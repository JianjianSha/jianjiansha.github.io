<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="DETR, SJJ">
    <meta name="description" content="论文：End-to-End Object Detection with Transformers
代码：facebookresearch/detr
首次将 transformer 应用于目标检测任务中。模型简称 DETR。

1. 简介
特">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>DETR | SJJ</title>
    <link rel="icon" type="image/png" href="/medias/logo.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">SJJ</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/resume" class="waves-effect waves-light">
      
      <i class="fas fa-file" style="zoom: 0.6;"></i>
      
      <span>简历（英）</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/jianli" class="waves-effect waves-light">
      
      <i class="fas fa-file" style="zoom: 0.6;"></i>
      
      <span>简历（中）</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">SJJ</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/resume" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-file"></i>
			
			简历（英）
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/jianli" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-file"></i>
			
			简历（中）
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/jianjiansha/jianjiansha.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/jianjiansha/jianjiansha.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/15.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">DETR</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/object-detection/">
                                <span class="chip bg-color">object detection</span>
                            </a>
                        
                            <a href="/tags/transformer/">
                                <span class="chip bg-color">transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-01-21
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr">facebookresearch/detr</a></p>
<p>首次将 transformer 应用于目标检测任务中。模型简称 <code>DETR</code>。</p>
<span id="more"></span>
<h1>1. 简介</h1>
<p>特点：</p>
<ol>
<li>
<p>端到端训练</p>
</li>
<li>
<p>可以基于任意的 CNN baseline</p>
</li>
<li>
<p>采用非自回归并行解码（non-autoregressive parallel decoding）</p>
<p>自回归模型例如机器翻译任务，每一 step，Decoder 的输出依赖之前的输出，即，每次 Decoder 输出一个新的 token 后，都附加到输入 sequence 之后，作为下一次 Decoder 的输入。</p>
<p>非自回归模型，则是并行生成所有的 tokens，这样解码速度更快。</p>
<p>在 DETR 中，对于一个 gt box，仅将一个预测 box 与它对应起来，得到一个 pair，并计算匹配 loss，这样预测 boxes 的顺序可以是任意，即 预测 boxes 之间是独立的，从而使得可以并行计算得到所有预测 boxes。</p>
</li>
<li>
<p>对大目标的检测上，DETR 比之前的目标检测模型效果更好，这是因为 transformer 的 attention 是 global 的，而 CNN 则是 local 的。相对的，在小目标检测上，则效果差些。</p>
</li>
<li>
<p>set prediction</p>
<p>DETR 一次性预测所有的 box 集合，需要一个 matching loss 函数，用于 预测 box 与 gt box 之间的匹配，采用基于匈牙利算法（Hungarian algorithm）的 loss 计算方式，一个 gt box 最多只有一个预测 box 与之匹配，从而省去了 NMS 等 postprocessing。</p>
<p>匈牙利算法可以参考这个 <a target="_blank" rel="noopener" href="https://gist.github.com/JianjianSha/ed5ea9022a8aa1217113dc7d30b52044">代码实现</a></p>
</li>
</ol>
<h1>2. DETR</h1>
<p>DETR 的结构示意图如下，</p>
<p><img src="/images/transformer/DETR1.png" alt=""></p>
<center>图 1. DETR 直接一次性（并行）预测所有的 box 集合。</center>
<ol>
<li>
<p>输入 image ，shape <code>(batch_size, 3, H, W)</code></p>
</li>
<li>
<p>经过一个 CNN 网络输出 features 。shape <code>(batch_size, c, h, w)</code></p>
<p>例如 ResNet-50，下采样率为 32，输出 channel 为 2048，即 <code>c=2048, h=H/32, w=W/32</code></p>
</li>
<li>
<p>backbone 的输出 features 作为 transformer 的输入，另外还使用了位置嵌入向量 <code>PE</code> 作为 transformer 的输入，具体参见下文分析</p>
</li>
<li>
<p>transformer decoder 输出经前馈网络 FFN 输出（feature map）上各 location 的预测分类得分和预测 box。（注：整个过程是并行的）</p>
</li>
</ol>
<h2 id="2-1-DETR-结构">2.1 DETR 结构</h2>
<p><img src="/images/transformer/DETR2.png" alt=""></p>
<center>图 2. DETR 包含：1. CNN backbone，输出 feature maps；2. encoder-decoder transformer；3. 前馈网络 FFN。</center>
<p><strong>Backbone</strong></p>
<p>CNN backbone 的输入 image $x \in \mathbb R^{3 \times H_0 \times W_0}$，输出 features 为 $f \in \mathbb R^{C \times H \times W}$。通常取，$C=2048$，$H,W=H_0/32, W_0/32$。</p>
<p>代码中 backbone 默认使用 <code>ResNet50</code>，（代码 1）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--backbone'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'resnet50'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">)</span>
backbone <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>models<span class="token punctuation">,</span> <span class="token string">'resnet50'</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token comment"># create resnet50</span>
<span class="token comment"># 标记 layer4 为 backbone 的输出层，其编号为 0</span>
<span class="token comment"># 对于 segmentation task，则有多个输出层</span>
return_layers <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'layer4'</span><span class="token punctuation">:</span> <span class="token string">'0'</span><span class="token punctuation">&#125;</span>
<span class="token comment"># self.body 输出 layer4 的 output features</span>
self<span class="token punctuation">.</span>body <span class="token operator">=</span> IntermediateLayerGetter<span class="token punctuation">(</span>backbone<span class="token punctuation">,</span> return_layers<span class="token operator">=</span>return_layers<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>机器翻译任务中，对短句子的末尾进行填充 <code>&lt;pad_tok&gt;</code>，然后再创建 <code>src_mask</code>，其中 <code>&lt;pad_tok&gt;</code> 对应 <code>mask=1</code>，这里对 image 采取类似的预处理，（代码 2）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 2</span>
<span class="token comment"># 将 list 中每个图像数据紧靠左上角填充（左上角为图像坐标系 O 点）</span>
<span class="token keyword">def</span> <span class="token function">nested_tensor_from_tensor_list</span><span class="token punctuation">(</span>tensor_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># tensor_list: a list of tensors. each tensor represents an image data</span>
    <span class="token comment"># each tensor has a shape of (C, H, W)</span>
    <span class="token comment"># max_size: [Cmax,Hmax,Wmax]。 tensor list中，最大 channel，最大 H，最大 W</span>
    max_size <span class="token operator">=</span> _max_by_axis<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> img <span class="token keyword">in</span> tensor_list<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 得到一个最大的 size，可以容纳 mini-batch 中所有的 images</span>
    <span class="token comment"># [batch_size, Cmax, Hmax, Wmax]，其实 Cmax=3，因为所有图像通道均为 3</span>
    batch_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>tensor_list<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> max_size
    b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> batch_shape
    dtype <span class="token operator">=</span> tensor_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dtype
    device <span class="token operator">=</span> tensor_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>batch_shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    <span class="token comment"># 创建 mask，指示哪些 spatial pixeles 是填充数据</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    <span class="token keyword">for</span> img<span class="token punctuation">,</span> pad_img<span class="token punctuation">,</span> m <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>tensor_list<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        pad_img<span class="token punctuation">[</span><span class="token punctuation">:</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        m<span class="token punctuation">[</span><span class="token punctuation">:</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">return</span> NestedTensor<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>   <span class="token comment"># 打包 image 数据和 mask</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（<em>为什么不将 image  resize 到相同 size 并保存各 image 的 scale 比例，而是使用 padding 和 mask？这是因为后者处理方法应该效果更好</em>）</p>
<p>经过 backbone 之后，image 转变成 features，其 spatial size 缩小了 $32$ 倍，故 mask 也需要等比例缩小 $32$ 倍，（代码 3）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 3</span>
<span class="token comment"># class BackboneBase(nn.Module)</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_list<span class="token punctuation">:</span> NestedTensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    xs <span class="token operator">=</span> self<span class="token punctuation">.</span>body<span class="token punctuation">(</span>tensor_list<span class="token punctuation">.</span>tensors<span class="token punctuation">)</span> <span class="token comment"># (batch_size, C=2048, H, W)</span>
    <span class="token comment"># xs: backbone 的输出 features</span>
    out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
    <span class="token comment"># 检测任务 xs -> &#123;'0':res&#125;</span>
    <span class="token comment"># 分割任务 xs -> &#123;'0':res0, '1':res1, '2':res2, '3':res3&#125;</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> x <span class="token keyword">in</span> xs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 对应上面 return_layers 的输出，name 为 编号</span>
        m <span class="token operator">=</span> tensor_list<span class="token punctuation">.</span>mask
        <span class="token comment"># mask 先从 3-D，转为 4-D，然后对最低的两个维度（spatial dimension）进行</span>
        <span class="token comment"># 插值，rescale 之后，再转为 3-D</span>
        <span class="token comment"># 这里使用最近邻插值，将 mask resize 到原来的 1/32</span>
        mask <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>m<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> size<span class="token operator">=</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        out<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> NestedTensor<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
    <span class="token comment"># 根据 return_layers 的输出，继续打包 features 与 masks</span>
    <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>代码 3 中关于检测任务和分割任务的 backbone 的输出不同，检测只需要最后的特征，而分割需要中间层特征和最后的特征，如下所示</p>
<pre class="line-numbers language-none"><code class="language-none">+-------------+    +---------+    +---------+    +---------+    +---------+
|conv1+bn    -|---&gt;| stage2 -|-+-&gt;| stage3 -|-+-&gt;| stage4 -|-+-&gt;| stage5 -|-+-&gt;
|+relu+maxpool|    |         | |  |         | |  |         | |  |         | |
+-------------+    +---------+ |  +---------+ |  +---------+ |  +---------+ |
                               v              v              v              v
(object detection)                                                         res
(segmentation)                res0           res1           res2           res3

res0: (B, 256, H0&#x2F;&#x2F;4, W0&#x2F;&#x2F;4)
res1: (B, 512, H0&#x2F;&#x2F;8, W0&#x2F;&#x2F;8)
res2: (B,1024, H0&#x2F;&#x2F;16, W0&#x2F;&#x2F;16)
res3: (B, 2048, H0&#x2F;&#x2F;32, W0&#x2F;&#x2F;32)
(H0, W0) 是网络的 input size<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>Transformer encoder</strong></p>
<ol>
<li>使用一个 <code>1x1 conv</code> 对 CNN backbone 的输出 features 进行降维，从维度 $C$ 降到 $d=256$，得到 features 为 $z_0 \in \mathbb R^{d \times H \times W}$</li>
<li>将 spatial 特征压缩至一维，即 $(d,H,W)\rightarrow (d,HW)$，这里 $d$ 就是(transformer)特征维度，$HW$ 则作为输入 sequence 的 <code>seq_len</code>。</li>
<li>Encoder 为标准结构，包含一个 multi-head self-attention 和 一个 FFN</li>
<li>对特征  $z_0 \in \mathbb R^{d \times H \times W}$ 进行 positional encoding，然后加到 $z_0$ 上</li>
</ol>
<p><strong>position encoding</strong></p>
<p>$$PE(pos_x, 2i)=\sin(pos_x / 10000^{2i/128})<br>
\\ PE(pos_x, 2i+1)=\cos(pos_x/10000^{2i/128})<br>
\\ PE(pos_y, 2i)=\sin(pos_y/10000^{2i/128})<br>
\\ PE(pos_y, 2i+1)=\cos(pos_y/10000^{2i/128})$$</p>
<p>考虑了二维 spatial 位置上 x 轴 与 y 轴的位置编码，$i \in [0, d//4)$，每个空间位置 <code>pos</code> 处，位置 encoding 向量维度为 $d=256$，前 <code>128</code> 维表示 <code>pos_y</code> 位置编码，sin 和 cos 间隔，后 <code>128</code> 维表示 <code>pos_x</code> 位置编码，sin 和 cos 间隔，记 <code>pos</code> 坐标为 $(x, y)$，那么此处位置 encoding 为</p>
<pre class="line-numbers language-none"><code class="language-none">[PE(y,0), PE(y,1), PE(y,2),PE(y,3), ..., PE(y,126),PE(y,127),PE(x,0),PE(x,1), ... ,PE(x,126),PE(x,127)]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>pixel 像素坐标为 (x, y)，其中 $x \in [1,W], \ y \in [1,H]$，如果需要归一化到 $[0, 2\pi]$ 之间，使用 $2\pi x / W, \ 2\pi y / H$ 。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 4</span>
<span class="token comment"># class PositionEmbeddingSine(nn.Module):</span>
<span class="token comment"># 获取 position embedding</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_list<span class="token punctuation">:</span> NestedTensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># tensor_list: the data pack of one return_layer(refer to return_layers)</span>
    x <span class="token operator">=</span> tensor_list<span class="token punctuation">.</span>tensors     <span class="token comment"># feartures of batch-images</span>
    mask <span class="token operator">=</span> tensor_list<span class="token punctuation">.</span>mask     <span class="token comment"># corresponding masks of features</span>
    <span class="token comment"># x: (B, C, H, W)</span>
    <span class="token comment"># mask: (B, H, W), where `1` elements represent padding pixels</span>
    not_mask <span class="token operator">=</span> <span class="token operator">~</span>mask

    <span class="token comment"># position of y-axis, (B, H, W)</span>
    <span class="token comment"># for one image features: [[1,1,...], </span>
    <span class="token comment">#                          [2,2,...],...]</span>
    <span class="token comment"># 对于i-th图像特征而言，y_embed[i] 沿y轴增 1</span>
    y_embed <span class="token operator">=</span> not_mask<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    <span class="token comment"># position of x_axis</span>
    <span class="token comment"># for one image feature: [[1,2,3,...],</span>
    <span class="token comment">#                         [1,2,3,...],...]</span>
    x_embed <span class="token operator">=</span> not_mask<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>normalize<span class="token punctuation">:</span>   <span class="token comment"># True, normalize position to [0, 1]*2*pi</span>
        eps <span class="token operator">=</span> <span class="token number">1e-6</span>
        <span class="token comment"># normalize the y-position。y_embed[:,-1,:] 是 H axis 最大值，相除使用 H axis 范围(0,1]</span>
        y_embed <span class="token operator">=</span> y_embed <span class="token operator">/</span> <span class="token punctuation">(</span>y_embed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> eps<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale   <span class="token comment"># scale: 2*math.pi</span>
        <span class="token comment"># x_embed 与 y_embed 类似处理，使得值范围 (0, 2*pi]</span>
        x_embed <span class="token operator">=</span> x_embed <span class="token operator">/</span> <span class="token punctuation">(</span>x_embed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> eps<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
    <span class="token comment"># self.temperature: 10000</span>
    <span class="token comment"># self.num_pos_feats = d//2 = 256/2=128</span>
    dim_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_pos_feats<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token comment"># dim_t // 2: 0, 0, 1, 1, 2, 2, ... , 63, 63</span>
    <span class="token comment"># 2 * (dim_t // 2): 0, 0, 2, 2, 4, 4, ... , 126, 126</span>
    <span class="token comment"># dim_t: 一维向量，size=128,  10000^&#123;2i/128&#125; ,  i = 0,1,...,63</span>
    dim_t <span class="token operator">=</span> self<span class="token punctuation">.</span>temperature <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>dim_t <span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_pos_features<span class="token punctuation">)</span>
    <span class="token comment"># pos_x / 10000^&#123;2i/128&#125;</span>
    <span class="token comment"># PE(pos_x, (2i, 2i+1)), PE(pos_y, (2i, 2i+1))</span>
    pos_x <span class="token operator">=</span> x_embed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> dim_t     <span class="token comment"># (B, H, W, 128)</span>
    pos_y <span class="token operator">=</span> y_embed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> dim_t     <span class="token comment"># (B, H, W, 128)</span>
    <span class="token comment"># cross: sin和cos间隔，[(B,H,W,64),(B,H,W,64)] => (B,H,W,64,2) => (B,H,W,128)</span>
    <span class="token comment">#   [sin, cos, sin, cos, sin, ...]</span>
    pos_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>pos_x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> pos_x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    pos_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>pos_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> pos_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dim<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token comment"># (B, H, W, 256) => (B, 256, H, W)</span>
    pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>pos_y<span class="token punctuation">,</span> pos_x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> pos <span class="token comment"># (B, 256, H, W)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>backbone 输出包含两部分，</p>
<ol>
<li>
<p>image 数据经过 ResNet 输出的 features</p>
</li>
<li>
<p>position embedding，与第 <code>1</code> 步中的 features 具有相同的 spatial size</p>
<p>分割任务有 4 个不同 spatial size 的特征，每个特征均执行一次 position embedding（除最后一个特征的 PE，其他 PE 并没有用到）</p>
</li>
</ol>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Joiner</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> backbone<span class="token punctuation">,</span> position_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>backbone<span class="token punctuation">,</span> position_embedding<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_list<span class="token punctuation">:</span> NestedTensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># self[0] 指的 ResNet</span>
        xs <span class="token operator">=</span> self<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>tensor_list<span class="token punctuation">)</span>   <span class="token comment"># 检测任务 -> &#123;'0': NestTensor0&#125;</span>
                                <span class="token comment"># 分割任务 -> &#123;'0': NT0, '1': NT1, '2': NT2, '3': NT3&#125;</span>
        out<span class="token punctuation">:</span> List<span class="token punctuation">[</span>NestedTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> x <span class="token keyword">in</span> xs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>tensors<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 对应的 PE</span>
    <span class="token keyword">return</span> out<span class="token punctuation">,</span> pos<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>整个 features 的位置编码 shape 为 $(d, H, W)$ （未考虑 batch_size 这一维度），而 features 的 shape 为 <code>(512,H,W)</code> 或者 <code>(2048,H,W)</code> (参考各ResNet的输出 channel)，故Backbone 的输出 features 经过 <code>1x1 Conv</code> 降维后特征 shape 为 $(d, H, W)$，两者执行 element-wise 相加，然后 flatten spatial，得到 $d \times HW$ 的特征，作为 encoder 的输入。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 5</span>
<span class="token comment"># class DETR(nn.Module):</span>
<span class="token comment"># decrease channels from 2048 to 256</span>
input_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>backbone<span class="token punctuation">.</span>num_channels<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token comment"># def forwrad(sef, samples: NestedTensor):</span>
<span class="token comment"># features, pos is just `out, pos` in last code snippet.</span>
features<span class="token punctuation">,</span> pos <span class="token operator">=</span> self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
src<span class="token punctuation">,</span> mask <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>decompose<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># param: src is the output of backbone, its shape (B, 2048, H, W)</span>
<span class="token comment"># return: src (B, 256, H, W)</span>
src <span class="token operator">=</span> input_proj<span class="token punctuation">(</span>src<span class="token punctuation">)</span>
hs <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>src<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>query_embed<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/images/transformer/DETR3.png" alt=""></p>
<center>图 3. DETR 中 Transformer 的具体结构</center>
<p>从图 3 中可见，backbone 输出特征经过 <code>1x1 conv</code> 降维后，直接作为 Encoder 的一个 input，记为 <code>f</code>，position encoding 作为另一个 input，记为 <code>PE</code>，这两个 tensor 的 shape 均为 $(d, HW)$（实际实现中，习惯按 <code>(seq_len, batch_size, feature_dim)</code> 的顺序 reshape，于是一个 mini-batch 中这两个 tensor 的 shape 为 $(HW,B,d)$ ），然后：</p>
<ol>
<li><code>f+PE</code> 作为 query, key；<code>f</code> 作为 value。value 中不需要 position encoding，可能是因为最终是一次性解码得到所有 object 列表，这个列表是无序的，例如原 image 上编号 <code>1</code> 的 object，其可以解码输出的列表中任意位置（index，下标），但是计算 attention 需要位置信息，故 <code>query</code> 和 <code>key</code> 上叠加了 <code>PE</code>。</li>
<li>multi-head self-attention 的输出与输入 <code>f</code> 做 Add&amp;Norm 操作，得到输出记为 <code>f1</code>，然后 <code>f1</code> 经过一个 FFN 得到的输出特征记为 <code>f2</code>，<code>f1</code> 与 <code>f2</code> 再次做 Add&amp;Norm 操作，得到单个 block 的输出。</li>
<li>Encoder 除了 <code>PE</code> 接入的位置不同，其他均与原生 transfromer 相同。</li>
</ol>
<p><strong>Encoder 总结：</strong></p>
<p><code>batch_size</code> 记为 $B$，考虑维度顺序 <code>(seq_len, batch_size, feature_dim)</code>。 $d=256$。</p>
<ol>
<li>输入 image backbone 的特征经过一个 <code>1x1 Conv</code> 降维，输出为 $z_0 \in \mathbb R^{HW \times B \times d}$，位置编码 $PE \in \mathbb R^{HW \times B \times d}$</li>
<li>PE 叠加到 <code>Q, K</code> 上</li>
<li>Block 输出 tensor 的 shape 为 $(HW, B, d)$ ，保持不变</li>
<li>第 <code>3</code> 步的输出继续作为下一个 block 的输入（仍使用一开始的那个 PE），重复步骤 <code>2~3</code> $N=6$ 次，最后得到整个 Encoder 的输出 shape 依然是 $(HW, B, d)$。</li>
</ol>
<p>Encoder 的代码：（代码 6）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 6</span>
<span class="token comment"># class TransformerEncoder(nn.Module):</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> src_key_padding_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> pos<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># src: `1x1 Conv` 输出特征 （reshape 之后） （HW, B, d)</span>
    <span class="token comment"># src_key_padding_mask: (HW, B, d)，backbone 输出的 mask，由于batch 内各 </span>
    <span class="token comment">#      image size 大小不一，使用了 zero-padding，故需要使用 mask</span>
    <span class="token comment"># pos: position embedding (HW, B, d)</span>
    <span class="token comment"># mask: 对 attention 是否需要做 mask。在 Encoder 对 attention 不需要做</span>
    <span class="token comment">#       mask，故这里 mask=None</span>
    output <span class="token operator">=</span> src    <span class="token comment"># `1x1 Conv` 输出特征 （reshape 之后）</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
        output <span class="token operator">=</span> layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> src_mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> \
            src_key_padding_mask<span class="token operator">=</span>src_key_padding_mask<span class="token punctuation">,</span> pos<span class="token operator">=</span>pos<span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>   <span class="token comment"># normalize_before is false, so self.norm is None</span>
        <span class="token comment"># normalize_after, so do not need norm here.</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>Encoder layer (block) 小结：</strong></p>
<ol>
<li>features 与 position embedding 相加，作为 <code>query</code> 和 <code>key</code>，features 作为 <code>value</code></li>
<li>计算 multi-head self-attn 的输出，然后与输入相加（残差结构，identity connection），然后计算 layer_norm</li>
<li>FFN 的输出再与 FFN 的输入相加，然后计算 layer_norm（沿 C,H,W 归一化，与 BatchNorm 不同）。</li>
<li>Encoder layer 的输入输出 shape 均为 $(HW, B, d)$。</li>
</ol>
<p>Encoder layer 代码：（代码 7）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 7</span>
<span class="token keyword">def</span> <span class="token function">forward_post</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> src_key_padding_mask<span class="token punctuation">,</span> pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># src: input embedding      (HW, B, d)</span>
    <span class="token comment"># pos: position embedding   (HW, B, d)</span>
    <span class="token comment"># src_mask：对 attention 参数矩阵做 mask</span>
    <span class="token comment"># src_key_padding_mask: 对 `key` 做 mask</span>
    q <span class="token operator">=</span> k <span class="token operator">=</span> self<span class="token punctuation">.</span>with_pos_embed<span class="token punctuation">(</span>src<span class="token punctuation">,</span> pos<span class="token punctuation">)</span>   <span class="token comment"># 叠加 position embedding</span>
    src2 <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> value<span class="token operator">=</span>src<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>src_mask<span class="token punctuation">,</span>
                          key_padding_mask<span class="token operator">=</span>src_key_padding_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    src <span class="token operator">=</span> src <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>src2<span class="token punctuation">)</span>      <span class="token comment"># residual 结构</span>
    src <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>src<span class="token punctuation">)</span>
    <span class="token comment"># linear -> act -> dropout -> linear</span>
    src2 <span class="token operator">=</span> self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>src<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    src <span class="token operator">=</span> src <span class="token operator">+</span> src2
    src <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>src<span class="token punctuation">)</span>
    <span class="token keyword">return</span> src  <span class="token comment"># (HW, B, d)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>Transformer decoder</strong></p>
<ol>
<li>
<p>decoder 采用标准结构，但是是并行解码得到 $N$ 个预测 objects，非自回归。</p>
<p>$N$ 是手动给出的，且需要大于单个 image 中的 object 数量，通常 $N \neq HW$（预先设置例如 <code>N=100</code>）。注意需要与图 3 Encoder 中的 block 数量 N 区分开来，这是两个不同的变量。</p>
</li>
<li>
<p>decoder 结构如图 3 所示，输入称为 object queries，这是 N 个 positional embedding（向量，维度为 $d$），是可学习的 positional embedding。</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># code snippet 8</span>
N  <span class="token operator">=</span>  <span class="token number">100</span>       <span class="token comment"># 默认为 100，大于单个 image 中可能的 object 数量</span>
<span class="token comment"># hidden_dim = 256，就是前面 Encoder 中的参数 `d` </span>
<span class="token comment"># Embedding.weight 根据标准正态分布进行初始化</span>
query_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>N<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>

<span class="token comment"># src: output of `backbone + 1x1 Conv`  (B,d, H, W)</span>
<span class="token comment"># mask: set mask=1 for all padding pixels in mini-batch  (B, H, W)</span>
<span class="token comment"># query_emb: N x d, object queries，N=100 是预设的目标数据</span>
<span class="token comment"># pos: position embeddings of all return_layers</span>
<span class="token comment">#   pos[-1] -> PE of the last return_layer, (B, d, H, W)</span>
transformer<span class="token punctuation">(</span>src<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> query_emb<span class="token punctuation">.</span>weights<span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>图 3 关于 Decoder 的输入标注会有些误导，其实 Decoder 还有一个输入，是与 object queries 相同 shape 的全 0 tensor，如下代码中的 <code>tgt</code>，因为还没见到 encoder 的输出（或者说没有见到 transformer 输入），故初值为全 0 tensor，</p>
<p>Transformer （Encoder+Decoder）代码实现：（代码 9）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># class Transformer(nn.Module):</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> query_embed<span class="token punctuation">,</span> pos_embed<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># src: output of input_proj(...), the input of encoder, (B, 256, H, W)</span>
    <span class="token comment"># mask: mask of backbone output, (B, H, W)</span>
    <span class="token comment"># query_embed: object query embedding of decoder, (N, 256)</span>
    <span class="token comment"># pos_embed: positional embedding, (B, 256, H, W)</span>
    b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> src
    <span class="token comment"># (B, 256, H, W) -> (B, 256, HW) -> (HW, B, 256)</span>
    src <span class="token operator">=</span> src<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    pos_embed <span class="token operator">=</span> pos_embed<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>   <span class="token comment"># (HW, B, 256)</span>
    <span class="token comment"># (N, 256) -> (N, B, 256)</span>
    <span class="token comment"># query_embed 是手动设置的 N 个目标的 object_queries</span>
    query_embed <span class="token operator">=</span> query_embed<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># (B, H, W) -> (B, HW)</span>
    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

    tgt <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>query_embed<span class="token punctuation">)</span>     <span class="token comment"># (N, B, 256)</span>
    <span class="token comment"># memory: output of encoder, (HW, B, 256)</span>
    memory <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>src<span class="token punctuation">,</span> src_key_padding_mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> pos<span class="token operator">=</span>pos_embed<span class="token punctuation">)</span>

    <span class="token comment"># hs：(M, N, B, d)，其中 M 为 decoder layer iteration number。参见代码 11 的返回结果</span>
    hs <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>tgt<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> memory_key_padding_mask<span class="token operator">=</span>mask<span class="token punctuation">,</span>
                    pos<span class="token operator">=</span>pos_embed<span class="token punctuation">,</span> query_pos<span class="token operator">=</span>query_embed<span class="token punctuation">)</span>
    <span class="token comment"># hs: (M, N, B, 256) -> (M, B, N, 256)</span>
    <span class="token comment"># memory: (HW, B, 256) -> (B, 256, HW) -> (B, C, H, W)</span>
    <span class="token keyword">return</span> hs<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> memory<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>Decoder 的第一个 mh self-attn 的 <code>query</code> 和 <code>key</code> 均为 <code>tgt</code> 与 object queries 相加，query embedding 相当于 pos embedding，</p>
<p>Decoder layer 的代码：（代码 10）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward_post</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">,</span> memory_mask<span class="token punctuation">,</span> tgt_key_padding_mask<span class="token punctuation">,</span> 
    memory_key_padding_mask<span class="token punctuation">,</span> pos<span class="token punctuation">,</span> query_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># query_pos: 就是前面说的 object queries `query_emb`， shape 为 (N, B, d)</span>
    <span class="token comment"># tgt: 初始时为全零 tensor，shape 为 (N, B, d)</span>
    <span class="token comment"># tgt_mask: 对 tgt 做 mask，这里不需要，为 None</span>
    <span class="token comment"># memory: encoder 的最终输出 (HW, B, d), </span>
    <span class="token comment"># memory_mask： 第二个 mh self-attn 中与 memory 计算 attention 之后的的 mask，这里为 None</span>
    <span class="token comment"># tgt_key_padding_mask: 第一个 mh self-attn 中 `key` 的 mask， 为 None</span>
    <span class="token comment"># memory_key_padding_mask: 第二个 mh self-attn 中 `key` 的 mask</span>
    <span class="token comment">#   由于 batch 中 image 大小各不相同，左上角对齐，右下防 padding，padding pixels 的 mask=1</span>
    <span class="token comment">#   memory_mask 缩放到 (H, W) 空间大小，(B,H,W) -> (B, HW)，参见代码 3 中的 mask</span>
    <span class="token comment"># pos: encoder 中的 position embedding，(HW, B, d)</span>
    q <span class="token operator">=</span> k <span class="token operator">=</span> self<span class="token punctuation">.</span>with_pos_embed<span class="token punctuation">(</span>tgt<span class="token punctuation">,</span> query_pos<span class="token punctuation">)</span>     <span class="token comment"># target 输入，Q, K 需要叠加 query embedding</span>
    <span class="token comment"># 调用第一个 mh self-attn，参数 tgt_mask, tgt_key_padding_mask 均为 None，即不做 mask</span>
    <span class="token comment"># 第一个 multi-head self-attn 因为没有用到 encoder 输出的特征，自然不需要 mask</span>
    tgt2 <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> value<span class="token operator">=</span>tgt<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>tgt_mask<span class="token punctuation">,</span> key_padding_mask<span class="token operator">=</span>tgt_key_padding_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    tgt <span class="token operator">=</span> tgt <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout1<span class="token punctuation">(</span>tgt2<span class="token punctuation">)</span>     <span class="token comment"># redisual 结构</span>
    tgt <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span>

    <span class="token comment"># 调用第二个 mh self-attn，first mh self-attn 的输出作为 query，encoder 的最终输出作为 key 和 value，</span>
    <span class="token comment"># query 和 key 分别使用 query_embedding 和 position embedding 叠加，value 保持不变</span>
    <span class="token comment"># memory_mask：为 None，计算出 attention 之后不需要做 mask；这跟 encoder 不同，因为 </span>
    <span class="token comment">#               encoder 的输入是经过填充的，所以对于填充的位置的 attention，需要置 0</span>
    <span class="token comment">#               decoder 是固定好 N 个位置的目标，N 个位置的 attention 都需要。</span>
    <span class="token comment"># memory_key_padding_mask：与 encoder 中 src mask 相同，(B, HW)，由于 images 大小各不相同，存在 padding，故需要 mask</span>
    tgt2 <span class="token operator">=</span> self<span class="token punctuation">.</span>multihead_attn<span class="token punctuation">(</span>query<span class="token operator">=</span>self<span class="token punctuation">.</span>with_pos_embed<span class="token punctuation">(</span>tgt<span class="token punctuation">,</span> query_pos<span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span>self<span class="token punctuation">.</span>with_pos_embed<span class="token punctuation">(</span>memory<span class="token punctuation">,</span> pos<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               value<span class="token operator">=</span>memory<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>memory_mask<span class="token punctuation">,</span> key_padding_mask<span class="token operator">=</span>memory_key_padding_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    tgt <span class="token operator">=</span> tgt <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout2<span class="token punctuation">(</span>tgt2<span class="token punctuation">)</span>
    tgt <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span>
    tgt2 <span class="token operator">=</span> self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    tgt <span class="token operator">=</span> tgt <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>tgt2<span class="token punctuation">)</span>
    tgt <span class="token operator">=</span> self<span class="token punctuation">.</span>norm3<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span>       <span class="token comment"># (N, B, d)</span>
    <span class="token keyword">return</span> tgt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>整个 Decoder 的前向过程：（代码 11）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">,</span> memory_mask<span class="token punctuation">,</span> tgt_key_padding_mask<span class="token punctuation">,</span> memory_key_padding_mask<span class="token punctuation">,</span>
            pos<span class="token punctuation">,</span> query_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 参数与 Decoder layer 的前向传播方法参数相同，略过解释</span>
    output <span class="token operator">=</span> tgt        <span class="token comment"># 全零 tensor，(N, B, d)，N=100</span>
    intermediate <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>   <span class="token comment"># 循环执行 Decoder layer 若干次</span>
        output <span class="token operator">=</span> layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> tgt_mask<span class="token operator">=</span>tgt_mask<span class="token punctuation">,</span> memory_mask<span class="token operator">=</span>memory_mask<span class="token punctuation">,</span>
                       tgt_key_padding_mask<span class="token operator">=</span>tgt_key_padding_mask<span class="token punctuation">,</span>
                       memory_key_padding_mask<span class="token operator">=</span>memory_key_padding_mask<span class="token punctuation">,</span>
                       pos<span class="token operator">=</span>pos<span class="token punctuation">,</span> query_pos<span class="token operator">=</span>query_pos<span class="token punctuation">)</span>
        <span class="token comment"># refer to the paper's section "Auxiliary decoding losses":</span>
        <span class="token comment">#   add prediction FFNs and Hungarian loss after each decoder layer...</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>return_intermediate<span class="token punctuation">:</span>
            intermediate<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>   <span class="token comment"># True</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>output<span class="token punctuation">)</span>  <span class="token comment"># the last (final) decoder layer's output</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>return_intermediate<span class="token punctuation">:</span>    <span class="token comment"># 默认为 True，即，使用辅助 decoding loss</span>
        <span class="token comment"># (M, N, B, d)，M is the total iteration number for decoder layer</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>intermediate<span class="token punctuation">)</span>
    <span class="token keyword">return</span> output<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>      <span class="token comment"># (1, N, B, d)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>将 Decoder 中每个 block（总共 $M=6$ 个 block）的输出均存储起来 <code>intermediate</code>，并 stack 后返回，每个 block 的输出 <code>(N, B, d)</code>，那么 stack 后 Decoder 的输出为 <code>(M, N, B, d)</code>，其中 $M=6, \ N=100$。</p>
</li>
<li>
<p>Transformer 的输出包含两部分</p>
<ul>
<li>Decoder 的输出 <code>(M, B, N, d)</code> （经过了 shape 转置）</li>
<li>Encoder 的输出 <code>(B, d, H, W)</code> （shape permute+view）</li>
</ul>
</li>
</ol>
<p><strong>prediction heads</strong></p>
<p>decoder 的输出 shape 为 $(M, B, N, d)$，其中 $M$ 为 decoder layer 循环次数，$B$ 为 <code>batch_size</code>，<code>d=256</code> 表示模型维度，$N$ 表示单个 image 中预测的 object 数量。</p>
<p>最后将 decoder 的输出分别经过 cls head 和 box head，预测分类得分和坐标，</p>
<ol>
<li>
<p>decoder 的输出经一个线性变换，使得维度从 <code>d</code> 变为 <code>C+1</code>，这里 <code>C</code> 表示 fg 分类数量，<code>1</code> 表示 bg 。（代码 12）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python">class_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> num_classes<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>
outputs_class <span class="token operator">=</span> class_embed<span class="token punctuation">(</span>hs<span class="token punctuation">)</span>         <span class="token comment"># hs 为 decoder layers 的输出，shape 为 (M, B, N, d)</span>
<span class="token comment"># 得到分类（非归一化）得分，(M, B, N, C+1)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>decoder 的输出经另一路分支即，由三层全连接层组成的分支，中间层的输出单元保持不变，输出层的输出单元数量为 4，表示坐标，坐标数据 shape 为 <code>(M, B, N, 4)</code>，（代码 13）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_embed <span class="token operator">=</span> MLP<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
outputs_coord <span class="token operator">=</span> bbox_embed<span class="token punctuation">(</span>hs<span class="token punctuation">)</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 归一化坐标</span>
<span class="token comment"># tensor 经 MLP，shape 变化为</span>
<span class="token comment"># MLP 输入 (M, B, N, d) -> (M, B, N, d) -> (M, B, N, d) -> (M, B, N, 4)</span>
<span class="token comment"># 每一个 "->" 表示一个全连接层</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>DETR 的输出</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用 Decoder 最后一个 block 进行预测</span>
<span class="token comment"># pred_logits: (B, N, C+1)</span>
<span class="token comment"># pred_boxes: (B, N, 4)</span>
out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'pred_logits'</span><span class="token punctuation">:</span> outputs_class<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'pred_boxes'</span><span class="token punctuation">:</span> outputs_coord<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>aux_loss<span class="token punctuation">:</span>   <span class="token comment"># 为 True，其他 Decoder block 输出用于辅助 loss 计算</span>
    out<span class="token punctuation">[</span><span class="token string">'aux_outputs'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_set_aux_loss<span class="token punctuation">(</span>outputs_class<span class="token punctuation">,</span> outputs_coord<span class="token punctuation">)</span>
<span class="token comment"># aux_outputs: [&#123;&#125;]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ol>
<h2 id="2-2-Loss">2.2 Loss</h2>
<blockquote>
<p>预测集损失用于反向传播，优化网络。Hungarian 匹配损失用于寻找匹配的预测 box，不用于反向传播。</p>
</blockquote>
<p>记 $y$ 为 gt box 集合，$\hat y = \lbrace \hat y_i \rbrace _ {i=1}^N$ 为预测 box 集合，</p>
<ol>
<li>$N$ 为某固定不变的值，表示对单个 image，预测 box 的数量。设置 $N$ 的值使得较大于一般意义上单个 image 中 object 数量。论文中设置 $N=100$</li>
<li>如果 gt box 数量不足 $N$，用 no-object进行填充，使得数量为 $N$。</li>
<li>填充的表示 no-object 的 gt boxes，其分类 index 为 <code>0</code>，表示背景 bg，坐标无所谓，因为坐标回归 loss 中只对正例预测 box 的坐标计算损失</li>
<li><strong><code>2~3</code> 是论文中原话，代码中并没有使用 no-object 进行填充使得 gt box 数量为 <code>N=100</code></strong>。参考 下方关于 <code>HungarianMatcher</code> 的代码</li>
</ol>
<h3 id="2-2-1-Hungarian-损失">2.2.1 Hungarian 损失</h3>
<p>在预测 box 集合和 gt box 集合上的二分匹配（bipartite matching）loss 为，</p>
<p>$$\hat {\sigma} = \arg \min_{\sigma \in \mathcal G _ N } \Sigma _ i ^ N \mathcal L _ {match}(y _ i, \hat y _ {\sigma(i)})$$</p>
<p>其中 $\sigma$ 表示 <code>1~N</code> 个自然数集合 $[N]$ 的一个 permutation（排列），$\sigma(i)$ 表示这个排列中第 $i$ 个数。$\mathcal G _ N$ 表示 $[N]$ 的所有排列的集合。$\mathcal L _ {match}(y _ i, \hat y _ {\sigma(i)})$ 表示 $y _ i$ 和 $\hat y _ {\sigma(i)}$ 的匹配 loss，这个 loss 包含了分类预测 loss 和 box 位置大小预测</p>
<p>记 gt box 为 $y _ i=(c _ i, b _ i)$，其中 $c _ i$ 表示分类 label index（约定 <code>0</code> 为 bg index），$b_i \in [0,1] ^ 4$ 表示 box 的 center 坐标和 height，width（相对于 image size 进行了归一化）。单个 matching pair 的损失包含两部分：分类损失和坐标损失</p>
<p>$$\mathcal L _ {match}(y _ i, \hat y _ {\sigma(i)})=-\hat p _ {\sigma(i)}(c _ i)+ \mathbb I _ {c _ i \neq 0} \cdot \mathcal L _ {box}(b _ i, \hat b _ {\sigma(i)})$$</p>
<p><strong>注：这里没有使用 NLL 损失，而是直接使用概率的负数作为损失</strong></p>
<p>对于单个 image，输出的预测分类概率应该类似于一个矩阵 $P \in [0, 1] ^ {N \times (C+1)}$，其中 $N=100$ 为单个 image 中预测 box 的数量，$C$ 为分类数量，$C+1$ 则包含了 bg。</p>
<p>第 <code>i</code> 个 gt box $y_i=(c_i, b_i)$ 与之匹配的预测 box 下标为 $\sigma(i)$，那么其对应到 $c_i$ 这个分类的预测概率为 $\hat p _ {\sigma(i)}(c _ i)=P _ {\sigma(i),c _ i}$</p>
<p>定义 Hungarian loss 表示单个 image 中所有 matching pairs 的损失，</p>
<p>$$\mathcal L _ {Hungarian}(y, \hat y)=\sum _ {i=1} ^ N \left[-\log \hat p _ {\hat \sigma(i)}(c _ i) + \mathbb I _ {c _ i \neq 0} \cdot \mathcal L _ {box}(b _ i, \hat b _ {\hat \sigma(i)})\right] \tag{1}$$</p>
<p>说明：</p>
<ol>
<li><font color="magenta">使用概率而非对数概率，即，去掉 （1）式中的 log，这样分类损失与坐标损失就比较相称。</font>（在下方的 Hungarian 代码中，没有对概率取对数操作）</li>
</ol>
<p><strong>Bound box loss</strong></p>
<p>DETR 直接预测 box，而非 box 相对于 anchor 的坐标偏差，故直接使用 $L_1$ 损失不合适，没有考虑到 scale 带来的影响，故 <strong>结合 $L_1$ 和 GIOU 作为坐标损失</strong>。</p>
<p>$$L_1(b, \hat b) = |b-\hat b|$$</p>
<p>GIOU 损失参考 <a href="/2019/06/13/GIoU">这篇文章</a>。</p>
<p>于是</p>
<p>$$\mathcal L _ {box}(b _ i, \hat b _ i)=\lambda_{iou} \mathcal L _ {iou}(b _ i, \hat b _ {\sigma(i)})+\lambda_{L _ 1}||b _ i - \hat b _ {\sigma(i)}|| _ 1$$</p>
<p>上式中使用了两个平衡因子 $\lambda _ {iou}, \ \lambda _ {L_i}$，代码中 $\lambda _ {iou}=2, \ \lambda _ {L _ 1}=5$，实际上分类损失也有平衡因子，只不过 $\lambda _ {cls}=1$。</p>
<h3 id="2-2-2-Hungarian-代码">2.2.2 Hungarian 代码</h3>
<p><strong>HungarianMatcher</strong></p>
<p>预测集与 target 集 的匹配采用匈牙利算法匹配，Hungarian 匹配算法仅仅是用于获取与 gt boxes 匹配的预测 boxes，这个匹配过程，也用到了一些损失计算，目标是求使得损失最小的二分图匹配，这个损失与上面求网络的优化目标损失不同，后者用于反向传播更新梯度，而前者（即 Hungarian 匹配损失）不是。（代码 17）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># class HungarianMatcher(nn.Module):</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># outputs 是 DETR 的输出，这是一个 dict 类型，key 可以是：</span>
    <span class="token comment">#   pred_logits: 最后一个 decoder layer 的输出分类未归一化得分  (B, N, C+1)</span>
    <span class="token comment">#   pred_boxes: 最后一个 decoder layer 的输出坐标       (B, N, 4)</span>
    <span class="token comment"># targets: dict list，每个 dict 表示一个 image 的 target，包含 key：</span>
    <span class="token comment">#   boxes: 某个 image 中 objects 的 (x, y, w, h)， shape 为 (M, 4)，</span>
    <span class="token comment">#           M 表示 object 数量，每个 image 的 M 均不同</span>
    <span class="token comment">#   labels：某个 image 中 objects 的分类 index（0 表示 bg），shape 为 (M, )</span>
    <span class="token comment">#   image_id：image 的 id（coco 数据集中每个 image 有一个数值编号）</span>
    <span class="token comment">#   ...：其他 keys 省略</span>
    bs<span class="token punctuation">,</span> num_queries <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_logits'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># B, N</span>
    out_prob <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_logits'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (B*N, C+1)</span>
    out_bbox <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_boxes'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (B*N, 4)</span>

    tgt_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>v<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> targets<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># (BM,), BM = M_1+M_2+...+M_B</span>
    tgt_bbox <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>v<span class="token punctuation">[</span><span class="token string">'boxes'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> targets<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># (BM, 4)</span>

    <span class="token comment"># =================================================</span>
    <span class="token comment"># 注意：以下三个损失计算理论上应分别在单个 image 内计算</span>
    <span class="token comment"># 但是为了计算效率提升，故将 mini-batch 内所有预测和 </span>
    <span class="token comment"># target 各自混合然后再计算这三种损失，最后取各 image</span>
    <span class="token comment"># 内的预测与 target 之间的匹配损失，参见下方 c[i] 变量</span>
    <span class="token comment"># =================================================</span>
    <span class="token comment"># 计算预测分类与 gt 分类 两两之间的损失</span>
    cost_class <span class="token operator">=</span> <span class="token operator">-</span>out_prob<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> tgt_ids<span class="token punctuation">]</span>      <span class="token comment"># (B*N, BM)</span>
    <span class="token comment"># 计算预测 boxes 与 gt boxes，两两 之间的 p1 范数 => 差的绝对值</span>
    cost_bbox <span class="token operator">=</span> torch<span class="token punctuation">.</span>cdist<span class="token punctuation">(</span>out_bbox<span class="token punctuation">,</span> tgt_bbox<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>     <span class="token comment"># (B*N, BM)</span>

    <span class="token comment">#（B*N, BM)</span>
    cost_giou <span class="token operator">=</span> <span class="token operator">-</span>generalized_box_iou<span class="token punctuation">(</span>box_cxcywh_to_xyxy<span class="token punctuation">(</span>out_bbox<span class="token punctuation">)</span><span class="token punctuation">,</span> box_cxcywh_to_xyxy<span class="token punctuation">(</span>tgt_bbox<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 计算总的损失，加权求和，损失 tensor shape: (B*N, BM)</span>
    <span class="token comment"># cost_bbox=5, cost_class=1, cost_giou=2</span>
    C <span class="token operator">=</span> self<span class="token punctuation">.</span>cost_bbox <span class="token operator">*</span> cost_bbox <span class="token operator">+</span> self<span class="token punctuation">.</span>cost_class <span class="token operator">*</span> cost_class <span class="token operator">+</span> self<span class="token punctuation">.</span>cost_giou <span class="token operator">*</span> cost_giou
    C <span class="token operator">=</span> C<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_queries<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment"># (B, N, BM)</span>

    sizes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>v<span class="token punctuation">[</span><span class="token string">"boxes"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> targets<span class="token punctuation">]</span>  <span class="token comment"># (B,)  gt number of all images in batch</span>
    <span class="token comment"># C.split(sizes, -1) -> ((B, N, M_1), (B, N, M_2), ... , (B, N, M_B))</span>
    <span class="token comment"># c[i] -> (N, M_i)  assignment the i-th image</span>
    <span class="token comment"># 注意：这里预测数量 N，target 数量 M_i，所以并没有将 target 数量通过</span>
    <span class="token comment">#   no-object 填充到 N</span>
    <span class="token comment"># linear_sum_assignment: 计算二分图匹配中最小损失的匹配对，返回结果：(row_ind, col_ind)</span>
    <span class="token comment"># row_ind 和 col_ind 均为长度为 M_i 的数量（这里假设了 N >= M_i）</span>
    <span class="token comment"># row_ind 表示匹配的 pairs 中预测 box 的索引</span>
    <span class="token comment"># col_ind 表示对应的 target 的索引</span>
    indices <span class="token operator">=</span> <span class="token punctuation">[</span>linear_sum_assignment<span class="token punctuation">(</span>c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> c <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>C<span class="token punctuation">.</span>split<span class="token punctuation">(</span>sizes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token comment"># tuple list，每个 tuple 表示对应 image 中，最佳匹配（loss 最小）的 预测 box ind 和 gt box ind</span>
    <span class="token comment">#       每个 tuple 的 shape ((M_i,), (M_i,))</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>i<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>j<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span>j <span class="token keyword">in</span> indices<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-2-3-目标函数">2.2.3 目标函数</h3>
<p>几种种损失（检测任务只有前三种，分割任务包含以下所有损失）：</p>
<ol>
<li>分类损失，平衡系数 $\lambda_{cls}=1$</li>
<li>坐标损失 $L_1$，平衡系数 $\lambda_{L_1}=5$</li>
<li>GIOU 损失，平衡系数 $\lambda_{iou}=2$</li>
<li>mask 损失，平衡系数 $1$ （分割任务中使用）</li>
<li>dice 损失，平衡系数 $1$ （分割任务中使用）</li>
<li>实际应用中，对于 bg 的分类损失，相较于 fg 的分类损失，我们使用一个权重因子 $\lambda_{no-object}=0.1$，以便缓和分类不平衡的问题。</li>
</ol>
<p><strong>分类损失：</strong></p>
<p>单个 image 的分类预测损失：</p>
<p>$$L _ {cls}=-\frac 1 N \sum_{i=1} ^ N w_i \log \hat p _ {\hat \sigma(i)}(c _ i)$$</p>
<p>其中权重</p>
<p>$$w _ i=\begin{cases} 1 &amp; 0\le c _ i &lt;C(\text{fg}) \\ 0.1 &amp; c _ i=C(\text{bg})\end{cases}$$</p>
<p><strong>L1 坐标损失：</strong></p>
<p>mini-batch 的 L1 坐标损失，</p>
<p>$$L _ {L _ 1}=\frac 1 {\sum _ i M _ i}\sum _ i \sum _ {j=1} ^ {M _ i} \sum _ {c \in {x,y,w,h}} ||\hat b _ {j,c}-b _ {j,c}||$$</p>
<p>其中 $i$ 表示 mini-batch 中第 <code>i</code> 个 image， $M_i$ 是 <code>i-th</code> image 中 targets 数量。$b_{j,c}$ 表示第 <code>j</code> 个 target 的某个坐标 (<code>cx,cy,w,h</code>)，$\hat b_{j,c}$ 表示与第 <code>j</code> 个 target 匹配的预测 box 的某个坐标。</p>
<p><strong>GIoU:</strong></p>
<p>$$L _ {iou}=\frac 1 {\sum _ i M _ i} \sum _ {j=1} ^ {M_i} GIoU(\hat b _ j, b _ j)$$</p>
<p>目标损失：</p>
<p>$$L=\lambda _ {cls}L _ {cls}+\lambda _ {L _ 1}L _ {L _ 1}+\lambda _ {iou}L _ {iou}$$</p>
<h3 id="2-2-4-目标函数的代码">2.2.4 目标函数的代码</h3>
<p><strong>SetCriterion</strong></p>
<p>集合匹配损失，用于反向传播优化网络（下面的 Hungarian 匹配损失则不参加反向传播，仅用于寻找匹配的 预测 box，注意区别）</p>
<p>（代码 14）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 只保留最后一个 decoder layer 的输出，辅助输出（非最后 decoder layer 的输出）的损失后面再计算</span>
    outputs_without_aux <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> outputs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token operator">!=</span> <span class="token string">'aux_outputs'</span><span class="token punctuation">&#125;</span>  
    indices <span class="token operator">=</span> self<span class="token punctuation">.</span>matcher<span class="token punctuation">(</span>output_without_aux<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>     <span class="token comment"># 计算 Hungarian Loss，见下文代码 17</span>

    num_boxes <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> targets<span class="token punctuation">)</span>      <span class="token comment"># 统计 minibatch 中所有 gt box 数量</span>
    <span class="token comment"># </span>
    num_boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>num_boxes<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    num_boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>num_boxes <span class="token operator">/</span> get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 不考虑分布式训练，get_world_size()=1</span>

    losses <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
    <span class="token keyword">for</span> loss <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>    <span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'boxes'</span><span class="token punctuation">,</span> <span class="token string">'cardinality'</span><span class="token punctuation">]</span>
        <span class="token comment"># get_loss: 根据指定的 loss 类型，获取相应的 loss 值；</span>
        <span class="token comment"># labels -> loss_labels(); boxes -> loss_boxes; cardinality -> loss_cardinality</span>
        losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token string">'aux_outputs'</span> <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> aux_outputs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token string">'aux_outputs'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 计算辅助 loss</span>
            indices <span class="token operator">=</span> self<span class="token punctuation">.</span>matcher<span class="token punctuation">(</span>aux_outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>            <span class="token comment"># 获取 匹配 indices</span>
            <span class="token keyword">for</span> loss <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>
                <span class="token keyword">if</span> loss <span class="token operator">==</span> <span class="token string">'masks'</span><span class="token punctuation">:</span> <span class="token keyword">continue</span>    <span class="token comment"># 分割任务，不计算辅助 mask loss</span>
                l_dict <span class="token operator">=</span> self<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> aux_outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">,</span> log<span class="token operator">=</span>loss<span class="token operator">!=</span><span class="token string">'labels'</span><span class="token punctuation">)</span>
                l_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token operator">+</span><span class="token string-interpolation"><span class="token string">f'_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> l_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
                losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>l_dict<span class="token punctuation">)</span>
    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最终返回一个 loss dict，以 <code>loss_ce</code> 开头的 key 表示交叉熵分类损失，<code>loss_bbox</code> 开头的 key 表示 l1 坐标（xywh）损失，以 <code>loss_giou</code> 开头的 key 表示 giou 损失。对于非最后一个 decoder layer 的损失，使用 <code>_&lt;i&gt;</code> 结尾，其中 <code>i</code> 为从 0 开始的编号。</p>
<ol>
<li>
<p>分类损失。注意，这里使用 NLL，用于反向传播更新梯度。（代码 15）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss_labels</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">,</span> log<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># outputs: &#123;'pred_logits': (B, N, C+1), 'pred_boxes': (B, N, 4)&#125;</span>
    <span class="token comment"># targets: dict list, 每个 dict 表示一个 image 的 target</span>
    <span class="token comment"># indices: tuple list，每个 tuple 表示一个 image 的预测 box ind 和 gt box ind</span>
    <span class="token comment"># num_boxes: minibatch 中所有 gt box 的数量</span>
    src_logits <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_logits'</span><span class="token punctuation">]</span>     <span class="token comment"># (B, N, C+1)</span>

    <span class="token comment"># batch_idx: (BM,)  where BM=M_1+M_2+...+M_B, first M_1 is `0`, and</span>
    <span class="token comment">#   then are M_2 `1`, and so on...</span>
    <span class="token comment"># src_idx: (BM,)   first M_1 are row ind(pred box ind) of first image, and so on...</span>
    <span class="token comment"># idx: (batch_idx, src_idx)</span>
    idx <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_src_permutation_idx<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>

    <span class="token comment"># t: i-th target, this is a dict. t['labels'] has a shape of (M_i,)</span>
    <span class="token comment"># J: gt box ind of matched pairs in i-th img, its shape is (M_i,)</span>
    <span class="token comment"># target_boxes_o: (BM,) ，minibatch 中所有匹配的 gt boxes 的 分类 id</span>
    target_classes_o <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>J<span class="token punctuation">]</span> <span class="token keyword">for</span> t<span class="token punctuation">,</span> <span class="token punctuation">(</span>_<span class="token punctuation">,</span> J<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>targets<span class="token punctuation">,</span> indices<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># target_classes: (B, N), 预测 box 对应的 gt 分类 id，</span>
    <span class="token comment">#       默认为 bg id，即 `num_classes`，不是 `0`，`0` 是第一个 fg 分类id</span>
    <span class="token comment">#       表示 预测 box 是 no-object（负例）</span>
    target_classes <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span>src_logits<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">,</span> 
                                dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">,</span> device<span class="token operator">=</span>src_logits<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token comment"># idx: mini-batch 中匹配对中的预测 box ind（范围 0~N-1）</span>
    <span class="token comment"># target_classes_o: mini-batch 中匹配对中的 target 分类 id</span>
    <span class="token comment"># 设置 target_classes 中被匹配中的预测 box 的分类，其分类为对应的 target 分类 id</span>
    target_classes<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> target_classes_o

    <span class="token comment"># 计算交叉熵，即 NLL 损失</span>
    <span class="token comment"># empty_weight: torch.ones(num_classes+1), 且 empty_weight[-1] = 0.1</span>
    <span class="token comment">#       正例损失系数 1.0， 负例损失系数为 0.1</span>
    <span class="token comment"># 交叉熵的 input shape：(B, C+1, N), target shape：(B, N)</span>
    <span class="token comment"># 交叉熵的各分类权重 shape：(C+1)</span>
    <span class="token comment"># loss_ce: (B, N) -> (reduction: mean) -> scalar</span>
    loss_ce <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>src_logits<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target_classes<span class="token punctuation">,</span> self<span class="token punctuation">.</span>empty_weight<span class="token punctuation">)</span>
    losses <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'loss_ce'</span><span class="token punctuation">:</span>loss_ce<span class="token punctuation">&#125;</span>
    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>坐标损失，包括 l1 损失和 GIOU 损失，（代码 16）</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss_boxes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># idx: 获取 batch 中所有匹配对中的预测 box ind</span>
    idx <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_src_permutation_idx<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>
    <span class="token comment"># predicted boxes，获取 batch 匹配对中的预测 box（归一化坐标，cx,cy,w,h）</span>
    src_boxes <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_boxes'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>idx<span class="token punctuation">]</span>  <span class="token comment"># (BM, 4)</span>
    <span class="token comment"># t: j-th target, this is a dict. t['boxes'] has a shape of (M_j, 4)</span>
    <span class="token comment"># i: j-th gt box ind, its shape is (M_j,)</span>
    <span class="token comment"># target_boxes: (BM, 4)，batch 中所有 target box 坐标（归一化，cx,cy,w,h)</span>
    target_boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">[</span><span class="token string">'boxes'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> t<span class="token punctuation">,</span> <span class="token punctuation">(</span>_<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>targets<span class="token punctuation">,</span> indices<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment"># 计算 L1 损失</span>
    loss_bbox <span class="token operator">=</span> F<span class="token punctuation">.</span>l1_loss<span class="token punctuation">(</span>src_boxes<span class="token punctuation">,</span> target_boxes<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>    <span class="token comment"># (BM, 4)</span>

    losses <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
    losses<span class="token punctuation">[</span><span class="token string">'loss_bbox'</span><span class="token punctuation">]</span> <span class="token operator">=</span> loss_bbox<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_boxes   <span class="token comment"># num_boxes 应该等于 src_boxes.shape[0]?</span>

    <span class="token comment"># GIOU loss = 1 - GIOU</span>
    loss_giou <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">-</span>torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>box_ops<span class="token punctuation">.</span>generalized_box_iou<span class="token punctuation">(</span>
        box_ops<span class="token punctuation">.</span>box_cxcywh_to_xyxy<span class="token punctuation">(</span>src_boxes<span class="token punctuation">)</span><span class="token punctuation">,</span>
        box_ops<span class="token punctuation">.</span>box_cxcywh_to_xyxy<span class="token punctuation">(</span>target_boxes<span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">)</span>
    losses<span class="token punctuation">[</span><span class="token string">'loss_giou'</span><span class="token punctuation">]</span> <span class="token operator">=</span> loss_giou<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_boxes
    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>cardinality loss：预测 fg box 数量与 gt box 数量（在一个 mini-batch 内）的平均差。</p>
<p>预测 box 为非 bg 的数量 <code>card_pred</code>，其 shape 为 $(B,)$，gt box 的数量 <code>tgt_lengths</code>，其 shape 为 $(B,)$，表示 mini-batch 中各个 image 中的预测为 fg 的数量和 gt box 数量，计算这两个 tensor 的 L1 损失，并求均值，这个损失 <strong>不用于反向传播</strong>。</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python">card_err <span class="token operator">=</span> F<span class="token punctuation">.</span>l1_loss<span class="token punctuation">(</span>card_pred<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tgt_lengths<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
</ol>
<p><strong>反向传播</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">loss_dict <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>     <span class="token comment"># 分类损失，l1 损失，giou 损失</span>
weight_dict <span class="token operator">=</span> criterion<span class="token punctuation">.</span>weight_dict     <span class="token comment"># 各损失的权重，分类损失为基准（其权值为 1），l1 权值为 5，giou 权值为 2</span>

<span class="token comment"># 计算所有损失的加权和，包括所有 decoder layer 的各项损失</span>
losses <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">*</span> weight_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> loss_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token keyword">in</span> weight_dict<span class="token punctuation">)</span>

optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
losses<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> max_norm <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>    <span class="token comment"># 默认 0.1</span>
    torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_norm<span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="2-3-测试">2.3 测试</h2>
<p>对一个新的 input image 进行预测时，根据前面分析，知道两个预测分支的输出为：1.分类得分$(1, N, C+1)$，2.预测坐标（xywh） $(1, N, 4)$。</p>
<p>首先根据分类得分得到 fg boxes 以及对应的分类 id</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># pred_logits, pred_boxes</span>
<span class="token comment"># batch_size  B=1</span>
pred_ind <span class="token operator">=</span> pred_logits<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>   <span class="token comment"># (B, N)</span>
<span class="token comment"># (B, N, C+1)，the last class id `C` represents bg</span>
fg_ind <span class="token operator">=</span> pred_ind <span class="token operator">!=</span> pred_logits<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>  <span class="token comment"># shape: (B, N)</span>
<span class="token comment"># (B,)  each element is the number of pred_fg boxes for some one image</span>
fg_num <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>fg_ind<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>   

<span class="token comment"># (n, 4)  n 为 mini-batch 中所有预测为 fg 的数量</span>
fg_boxes <span class="token operator">=</span> pred_boxes<span class="token punctuation">[</span>fg_ind<span class="token punctuation">]</span>       
x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h <span class="token operator">=</span> fg_boxes<span class="token punctuation">.</span>unbind<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 4 个变量 shape 均为 (n,)</span>
<span class="token comment"># (cx, cy, w, h) -> (x1, y1, x2, y2)</span>
xyxy <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token operator">-</span><span class="token number">0.5</span><span class="token operator">*</span>w<span class="token punctuation">,</span> y<span class="token operator">-</span><span class="token number">0.5</span><span class="token operator">*</span>h<span class="token punctuation">,</span> x<span class="token operator">+</span><span class="token number">0.5</span><span class="token operator">*</span>w<span class="token punctuation">,</span> y<span class="token operator">+</span><span class="token number">0.5</span><span class="token operator">*</span>h<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># (n, 4)</span>
cls_id <span class="token operator">=</span> pred_ind<span class="token punctuation">[</span>fg_ind<span class="token punctuation">]</span>           <span class="token comment"># (n,)   class id of predicted box</span>
<span class="token comment"># normalized (x1, y1, x2, y2)</span>
<span class="token comment"># split x1y1x2y2 into a tuple, each element represents predicted coords of one image in mini-batch</span>
xyxy_tuple <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>xyxy<span class="token punctuation">,</span> fg_num<span class="token punctuation">)</span>  <span class="token comment"># (tensor_1,...,tensor_B), each tensor has a shape (n_i, 4), s.t. sum_i n_i = n</span>
<span class="token comment"># fg predicted class id</span>
cls_id_tuple <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>cls_id<span class="token punctuation">,</span> fg_num<span class="token punctuation">)</span>  <span class="token comment"># (tensor1,...,tensor_B), each tensor's shape (n_i,), s.t. sum_i n_i = n</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>DETR 没有 post processing，故获取预测结果非常简单。</p>
<h1>3. 总结</h1>
<p>DETR 首次（不是？）将 Transformer 用于目标检测，后面有很多研究均基于 DETR 进行改进，故对 DETR 研究透彻非常有必要，为了以后能快速恢复对 DETR 的了解，对 DETR 各关键点进行总结。</p>
<h2 id="3-1-结构">3.1 结构</h2>
<p>DETR 结构图如图 1 和图 2。</p>
<h3 id="3-1-1-Network-Input">3.1.1 Network Input</h3>
<p>对于一个 mini batch 中图像数据，左上对齐，右下 zero-padding，得到一个 batch 数据，以及一个相同 spatial size 的 mask。</p>
<p>对于 target bbox 坐标，从 COCO anno 文件加载的是 <code>(x1, y1, w, h)</code>，经过 <code>ConvertCocoPolysToMask</code> 转换后为 <code>(x1, y1, x2, y2)</code>，然后再经 <code>transforms.Normalize</code> 转换为 <strong>归一化后的</strong> <code>(cx, cy, w, h)</code>。（归一化指 将 <code>x, y</code> 除以 <code>w, h</code>）。</p>
<h3 id="3-1-2-Backbone">3.1.2 Backbone</h3>
<p>ResNet50/ResNet101（这两个常用），下采样率 <code>32</code>。</p>
<p>预处理的数据经过 backbone，输出 feature，然后将 Network Input 中的 mask 数据 rescale 到原来的 <code>1/32</code>。两个输出：</p>
<ol>
<li>features</li>
<li>PE</li>
</ol>
<h3 id="3-1-3-Encoder">3.1.3 Encoder</h3>
<p>上一步的 features 和 PE 的 channel 数不同，对 features 采用 <code>1x1 conv</code> 降维。</p>
<p>Encoder 输入：</p>
<ol>
<li>降维后的 features</li>
<li>PE</li>
<li>src_key_padding_mask，这是由于 batch 内各 image size 不同而进行 zero padding，由此需要引入 mask，对部分位置上的 attention 进行 mask。</li>
</ol>
<p>Encoder 结构如图 3 所示，为了方便查看，这里在下方再贴出来。features 为 query, key, value，其中 query 和 key 还需要另外 element-wise 加上 PE，value 不需要叠加 PE。</p>
<p>单个 block 的输出 <code>output</code> shape 为 <code>(HW, B, d)</code>，保持不变，这个 <code>output</code> 继续作为下一个 block 的 query, key, value，且同时 query 和 key 需要叠加 PE（与最开始的 PE 相同，value 不需要叠加 PE。</p>
<p>重复 $N=6$ 次 Encoder block 后输出 <code>output</code>，其 shape 为 <code>(HW, B, d)</code> 。</p>
<h3 id="3-1-4-Decoder">3.1.4 Decoder</h3>
<p>Decoder 结构如图 3 所示，输入包含：</p>
<ol>
<li>object_query：用于表征 $N=100$ 个预测目标，这是一个可学习的 Embedding（weight使用 $\mathcal N(0,1)$ 进行初始化）。shape 为 <code>(N=100, d)</code>，其中 $d$ 为模型维度，维度调整后 shape 为 <code>(N, B, d)</code>。</li>
<li>tgt：与 object_query 相同 shape，初始化为全零 tensor。</li>
<li>Encoder 的最后一个 block 的输出，记为 <code>memory</code>，shape 为 <code>(HW, B, d)</code>。</li>
<li>memory_key_padding_mask：这是对 <code>memory</code> 进行 mask。仍然是因为 network input image 大小不一，导致 zero-padding，从而需要对 padded position 进行 mask。</li>
<li>PE：position embedding，与 Encoder 中所用 PE 相同。</li>
</ol>
<p>注意 Encoder block 中只有一个 attn layer，而 Decoder 中有两个 attn layer，这两个 attn layer 的输入 <strong>不同</strong>。</p>
<p><strong>第一个 attn layer：</strong></p>
<p>query 和 key 均为 <code>tgt</code> 与 <code>object_query</code> 的叠加。value 为 <code>tgt</code>，attn layer 的输出，记为 <code>tgt2</code>，其 shape 仍然是 <code>(N, B, d)</code>。</p>
<p>这个输出 <code>tgt2</code> 依次经过 dropout，residual connect 和 norm 处理之后，记为 <code>tgt</code>，准备进入 第二个 attn layer。</p>
<p><strong>第二个 attn layer：</strong></p>
<p>注意看图 3， q, k, v 均与第一个 attn layer 不同。</p>
<ol>
<li>将 <code>tgt</code>（上一个步的输出） 和 <code>object_query</code> （与上一步的相同）叠加，作为 query，shape 为 <code>(N, B, d)</code></li>
<li>将 Encoder 的最终输出 <code>memory</code> 与 PE 叠加，作为 key，shape 为 <code>(HW,B,d)</code>。</li>
<li>将 Encoder 的最终输出 <code>memory</code> 作为 value，shape 为 <code>(HW, B, d)</code>。</li>
</ol>
<p>Q 与 K 的 attention weight，<code>(B, N, HW)</code>，与  value 作用后结果 shape 为 <code>(N, B, d)</code>，然后再依次经过 dropout，residual connect 和 norm 操作，记这一步结果为 <code>tgt</code>。</p>
<p><strong>FFN：</strong></p>
<p>上一步结果 <code>tgt</code> 经一个双 fc layer 组成的前馈网络，输出的 shape 保持不变为 <code>(N, B, d)</code>，然后再依次经过 dropout，residual connect 和 norm 操作，得到 Decoder 中单个 block 的输出，记结果为 <code>tgt</code>。</p>
<p>将第一个 block 的输出 <code>tgt</code> 代替原始的全零 tensor 的 <code>tgt</code>，其他参数保持不变，送入第二个 block。重复 $M=6$ 次 block。</p>
<p>每个 block 的输出仅保存起来，最后再 stack，得到 Decoder 的输出 <code>(M, N, B, d)</code>。</p>
<h3 id="3-1-6-检测-heads">3.1.6 检测 heads</h3>
<p>分类和位置两个 heads。</p>
<ol>
<li>
<p>分类 head：一个 fc 层，输出 channel 为 <code>num_classes+1</code>，因为 $N=100$ 的预测目标通常有 bg。输出 shape 为 <code>(M, B, N, num_classes+1)</code></p>
</li>
<li>
<p>坐标 head：三个 fc 层组成的 MLP。输出 shape <code>(M, B, N, 4)</code></p>
<p>MLP 输入 channel 和 hidden channel 均为 d，输出 channel 为 4 （bbox 坐标）<br>
MLP 中前两个 fc 有 relu，最后一个 fc 没有 relu</p>
</li>
</ol>
<h2 id="3-2-LOSS">3.2 LOSS</h2>
<p>见上面 <code>2.2</code> 节。</p>
<h2 id="3-3-Prediction">3.3 Prediction</h2>
<p>见上面 <code>2.3</code> 节。</p>
<h1>4. Segmentation</h1>
<p>在 decoder 输出之后增加一个 mask head，实现分割功能。对于全景分割，则需要将 thing 和 stuff（除目标之外的东西，如天空、草地、建筑等）统一看待，对它们所在的区域（region）均需要进行检测。COCO 全景数据包含 80 个目标分类和 53 个 stuff 分类。</p>
<p><img src="/images/transformer/DETR4.png" alt=""></p>
<center>图 4. 全景分割的 mask head</center>
<p>分割任务的实现思路：</p>
<ol>
<li>
<p>使用前面 DETR 在目标检测集上训练</p>
</li>
<li>
<p>冻结 DETR 的参数，然后增加一个 mask head，在分割数据集上训练这个 mask head</p>
</li>
</ol>
<p>具体而言，</p>
<ol>
<li>
<p>在 decoder 之后继续使用 multi-head self-attention，，如图 4 左起第一个 attention，其 Q 为 decoder 的输出，K 为 encoder 的输出（参考图 3 decoder 的第二个 attention），计算 $QK^{\top}$ 得到 attention 作为输出（没有 value），shape 为 <code>(B, N, 8, H, W)</code>，其中 N=100，8 是 multi-heads 数量。</p>
</li>
<li>
<p>使用 FPN 网络，输入是 backbone 经过一个 conv 适配后的输出以及第 <code>1</code> 步得到的 attention：</p>
<ul>
<li>前者 shape 为 <code>(B, d, H, W)</code>，增加一个维度后为 <code>(B, 1, d, H, W)</code>，repeat 为 <code>(B, N, d, H, W)</code>，最后 flatten 为 <code>(BN, d, H, W)</code></li>
<li>后者 shape 为 <code>(B, N, 8, H, W)</code>，flatten 为 <code>(BN, 8, H, W)</code>，</li>
<li>两者 concatenate，得到 <code>(BN, d+8, H, W)</code></li>
<li>然后经过 FPN ，得到输出 <code>pred_masks</code>，其 shape 为 <code>(BN, 1, 8H, 8W)</code>，review 为 <code>(B, N, 8H, 8W)</code></li>
</ul>
<p>整个过程如图 5 所示，</p>
<p><img src="/images/transformer/DETR5.png" alt=""></p>
 <center>图 5. FPN-style 的 mask head</center>
</li>
</ol>
<h2 id="4-1-代码">4.1 代码</h2>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DETRsegm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> detr<span class="token punctuation">,</span> freeze_detr<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>detr <span class="token operator">=</span> detr    <span class="token comment"># 目标检测任务的 DETR</span>
        <span class="token keyword">if</span> freeze_detr<span class="token punctuation">:</span>
            <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># transformer 模型 dim，例如 256</span>
        <span class="token comment"># multi-head self-attn 的 head 数量</span>
        hidden_dim<span class="token punctuation">,</span> nheads <span class="token operator">=</span> detr<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> detr<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>nhead
        self<span class="token punctuation">.</span>bbox_attention <span class="token operator">=</span> MHAttentionMap<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> nheads<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mask_head <span class="token operator">=</span> MaskHeadSmallConv<span class="token punctuation">(</span>hidden_dim <span class="token operator">+</span> nheads<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>samples<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            samples <span class="token operator">=</span> nested_tensor_from_tensor_list<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>   <span class="token comment"># size 不同的图像填充</span>
        <span class="token comment"># 对于分割任务，backbone 返回 4 个 layer 的输出特征</span>
        <span class="token comment"># 4 个 PE，PE 与 feature 一一对应</span>
        features<span class="token punctuation">,</span> pos <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
        bs <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># B, batch_size</span>
        <span class="token comment"># src: (B, 2048, H, W), mask: (B, H, W)</span>
        src<span class="token punctuation">,</span> mask <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>decompose<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 最后一个特征 H0/32,W0/32</span>
        src_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>input_proj<span class="token punctuation">(</span>src<span class="token punctuation">)</span>    <span class="token comment"># 维度适配，(B, d, H, W)</span>
        <span class="token comment"># 最小 size 的特征输入到 transformer</span>
        <span class="token comment"># hs: decoder 输出（经过 review）(M, B, N, d)，其中 N=100，query_emb 数量</span>
        <span class="token comment"># memory: encoder 输出（经过 reshape）(B, d, H, W)</span>
        hs<span class="token punctuation">,</span> memory <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>src_proj<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>query_embed<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># M 是 decoder attention block 的数量，所有 attention block 的输出</span>
        outputs_class <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>class_embed<span class="token punctuation">(</span>hs<span class="token punctuation">)</span>   <span class="token comment"># cls head 输出 (M, B, N, C+1)</span>
        outputs_coord <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>bbox_embed<span class="token punctuation">(</span>hs<span class="token punctuation">)</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># (M, B, N, 4)</span>
        <span class="token comment"># 记录最后一个 decoder atten block 对应的 cls 和 box 输出</span>
        out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'pred_logits'</span><span class="token punctuation">:</span> outputs_class<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'pred_boxes'</span><span class="token punctuation">:</span> outputs_coord<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>aux_loss<span class="token punctuation">:</span>
            <span class="token comment"># 保存其他 decoder atten block 对应的 cls 和 box 输出</span>
            output<span class="token punctuation">[</span><span class="token string">'aux_outputs'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>_set_aux_loss<span class="token punctuation">(</span>outputs_class<span class="token punctuation">,</span> outputs_coord<span class="token punctuation">)</span>
        
        <span class="token comment"># 在这之前，执行逻辑与目标检测的 DETR 完全一样</span>
        <span class="token comment"># 在这之后，开始 mask head 前向传播</span>

        <span class="token comment"># 输入为最后一个 decoder atten block 的输出，encoder 输出，输入图像 mask</span>
        <span class="token comment">#                           (B, N, d),   (B, d, H, W),  (B, H, W)</span>
        <span class="token comment"># bbox_mask: (B, 1, 1, H, W)</span>
        bbox_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_attention<span class="token punctuation">(</span>hs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> memory<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>

        <span class="token comment"># features 中各 feature 的 shape 为 </span>
        <span class="token comment"># (B, 256, H0//4, W0//4)</span>
        <span class="token comment"># (B, 512, H0//8, W0//8)</span>
        <span class="token comment"># (B,1024, H0//16, W0//16)</span>
        <span class="token comment"># (B, 2048, H0//32, W0//32)</span>
        <span class="token comment"># (H0, W0) 是网络的 input size</span>
        <span class="token comment"># seg_masks: (BN, 1, H0//4, W0//4)。二分类，非归一化得分</span>
        seg_masks <span class="token operator">=</span> self<span class="token punctuation">.</span>mask_head<span class="token punctuation">(</span>src_proj<span class="token punctuation">,</span> bbox_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span>features<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tensors<span class="token punctuation">,</span> features<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tensors<span class="token punctuation">,</span> features<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tensors<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># seg_masks: (B, N, H0//4, W0//4)</span>
        outputs_seg_masks <span class="token operator">=</span> seg_masks<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>detr<span class="token punctuation">.</span>num_queries<span class="token punctuation">,</span> seg_masks<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> seg_masks<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        out<span class="token punctuation">[</span><span class="token string">'pred_masks'</span><span class="token punctuation">]</span> <span class="token operator">=</span> outputs_seg_masks
        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong># Dataset</strong></p>
<p>数据集代码，这里我们主要看 target 的构造，因为 image 输入与检测任务一样，target 中包含 mask 的数据。</p>
<p>anno 文件中，记录了每个目标/stuff 的多边形区域，每个目标/stuff 可能会有多个多边形区域，这是因为目标/stuff 被其他东西遮挡。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># </span>
<span class="token keyword">class</span> <span class="token class-name">ConvertCocoPolysToMask</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> image<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        一个图像中可能有多个 目标/stuff，每个 目标/stuff 可能有多个多边形
        image: 单个图像
        target: dict 类型

        返回：dict 类型
        '''</span>
        w<span class="token punctuation">,</span> h <span class="token operator">=</span> image<span class="token punctuation">.</span>size

        image_id <span class="token operator">=</span> target<span class="token punctuation">[</span><span class="token string">'image_id'</span><span class="token punctuation">]</span>   <span class="token comment"># 图像 id</span>
        image_id <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>image_id<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 转为 tensor</span>

        <span class="token comment"># anno 是一个 list[dict] 对象，每个 dict 记录一个目标/stuff 的标注数据</span>
        anno <span class="token operator">=</span> target<span class="token punctuation">[</span><span class="token string">'annotations'</span><span class="token punctuation">]</span>

        <span class="token comment"># 筛选那些不是 拥挤的目标/stuff（太难，使用了反而影响学习效果）</span>
        anno <span class="token operator">=</span> <span class="token punctuation">[</span>obj <span class="token keyword">for</span> obj <span class="token keyword">in</span> anno <span class="token keyword">if</span> <span class="token string">'iscrowd'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> obj <span class="token keyword">or</span> obj<span class="token punctuation">[</span><span class="token string">'iscrowd'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span>

        <span class="token comment"># 所有 目标/stuff 的 bbox。一个目标/stuff 仅有一个 bbox</span>
        boxes <span class="token operator">=</span> <span class="token punctuation">[</span>obj<span class="token punctuation">[</span><span class="token string">'bbox'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> obj <span class="token keyword">in</span> anno<span class="token punctuation">]</span>

        <span class="token comment"># (N, 4)    其中 N 为当前图像中的 目标/stuff 数量</span>
        boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>boxes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>    <span class="token comment"># (x,y,w,h) -> (x1,y1,x2,y2)</span>
        boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clamp_<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>w<span class="token punctuation">)</span>
        boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clamp_<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>h<span class="token punctuation">)</span>

        classes <span class="token operator">=</span> <span class="token punctuation">[</span>obj<span class="token punctuation">[</span><span class="token string">'category_id'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> obj <span class="token keyword">in</span> anno<span class="token punctuation">]</span>  <span class="token comment"># 所有 目标/stuff 的分类</span>
        classes <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>classes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>  <span class="token comment"># (N, )</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>return_masks<span class="token punctuation">:</span>   <span class="token comment"># 这里是分割任务，所以为 True</span>
            <span class="token comment"># list[list[float]]，内层的 list[float] 表示一个多边形</span>
            segmentations <span class="token operator">=</span> <span class="token punctuation">[</span>obj<span class="token punctuation">[</span><span class="token string">'segmentation'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> obj <span class="token keyword">in</span> anno<span class="token punctuation">]</span>
            <span class="token comment"># masks: (N, h, w) ，这里 N 表示当前图像中 目标/stuff 数量</span>
            <span class="token comment"># 每个 目标/stuff 均使用与图像 size 相等的 binary map 表示 mask</span>
            masks <span class="token operator">=</span> convert_coco_poly_to_mask<span class="token punctuation">(</span>segmentations<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
        
        <span class="token comment"># x2 > x1, y2 > y1</span>
        keep <span class="token operator">=</span> <span class="token punctuation">(</span>boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">></span> boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        boxes <span class="token operator">=</span> boxes<span class="token punctuation">[</span>keep<span class="token punctuation">]</span>         <span class="token comment"># (n, 4)</span>
        classes <span class="token operator">=</span> classes<span class="token punctuation">[</span>keep<span class="token punctuation">]</span>     <span class="token comment"># (n, )</span>

        target <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'boxes'</span><span class="token punctuation">:</span> boxes<span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">:</span> classes<span class="token punctuation">&#125;</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>return_masks<span class="token punctuation">:</span>
            masks <span class="token operator">=</span> masks<span class="token punctuation">[</span>keep<span class="token punctuation">]</span>     <span class="token comment"># (n, h, w)</span>
            target<span class="token punctuation">[</span><span class="token string">'masks'</span><span class="token punctuation">]</span> <span class="token operator">=</span> masks
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token keyword">return</span> image<span class="token punctuation">,</span> target<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>分析了 model 和 dataset 的代码，然后我们再看训练代码（其中关键的部分），</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_one_epoch</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> device
    epoch<span class="token punctuation">,</span> max_norm<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">for</span> samples<span class="token punctuation">,</span> targets <span class="token keyword">in</span> metric_logger<span class="token punctuation">.</span>log_every<span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> print_freq<span class="token punctuation">,</span> header<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token comment"># targets: list[dict]，每个 dict 表示一个图像 target，参考 ConvertCocoPolysToMask 输出</span>

        <span class="token comment"># outputs 是一个 dict，包含：</span>
        <span class="token comment"># pred_logits: (B, N, C+1)  # 最后一个 attn block 经 cls head 的输出</span>
        <span class="token comment"># pred_boxes: (B, N, 4)     # 最后一个 attn block 经 box head 的输出</span>
        <span class="token comment"># pred_masks: (B, N, H0//4, W0//4)</span>
        <span class="token comment"># aux_outputs: 其余 attn blocks 经 cls 和 box head 输出的loss</span>
        <span class="token comment">#               [&#123;'pred_logits': xxx, 'pred_boxes': xxx&#125;]</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
        loss_dict <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span> <span class="token comment"># 计算各种损失</span>
        weight_dict <span class="token operator">=</span> criterion<span class="token punctuation">.</span>weight_dict     <span class="token comment"># 每种损失的权重不同</span>
        losses <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">*</span> weight_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> loss_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token keyword">in</span> weight_dict<span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        losses<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> max_norm <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_norm<span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong># 损失</strong></p>
<p>mask loss 使用两个损失：1. Focal loss；2. DICE loss</p>
<ol>
<li>
<p>flocal loss</p>
<p>$$L = -\alpha _ t (1-x _ t) ^ {\gamma} \log x _ t$$</p>
<p>其中 $x _ t= \begin{cases} x &amp; y=1 \\ 1-x &amp; y=0 \end{cases}$</p>
</li>
<li>
<p>DICE loss</p>
<p>$$L = 1-  \frac {2 \sum _ i ^ N p _ i g _ i}{\sum _ i ^ N p _ i + \sum _ i ^ N g _ i}$$</p>
</li>
</ol>
<p>其他损失包括分类和坐标损失，与检测任务一样，这里不再细说。</p>
<p>计算损失的代码如下，</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SetCriterion</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs_without_aux <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> outputs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token operator">!=</span> <span class="token string">'aux_outputs'</span><span class="token punctuation">&#125;</span>
        <span class="token comment"># indices: [(tensor, tensor)]，匈牙利匹配结果</span>
        <span class="token comment"># 一共 B 个二元素 tuple，tuple[0] 表示匹配的预测box 下标，tuple[1] 表示gt box 下标</span>
        indices <span class="token operator">=</span> self<span class="token punctuation">.</span>matcher<span class="token punctuation">(</span>outputs_without_aux<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>

        <span class="token comment"># 这个 batch 内所有图像中的 box 数量之和</span>
        num_boxes <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> targets<span class="token punctuation">)</span>
        num_boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>num_boxes<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
        nux_boxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>num_boxes <span class="token operator">/</span> get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        losses <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">for</span> loss <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>    <span class="token comment"># labels, boxes, cardinality, masks</span>
            losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 使用 aux_outputs ，decoder 其他  attn blocks 的输出经过 cls head</span>
        <span class="token comment"># 和 box head 的分类预测和坐标预测，计算相关损失</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

        <span class="token keyword">return</span> losses
    
    <span class="token keyword">def</span> <span class="token function">loss_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 根据匈牙利算法匹配的结果中提取匹配的预测 box id</span>
        <span class="token comment"># src_idx: (batch_idx, src_box_idx)，两个元素均为 1-D tensor</span>
        src_idx <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_src_permutation_idx<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>
        <span class="token comment"># tgt_idx: (batch_idx, tgt_box_idx)，匹配的 gt box id</span>
        tgt_idx <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_tgt_permutation_idx<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>

        src_masks <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'pred_masks'</span><span class="token punctuation">]</span>   <span class="token comment"># (B, N=100, H0//4, W0//4)</span>
        <span class="token comment"># 筛选出匹配的预测 box。这里 m 表示 batch 中所有匹配的 box pair 的数量</span>
        src_masks <span class="token operator">=</span> src_masks<span class="token punctuation">[</span>src_idx<span class="token punctuation">]</span>  <span class="token comment"># (m, H0//4, W0//4)</span>
        masks <span class="token operator">=</span> <span class="token punctuation">[</span>t<span class="token punctuation">[</span><span class="token string">'masks'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> targets<span class="token punctuation">]</span>   <span class="token comment"># [(n, H0, W0)]</span>
        <span class="token comment"># masks 是 batch 中所有图像的 box mask，由于每个图像中 box 数量不等</span>
        <span class="token comment"># 所以使用具有最多 box 数量 n1，即 target_masks: (B, n1, H0, W0)</span>
        target_masks<span class="token punctuation">,</span> valid <span class="token operator">=</span> nested_tensor_from_tensor_list<span class="token punctuation">(</span>masks<span class="token punctuation">)</span><span class="token punctuation">.</span>decompose<span class="token punctuation">(</span><span class="token punctuation">)</span>
        target_masks <span class="token operator">=</span> target_masks<span class="token punctuation">.</span>to<span class="token punctuation">(</span>src_masks<span class="token punctuation">)</span>
        <span class="token comment"># 筛选出匹配的 gt box</span>
        target_masks <span class="token operator">=</span> target_masks<span class="token punctuation">[</span>tgt_idx<span class="token punctuation">]</span>    <span class="token comment"># (m, H0, W0)</span>

        <span class="token comment"># 插值。二维 spatial size (H0//4, W0//4) 放大到 (H0, W0)</span>
        <span class="token comment"># 输入必须是 4-d tensor，所以 src_mask 从 (m, H0//4, W0//4) </span>
        <span class="token comment"># 转为 (m, 1, H0//4, W0//4)，然后插值上采样，得到 (m, 1, H0, W0)</span>
        src_masks <span class="token operator">=</span> interpolate<span class="token punctuation">(</span>src_masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                                size<span class="token operator">=</span>target_masks<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> 
                                align_corners<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># (m, 1, H0, W0)</span>
        src_masks <span class="token operator">=</span> src_masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (m, H0*W0)</span>
        target_masks <span class="token operator">=</span> target_masks<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (m, H0*W0)</span>
        target_masks <span class="token operator">=</span> target_masks<span class="token punctuation">.</span>view<span class="token punctuation">(</span>src_masks<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment"># src_masks 表示非归一化预测得分</span>
        <span class="token comment"># target_masks，binary map： 0/1</span>
        losses <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
            <span class="token string">'loss_mask'</span><span class="token punctuation">:</span> sigmoid_focal_loss<span class="token punctuation">(</span>src_masks<span class="token punctuation">,</span> target_masks<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'loss_dice'</span><span class="token punctuation">:</span> dice_loss<span class="token punctuation">(</span>src_masks<span class="token punctuation">,</span> target_masks<span class="token punctuation">,</span> num_boxes<span class="token punctuation">)</span>
        <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">shajianjian</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jianjiansha.github.io/2022/01/21/transformer/detr/">https://jianjiansha.github.io/2022/01/21/transformer/detr/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">shajianjian</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/object-detection/">
                                    <span class="chip bg-color">object detection</span>
                                </a>
                            
                                <a href="/tags/transformer/">
                                    <span class="chip bg-color">transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/01/25/pytorch/attention/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="Attention">
                        
                        <span class="card-title">Attention</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            相关论文：Attention Is All You Need
论文解读
1. 流程简介
为了方便理解，这里我简洁地进行总结。以机器翻译任务为例说明。
将 multi-head self-attention 简记为 mh self-attn

                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            shajianjian
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/PyTorch/">
                        <span class="chip bg-color">PyTorch</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/01/19/pytorch/embedding/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/12.jpg" class="responsive-img" alt="词嵌入向量">
                        
                        <span class="card-title">词嵌入向量</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            embedding 就是将一个 word 转换为 一个 vector，最简单的方法就是使用 one-hot vector，其长度为词汇表大小，但是 one-hot vector 无法表示词的语义，比如两个词是相近还是相反含义，或者无关，em
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-01-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            shajianjian
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/PyTorch/">
                        <span class="chip bg-color">PyTorch</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2024</span>
            
            <a href="/about" target="_blank">shajianjian</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/jianjiansha" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:501834524@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=501834524" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 501834524" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    
        <!-- <script src='https://unpkg.com/mermaid@latest/dist/mermaid.min.js'></script> -->
        <script src='/libs/mermaid/mermaid.min.js'></script>
        <script>
          if (window.mermaid) {
            mermaid.initialize({theme: 'forest'});
          }
        </script>
    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
