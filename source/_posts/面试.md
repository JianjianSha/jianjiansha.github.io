---
layout: interview
title: 面试
date: 2023-08-07 20:32:32
tags:
---
# 1. 传统算法

B 树（平衡多叉树），红黑树（平衡二叉树），存储/搜索 公司名等文本

Huffman 编码，压缩文本

最短路径：Dijstra 算法 。挖掘公司关系时，找到两个公司的最大（权重取负）投资比例的路径

挖掘公司与公司，人与人，公司与人的关系，并判断是否同一个人。

# 2. 机器学习

分类：

1. 行业分类：NB，多分类-SVM（企业行业分类 multi label multi class）
2. 分词：HMM，CRF

    - EM 算法：训练 HMM 等模型用到。
    - E step，求后验分布 $p(z _ i|x _ i;\theta)$，对应 responsibility 矩阵的第 `i` 行：观测数据 $x _ i$ 来自隐变量 $z _ i$ 各个值的概率
    - M step：最大化 $\max_{\theta} \sum_i \sum_{z_i} Q(z_i) \log P(x_i,z_i;\theta)$

搜索排名：TFIDF

数据降维：PCA，计算企业搜索排名权重，企业维度很大，需要降维。各维度需要先归一化，然后做变换 $\mathbf z = B \mathbf x$ 。

- 原理：最大化降维之后的方差 $\mathbb V[\mathbf z]$

图像压缩：SVD，取 topk 大奇异值对应的特征向量矩阵乘积 $\sum _ {i=1} ^ k \mathbf u _ i \mathbf v _ i ^ {\top}$

# 3. 深度学习

## 3.1 图像分类

损失：交叉熵，focal loss


1. VGG：conv+bn+relu，然后适当的 maxpool，注意 maxpool 之后增大 channel，避免过多的信息损失掉

2. ResNet：ResLayer 有skip连接，可以叠更多layer，网络更deep，避免梯度消失

3. DenseNet：DenseBlock，前面layer全部连接到后面layer，可以降低 channel，从而降低计算量，内存占用

4. SENet：一条分支，计算channel weight

5. shufflenet：分组卷积，减少计算量，两个分组卷积之间需要 shuffle channel

6. ViT：切分为 16x16 的patches，然后组成 `(B, n_embd, 1+256)`，其中 `1` 表示 cls token，送入 transformer encoder，需要巨大的数据集（上亿级别），否则效果不如 CNN

7. DeiT：解决 ViT 数据集巨大的问题。prepend cls token，append distill token

    - teacher：预训练好的 RegNet
    - teacher 输出作为 soft label，与 distill token 对应的分类预测做 KL loss（硬蒸馏则使用 CE loss）
    - true label 与 cls token 对应的分类预测做 CE loss
    - inference 时，cls token 预测概率与 distill token 预测概率求平均

8. Visual Transformer：仅使用 $L=16$ 个视觉 tokens 表示图像，而 CNN 需要成百上千的 filters
    
    - 卷积的缺点：无法解决 long range dependency，channel size 巨大为了包含特征中的所有模式，导致计算量，参数量也巨大
    - 将上一个 block 输出特征经过 `1x1` 卷积输出特征，reshape 为 `X: (B, HW, C)`
    - tokenizer：$T=\text{softmax}(XW _ A) ^ {\top} \ X \in \mathbb R ^ {B \times L \times C}$
    - transformer encoder：Q, K 是 T 分别经过线性变换，V 就是 T 本身
    - projector：$X _ {out} = X _ {in} + \text{softmax} ((X _ {in} W _ Q)(T W _ K)^{\top}) T$
    - 分类模型：ResNet backbone，最后一个 stage 中的每个 res block 均替换为 VT block。$L$ 个 tokens 向量均值池化得到 $(B, C)$，然后经过线性变换得到 `(B, n_cls)` 得到分类预测得分
    - 分割模型：使用 FPN baseline，将 FPN 中的各 layer 的横向连接（卷积）改为 VT 。
    - 聚类：将输入特征 $X \in \mathbb R ^ {C \times HW}$ 通过聚类方式得到 $L$ 个中心的向量作为 tokenizer 的变换矩阵 $W _K \in \mathbb R ^ {C \times L}$

9. T2T-ViT：提取 tokens，然后使用若干个 transformer 提取注意力特征

    - 解决 ViT 训练集巨大的问题， ViT 中将 Image 划分为 $16 \times 16$ 过于简单粗暴，不能很好的对局部结构进行 model
    - T2T 分两步：重建+soft plit 。使用较小的维度（32 或 64）
    
        - soft plit：对于一个图像或特征 $(h,w,c)$，使用 soft plit（类似于卷积 filter 那样滑动），得到一个 spatial size 小的输出 map $(h _ o, w _ o, c)$，每个 location 是一个 patch $(k, k, c)$，unfold 为 $k k c$，那么一共得到 $h _ o w _ o$ 个 tokens，每个 token 维度 $kkc$，即，数据 $T: (B, \ h _ o w _ o , \ kkc)$
        - 重建：上述 tokens $T$ 经 transformer `MLP(MSA(T))`，提取注意力特征，然后 reshape 为 $(B, h _ o, w _ o, c)$（注意维度为 $c$，因为 transformer 之前使用 QKV 变换矩阵将维度从 $kkc$ 转为 $c$） 。
    - T2T-ViT backbone：上个模块最后提取得到的 tokens（soft split 输出），append 一个 cls token，然后加上 PE，送入 transformer（基于 densenet 设计，即前面 transformer layer 连接到后面 transformer layer）

## 3.2 目标检测

### 3.2.1 one-stage

**# YOLO**

1. v1：下采样，得到 7x7 的 grid。 cell 负责预测落入此处的目标。

    - 每个 cell 预测 `B=3` 个 box 和 `C` 个分类，每个 box 有 5 个数据（x,y,w,h,IOU），一共 `5B+C` 个数据
    - 为何 `B>1`，因为不同目标具有不同的 aspect ratio。假设多个目标落入同一个 cell，那也只考虑与目标 IOU 最大的预测框，其他目标被忽略
    - 损失：
        - 正例，中心坐标差的平方和宽高平方根的差平方，取平方根是降低尺度的影响
        - 正例和负例 IOU 的差平方。当前 cell 有 gt box 则为 `1`，否则为 `0`
        - 正例的分类损失，预测为 `C` 长度得分向量，gt class 为 one-hot 向量，计算差平方

2. v2：对 v1 的改进：增加 BN；使用 anchor box。使用聚类计算 `B=5` 个 anchor box 的 size 。
    - 网络输出 `N, B, 4+1+C, S, S`，每个 cell 有 B 个 box，每个 box 单独预测分类
    - 预测目标中心与 cell 左上角的距离（归一化），预测 box 与 anchor box 的 H W 相除取对数
    - 损失：
        - 正例的分类损失，预测分类得分向量与 gt one=hot 向量的差的平方
        - 正例和负例置信度损失，IOU 预测值与真实值差的平方。负例 gt IOU 为 0，正例 gt IOU 为 1
        - 正例坐标损失：预测值与 gt box 坐标差平方；负例坐标损失：预测值与 anchor box 坐标差平方（anchor box label：`(0.5, 0.5, 0, 0)`）
3. v3：基于 v2 做修改：输出多个 scale 的特征。增加 residual 连接

    - 每个 scale map 上的每个 cell 使用 3 个 anchor box，每个 cell 输出 `3x4x(4+1+C)` 的数据。

4. v5：

**# SSD**

multi scale 的输出特征，每个 scale 独立用于预测

**# FPN**

multi scale 的输出特征，每个 scale 均用于预测。自顶向下通过 upsample 与自底向上的特征在同一 level 进行融合。

**# DSSD**

在 SSD 的基础上增加 deconvolution layer，类似于 FPN，使用 deconv 替换 upsample 。形成一个 encoder-decoder 结构。

**# RetinaNet**

基于 FPN，使用了 focal loss

**# PANet** 基于 FPN 修改，双向信息流融合（类似于 BiRNN）

### 3.2.2 two-stage

**# Faster RCNN**

结构：backbone + RPN + RCNN

RPN：backbone 的输出 map 经过一个 conv 映射，然后使用两个 FC，得到 `2k` 二分类预测得分，和 `4k` 个坐标。 

- 正例：IOU > 0.7；负例：IOU < 0.3 。正例负例均选择为 128 个 。
- 损失：分类损失-交叉熵；回归损失：smooth L1

    分类 target 为 $\lbrace 0, 1 \rbrace$ 。坐标 target 如下（gt box 与 anchor box 计算得到），

    - $$t _ {x|y} = \frac {x _ p|y _ p - x _ a|y _ a} {w _ a|h _ a}, \ t _ {w|h}=\log \frac {w _ p|h _ p}{w _ a| y _ a}$$

RCNN:

- 选择单张 image，或者两张 images，从中选择正负样本。
- 根据 RPN 预测为正例的得分，取 top-6000，然后 NMS 后取 top-300 预测 boxes
- 上一步的 top-300 预测 boxes 和 gt boxes 一起作为 proposals，然后再与 gt boxes 计算 IOU 矩阵，有最大 IOU 或者 IOU > 0.5 的 proposals 为正例。与所有 gt boxes 最大 IOU 在 `[0.1,0.5]` 之间的为负例。然后按 1:3 随机选择正负例，batch size 为 128 。
- 上述选择的正负例 proposals 经过 ROIpool 得到 `7x7` 的 feature maps，然后见过 MLP，以及两个 heads，输出分类预测 `(B=128, C+1)` 和坐标预测 `(B=128, 4x(1+C))` 。
- 损失：

    - 正负例的交叉熵损失（=负对数似然）。负样本分类 target 为 `0`，正样本分类 target 范围为 `1 ~ C`。每个 proposal，仅计算 target 对应的分类的对数似然 $-\log p _ i$，$i$ 为 proposal target 。
    - 正例的 smooth L1 损失。坐标 target 通过 gt box 与 proposal 计算得到。

inference：

RPN 生成的 proposals，根据预测得分选择 top-12000，然后 NMS 选出 2000 proposals，然后经过 RCNN，筛选最终的预测得分 > 阈值的 proposals 作为预测结果，其 box 则根据 RCNN 输出坐标分支结果对 proposals 进行 fine-tuning 。


### 3.2.3 anchor free

**# FCOS**


使用 FPN 作为 backbone，multi scale feature maps。每个 feature map 上的每个 location 预测 `C` 个分类（每个分类独立预测，逻辑回归二分类）和 4 个坐标，以及一个 center 得分。

正例：位于目标内部的 location
负例：不在任何目标内部的 location

解决 location 同属于多个目标的问题：

- 不同 scale 的 feature map 上 locations 负责预测不同 size 的目标。
- 如果 location 属于 size 差不多的多个目标，那么使用 area 最小的目标作为 gt box。

记 location 坐标 `(x,y)`，那么 location 距目标上下左右的长度，即坐标 target 为

$$\begin{aligned}l^{*}=x-x_0^{(i)}, \quad t^{*}=y-y_0^{(i)} \\\\
r^{*}=x_1^{(i)} - x, \quad b^{*}=y_1^{(i)} - y \end{aligned}$$

center 得分表示 location 是目标中心的置信度。target 值为

$$\text{centerness} ^ {\star}=\sqrt{\frac {\min(l ^ {\star}, r ^ \star)} {\max(l ^ {\star},r ^ {\star})} \times \frac {\min(t ^ {\star}, b ^ {\star})} {\max(t ^ {\star},b ^ {\star})}}$$

损失：

- 分类损失：focal loss
- 坐标损失：正例的 IOU loss
- 中心置信度损失：BCE loss

inference：置信度得分与分类得分相乘，然后使用 NMS 。

### 3.2.4 transformer

**# DETR**

1. 非自回归并行解码
2. 大目标检测效果更小，因为 global attention
3. set prediction，基于匈牙利匹配算法，一个 gt box 最多只有一个预测 box 与之匹配，不需要 NMS 。

输入 batch：经过填充，是个每个 image size 相同，然后记录一个 mask

backbone：ResNet，输出特征 $(B, C, H, W)$，下采样率 `32`，mask 也需要缩小 `32`。

一个 conv，将特征维度 $C$ 适配到 transformer 模型维度 $d=256$ ，记特征为 `f` 。

`(H, W)` size 的 PE ，可使用正余弦函数或 learnable embedding matrix。

encoder：

- `f` 与 `PE` 相加 作为 q 和 k，`f` 作为 v，然后送入 MSA + MLP（这里计算 attention 用到 mask），循环 M 次，得到 encoder 的输出特征，记为 memory 。

decoder： M 个 transformer block，每个 block 包含 2 个 transformer layer 。

- object queries $(B, N=100, d)$ （B 为 batch size）充当 PE 。

- tgt $(B, N, d)$ 初始化为全 0 tensor（类似于自回归任务中 $t _ 0$ 时刻的输入值）

- tgt 与 object queries 相加作为 q 和 k，然后 tgt 作为 v，送入 MSA + MLP （第一个 transformer layer），输出为 tgt。

- 第二个 transformer layer：q 为 tgt + object queries， k 为 memory + PE（与 encoder 的相同）， v 为 memory。 送入 MSA+MLP （这里用到 memory mask），输出依然记为 tgt 。

- 输出：

    - decoder 的输出 shape 为 $(M,B,N,d)$，分别经过：1. cls head （一个 FC），输出 `C+1` 维度预测得分；2. box head （MLP），输出 `4` 维度坐标（归一化的 cx,cy,w,h）。

- 匹配损失（Hungarian Loss）：根据匹配损失最小，为 gt boxes 选择对应的预测 boxes 。匹配损失包含
    
    - 分类损失：负预测概率。概率为对应 gt box 的那个分类概率，如果预测 box 未能匹配 gt box，那么分类为 bg 的预测概率。
    - 坐标损失：只有正例的 GIOU + L1

- 目标损失：根据匹配损失最小原则，得到 gt boxes 与匹配的预测 boxes

    - 分类损失：加权平均负预测概率。正例使用 gt box 的分类对应的预测概率，权重为 `1`；负例使用 bg 对应的预测概率，权重为 `0.1` 。
    - 坐标损失：正例的 GIOU + L1

inference：

根据 decoder 的两个输出：分类预测概率 `(B, N, 1+C)` 和坐标预测 `(B, N, 4)`

1. 根据分类预测概率求 argmax(dim=-1)，得到预测为 fg 的 boxes
2. 上一步预测为 fg 的 boxes，提取对应的坐标

