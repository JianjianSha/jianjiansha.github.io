---
title: TODO
date: 2021-02-02 11:51:13
tags:
---

牛顿法/拟牛顿法

https://blog.csdn.net/songbinxu/article/details/79677948

https://blog.csdn.net/itplus/article/details/21896453

https://blog.csdn.net/itplus/article/details/21896619

https://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html

https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/100680790

https://blog.csdn.net/g11d111/article/details/118026427

https://towardsdatascience.com/mcmc-a-visual-introduction-38e1d6131e86

http://accu.cc/

https://nbviewer.org/github/ctgk/PRML/tree/main/

可解释人工智能导论（全彩）(博文视点出品)
张钹院士作序，沈向洋、周志华倾情推荐。杨强、范力欣、朱军、陈一昕、张拳石、朱松纯、陶大程、崔鹏、周少华、刘琦、黄萱菁、张永锋等顶级专家扛鼎之作

第1章可解释人工智能概述

1.1为什么人工智能需要可解释性

1.2可解释人工智能

1.2.1目的、定义及范式

1.2.2层次、分类及应用场景

1.2.3解释的范畴

1.2.4解释的评价与度量

1.3可解释AI的历史及发展现状

1.3.1可解释AI历史回顾

1.3.2可解释AI发展现状

1.4本书结构及阅读建议



第2章贝叶斯方法

2.1贝叶斯网络

2.1.1贝叶斯网络的表示

2.1.2贝叶斯网络的推断

2.1.3贝叶斯网络的学习

2.1.4贝叶斯规划学习

2.2贝叶斯深度学习

2.2.1深度生成模型

2.2.2贝叶斯神经网络

2.3从贝叶斯网络到可解释的因果模型

2.4延伸阅读

2.5小结



第3章基于因果启发的稳定学习和反事实推理

3.1将因果引入机器学习的增益

3.1.1制约人工智能技术的可解释性和稳定性问题

3.1.2关联性和因果性

3.2挖掘数据中的因果关联

3.2.1因果推理框架和因果效应定义

3.2.2潜在结果框架下的因果效应评估

3.3稳定学习

3.3.1二值特征下的稳定学习

3.3.2连续特征下的稳定学习

3.3.3从统计学习角度的解释

3.3.4区分性变量去关联的稳定学习

3.3.5与深度神经网络相结合的稳定学习

3.4反事实推理

3.4.1二值类型干预的反事实推理

3.4.2多维类型干预下的反事实推理

3.4.3存在未观测混淆变量的反事实推理

3.5小结



第4章基于与或图模型的人机协作解释

4.1与或图模型

4.2基于与或图的多路径认知过程

4.3人机协作对齐人类认知结构和与或图模型

4.3.1通过交互式问答构建与人类认知系统对齐的与或图模型

4.3.2评价模型的可解读性：“气泡游戏”实验

4.3.3模型通过主动建模用户认知提升可解读性

4.4小结



第5章对深度神经网络的解释

5.1神经网络特征可视化

5.1.1最大激活响应可视化

5.1.2网络解剖与特征语义分析

5.1.3基于反向传播的输入重建可视化

5.1.4CAM/Grad-CAM

5.2输入单元重要性归因

5.2.1SHAP算法

5.2.2导向反向传播算法

5.2.3逐层相关性传播算法

5.2.4积分梯度算法

5.2.5LIME

5.3博弈交互解释性理论

5.3.1理论基础：沙普利值

5.3.2博弈交互的定义

5.3.3博弈交互的性质

5.3.4博弈交互与语义表达

5.3.5解释随机失活操作

5.3.6解释批规范化操作

5.3.7解释对抗迁移性和对抗鲁棒性

5.4对神经网络特征质量解构、解释和可视化

5.4.1解释表征一致性

5.4.2解释复杂度

5.5对表达结构的解释

5.5.1代理模型解释

5.5.2对自然语言网络中语言结构的提取和解释

5.6可解释的神经网络

5.6.1胶囊网络

5.6.2β-变分自编码器

5.6.3可解释的卷积神经网络

5.6.4可解释的组成卷积神经网络

5.7小结



第6章生物医疗应用中的可解释人工智能

6.1基因编辑系统优化设计中的可解释人工智能

6.1.1基因编辑系统背景介绍

6.1.2基因编辑系统优化设计可解释AI模型构建

6.2医学影像中的可解释性

6.2.1概述

6.2.2可解释性胸片诊断

6.2.3具有自适应性的通用模型学习

6.3小结



第7章金融应用中的可解释人工智能

7.1简介

7.1.1金融行业背景介绍

7.1.2金融市场介绍

7.1.3可解释AI面向各金融行业对象的必要性

7.1.4金融监管对于可解释性的要求

7.2金融可解释AI的案例

7.2.1事后可解释模型解释人工智能量化模型

7.2.2高风险客户信用违约预测

7.2.3对金融人工智能模型可解释性的监管

7.3金融可解释AI的发展方向

7.3.1安全性

7.3.2平衡性

7.3.3完整性

7.3.4交互性

7.3.5时效性

7.3.6深化推广应用

7.4延伸阅读

7.5小结



第8章计算机视觉应用中的可解释人工智能

8.1背景

8.1.1机器视觉与可解释性

8.1.2可解释性与机器视觉发展

8.2视觉关系抽取

8.2.1基本概念

8.2.2视觉关系检测中可解释性的重要性

8.2.3可解释视觉关系抽取

8.3视觉推理

8.3.1基本概念

8.3.2可解释视觉推理示例

8.4视觉鲁棒性

8.4.1动态与静态可解释性分析

8.4.2数字世界与物理世界模型安全可解释性

8.5视觉问答

8.5.1基本概念

8.5.2视觉问答中可解释性的重要性

8.5.3可解释性视觉问答示例

8.6知识发现

8.6.1基本概念

8.6.2视觉可解释性与知识发现的关系

8.6.3可解释性知识发现案例

8.7小结



第9章自然语言处理中的可解释人工智能

9.1简介.243

9.2可解释自然语言处理中的模型结构分析

9.2.1为什么模型结构分析很重要

9.2.2设置探针任务窥探模型结构的功能

9.2.3错误类型分析

9.2.4可解释评估

9.3可解释自然语言处理中的模型行为分析

9.3.1为什么模型行为分析很重要

9.3.2预测行为分析

9.4自然语言处理任务中的可解释性

9.4.1对话系统

9.4.2智能问答系统

9.4.3情感分析系统

9.4.4自动文摘系统

9.5延伸阅读

9.5.1鲁棒性分析

9.5.2泛化性分析

9.6小结



第10章推荐系统中的可解释人工智能

10.1简介

10.2初探可解释推荐

10.3可解释推荐的历史与背景

10.4推荐系统基础

10.4.1推荐系统的输入

10.4.2推荐系统的输出

10.4.3推荐系统的三大核心问题

10.5基本的推荐模型

10.5.1协同过滤

10.5.2协同推理

10.6可解释的推荐模型

10.7可解释推荐的应用

10.7.1电子商务

10.7.2社交网站

10.7.3基于位置的服务

10.7.4多媒体系统

10.7.5其他应用

10.8延伸阅读：其他可解释推荐模型

10.8.1基于图和知识图谱的可解释推荐模型

10.8.2深度学习推荐系统的可解释性

10.8.3基于自然语言生成的解释

10.8.4基于因果和反事实推理的解释

10.9小结



第11章结论



附录A传统机器学习中的可解释模型

A.1线性回归

A.2逻辑回归

A.3决策树



附录B可解释人工智能相关研究资源

B.1图书

B.2综述论文

B.3Workshop及论文集

B.4Tutorial

B.5代码


将20×20点矩阵分割成1×1或2×2的矩阵，有多少种分法？
相似的一维问题是上台阶问题，20级台阶，每次走一级或两级，有多少种最短走法。

先考虑一维情况。记 i 级台阶有 N(i) 种走法（数量使用大写 N 表示），考虑
i=1，1 个台阶，显然只有 1 种走法， N(1)=1
i=2，2 个台阶，显然有 2 种走法，即 1,1 或者 2 两种，故 N(2)=2
i=3，3 个台阶，N(3)=N(2)+N(1)=3，对应以下两种情况：
第一步固定走 1，那么剩下 2 个台阶，这种情况有 N(2) 种走法
第一步固定走 2，那么剩下 1 个台阶，这种情况有 N(1) 种走法 
i=4，4 个台阶，类似地可知，N(4)=N(3)+N(2)=5
所以，N(i)=N(i-1)+N(i-2)
二维情况，记矩阵 size 为 m x n，可以有 N(m,n) 种分法（这里大写 N 有两个参数）那么
m=1，只有 1 种情况，即全部都是 1x1 的子矩阵，故 N(1,n)=1
m=2，有 N(n) 种情况，可看作是一维情况，即 n 个台阶的走法，故 N(2,n)=N(n)
m=3，考虑以下几种情况：
第一行全部都是1x1，矩阵剩余部分为2xn，故有 N(2,n)=N(n)  种分法
第一列全部都是1x1，矩阵剩余部分为mx(n-1)，故有 N(m,n-1) 种分法
以上两种情况，均包含了第一行和第一列全部都是1x1的情况，即，需要减去N(m-1,n-1)