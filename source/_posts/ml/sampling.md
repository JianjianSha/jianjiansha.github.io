---
title: 随机采样
date: 2022-06-10 14:03:44
tags: machine learning
---

# 1. MC 数值积分

要求积分 

$$\int_a^b f(x) dx \tag{1}$$

如果 $f(x)$ 的形式非常复杂难以求积分，那么考虑数值法求解。采用蒙特卡洛积分

$$\int_a^b \frac {f(x)} {q(x)} q(x) dx \tag{2}$$


其中 $q(x)$ 是区间内随机变量 $x$ 的某个概率分布，此时上式就是期望 

$$\mathbb E_{q(x)} \left[\frac {f(x)} {q(x)}\right] \tag{3}$$

观察 (2) 式，$q(x)$ 是概率密度，$q(x)dx$ 可看作是取值为 $x$ 的概率，于是蒙特卡洛采样求积分为

$$\frac 1 n \sum_{i=1}^n \frac {f(x^{(i)})} {q(x^{(i)})} \tag{4}$$

# 2. Box-Muller 变换

如果随机变量 $X_1, \ X_2$ 相互独立，且均为均匀分布 $U(0,1)$，那么

$$Z_1=\sqrt {-2 \log X_1} \cos (2\pi X_2)
\\Z_2=\sqrt {-2 \log X_1} \sin (2\pi X_2) \tag{5}$$

$Z_1, \ Z_2$ 独立且服从标准正态分布 $\mathcal N(0, 1)$。

# 3. 接受-拒绝采样

如果某分布 $p(x)$ 很难采样，则需要借助其他手段完成采样。既然 $p(x)$ 太复杂无法直接采样，那么寻找一个可采样的分布 $q(x)$，例如高斯分布，以及一个常量 $k$，使得 $p(x)$ 总是在 $kq(x)$ 下方，如图 1，

![](/images/ml/sampling_1.png)

具体操作：

1. 从 $q(x)$ 上分布采样得到采样值 $a$
2. 从均匀分布 $U(0, kq(a))$ 上采样得到 $u$
3. 如果 $u > p(a)$ 那么拒绝采样，即不使用这次的采样值；否则接受这次采样值 $a$ 。

# 4. Markov Chain

马尔可夫链假设当前状态只与前一个状态有关。状态转移矩阵为 $P$，那么无论初始状态是什么，经过状态转移后，最终都会到达一个稳定状态。

记初始状态的概率向量为 $\pi_0=[\pi_0(1), \pi_0(1), \ldots]$，状态转移矩阵为 $P$，$P_{ij}$ 表示从状态 `i` 转移到 `j` 的概率，于是 

$$\pi_k=\pi_{k-1} P=\pi_0 P^k \tag{6}$$

当 $k=n$ 时，达到稳态，即 $\pi_{n+1} = \pi_n P = \pi_n = \pi_0 P^n$ ，不妨记稳态为 $\pi$，那么

$$\pi(j)=\pi_0 P_{:,j}^n \tag{7}$$

(7) 式对任意的 $\pi_0$ 均成立，那么必然有 $P_{i,j}^n=\pi(j), \ \forall i=1,2,\ldots$，此时 (7) 式为 $\pi(j)=\pi(j) \sum_i\pi_0(i)= \pi(j)$。于是将 $P^n$ 写成如下形式

$$P^n=\begin{bmatrix} \pi(1) & \pi(2) & \cdots \\ \vdots & \vdots & \vdots \\ \pi(1) & \pi(2) & \cdots \end{bmatrix} \tag{8}$$

对于概率分布 $p(x)$，要依其进行采样。如果能构造一个状态转移矩阵 $P$，使得最终的平稳分布恰好为 $p(x)$，那么从任何一个初始状态 $x_0$，沿着马氏链转移，得到转移序列 

$x_0, x_1, \ldots, x_n, x_{n+1}, \ldots$

在第 $n$ 步已经收敛，那么 $x_n, x_{n+1}, \ldots$ 就是依概率 $p(x)$ 的采样样本。

现在问题是，根据稳态分布 $p(x)$，如何构造符合要求的状态转移矩阵 $P$？

**细致平稳条件**

如果非周期马氏链的转移矩阵 $P$ 和分布 $\pi$ 满足

$$\pi(i) P_{ij}=\pi(j) P_{ji}, \quad \forall i, j \tag{9}$$

那么 $\pi$ 是马氏链的稳态分布。

**证：**

$$\sum_i \pi(i) P_{ij} = \sum_i \pi(j) P_{ji}=\pi(j) \sum_i P_{ji}=\pi(j)$$

上式最后一个等号成立是因为 $\sum_i P_{ji}=1$，表示从状态 $j$ 必然能转移到某个状态 $i$。根据上式推导可知

$$\pi P = \pi$$

故 $\pi$ 是稳态分布。

我们随机初始化一个马氏链的状态转移矩阵 $Q$，显然通常情况下

$$p(i) Q_{ij} \ne p(j) Q_{ji}$$

即细致平稳条件不满足。现在引入一个 $\alpha_{ij}$，使得

$$p(i)Q_{ij} \alpha_{ij} = p(j)Q_{ji} \alpha_{ji} \tag{10}$$

例如 

$$\alpha_{ij}=p(j) Q_{ji}, \quad \alpha_{ji}=p(i) Q_{ij} \tag{11}$$ 

是一个满足上式的选择，于是

$$Q'=Q \circ \alpha \tag{12}$$

其中 $\circ$ 表示 Madamard 乘积，即按元素相乘。

转移矩阵 $Q'$ 的稳态分布就是 $p(x)$ 。

$\alpha_{ij}$ 可以看作是从状态 $i$ 以概率 $Q_{ij}$ 转移到状态 $j$ 的接受率。


**MCMC 采样算法**

---
1. 初始化马氏链的转移矩阵 $Q$，这个矩阵满足每行之和为 $\sum_j Q_{ij}=1$ 。
1. 初始化马氏链的初始状态 $X_0=x_0$ （状态值，下标 $0$ 表示初始时刻）
2. 对时刻 $t=0,1,2,\ldots$

    - 根据 $t$ 时刻状态进行采样 $y \sim Q(x|x_t)$
    - 从均匀分布采样 $u \sim U(0,1)$
    - 如果 $u < \alpha(x_t,y)=p(y) Q(x_t|y)$，那么接受转移，即 $x_{t+1}=y$
    - 否则不接受转移，即 $x_{t+1}=x_t$
---

**连续型分布**

对于连续型分布，上述 MCMC 采样算法依然有效，此时 $Q(x|y)$ 表示连续二元概率分布对应的条件分布。

如果 $\alpha_{ij}$ 太小，会导致大量的转移被拒绝，使得收敛速度太慢。根据 (10) 式，易知如果 $\alpha_{ij}, \ \alpha_{ji}$ 同乘以一个比例因子进行扩大，(10) 式保持不变，即细致平稳条件依然满足。但是由于我们这里的接受拒绝所用的均值分布是固定在  $[0,1]$ 区间的，所以 $\alpha$ 也不能随意放大，令扩大后的 $\alpha_{ij}, \ \alpha_{ji}$ 均不超过 $1$ 。

由于是同比例扩大，故接受率比例保持不变，

$$\frac {\alpha_{ij}}{\alpha_{ji}}= \frac {p(j) Q_{ji}}{p(i) Q_{ij}}$$

如果 $\alpha_{ij} \ge \alpha_{ji}$，那么扩大后 $\alpha_{ij}=1$，此时 $1/\alpha_{ji}=\frac {p(j) Q_{ji}}{p(i) Q_{ij}} > 1$

如果 $\alpha_{ij} \le \alpha_{ji}$，那么扩大后 $\alpha_{ji}=1$，此时 $\alpha_{ij}=\frac {p(j) Q_{ji}}{p(i) Q_{ij}} < 1$

所以，

$$\alpha_{ij}=\min \left(\frac {p(j) Q_{ji}}{p(i) Q_{ij}}, 1\right) \tag{13}$$

这就得到下面的算法，

**Metropolis-Hastings 采样算法**

---
1. 初始化马氏链的转移矩阵 $Q$，这个矩阵满足每行之和为 $\sum_j Q_{ij}=1$ 。
1. 初始化马氏链的初始状态 $X_0=x_0$ （状态值，下标 $0$ 表示初始时刻）
2. 对时刻 $t=0,1,2,\ldots$

    - 根据 $t$ 时刻状态进行采样 $y \sim Q(x|x_t)$
    - 从均匀分布采样 $u \sim U(0,1)$
    - 如果 $u < \alpha(x_t,y)=\min \left(\frac {p(y) Q(y,x_t)}{p(x_t) Q(x_t,y)}, 1\right)$，那么接受转移，即 $x_{t+1}=y$
    - 否则不接受转移，即 $x_{t+1}=x_t$
---

其中 $Q_{ij}=Q(i,j)=Q(j|i)=Q(i \rightarrow j)$ 表示同一个意思。

# 5. Gibbs 采样

对于多维的情况，由于 $\alpha < 1$，上述 Metropolis-Hastings 算法效率还不够高。并且，由于特征维度大，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布。

我们寻找一个合适的状态转移矩阵 $Q$ 使得接受率 $\alpha=1$，避开拒绝转移从而提高效率。

考虑二维情况，概率分布为 $p(x,y)$。考察两个点 $A(x_1,y_1)$ 和 $B(x_1, y_2)$，发现

$$p(x_1,y_1)p(y_2|x_1)=p(x_1) p(y_1|x_1) p(y_2|x_1)
\\p(x_1,y_2)p(y_1|x_1)=p(x_1)p(y_2|x_1)p(y_1|x_1)$$

所以

$$p(x_1,y_1)p(y_2|x_1)=p(x_1,y_2)p(y_1|x_1) \tag{14}$$

即 $p(A)p(y_2|x_1) = p(B) p(y_1|x_1)$ 。

同样地，对于 $A(x_1,y_1)$ 和 $C(x_2,y_1)$ 两点，有 $p(A)p(x_2|y_1) = p(C) p(x_1|y_1)$ 。

![](/images/ml/sampling_2.png)

<center>图2 . 平面上马氏链转移矩阵的构造</center>

于是构造平面上任意两点之间的转移概率矩阵 $Q$，

$$\begin{aligned} Q(A \rightarrow B)& =p(y_B|x_1) & x_A=x_B=x_1
\\Q(A \rightarrow C)&=p(x_C|y_1) & y_A=y_C=y_1
\\Q(A \rightarrow D)&=0 & \text{otherwise}
\end{aligned}$$

**两个点如果同在 x 轴或者同在 y 轴上有连接，此时可以进行状态转移；否则两点之间无连接，即不可进行状态转移。**

这是我们人为构造的规则，根据这个规则得到的转移矩阵 $Q$，满足细致平稳条件：

$$p(X)Q(X \rightarrow Y)=p(Y) Q(Y \rightarrow X)$$

根据 (9) 式，这个二维空间的马氏链将收敛到平稳分布 $p(x,y)$，这就是 Gibbs Sampling 算法。

**二维 Gibbs Sampling 算法**

---
1. 要求依 $p(x,y)$ 采样值。目标分布 $p(x,y)$ 可能不易求得，但其条件分布 $p(y|x), \ p(x|y）$ 容易得到 。
2. 随机初始化 $X_0=x_0, \ Y_0=y_0$
3. 对 $t=0,1,2,\ldots$ ，循环采样

    - $y_{t+1} \sim p(y|x_t)$
    - $x_{t+1} \sim p(x|y_{t+1})$
---

以上采样过程，轮换着沿 x 轴和 y 轴做马氏链转移，得到样本 $(x_0, y_0), (x_0, y_1), (x_1, y_1), (x_1, y_2), (x_2,y_2), \ldots$ 

当然，初始样本值并非依概率 $p(x,y)$ 进行采样而得，所以在采样一段时间后，马氏链收敛。上述算法中，x 轴和 y 轴轮流采样，实际上这不是必须的，也可以随机选择一个坐标轴进行采样。


## 5.1 多维 Gibbs 采样

将坐标 $x_1$ 扩展到多维情况 $\mathbf x_1$，容易知道细致平稳条件仍然成立，

$$p(\mathbf x_1,y_1)p(y_2|\mathbf x_1)=p(\mathbf x_1,y_2) p(y_1|\mathbf x_1)$$

一般地，对于 $n$ 维空间中概率分布 $p(x_1,x_2,\ldots, x_n)$，可以定义如下转移矩阵：

1. 如果当前状态 $(x_1,x_2,\ldots, x_n)$，可沿着 $x_i, \ i=1,2,\ldots,n$ 坐标轴进行转移，转移概率由条件概率 $p(x_i|x_1,\ldots , x_{i-1},x_{i+1}, \ldots, x_n)$ 定义
2. 其他无法沿着单根坐标轴进行的转移，其转移概率均为 $0$ 。

**n 维 Gibbs Sampling 算法**

---
1. 随机初始化 $\{x_i:i=1,2,\ldots, n\}$
2. 对 $t=0, 1,2,\cdots$ ， 循环采样

    - $x_1^{(t+1)} \sim p(x_1|x_2^{(t)}, x_3^{(t)}, \cdots, x_n^{(t)})$
    - $x_2^{(t+1)} \sim p(x_2|x_1^{(t+1)}, x_3^{(t)}, \cdots, x_n^{(t)})$
    - $\cdots$
    - $x_j^{(t+1)} \sim p(x_j|x_1^{(t+1)}, \cdots, x_{j-1}^{(t+1)}, x_{j+1}^{(t)} \cdots, x_n^{(t)})$
    - $\cdots$
    - $x_n^{(t+1)} \sim p(x_n|x_1^{(t+1)}, \cdots, x_{n-1}^{(t+1)})$
---

以上算法收敛后（采样 $n$ 次后收敛），得到的就是概率分布p(x1,x2,⋯,xn)的样本，当然这些样本并不独立，但是我们此处要求的是采样得到的样本符合给定的概率分布，并不要求独立。同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵 Q 中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻t，在一根固定的坐标轴上转移的概率是1。

参考：

https://www.cnblogs.com/xbinworld/p/4266146.html