{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/images/BBox-reg_fig3.png","path":"images/BBox-reg_fig3.png","modified":0,"renderable":0},{"_id":"source/images/BBox-reg_fig4.png","path":"images/BBox-reg_fig4.png","modified":0,"renderable":0},{"_id":"source/images/DP1_fig1.png","path":"images/DP1_fig1.png","modified":0,"renderable":0},{"_id":"source/images/CGAN_fig1.png","path":"images/CGAN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/DP2_fig1.png","path":"images/DP2_fig1.png","modified":0,"renderable":0},{"_id":"source/images/DetNet_fig2.png","path":"images/DetNet_fig2.png","modified":0,"renderable":0},{"_id":"source/images/DetNet_fig1.png","path":"images/DetNet_fig1.png","modified":0,"renderable":0},{"_id":"source/images/FSAF_fig2.png","path":"images/FSAF_fig2.png","modified":0,"renderable":0},{"_id":"source/images/FSAF_fig3.png","path":"images/FSAF_fig3.png","modified":0,"renderable":0},{"_id":"source/images/FSAF_fig4.png","path":"images/FSAF_fig4.png","modified":0,"renderable":0},{"_id":"source/images/FSAF_fig5.png","path":"images/FSAF_fig5.png","modified":0,"renderable":0},{"_id":"source/images/FSAF_fig6.png","path":"images/FSAF_fig6.png","modified":0,"renderable":0},{"_id":"source/images/GA-RPN_fig3.png","path":"images/GA-RPN_fig3.png","modified":0,"renderable":0},{"_id":"source/images/GAN_fig1.png","path":"images/GAN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/GIoU_fig2.png","path":"images/GIoU_fig2.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN_fig1.png","path":"images/Grid-RCNN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/ImprovedGAN_fig1.png","path":"images/ImprovedGAN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/M2Det_fig3.png","path":"images/M2Det_fig3.png","modified":0,"renderable":0},{"_id":"source/images/TridentNet_fig1(b).png","path":"images/TridentNet_fig1(b).png","modified":0,"renderable":0},{"_id":"source/images/TridentNet_fig1(c).png","path":"images/TridentNet_fig1(c).png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_fig3.png","path":"images/libra-rcnn_fig3.png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_figa.png","path":"images/libra-rcnn_figa.png","modified":0,"renderable":0},{"_id":"source/images/mAP_fig1.png","path":"images/mAP_fig1.png","modified":0,"renderable":0},{"_id":"source/images/mAP_fig2.png","path":"images/mAP_fig2.png","modified":0,"renderable":0},{"_id":"source/images/mAP_fig3.png","path":"images/mAP_fig3.png","modified":0,"renderable":0},{"_id":"source/images/mAP_fig4.png","path":"images/mAP_fig4.png","modified":0,"renderable":0},{"_id":"source/images/mAP_fig5.png","path":"images/mAP_fig5.png","modified":0,"renderable":0},{"_id":"source/images/mask-rcnn_fig3.png","path":"images/mask-rcnn_fig3.png","modified":0,"renderable":0},{"_id":"source/images/pytorch_mtd_aligncorners.png","path":"images/pytorch_mtd_aligncorners.png","modified":0,"renderable":0},{"_id":"source/images/pytorch_mtd_conv_t_1.png","path":"images/pytorch_mtd_conv_t_1.png","modified":0,"renderable":0},{"_id":"source/images/pytorch_mth_conv.png","path":"images/pytorch_mth_conv.png","modified":0,"renderable":0},{"_id":"source/images/BBox-reg_fig2.png","path":"images/BBox-reg_fig2.png","modified":0,"renderable":0},{"_id":"source/images/CGAN_fig2.png","path":"images/CGAN_fig2.png","modified":0,"renderable":0},{"_id":"source/images/DeRPN_fig1.png","path":"images/DeRPN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/GA-RPN_fig1.png","path":"images/GA-RPN_fig1.png","modified":0,"renderable":0},{"_id":"source/images/GA-RPN_fig2.png","path":"images/GA-RPN_fig2.png","modified":0,"renderable":0},{"_id":"source/images/GA-RPN_fig4.png","path":"images/GA-RPN_fig4.png","modified":0,"renderable":0},{"_id":"source/images/GAN_alg1.png","path":"images/GAN_alg1.png","modified":0,"renderable":0},{"_id":"source/images/GAN_fig2.png","path":"images/GAN_fig2.png","modified":0,"renderable":0},{"_id":"source/images/GIoU_fig1.png","path":"images/GIoU_fig1.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN-Plus_fig1.png","path":"images/Grid-RCNN-Plus_fig1.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN_fig3.png","path":"images/Grid-RCNN_fig3.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN_fig4.png","path":"images/Grid-RCNN_fig4.png","modified":0,"renderable":0},{"_id":"source/images/M2Det_fig1.png","path":"images/M2Det_fig1.png","modified":0,"renderable":0},{"_id":"source/images/M2Det_fig4.png","path":"images/M2Det_fig4.png","modified":0,"renderable":0},{"_id":"source/images/RepPoints_fig2.png","path":"images/RepPoints_fig2.png","modified":0,"renderable":0},{"_id":"source/images/TridentNet_fig1(a).png","path":"images/TridentNet_fig1(a).png","modified":0,"renderable":0},{"_id":"source/images/TridentNet_fig2.png","path":"images/TridentNet_fig2.png","modified":0,"renderable":0},{"_id":"source/images/TridentNet_fig3.png","path":"images/TridentNet_fig3.png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_fig1.png","path":"images/libra-rcnn_fig1.png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_fig2.png","path":"images/libra-rcnn_fig2.png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_fig5.png","path":"images/libra-rcnn_fig5.png","modified":0,"renderable":0},{"_id":"source/images/libra-rcnn_fig4.png","path":"images/libra-rcnn_fig4.png","modified":0,"renderable":0},{"_id":"source/images/mask-rcnn_fig1.png","path":"images/mask-rcnn_fig1.png","modified":0,"renderable":0},{"_id":"source/images/mask-rcnn_fig4.png","path":"images/mask-rcnn_fig4.png","modified":0,"renderable":0},{"_id":"source/images/pytorch_mtd_conv_t.png","path":"images/pytorch_mtd_conv_t.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN-Plus_fig2.png","path":"images/Grid-RCNN-Plus_fig2.png","modified":0,"renderable":0},{"_id":"source/images/RepPoints_fig1.png","path":"images/RepPoints_fig1.png","modified":0,"renderable":0},{"_id":"source/images/obj_det/YOLOv2_fig2.png","path":"images/obj_det/YOLOv2_fig2.png","modified":0,"renderable":0},{"_id":"source/images/pytorch/NAG_0.png","path":"images/pytorch/NAG_0.png","modified":0,"renderable":0},{"_id":"source/images/pytorch/momentum.png","path":"images/pytorch/momentum.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"source/images/BBox-reg_fig1.png","path":"images/BBox-reg_fig1.png","modified":0,"renderable":0},{"_id":"source/images/DeRPN_fig2.png","path":"images/DeRPN_fig2.png","modified":0,"renderable":0},{"_id":"source/images/Grid-RCNN_fig2.png","path":"images/Grid-RCNN_fig2.png","modified":0,"renderable":0},{"_id":"source/images/M2Det_fig2.png","path":"images/M2Det_fig2.png","modified":0,"renderable":0},{"_id":"source/images/img_cls/densenet_2.png","path":"images/img_cls/densenet_2.png","modified":0,"renderable":0},{"_id":"source/images/pytorch/NAG.png","path":"images/pytorch/NAG.png","modified":0,"renderable":0},{"_id":"source/images/pytorch/overfitting.png","path":"images/pytorch/overfitting.png","modified":0,"renderable":0},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"source/images/DSOD_fig1.png","path":"images/DSOD_fig1.png","modified":0,"renderable":0},{"_id":"source/images/obj_det/YOLOv1_fig1.jpg","path":"images/obj_det/YOLOv1_fig1.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"source/images/img_cls/densenet_3.png","path":"images/img_cls/densenet_3.png","modified":0,"renderable":0},{"_id":"source/images/img_cls/densenet_1.png","path":"images/img_cls/densenet_1.png","modified":0,"renderable":0},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"source/images/img_cls/resnet_4.png","path":"images/img_cls/resnet_4.png","modified":1,"renderable":0},{"_id":"source/images/img_cls/resnet_2.png","path":"images/img_cls/resnet_2.png","modified":1,"renderable":0},{"_id":"source/images/img_cls/resnet_1.png","path":"images/img_cls/resnet_1.png","modified":1,"renderable":0},{"_id":"source/images/img_cls/resnet_3.png","path":"images/img_cls/resnet_3.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/next/.all-contributorsrc","hash":"43eb0149c78e464c695f0dd758bb8c59353182b3","modified":1587361552683},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1587361552686},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1587722949386},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1587361552688},{"_id":"themes/next/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1587722949386},{"_id":"themes/next/.gitignore","hash":"83418530da80e6a78501e1d62a89c3bf5cbaec3d","modified":1587722949391},{"_id":"themes/next/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1587722949392},{"_id":"themes/next/.travis.yml","hash":"379f31a140ce41e441442add6f673bf397d863ea","modified":1587722949392},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1587361552835},{"_id":"themes/next/README.md","hash":"7d56751b580d042559b2acf904fca4b42bcb30a7","modified":1587722949393},{"_id":"themes/next/_config.yml","hash":"ad947b4a5bc64bee8aceaed79f47f0e5bfefded3","modified":1587723680740},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1587721825054},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1587361552839},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1587721825054},{"_id":"themes/next/package.json","hash":"b099e7cea4406e209130410d13de87988ba37b2a","modified":1587722949427},{"_id":"source/_posts/BBox-Reg-Uncertainty.md","hash":"7e629be404c79ac1d8b63dc6478f75aa49503a35","modified":1587724517063},{"_id":"source/_posts/CGAN.md","hash":"f10bb3f69d370244bc8ffb0d6f5f0fcc84494550","modified":1587724547246},{"_id":"source/_posts/DIP-1.md","hash":"4900be1b46bf1738dd1b559d7d7b9dc4839150a0","modified":1587724616327},{"_id":"source/_posts/DL-env.md","hash":"d376372420e0f982034bf50833a785d83093fdd8","modified":1587724647789},{"_id":"source/_posts/DIP-2.md","hash":"b613e3d6c3f72e4604b6e768c833d7a33644f1eb","modified":1587724624704},{"_id":"source/_posts/DSOD.md","hash":"216a994cfac7b88cd2edfe0863296025e8a772ee","modified":1587724653289},{"_id":"source/_posts/DeRPN.md","hash":"f1b4cefe1c47859626259b55d21d824ba90bc201","modified":1587724549189},{"_id":"source/_posts/DetNet.md","hash":"b40dfc488d450dc059f422ef20174378d9acce22","modified":1587724608330},{"_id":"source/_posts/FSAF.md","hash":"19f76265e71a60fc5c2c422884d65c43e3410791","modified":1587724658161},{"_id":"source/_posts/GA-RPN.md","hash":"0661834aa008c32e80c2e43f782b0f6eaacd3683","modified":1587724662094},{"_id":"source/_posts/GAN.md","hash":"b212be2661b6c61aafca80c68c17d25e4438bf30","modified":1587724666726},{"_id":"source/_posts/GIoU.md","hash":"3e70d0daed924dc871987f2c6ab00f101249ded3","modified":1589505889573},{"_id":"source/_posts/Grid-RCNN.md","hash":"d5b1989b6bb93edf7ee4585e1c67b62250c17dab","modified":1587724683391},{"_id":"source/_posts/ImprovedGAN.md","hash":"885776b42fa49975e90131418d8852306c1bc9ba","modified":1587724698844},{"_id":"source/_posts/M2Det.md","hash":"defe534aa5fc8e2729c43e6424b230fb469be7ed","modified":1587724591648},{"_id":"source/_posts/TridentNet.md","hash":"bb01d8181a5e4846d95911432cc4895b6eda5f39","modified":1587724572120},{"_id":"source/_posts/RepPoints.md","hash":"48a1a1d93224a738c7f7eee6660523998589407c","modified":1587724577184},{"_id":"source/_posts/WGAN.md","hash":"d087dd74799c6606f923850d7dee077ba74d3fca","modified":1587361552256},{"_id":"source/_posts/cv-mtds.md","hash":"c0b6021c9c650f83a0a23abfe92a9685244c71f0","modified":1587724535883},{"_id":"source/_posts/cpp-aux-tools.md","hash":"94eaf00502e41d4369d9b995299026afb8a7e36b","modified":1595322770733},{"_id":"source/_posts/gcc-src.md","hash":"3a0169396aec02a7ac32af85d7d990444fc96068","modified":1587724673028},{"_id":"source/_posts/libra-rcnn.md","hash":"e30150614c37fe490fa8b5f840bdd0ca1b410714","modified":1587724601422},{"_id":"source/_posts/loss.md","hash":"1a30c6f947cc8a0b711294c5ac7098b3086c61f0","modified":1587724596189},{"_id":"source/_posts/mAP.md","hash":"9e2b742de01933583676c894d34704a3c0484964","modified":1587724587200},{"_id":"source/_posts/mask-rcnn.md","hash":"e86c7e2564b2b4415cf4010822eed40f8b93534f","modified":1587724581610},{"_id":"source/about/index.md","hash":"a4051fc00d169485d93ddb50967415343e1b0075","modified":1587361552303},{"_id":"source/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1587361552312},{"_id":"source/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1587361552316},{"_id":"source/images/DP1_fig1.png","hash":"ed9dd7cbecff94be6778831a494b1295482bfe3e","modified":1587361552324},{"_id":"source/images/CGAN_fig1.png","hash":"94d2eb94087c072188200f54dfccb665a4bb0bc3","modified":1587361552318},{"_id":"source/images/DP2_fig1.png","hash":"a90b55aa55d0cfdd147c5b00feaf432300f2854b","modified":1587361552328},{"_id":"source/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1587361552345},{"_id":"source/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1587361552344},{"_id":"source/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1587361552350},{"_id":"source/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1587361552351},{"_id":"source/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1587361552361},{"_id":"source/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1587361552363},{"_id":"source/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1587361552364},{"_id":"source/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1587361552369},{"_id":"source/images/GAN_fig1.png","hash":"c6f2c668d1b4d98d4bd7fdc27851faed5c7ffe9c","modified":1587361552380},{"_id":"source/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1587361552389},{"_id":"source/images/Grid-RCNN_fig1.png","hash":"4c257e0b6f9f569b1a60ac27af54ca440812bc9c","modified":1587361552401},{"_id":"source/images/ImprovedGAN_fig1.png","hash":"9496ef5958922774f45112291e76fc421ba33ad5","modified":1587361552418},{"_id":"source/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1587361552426},{"_id":"source/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1587361552446},{"_id":"source/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1587361552450},{"_id":"source/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1587361552476},{"_id":"source/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1587361552482},{"_id":"source/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1587361552487},{"_id":"source/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1587361552488},{"_id":"source/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1587361552489},{"_id":"source/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1587361552491},{"_id":"source/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1587361552492},{"_id":"source/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1587361552498},{"_id":"source/images/pytorch_mtd_aligncorners.png","hash":"8e1fff6ffbefc3b3bfb3fabbb2aa34c035b922a4","modified":1587361552509},{"_id":"source/images/pytorch_mtd_conv_t_1.png","hash":"68104100110da86678ff50bbc0d7418946c68f10","modified":1587361552516},{"_id":"source/images/pytorch_mth_conv.png","hash":"789dc01b7cd1d19267d690b001bf45582d720085","modified":1587361552520},{"_id":"source/categories/index.md","hash":"f543c292a5425d3bdeeb4eb223c39d17b75c3296","modified":1587361552304},{"_id":"source/tags/index.md","hash":"187b2cc91b3f34577e2b97e1e10051a0eb5a0854","modified":1587361552521},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"778b7e052993ed59f21ed266ba7119ee2e5253fb","modified":1587722949387},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ddde54fb50d11dc08cec899a3588addb56aa386","modified":1587722949387},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1587721825051},{"_id":"source/_posts/Hexo-Sync.md","hash":"02083cbfb60bc19000fac37f2aba28d6d6e41d80","modified":1587724692942},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"d2f8e6b65783e31787feb05d2ccea86151f53f35","modified":1587722949389},{"_id":"themes/next/.github/auto_assign.yml","hash":"9fe0dbe3f6edc59bf10ea25b14eba0e92e2c8f42","modified":1587361552813},{"_id":"themes/next/.github/config.yml","hash":"df3d970700e6b409edc3d23be8d553db78d5ba3f","modified":1587722949389},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"e06053d417579ed967a94166deb6bda5ce41d805","modified":1587361552815},{"_id":"themes/next/.github/lock.yml","hash":"3ce3d0a26030a1cd52b273cc6a6d444d7c8d85c2","modified":1587722949390},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1587361552817},{"_id":"themes/next/.github/release-drafter.yml","hash":"09c3352b2d643acdc6839601ceb38abc38ab97c5","modified":1587722949391},{"_id":"themes/next/.github/stale.yml","hash":"590b65aca710e0fba75d3cf5361a64d13b6b0f63","modified":1587722949391},{"_id":"themes/next/.github/topissuebot.yml","hash":"5091c3bc6f3df303d16d853ce65a302601c1e875","modified":1587361552819},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1587361552819},{"_id":"themes/next/.github/weekly-digest.yml","hash":"6db3bcad65c3156de298f6a3ffd3ba887af4aa4f","modified":1587361552820},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1587361552840},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"60c7e9ef0c578deebad43e9395c958fa61096baf","modified":1587722949394},{"_id":"themes/next/docs/AUTHORS.md","hash":"cde7cc095ac31b421a573042cf61060f90d9ad0d","modified":1587722949395},{"_id":"themes/next/docs/DATA-FILES.md","hash":"980fb8d37701f7fd96b30bb911519de3bbb473d1","modified":1587722949395},{"_id":"themes/next/docs/INSTALLATION.md","hash":"07ea00bee149a1bdc9073e903ee6b411e9f2f818","modified":1587722949395},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"6cc663db5e99fd86bb993c10d446ad26ada88e58","modified":1587722949396},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1587361552849},{"_id":"themes/next/docs/MATH.md","hash":"f56946053ade0915ff7efa74d43c38b8dd9e63bb","modified":1587722949396},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"1e86d32063b490d204baa9d45d8d3cb22c24a37d","modified":1587722949397},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1587722949402},{"_id":"themes/next/languages/de.yml","hash":"15078b7ede1b084e8a6a15d271f0db9c325bd698","modified":1587722949402},{"_id":"themes/next/languages/en.yml","hash":"dbb64776f9c001c54d0058256c415a9a0724ed5d","modified":1587722949403},{"_id":"themes/next/languages/es.yml","hash":"f064c793d56a5e0f20cda93b6f0e355044efc7d8","modified":1587722949403},{"_id":"themes/next/languages/fa.yml","hash":"6c0a7d5bcc26eb45a9f3e02f13117c668e77fffd","modified":1587722949403},{"_id":"themes/next/languages/fr.yml","hash":"3e2f89d4bb4441d33ecc7b5a4ee114f627603391","modified":1587722949404},{"_id":"themes/next/languages/it.yml","hash":"46222f468e66789e9ba13095809eb5e5b63edf30","modified":1587722949405},{"_id":"themes/next/languages/id.yml","hash":"7599bb0ecf278beb8fde3d17bfc148a3241aef82","modified":1587722949404},{"_id":"themes/next/languages/ja.yml","hash":"bf279d0eb1911806d01a12f27261fbc76a3bb3f9","modified":1587722949405},{"_id":"themes/next/languages/nl.yml","hash":"9749cf90b250e631dd550a4f32ada3bb20f66dd0","modified":1587722949405},{"_id":"themes/next/languages/ko.yml","hash":"af4be6cb394abd4e2e9a728418897d2ed4cc5315","modified":1587722949405},{"_id":"themes/next/languages/pt.yml","hash":"f6606dd0b916a465c233f24bd9a70adce34dc8d6","modified":1587722949406},{"_id":"themes/next/languages/pt-BR.yml","hash":"69aa3bef5710b61dc9a0f3b3a8f52f88c4d08c00","modified":1587722949406},{"_id":"themes/next/languages/ru.yml","hash":"012abc694cf9de281a0610f95f79c594f0a16562","modified":1587722949406},{"_id":"themes/next/languages/uk.yml","hash":"69ef00b1b8225920fcefff6a6b6f2f3aad00b4ce","modified":1587722949407},{"_id":"themes/next/languages/tr.yml","hash":"46e09f2119cbfbcf93fb8dbd267dccabeb8b0cda","modified":1587722949406},{"_id":"themes/next/languages/zh-CN.yml","hash":"81d73e21402dad729053a3041390435f43136a68","modified":1587722949407},{"_id":"themes/next/languages/vi.yml","hash":"6a578cc28773bd764f4418110500478f185d6efa","modified":1587722949407},{"_id":"themes/next/languages/zh-TW.yml","hash":"cf0740648725983fb88409d6501876f8b79db41d","modified":1587722949407},{"_id":"themes/next/languages/zh-HK.yml","hash":"92ccee40c234626bf0142152949811ebe39fcef2","modified":1587722949407},{"_id":"themes/next/layout/_layout.swig","hash":"9554bd0f5c5a0438aa7b64065be5561c374d260e","modified":1587722949408},{"_id":"themes/next/layout/category.swig","hash":"c546b017a956faaa5f5643c7c8a363af7ac9d6b9","modified":1587722949426},{"_id":"themes/next/layout/archive.swig","hash":"d9bca77f6dcfef71e300a294f731bead11ce199f","modified":1587722949425},{"_id":"themes/next/layout/index.swig","hash":"8dfd96fb6f833dd5d037de800813105654e8e8e6","modified":1587722949426},{"_id":"themes/next/layout/page.swig","hash":"357d916694d4c9a0fd1140fa56d3d17e067d8b52","modified":1587722949426},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1587721825071},{"_id":"themes/next/layout/post.swig","hash":"5f0b5ba2e0a5b763be5e7e96611865e33bba24d7","modified":1587722949426},{"_id":"themes/next/layout/tag.swig","hash":"d44ff8755727f6532e86fc9fc8dc631200ffe161","modified":1587722949426},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1587721825072},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1587361553045},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1587361553315},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1587361553315},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1587361553319},{"_id":"source/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1587361552310},{"_id":"source/images/CGAN_fig2.png","hash":"d690cf996f276f489982f47ed689683d1c48de1a","modified":1587361552322},{"_id":"source/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1587361552338},{"_id":"source/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1587361552366},{"_id":"source/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1587361552368},{"_id":"source/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1587361552376},{"_id":"source/images/GAN_alg1.png","hash":"d535c3b65a4c0c8eb331c79dc5032bf754e7084a","modified":1587361552379},{"_id":"source/images/GAN_fig2.png","hash":"86a432fab3c2df0b7e84f789971e5ced612cc2aa","modified":1587361552385},{"_id":"source/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1587361552387},{"_id":"source/images/Grid-RCNN-Plus_fig1.png","hash":"7255da83a2dadae1b8832adf87b3f60e6ed211d5","modified":1587361552391},{"_id":"source/images/Grid-RCNN_fig3.png","hash":"b5e40a1ca3df03b69c06e573086ce5af7f91eec1","modified":1587361552410},{"_id":"source/images/Grid-RCNN_fig4.png","hash":"f06e5e1250a7e59b4d04fbe316592745e65b6650","modified":1587361552413},{"_id":"source/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1587361552421},{"_id":"source/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1587361552428},{"_id":"source/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1587361552437},{"_id":"source/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1587361552438},{"_id":"source/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1587361552452},{"_id":"source/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1587361552456},{"_id":"source/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1587361552470},{"_id":"source/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1587361552472},{"_id":"source/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1587361552481},{"_id":"source/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1587361552478},{"_id":"source/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1587361552497},{"_id":"source/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1587361552501},{"_id":"source/images/pytorch_mtd_conv_t.png","hash":"158a3311f03a494bb7bd1a81a5712cab7aa1c37b","modified":1587361552516},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1587361553221},{"_id":"source/_posts/dp/DP1.md","hash":"f4240b4eba164b3597345db784af0bc301ae5d39","modified":1587724265843},{"_id":"source/_posts/dp/DP2.md","hash":"1464a94ecdc9376dde946b7193db04d19a127969","modified":1587724274130},{"_id":"source/_posts/dp/DP3.md","hash":"1c572021c6c6fc05f735e640fa0aa105576b41b0","modified":1587724378204},{"_id":"source/_posts/dp/DP4.md","hash":"528ab635d06ad098842cdb57abcefb16be935c88","modified":1587724386640},{"_id":"source/_posts/img_cls/densenet.md","hash":"a6553202edceb6945135f85c018d82187f0a8527","modified":1587724395098},{"_id":"source/_posts/obj_det/YOLO.md","hash":"717457f459b3cbb289030850b437e8bd56f3069f","modified":1587724200129},{"_id":"source/_posts/pytorch/PyTorch-1.md","hash":"7642eced7556a6d93fa5330590b5af2dbceaf556","modified":1587724458685},{"_id":"source/_posts/pytorch/PyTorch-2.md","hash":"270fee7d66050bf260227f129ec131c3158e6ac0","modified":1587724468998},{"_id":"source/_posts/pytorch/PyTorch-3.md","hash":"5d3e2de7b67f4f7a9fb1f2c2e833ee78e95a43f3","modified":1587724475902},{"_id":"source/_posts/pytorch/PyTorch-4.md","hash":"007fddc258aca5bf032a67e74c121c372d9cf958","modified":1587724485043},{"_id":"source/_posts/pytorch/PyTorch-5.md","hash":"6eb3f62ee09e3798539c0a5e90847aedd13baa64","modified":1587724491985},{"_id":"source/_posts/pytorch/PyTorch-loss.md","hash":"a3f94930209372642df89281907376cbd89d5e92","modified":1587724497124},{"_id":"source/_posts/pytorch/PyTorch-mtd.md","hash":"c9f79f0b9d62ddf876b2b5d7a3df0f3b4e25671d","modified":1587724509355},{"_id":"source/_posts/pytorch/optim-1.md","hash":"e271d1277ed7b149a569704e551b5b222b461044","modified":1587724445306},{"_id":"source/_posts/pytorch/optim-2.md","hash":"f6bd0b830be2b4f1e1a6ce38badc0cf9e19636e7","modified":1587724453810},{"_id":"source/_posts/pytorch/optim_SGD.md","hash":"48612c41c9cb8297c361a446b1222e47aafe8153","modified":1587724415048},{"_id":"source/images/Grid-RCNN-Plus_fig2.png","hash":"801abc90e532b83bc123e58df681d79b9fc8ad4b","modified":1587361552399},{"_id":"source/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1587361552434},{"_id":"source/images/obj_det/YOLOv2_fig2.png","hash":"5c64920430b221ffaee62e991ae509c2410babb6","modified":1587614113384},{"_id":"source/images/pytorch/NAG_0.png","hash":"95aa0c0197c730db1bda3ffffe8895287826132e","modified":1587361552506},{"_id":"source/images/pytorch/momentum.png","hash":"0b274a844e87ac5c878a8e5b66848288d36d1aea","modified":1587361552507},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"e67146befddec3a0dc47dc80d1109070c71d5d04","modified":1587722949387},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"245917ffaa296bc2d9a85444acf639077ca25944","modified":1587361552689},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"ae22e700b7c63c60746321719a20d34022ad78d9","modified":1587361552707},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"6beeca0f45a429cd932b6e648617f548ff64c27c","modified":1587722949388},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"54e6a067ed95268eab6be2ba040a7e9b1907928e","modified":1587722949397},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"a9cfe5ac9ef727a8650b2b6584482751a26b1460","modified":1587722949398},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"cb8e39c377fc4a14aaf133b4d1338a48560e9e65","modified":1587722949398},{"_id":"themes/next/docs/ru/README.md","hash":"1e5ddb26ad6f931f8c06ce2120f257ff38b74fdf","modified":1587722949398},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"3202be9a8d31986caac640e7a4c7ce22e99917eb","modified":1587722949399},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7e6f227f2aaf30f400d4c065650a4e3d0d61b9e1","modified":1587722949399},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"611f2930c2b281b80543531b1bf33d082531456a","modified":1587722949399},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"2d868cd271d78b08775e28c5b976de8836da4455","modified":1587722949399},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"716111dd36d276f463c707dfcc9937fea2a1cf7a","modified":1587722949400},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"50ab381c27611d5bf97bb3907b5ca9998f28187d","modified":1587722949400},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"0d46f9f50cf2e4183970adce705d1041155b0d37","modified":1587722949401},{"_id":"themes/next/docs/zh-CN/README.md","hash":"8f7c0d0b766024152591d4ccfac715c8e18b37f3","modified":1587722949401},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"b3201934b966bc731eaf8a4dad4ba4bdcd300c10","modified":1587722949401},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1587361552895},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1587361552896},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1587361552897},{"_id":"themes/next/layout/_partials/comments.swig","hash":"142efb4c6b73d8f736f6784804b40d5871333172","modified":1587722949409},{"_id":"themes/next/layout/_partials/footer.swig","hash":"0e650e97d5fadc4b8a9a0fec00fe7db642dc3f76","modified":1587722949409},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"1ad13269b43b900356f3bdab7947d6a86f035a2c","modified":1587361552907},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1587722949412},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1587361552944},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"30ade8c806d7826cc50a4a3e46a9e6213fddf333","modified":1587722949408},{"_id":"themes/next/layout/_macro/post.swig","hash":"c3fd56bac90ce45a0c79ddfe68beb223ad0d72b4","modified":1587722949408},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"5bffdb1448caca7db7b1f84e1693e6657a106d50","modified":1587722949409},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1587721825063},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"c2e8f4b3a2bf991320ecc827dcdc227399ad5b51","modified":1587361552959},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"50c3ae6b50f173ae70f8c3312f7c6da1097eb9b6","modified":1587361552960},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"7b9e0f776a5be6c3f95bc7f394e1424ba02ba93b","modified":1587722949415},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"8a992b7fe42b9c1a5eb9d937b0827aed91586d94","modified":1587361552979},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"244ca2d74ee0d497c87572c6a26b43c62a952673","modified":1587722949418},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"bd9ba0bf60cc3008ee14339fa395ee6af188e879","modified":1587722949419},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1587361553001},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"87182367d7954457cb2498bbfa9445c03c2d619e","modified":1587361553001},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"12bf51c55449d0e838f93a4aae9f6d25c0a27ba2","modified":1587361553017},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"80dfc0879866e6512cb67590a3b2d8741a66f980","modified":1587361553021},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1587361553023},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1587721825068},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"4ae61c7efb16e962385bfe522a38c4d29cdcccbe","modified":1587361553023},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"5ae5adcd6f63ed98b2071e4f7e5e38c4d7d24e1b","modified":1587722949422},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1587721825069},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"269102fc5e46bd1ce75abdcce161f0570ae70e2f","modified":1587722949422},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"b44010cd577e4d063c3406772938c4b117ec7b7b","modified":1587361553028},{"_id":"themes/next/scripts/filters/exturl.js","hash":"b19c7c1021e57367b3b3bbf5678381017ed5667d","modified":1587361553039},{"_id":"themes/next/scripts/helpers/engine.js","hash":"eb6b8bbc1dce4846cd5e0fac0452dbff56d07b5d","modified":1587722949432},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"4044129368d0e2811859a9661cad8ab47118bc32","modified":1587722949433},{"_id":"themes/next/scripts/tags/button.js","hash":"bb0e8abbc0a6d5b3a1a75a23976f2ac3075aab31","modified":1587722949433},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"e2d0184bc4a557e1017395b80ff46880078d8537","modified":1587722949434},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1587721825073},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1587721825073},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"93ccd3f99d3cb42674f29183c756df63acb5d7f8","modified":1587722949434},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1587361553052},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1587722949434},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1587722949435},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1587361553059},{"_id":"themes/next/scripts/tags/tabs.js","hash":"c70a4a66fd0c28c98ccb6c5d5f398972e5574d28","modified":1587722949435},{"_id":"themes/next/scripts/tags/pdf.js","hash":"37b53661ad00a01a2ca7d2e4a5ad3a926073f8e2","modified":1587722949435},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1587361553061},{"_id":"themes/next/source/css/main.styl","hash":"815ef30987d02f3d76dbe4b5ee3a72135a152678","modified":1587722949460},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1587361553225},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1587722949461},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1587361553226},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1587361553227},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1587361553246},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1587361553248},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1587361553249},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1587361553249},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1587361553250},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1587361553250},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1587361553251},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1587361553251},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1587361553252},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1587361553253},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1587361553253},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1587721825099},{"_id":"themes/next/source/js/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1587361553255},{"_id":"themes/next/source/js/algolia-search.js","hash":"f0cee802b4d48d5e78ba88d77d4257cb8a88dd6e","modified":1587722949464},{"_id":"themes/next/source/js/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1587361553259},{"_id":"themes/next/source/js/js.cookie.js","hash":"f11e84def0352b7dd6393f1b83e55a40ab468686","modified":1587361553260},{"_id":"themes/next/source/js/motion.js","hash":"d5aa1a08cdf3c8d1d8d550fb1801274cc41e5874","modified":1587722949465},{"_id":"themes/next/source/js/next-boot.js","hash":"509c5b02446d4989a6ef3081cafeb9497cdde4e5","modified":1587722949465},{"_id":"themes/next/source/js/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1587361553263},{"_id":"themes/next/source/js/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1587361553265},{"_id":"themes/next/source/js/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1587361553266},{"_id":"themes/next/source/js/utils.js","hash":"2c5bc91559a19a5d1c9e691a46f472fe82fdcc4e","modified":1587722949466},{"_id":"source/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1587361552308},{"_id":"source/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1587361552342},{"_id":"source/images/Grid-RCNN_fig2.png","hash":"65e1e9f69e1d3e7b1b7b8b95d5cbb1859a99e897","modified":1587361552408},{"_id":"source/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1587361552424},{"_id":"source/images/img_cls/densenet_2.png","hash":"0d027c4897b024f9b9d20b21b5d3afaf1dd6e70c","modified":1587361552461},{"_id":"source/images/pytorch/NAG.png","hash":"54fcfa655d287fe22d00a6b60f216dafd628b71e","modified":1587361552502},{"_id":"source/images/pytorch/overfitting.png","hash":"65c45d4a6a92b9664ab1454535350c1bb8423083","modified":1587361552509},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1587361553181},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1587361553181},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1587361553183},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c22b58af3327236ec54d5706501aa5a20e15012e","modified":1587722949459},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1587361553220},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1587361553228},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1587361553247},{"_id":"source/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1587361552333},{"_id":"source/images/obj_det/YOLOv1_fig1.jpg","hash":"7da82e62769ef15baafce9848f5164c00fab3b1b","modified":1587442590313},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1587721825060},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"7d638e413f2548fc990c4a467dd03de6c81fc960","modified":1587722949409},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"90cce9f407e9490756ba99580e3eb09f55b05eaa","modified":1587722949410},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"91056a6c98cca63ff8cc6956e531ee3faf4b8ad9","modified":1587722949410},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"0dd316f153c492c0a03bd0273d50fa322bc81f11","modified":1587722949410},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"90d3eaba6fbe69bee465ddd67c467fd2c0239dc4","modified":1587722949411},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"bed6cc2b48cf2655036ba39c9bae73a295228a4d","modified":1587722949411},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"91c0addb33006619faa4c32e5d66874e25f1e9b3","modified":1587722949411},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"8d4e3dd0d3631ce0b21bc15c259f6ac886de631d","modified":1587722949412},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"f2eb455c8bf13533427254f0c9b4b17b2498168b","modified":1587722949413},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"bc7b047a6246df07767373644b1637d91c3a88b1","modified":1587722949413},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"f62b801c7999da67b4bdca9c5e373b9b5ed039dc","modified":1587361552947},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"fb7727e8ec63a58238a7206bf70eb273c8879993","modified":1587361552947},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"98fd1f5df044f4534e1d4ca9ab092ba5761739a9","modified":1587722949414},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1587721825062},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a6c761d5193cb6f22e9422dbbcf209e05471b0ed","modified":1587722949414},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"128f7d679bb4d53b29203d598d217f029a66dee7","modified":1587722949414},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1587721825062},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1587721825062},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"647e8677d1ccfb3f7918dd3ea2ff7078504a845d","modified":1587361552954},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1587361552899},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1587361552902},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1587721825063},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1587722949416},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1587722949417},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1587722949416},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1587722949417},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1587721825064},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1587721825064},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"84adaadd83ce447fa9da2cff19006334c9fcbff9","modified":1587722949418},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1587721825064},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1587721825064},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1587721825065},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1587721825065},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b8819bd056f8a580c5556d4415836a906ed5d7a4","modified":1587722949419},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"91c2cb900c76224c5814eeb842d1d5f517f9bf05","modified":1587722949419},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"85b60e222712ca3b2c4dc2039de2dc36b8d82940","modified":1587722949419},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1587721825065},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1587721825066},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1587721825066},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1587721825065},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"9298e6d6c4a62a0862fc0f4060ed99779d7b68cb","modified":1587722949420},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1b29b99fa921f12c25d3dc95facdf84ef7bb1b5c","modified":1587722949420},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"a42f97eda3748583bac2253c47fe5dfa54f07b8f","modified":1587722949420},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"606ad14a29320157df9b8f33738282c51bb393d9","modified":1587722949420},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1587721825066},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1587721825067},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"3d91899ca079e84d95087b882526d291e6f53918","modified":1587722949420},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"ae2707d6e47582bb470c075649ec7bad86a6d5a9","modified":1587722949421},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"59df21fcfe9d0ada8cee3188cb1075529c1c3eb8","modified":1587722949421},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"276f523e414d4aa7f350a8f2fd3df8a3d8ea9656","modified":1587722949421},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"1f34b2d3c753a3589ab6c462880bd4eb7df09914","modified":1587722949421},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"fd726aad77a57b288f07d6998ec29291c67c7cbb","modified":1587722949423},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1587721825069},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"58296a5c1883f26464c2a5ccf734c19f5fbf395a","modified":1587722949423},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1587361553180},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1587361553181},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1587721825090},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1587721825090},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"583ff1e7a2ca889f1f54eb0ca793894466823c7c","modified":1587722949459},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"5980abbbbeacd8541121f436fa414d24ad5e97c2","modified":1587722949459},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4e33774b1fe6d0a51f3a428c54c5e600e83bf154","modified":1587722949459},{"_id":"themes/next/source/js/schemes/muse.js","hash":"78c77614b9fe0d7d97aa08468c6cffbcbda96b75","modified":1587722949466},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"1a9741145938e2c754a808381350723cbebf43c5","modified":1587722949466},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1587361553271},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1587361553271},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1587361553272},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1587361553272},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1587361553273},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1587361553311},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1587361553314},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1587361553313},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ad680efdfb2f86546182bf3f59886efbcf3c1b2d","modified":1587722949460},{"_id":"source/images/img_cls/densenet_3.png","hash":"b456a005625516ce3e9acf66011d5fd4d9723e7a","modified":1587361552467},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"510a6f0ba7485dd54ce347cca890ab52c4957081","modified":1587722949436},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"0534b329d279a6f255112b3305ff92c810f31724","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1587361553066},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"d17236df3b4d6def1e4e81133ef4729c390de3ac","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1587721825075},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1587721825078},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"cfa64bd8ee2ff9f943673e339d69341e76fbf031","modified":1587361553132},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"afdd21533db18d846e1a2663b1199761b1bd2c1e","modified":1587361553135},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7a95c27762e1303bf06ee808c63f616cb192fcaf","modified":1587722949446},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5540c9259cb7895a5f10a289c7937e5470a7c134","modified":1587722949449},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1587721825088},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1587721825088},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1587722949452},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1587722949451},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"7ed4733240206d1aa729c835e69a85f8f3c73cd6","modified":1587722949452},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"e342b8f8e11a3a6aa5a029912c9778c25bf5d135","modified":1587722949455},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1587721825090},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"b9e87d32da24264bda247c1526afe140c858b0ef","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1587361553190},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"716e8b0f056bf6393e6bc6969ac84598ab8e7a6f","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1587721825091},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1587721825091},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"c5142739e01e9f25c8b32b2209af85c787bb2b42","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"49c76bc723d3952abb613d9d68398ed7305da999","modified":1587722949457},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1587721825092},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4b7f057dbb53efd7cbe7eac7835a793ab3cbb135","modified":1587722949457},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1587722949457},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1587721825093},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1587721825093},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5de34e1d8a290751641ae456c942410852d5e809","modified":1587722949458},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0a9f0d9eb042595502d200fb8c65efb0e6c89aa9","modified":1587722949458},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"dc9318992ce2eb086ebaa2fe56b325e56d24098b","modified":1587722949458},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b69ac38b9da8c9c1b7de696fdeea7f9d7705213a","modified":1587722949458},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1587722949459},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1587361553275},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1587361553279},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1587361553280},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1587721825129},{"_id":"source/images/img_cls/densenet_1.png","hash":"f33ee6bfa0d34b8f14aaf6881ae878b9503d7cd4","modified":1587361552459},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1587361553296},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1587361553310},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1587721825076},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"a8f4d4b86acaa34c99111b2dde5d0779cc7e0de6","modified":1587361553087},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1587721825076},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1587361553088},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1587721825076},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1587721825076},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1587721825076},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1587721825077},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1587721825077},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1587361553093},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1587721825077},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"236a039b0900f4267de566b46f62314ad967d30f","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"18edddb2ffb3f85a68e4367f81e06c461e07bc25","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1587361553100},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"f6f05f02d50f742c84ee5122016c0563a8bb2cf9","modified":1587722949437},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"97974c231b4659b8aa5e9321c4d54db5c816d0db","modified":1587722949438},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1587361553115},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"9af620eba5ccceea21a0e3bc69f6f1fa7637c2f3","modified":1587722949438},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"70b3eb9d36543ab92796ac163544e9cf51b7c1e6","modified":1587722949438},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"97dec98d0403097d66822f1c90b50b2890c84698","modified":1587722949438},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0dfb97703a519d9438f64f9e41ab1dd37381f733","modified":1587722949439},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1587721825080},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"2c24829d95c742eb9e8316ebf2fbe9f2c168b59a","modified":1587722949440},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"82a275ca74086a46b8e82d5ebf78c7a807cd9c8b","modified":1587722949440},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"09dda2667628d1f91b2e37d8fc6df1413f961b64","modified":1587722949440},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"4aad8e36178faaa71a767af0084d578df4c09f73","modified":1587361553126},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5cc9e7394c927065c688cba5edd6e0a27587f1d8","modified":1587722949440},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1587721825081},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1587721825081},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"b266d2ce5e2b117be01537889e839a69004dc0bb","modified":1587722949441},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"fcd64c23d17775b3635325f6758b648d932e79b5","modified":1587722949441},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1587721825082},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1587721825082},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"b36eea093bd4b32056b5de6f370ff57e50b25a49","modified":1587361553139},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1587721825082},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1587721825083},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1587721825083},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1587721825083},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1587361553151},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1587721825085},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1587721825085},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"3baeeb51cfe123e99235ee1816d0e1f6a97c7852","modified":1587361553153},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"d9c244b1c3a09a7fccd3c3f732e6fb112a8cd565","modified":1587361553161},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"b87f4a06c0db893df4f756f24be182e1a4751f24","modified":1587722949442},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1587721825087},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1587721825087},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1587721825087},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"d83102771df652769e51ddfd041cf5f4ca1a041d","modified":1587722949442},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1587721825088},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1f6b0d3ab227697ca115e57fd61122ea7950e19d","modified":1587722949443},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"8ed7a9d5dfac592de703421b543978095129aa5b","modified":1587722949443},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1587361553205},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1587361553206},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1587361553209},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1587361553292},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"a52f8cae599099231866298ed831fdf76c9b6717","modified":1587722949438},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1587361553290},{"_id":"public/about/index.html","hash":"cd94b5d13901a7322b9d0e2a0d597db711147dde","modified":1613701256929},{"_id":"public/categories/index.html","hash":"80f95621f9e805ca0cd4c93ce0270de632a82cfe","modified":1613701256929},{"_id":"public/tags/index.html","hash":"a97f2f8beac470440a742e8fa443f5b9259cf214","modified":1613701256929},{"_id":"public/2019/07/25/WGAN/index.html","hash":"9247c9574d78241bc3c915d9e5c29198aa1dce5e","modified":1613701256929},{"_id":"public/2019/07/11/cpp-aux-tools/index.html","hash":"e7d360d3d2cb5f854a8d8cb7c05095735a505fee","modified":1613701256929},{"_id":"public/categories/DL-Framework/index.html","hash":"fe511cbe5844e5808df453fea1a48ff71b83f217","modified":1613701256929},{"_id":"public/categories/math/index.html","hash":"6a042978462b38f80052f37a22447371d5c5ef15","modified":1613701256929},{"_id":"public/archives/page/3/index.html","hash":"95d6e1382c34d7d7b1dd23015f219441c748426e","modified":1613701256929},{"_id":"public/archives/2019/08/index.html","hash":"938e59c4cd326edc9e8c9678115f5a7ce2cd9772","modified":1613701256929},{"_id":"public/archives/2019/09/index.html","hash":"ae89f8cf2e6762f769e6d1b2f6b406dfe372a3fd","modified":1613701256929},{"_id":"public/archives/2019/10/index.html","hash":"d1f34c0f28d5e4c0a0adf3c7e2890cf03d6bf977","modified":1610005048688},{"_id":"public/archives/2019/11/index.html","hash":"db6d4af907ef688a5a489a3a7c7ad8291ef2c0ba","modified":1613701256929},{"_id":"public/archives/2019/12/index.html","hash":"6a17e07f10b90d5be32886c8a7718f0278b28a8c","modified":1613701256929},{"_id":"public/archives/2020/index.html","hash":"7187d6bb51ad890fa09ff07882974b788ccf4cf3","modified":1613701256929},{"_id":"public/archives/2020/01/index.html","hash":"c44ecd4d491d172fde139feb9fe8b60164e0682d","modified":1613701256929},{"_id":"public/tags/GAN/index.html","hash":"99a648b11cb4a24545fa465d7850e93e59a71edd","modified":1613701256929},{"_id":"public/tags/DIP/index.html","hash":"5d92664a507016588c1696492b891ce5434f2f0b","modified":1613701256929},{"_id":"public/tags/object-detection/page/2/index.html","hash":"93f7c9bc3cff5f4bd279b4afd47c37a6860af99d","modified":1613701256929},{"_id":"public/tags/CV/index.html","hash":"a9e2cd730451ca5a4dcdc475723c28030a79dc34","modified":1613701256929},{"_id":"public/tags/c/index.html","hash":"2e354acfb93654d3fae96d788dc1869fa1bd635d","modified":1613701256929},{"_id":"public/tags/math/index.html","hash":"1d3fce5b853a62eafe407a76cce38c544b0eef3e","modified":1613701256929},{"_id":"public/tags/tool/index.html","hash":"463ab262b54e739ffd010bbfce1de98053951a00","modified":1613701256929},{"_id":"public/tags/DP/index.html","hash":"3abe7229be4e0547bad183915a1e9d967a6223ee","modified":1613701256929},{"_id":"public/tags/DL/index.html","hash":"420e336b4d83fbbdb26db7876dd899c03dace2d3","modified":1613701256929},{"_id":"public/tags/image-classification/index.html","hash":"30b8510fe4f609c52aa1f7193135c9efe11bfc00","modified":1613701256929},{"_id":"public/2020/01/08/pytorch/optim-2/index.html","hash":"ab92c92cbc759470b6767f3ab5aee274cf713f74","modified":1613701256929},{"_id":"public/2020/01/06/pytorch/optim-1/index.html","hash":"159d92f00dec67658c64e7b4971d008bcd25143e","modified":1613701256929},{"_id":"public/2020/01/02/pytorch/optim_SGD/index.html","hash":"0e6e97867fa4d628bac68a0304b990ab311edd20","modified":1613701256929},{"_id":"public/2019/12/07/DIP-2/index.html","hash":"9531abfc6991571bed90eb40e9a888915856f332","modified":1613701256929},{"_id":"public/2019/12/31/img_cls/densenet/index.html","hash":"ca916f3f16e9d92699bb83905f6cba46465d4027","modified":1613701256929},{"_id":"public/2019/12/05/DIP-1/index.html","hash":"28d049d0233c8a115c3f20ebef241bdc2d8e82e5","modified":1613701256929},{"_id":"public/2019/11/01/pytorch/PyTorch-mtd/index.html","hash":"d635f08a26c4d154bd0be2793e4e5b35d9d416e3","modified":1613701256929},{"_id":"public/2019/09/09/DL-env/index.html","hash":"8526c30bc7ef74e2d3141744df203fec097c9789","modified":1587724712866},{"_id":"public/2019/08/27/pytorch/PyTorch-5/index.html","hash":"daa3cb348fdd325bff55304135216b7433d09398","modified":1613701256929},{"_id":"public/2019/08/22/pytorch/PyTorch-4/index.html","hash":"6b61e440363af5812dbd013dd9e755cf076143fe","modified":1613701256929},{"_id":"public/2019/10/29/pytorch/PyTorch-loss/index.html","hash":"0987ca7865f1394739ee712cd1c389d90d9eb096","modified":1610005048688},{"_id":"public/2019/07/29/CGAN/index.html","hash":"a1cc50484a630732103a17e13582eedec4b8d315","modified":1613701256929},{"_id":"public/2019/07/23/GAN/index.html","hash":"073f1d08f8ec43d177298edc9a79b867f80408b1","modified":1613701256929},{"_id":"public/2019/07/19/Grid-RCNN/index.html","hash":"0cdb1f71133f1739029f4cb0b6ce4010e50c6f31","modified":1613701256929},{"_id":"public/2019/07/17/RepPoints/index.html","hash":"dcbbaf92bf8ac88487429d8252b4f22192a6d207","modified":1613701256929},{"_id":"public/2019/08/01/ImprovedGAN/index.html","hash":"95e208a05c90d3c6d324b4dc58909994d8122a2b","modified":1613701256929},{"_id":"public/2019/07/17/DetNet/index.html","hash":"f26a978ac513e0b99652c97b22eea51e4e7732e8","modified":1613701256929},{"_id":"public/2019/07/16/loss/index.html","hash":"630d141949e79613a9ea8ec3c0e4185e859bcf49","modified":1613701256929},{"_id":"public/2019/07/15/DeRPN/index.html","hash":"5f1ef3537efb6e09702c421724d713c6d0de270c","modified":1613701256929},{"_id":"public/2019/07/08/mask-rcnn/index.html","hash":"9665bde84c49d957291094458e6f899d47348fff","modified":1613701256929},{"_id":"public/2019/07/08/DSOD/index.html","hash":"e793446f1e68ecad709395f0ab0352ad893f53a8","modified":1613701256929},{"_id":"public/2019/07/03/libra-rcnn/index.html","hash":"955992f10bbbd98d9a0580ad7a2f08bd5f88b8a1","modified":1613701256929},{"_id":"public/2019/07/02/gcc-src/index.html","hash":"2d2a8dd0fe40ceed108a7918cd5ee09f8792d07f","modified":1613701256929},{"_id":"public/2019/06/28/M2Det/index.html","hash":"b4a625da8bd4e1328afea913474d5646745523c3","modified":1613701256929},{"_id":"public/2019/06/28/BBox-Reg-Uncertainty/index.html","hash":"553c894695d4686e9a819d8c8a89d7d931c2ee24","modified":1613701256929},{"_id":"public/2019/06/25/GA-RPN/index.html","hash":"8219eb343f6c7c0ef17eea6289418062a8a9a932","modified":1613701256929},{"_id":"public/2019/06/24/cv-mtds/index.html","hash":"6b040be3817c2197357c1a93eca4fdddff44afc8","modified":1613701256929},{"_id":"public/2019/06/27/FSAF/index.html","hash":"335389cb20f637591b0a282c01e0cb9d0b2be17d","modified":1613701256929},{"_id":"public/2019/06/21/TridentNet/index.html","hash":"37d2b5afa4cda3290542466e66c6a09ac7f1f390","modified":1613701256929},{"_id":"public/2019/06/18/pytorch/PyTorch-3/index.html","hash":"f93c6b584af26adecbed1c0d2ca51e9f4d5c84f5","modified":1613701256929},{"_id":"public/2019/06/13/GIoU/index.html","hash":"b1fa61bb6c9db46432d7f95aace078b395a6752c","modified":1613701256929},{"_id":"public/2019/06/13/Hexo-Sync/index.html","hash":"1f739171933ab06b046025d3ae89851c08b8fcb2","modified":1610683247106},{"_id":"public/2019/06/13/pytorch/PyTorch-2/index.html","hash":"e60605085c02a16a877d0030ff8e3790e498cfd1","modified":1613701256929},{"_id":"public/2019/06/12/pytorch/PyTorch-1/index.html","hash":"9c863fe8deb49e1ef0252843c19d1ed868ac9773","modified":1613701256929},{"_id":"public/2019/06/16/mAP/index.html","hash":"adeed825a0866b1bd260f2931956d1e211e0e54d","modified":1613701256929},{"_id":"public/index.html","hash":"142b87b08904df9a28cc06735da729e56f46afa6","modified":1613701256929},{"_id":"public/page/2/index.html","hash":"adffd545576ad5b019011f5b1d9c73b86cce1b05","modified":1613701256929},{"_id":"public/page/3/index.html","hash":"94d28dbcb7fdda8e3f2b095ca4943265cdf320c7","modified":1613701256929},{"_id":"public/page/4/index.html","hash":"0b6a567b5fd36d9e265e81122b422ac8905b0f40","modified":1613701256929},{"_id":"public/page/5/index.html","hash":"caa531069b320b3e8589a5e36795017e9f7cc836","modified":1613701256929},{"_id":"public/archives/index.html","hash":"f9df36b1c6f5514af90f898f475d6f61ac89199d","modified":1613701256929},{"_id":"public/archives/page/2/index.html","hash":"2ee096a6429e895d91da7065eac3cca20d4f5a1b","modified":1613701256929},{"_id":"public/archives/2019/index.html","hash":"9a00583db6a4976b5a87f6f2806fa98185b9fdfa","modified":1613701256929},{"_id":"public/archives/2019/page/2/index.html","hash":"21042d329a7fb637001c8515bbe437109ff92f69","modified":1613701256929},{"_id":"public/archives/2019/06/index.html","hash":"91fca873cbf0eef4df25819feb5eabf81def30b9","modified":1613701256929},{"_id":"public/archives/2019/07/index.html","hash":"4559b34eb1b6efe00d817edac415bfbe6b6157ce","modified":1613701256929},{"_id":"public/tags/object-detection/index.html","hash":"64e9a7344d0b54e5e325032c935d803bfd26c2b4","modified":1613701256929},{"_id":"public/tags/PyTorch/index.html","hash":"fe9aa7edaf3356ee54bd5b65f3552eea72c9b602","modified":1613701256929},{"_id":"public/archives/2020/04/index.html","hash":"649cd551c6f292d9f43a8e0ef3cb58f8e67e73b4","modified":1613701256929},{"_id":"public/2019/12/20/dp/DP4/index.html","hash":"508b4d8495ff3bf0dcfc3909bc65efe4242323d4","modified":1613701256929},{"_id":"public/2019/08/14/dp/DP2/index.html","hash":"85246941363aae2b576f83dc87929678a223a711","modified":1613701256929},{"_id":"public/2019/08/07/dp/DP1/index.html","hash":"435c4efded2f98ac022cbed950b2dec6a208552d","modified":1613701256929},{"_id":"public/2019/08/27/dp/DP3/index.html","hash":"1373ac6d6e6224180f3c73a109e7491a5883bb47","modified":1613701256929},{"_id":"public/2020/04/20/obj_det/YOLO/index.html","hash":"8ed6d8e38ee92d3cf1b3696d88dbb369218e8a96","modified":1613701256929},{"_id":"public/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1587719748702},{"_id":"public/images/CGAN_fig1.png","hash":"94d2eb94087c072188200f54dfccb665a4bb0bc3","modified":1587719748702},{"_id":"public/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1587719748702},{"_id":"public/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1587719748702},{"_id":"public/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1587719748702},{"_id":"public/images/Grid-RCNN_fig1.png","hash":"4c257e0b6f9f569b1a60ac27af54ca440812bc9c","modified":1587719748702},{"_id":"public/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1587719748702},{"_id":"public/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1587719748702},{"_id":"public/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1587719748702},{"_id":"public/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1587719748702},{"_id":"public/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1587719748702},{"_id":"public/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1587719748702},{"_id":"public/images/pytorch_mtd_aligncorners.png","hash":"8e1fff6ffbefc3b3bfb3fabbb2aa34c035b922a4","modified":1587719748702},{"_id":"public/images/pytorch_mtd_conv_t_1.png","hash":"68104100110da86678ff50bbc0d7418946c68f10","modified":1587719748702},{"_id":"public/images/pytorch_mth_conv.png","hash":"789dc01b7cd1d19267d690b001bf45582d720085","modified":1587719748702},{"_id":"public/images/pytorch/NAG_0.png","hash":"95aa0c0197c730db1bda3ffffe8895287826132e","modified":1587719748702},{"_id":"public/images/pytorch/momentum.png","hash":"0b274a844e87ac5c878a8e5b66848288d36d1aea","modified":1587719748702},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1587719748702},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1587723354499},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1587719748702},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1587719748702},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1587719748702},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1587719748702},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1587719748702},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1587719748702},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1587719748702},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1587719748702},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1587719748702},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1587719748702},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1587719748702},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1587719748702},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1587719748702},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1587721857222},{"_id":"public/images/pytorch/NAG.png","hash":"54fcfa655d287fe22d00a6b60f216dafd628b71e","modified":1587719748702},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1587719748702},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1587719748702},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1587719748702},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1587719748702},{"_id":"public/images/obj_det/YOLOv2_fig2.png","hash":"5c64920430b221ffaee62e991ae509c2410babb6","modified":1587719748702},{"_id":"public/images/obj_det/YOLOv1_fig1.jpg","hash":"7da82e62769ef15baafce9848f5164c00fab3b1b","modified":1587719748702},{"_id":"public/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1587719748702},{"_id":"public/images/DP1_fig1.png","hash":"ed9dd7cbecff94be6778831a494b1295482bfe3e","modified":1587719748702},{"_id":"public/images/DP2_fig1.png","hash":"a90b55aa55d0cfdd147c5b00feaf432300f2854b","modified":1587719748702},{"_id":"public/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1587719748702},{"_id":"public/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1587719748702},{"_id":"public/images/GAN_fig1.png","hash":"c6f2c668d1b4d98d4bd7fdc27851faed5c7ffe9c","modified":1587719748702},{"_id":"public/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1587719748702},{"_id":"public/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1587719748702},{"_id":"public/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1587719748702},{"_id":"public/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1587719748702},{"_id":"public/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1587719748702},{"_id":"public/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1587719748702},{"_id":"public/images/CGAN_fig2.png","hash":"d690cf996f276f489982f47ed689683d1c48de1a","modified":1587719748702},{"_id":"public/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1587719748702},{"_id":"public/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1587719748702},{"_id":"public/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1587719748702},{"_id":"public/images/GAN_alg1.png","hash":"d535c3b65a4c0c8eb331c79dc5032bf754e7084a","modified":1587719748702},{"_id":"public/images/GAN_fig2.png","hash":"86a432fab3c2df0b7e84f789971e5ced612cc2aa","modified":1587719748702},{"_id":"public/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1587719748702},{"_id":"public/images/Grid-RCNN-Plus_fig1.png","hash":"7255da83a2dadae1b8832adf87b3f60e6ed211d5","modified":1587719748702},{"_id":"public/images/Grid-RCNN_fig3.png","hash":"b5e40a1ca3df03b69c06e573086ce5af7f91eec1","modified":1587719748702},{"_id":"public/images/Grid-RCNN_fig4.png","hash":"f06e5e1250a7e59b4d04fbe316592745e65b6650","modified":1587719748702},{"_id":"public/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1587719748702},{"_id":"public/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1587719748702},{"_id":"public/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1587719748702},{"_id":"public/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1587719748702},{"_id":"public/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1587719748702},{"_id":"public/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1587719748702},{"_id":"public/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1587719748702},{"_id":"public/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1587719748702},{"_id":"public/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1587719748702},{"_id":"public/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1587719748702},{"_id":"public/images/pytorch_mtd_conv_t.png","hash":"158a3311f03a494bb7bd1a81a5712cab7aa1c37b","modified":1587719748702},{"_id":"public/images/img_cls/densenet_2.png","hash":"0d027c4897b024f9b9d20b21b5d3afaf1dd6e70c","modified":1587719748702},{"_id":"public/images/pytorch/overfitting.png","hash":"65c45d4a6a92b9664ab1454535350c1bb8423083","modified":1587719748702},{"_id":"public/images/img_cls/densenet_3.png","hash":"b456a005625516ce3e9acf66011d5fd4d9723e7a","modified":1587719748702},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1587719748702},{"_id":"public/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1587719748702},{"_id":"public/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1587719748702},{"_id":"public/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1587719748702},{"_id":"public/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1587719748702},{"_id":"public/js/algolia-search.js","hash":"23cc3c013185eb97ef347c3b4c92d928f2f3398f","modified":1587723354499},{"_id":"public/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1587719748702},{"_id":"public/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1587719748702},{"_id":"public/js/next-boot.js","hash":"a22eeb6048ddd6b9224c8a671cbcfa303a2f7a1a","modified":1587723354499},{"_id":"public/js/schemes/muse.js","hash":"47c4f60eb7f7dc3303e84914b611dc34827069e1","modified":1587723354499},{"_id":"public/js/schemes/pisces.js","hash":"3d9d3c14b77044d66be1898a9a934696e9127c82","modified":1587723354499},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1587719748702},{"_id":"public/css/main.css","hash":"73c2fa13cea5f9fdbc7e4185c3c30e7523f27597","modified":1587723354499},{"_id":"public/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1587719748702},{"_id":"public/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1587719748702},{"_id":"public/images/ImprovedGAN_fig1.png","hash":"9496ef5958922774f45112291e76fc421ba33ad5","modified":1587719748702},{"_id":"public/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1587719748702},{"_id":"public/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1587719748702},{"_id":"public/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1587719748702},{"_id":"public/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1587719748702},{"_id":"public/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1587719748702},{"_id":"public/images/img_cls/densenet_1.png","hash":"f33ee6bfa0d34b8f14aaf6881ae878b9503d7cd4","modified":1587719748702},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1587719748702},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1587723354499},{"_id":"public/js/utils.js","hash":"91d174e12c61c332f3b06085d635c2b0f686a758","modified":1587723354499},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1587719748702},{"_id":"public/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1587719748702},{"_id":"public/images/Grid-RCNN-Plus_fig2.png","hash":"801abc90e532b83bc123e58df681d79b9fc8ad4b","modified":1587719748702},{"_id":"public/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1587719748702},{"_id":"public/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1587719748702},{"_id":"public/images/Grid-RCNN_fig2.png","hash":"65e1e9f69e1d3e7b1b7b8b95d5cbb1859a99e897","modified":1587719748702},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1587719748702},{"_id":"public/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1587719748702},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1587719748702},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1587719748702},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1587719748702},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1587719748702},{"_id":"public/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1587719748702},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1587721857222},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1587719748702},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1587721825052},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1587721825052},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1587721825052},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1587721825053},{"_id":"themes/next/README.cn.md","hash":"b878b73f3fcdef47849453c94420871903d487b3","modified":1587721825053},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1587721825051},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1587721825055},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1587721825056},{"_id":"themes/next/languages/zh-Hans.yml","hash":"66b9b42f143c3cb2f782a94abd4c4cbd5fd7f55f","modified":1587721825057},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1587721825057},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1587721825057},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1587721825058},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1587721825059},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1587721825059},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1587721825060},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1587721825060},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1587721825060},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1587721825061},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1587721825062},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1587721825068},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1587721825068},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1587721825068},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1587721825069},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1587721825074},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1587721825060},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1587721825062},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1587721825062},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1587721825062},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1587721825066},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1587721825067},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1587721825067},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1587721825070},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1587721825070},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1587721825106},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1587721825108},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1587721825109},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1587721825109},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1587721825109},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1587721825113},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1587721825114},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1587721825114},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1587721825114},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1587721825128},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1587721825129},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1587721825130},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1587721825130},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1587721825130},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1587721825130},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1587721825131},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1587721825132},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1587721825132},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1587721825133},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1587721825133},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1587721825133},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1587721825133},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1587721825134},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1587721825134},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1587721825134},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1587721825134},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1587721825134},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1587721825135},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1587721825135},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1587721825135},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1587721825135},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1587721825136},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1587721825136},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1587721825136},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1587721825137},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1587721825141},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1587721825142},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1587721825099},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1587721825100},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1587721825100},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1587721825100},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1587721825100},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1587721825100},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1587721825101},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1587721825101},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1587721825102},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1587721825101},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1587721825102},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1587721825069},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1587721825069},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1587721825084},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1587721825094},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1587721825104},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1587721825105},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1587721825105},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1587721825109},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1587721825109},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1587721825110},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1587721825110},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1587721825110},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1587721825112},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1587721825113},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1587721825113},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1587721825114},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1587721825114},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1587721825132},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1587721825141},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1587721825141},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1587721825101},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1587721825105},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1587721825083},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1587721825084},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1587721825085},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1587721825086},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1587721825087},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1587721825103},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1587721825103},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1587721825103},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1587721825103},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1587721825103},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1587721825111},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1587721825112},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1587721825126},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1587721825119},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1587721825108},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1587721825124},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1587721825140},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1587721857222},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1587721857222},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1587721857222},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1587721857222},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1587721857222},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1587721857222},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1587721857222},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1587721857222},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1587721857222},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1587721857222},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1587721857222},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1587721857222},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1587721857222},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1587721857222},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1587721857222},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1587721857222},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1587721857222},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1587721857222},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1587721857222},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1587721857222},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1587721857222},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1587721857222},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1587721857222},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1587721857222},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1587721857222},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1587721857222},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1587721857222},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1587721857222},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1587721857222},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1587721857222},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1587721857222},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1587721857222},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1587721857222},{"_id":"public/lib/fastclick/README.html","hash":"b9e008af0866799103e596e2eda466a33d7dde18","modified":1587721857222},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"7eecc5e472416b9870580c53b7ed0c44474ed23a","modified":1587721857222},{"_id":"public/lib/jquery_lazyload/README.html","hash":"9c91bba4263a20004f2c10772d438cfa8312c308","modified":1587721857222},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1587721857222},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1587721857222},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1587721857222},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1587721857222},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1587721857222},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1587721857222},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1587721857222},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1587721857222},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1587721857222},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1587721857222},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1587721857222},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1587721857222},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1587721857222},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1587721857222},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1587721857222},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1587721857222},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1587721857222},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1587721857222},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1587721857222},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1587721857222},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1587721857222},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1587721857222},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1587721857222},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1587721857222},{"_id":"themes/next/gulpfile.js","hash":"0c76a1ac610ee8cbe8e2cc9cca1c925ffd0edf98","modified":1587722949402},{"_id":"themes/next/.github/issue-close-app.yml","hash":"b14756e65546eb9ecc9d4393f0c9a84a3dac1824","modified":1587722949390},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1587722949390},{"_id":"themes/next/languages/ar.yml","hash":"abcf220bd615cec0dd50e4d98da56580169d77e1","modified":1587722949402},{"_id":"themes/next/languages/hu.yml","hash":"0ea89ffaefd02a10494995f05a2a59d5e5679a28","modified":1587722949404},{"_id":"themes/next/scripts/renderer.js","hash":"e3658eea97b1183ee2e9f676231e53f7994741f6","modified":1587722949433},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d5aa1a3323639a36bcd9a401484b67537043cd3c","modified":1587722949388},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"59275aa0582f793fee7be67904dcf52ad33a7181","modified":1587722949388},{"_id":"themes/next/layout/_partials/languages.swig","hash":"c3ea82604a5853fb44c5f4e4663cbe912aa5dcf8","modified":1587722949411},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"5392dcbb504266f0f61d5b8219914068ef9cdc25","modified":1587722949415},{"_id":"themes/next/layout/_scripts/index.swig","hash":"1822eaf55bbb4bec88871c324fc18ad95580ccb4","modified":1587722949415},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"ccff5a773644d33ff22f6b45b6734f52b048f22b","modified":1587722949416},{"_id":"themes/next/layout/_scripts/three.swig","hash":"6b092c6d882b2dfa5273e1b3f60b244cb7c29fcd","modified":1587722949417},{"_id":"themes/next/layout/_third-party/index.swig","hash":"c6b63cbc80938e6e09578b8c67e01adf13a9e3bd","modified":1587722949421},{"_id":"themes/next/scripts/events/index.js","hash":"7baf362743b3d30626066614d877891fc140c502","modified":1587722949427},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"ad321db012cea520066deb0639335e9bc0dcc343","modified":1587722949431},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"305d03c1e45782988809298c3e3b3c5d5ee438aa","modified":1587722949431},{"_id":"themes/next/scripts/filters/locals.js","hash":"a5e7d05d3bd2ae6dcffad5a8ea0f72c6e55dbd02","modified":1587722949431},{"_id":"themes/next/scripts/filters/minify.js","hash":"21196a48cb127bf476ce598f25f24e8a53ef50c2","modified":1587722949432},{"_id":"themes/next/scripts/filters/post.js","hash":"57f2d817578dd97e206942604365e936a49854de","modified":1587722949432},{"_id":"themes/next/scripts/helpers/font.js","hash":"8fb1c0fc745df28e20b96222974402aab6d13a79","modified":1587722949432},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"b8d7ddfa4baa9b8d6b9066a634aa81c6243beec9","modified":1587722949433},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"840536754121e0da5968f5ad235f29200fc5d769","modified":1587722949433},{"_id":"themes/next/source/css/_colors.styl","hash":"11aef31a8e76f0f332a274a8bfd4537b73d4f88f","modified":1587722949436},{"_id":"themes/next/source/css/_mixins.styl","hash":"072a3fa473c19b20ccd7536a656cda044dbdae0a","modified":1587722949455},{"_id":"themes/next/source/js/bookmark.js","hash":"2268bfcab8cf9019e590e2d356b08a3d4a0cf791","modified":1587722949464},{"_id":"themes/next/source/js/local-search.js","hash":"f43b891fcd55b8153a8999a39d801c74ef5da0f7","modified":1587722949465},{"_id":"themes/next/source/lib/anime.min.js","hash":"960be51132134acd65c2017cc8a5d69cb419a0cd","modified":1587722949467},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"4baa86ca631168fc6388d27f4b1b501b40c877a8","modified":1587722949410},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"d8f785c062c6b0763a778bd4a252e6f5fee0e432","modified":1587722949413},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"ce712c110b5ce8aacba7a86b0558ff89700675c9","modified":1587722949413},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"f349a226e5370075bb6924e60da8b0170c7cfcc1","modified":1587722949413},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"7b2ef5db9615267a24b884388925de1e9b447c1f","modified":1587722949415},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"34c05e9d73b0f081db70990c296b6d6a0f8ea2ca","modified":1587722949416},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"2642e8aef5afbe23a2a76efdc955dab2ee04ed48","modified":1587722949419},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"fb94ee487d75e484e59b7fba96e989f699ff8a83","modified":1587722949420},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"aa6ab95b8b76611694613defb4bf25003d1b927f","modified":1587722949423},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d2f0e4c598410ec33785abe302c7ea7492bb791a","modified":1587722949424},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"53a0760c75d5aaabb3ce8e8aa8e003510d59807f","modified":1587722949424},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"01d94354d07e72cad47100482068b6be69fcc033","modified":1587722949424},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"964cd6bac668cf6d211a2624fbef3948cfdece55","modified":1587722949424},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"619338ddacf01e3df812e66a997e778f672f4726","modified":1587722949425},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"c171ea94e9afbba97f06856904264da331559463","modified":1587722949424},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"5a223b60406cee7438cfe3a5e41d1284425aa7a5","modified":1587722949425},{"_id":"themes/next/scripts/events/lib/config.js","hash":"aefe3b38a22bc155d485e39187f23e4f2ee5680a","modified":1587722949428},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"08496b71c9939718e7955704d219e44d7109247b","modified":1587722949428},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"e73f697bb160b223fdde783237148be5f41c1d78","modified":1587722949428},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"2f22f48f7370470cef78561a47c2a47c78035385","modified":1587722949429},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1587722949429},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"0c3bea89d64bc12c1bbe6f208a83773c6fb5375a","modified":1587722949429},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3a80559df0b670ccb065ea9d3bb587d0b61be3a4","modified":1587722949429},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"67cf90d9a2428c14eb113a64bdd213c22a019aef","modified":1587722949429},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"323a47df6ded894944a2647db44556d6163e67c4","modified":1587722949430},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"a4f3153ac76a7ffdf6cc70f52f1b2cc218ed393e","modified":1587722949430},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"851359f5ff90f733a9bd7fe677edbee8b8ac714c","modified":1587722949431},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"c52648a7b09f9fe37858f5694fcc1ffc709ad147","modified":1587722949441},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"a2ee16cac29a82cfce26804c160286fcbee94161","modified":1587722949446},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"45f4badac6ec45cf24355f6157aece1d4d3f1134","modified":1587722949450},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"4b068d0d898f4e624937503f0e1428993050bd65","modified":1587722949450},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"b619f39e18398422e0ac4999d8f042a5eaebe9cd","modified":1587722949451},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"20e0e3e3eba384930c022e21511214d244b4c9e7","modified":1587722949454},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"12b265f82840f27112ca2b1be497677f20f87545","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"e1c29b81a32273a0dedd926cda199a71aea72624","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"8674bd88df076a1dfe4023ed6750ded1f5b00223","modified":1587722949456},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"9898323ee5a7ac2a5d4f633c653112280beb2643","modified":1587722949457},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1587722949457},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"558794fced306339b98dc2b0ee7f0576802f1355","modified":1587722949458},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1587722949469},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1587722949469},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"57b9a179675f1536e017cba457b6ac575e397c4f","modified":1587722949439},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"93ba8172c0d2c37d738e6dbd44fcd5a2e23b92f3","modified":1587722949440},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"bad99f4cccb93b3cefe990a2c85124e60698d32e","modified":1587722949443},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"7eeb22c5696f8e0c95161dc57703973cf81c8c12","modified":1587722949443},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"b4f4bae437d4f994af93cf142494ffcd86bae46b","modified":1587722949444},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"b31c86d1a4f89837f9187bed646bda96b2cd286c","modified":1587722949444},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"300058ca12e81013e77ba01fe66ac210525768b6","modified":1587722949444},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"6d5f26646e2914474f295de8bf6dc327d4acd529","modified":1587722949444},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"7a3a56b10ab714c0e2ed240d0939deeecdcad167","modified":1587722949445},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"3d16ac0f4ccaeed868c246d4d49bde543d1f62cb","modified":1587722949445},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b8c816fba0a9b4a35fbae03ba5b1b2da96ba2687","modified":1587722949446},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"49722d555a2edb18094bb2cb3d7336dd72051b93","modified":1587722949447},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"357f825f0a649b2e28cba1481d4c9a0cb402e43a","modified":1587722949447},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"096f908c08ce553e482aadfd3e767a0145191093","modified":1587722949447},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"525242ce9e912c4adfe5134347c67dbdb9e98e3d","modified":1587722949447},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"12f7eaf6b56624cbc411528562d6bb848ff97039","modified":1587722949447},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"b11b04737a1a0fea3bd9f0081d96ee6c015358d4","modified":1587722949448},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"fa0a2ea57b7b4ce75b5d18c264af2d92ea3192f9","modified":1587722949448},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"098b4bdf49c7300490f959386d5d1185a32543f6","modified":1587722949448},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"5d540f683018745a5ed1d6f635df28ea610c1244","modified":1587722949449},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"67a1fcb33535122d41acd24f1f49cf02c89b88fa","modified":1587722949449},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4079e616fbf36112dec0674c1e0713d1d9769068","modified":1587722949450},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"83bd737f663a8461e66985af8ddbfc0a731fc939","modified":1587722949450},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"80488259271bcfe38031f4c2e902463daba9336b","modified":1587722949451},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"c911045b2ce9a66e38d9dd30c7ed078abbc10cbf","modified":1587722949451},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"ceacfa6218f6084c71a230b086e5d2708d29927e","modified":1587722949453},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"aca7bb220fc14ef2a8f96282d2a95a96a9238d46","modified":1587722949453},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"8b7aafb911850c73074cdb6cc87abe4ac8c12e99","modified":1587722949453},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"adaf0f580fccf4158169eeaf534a18005b39a760","modified":1587722949453},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"03a5bcecc0b12231462ef6ffe432fa77ee71beff","modified":1587722949453},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"3256e39f281f06751a1c0145d9806a0e56d68170","modified":1587722949454},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"51d46fa3c7c6b691c61a2c2b0ac005c97cfbf72b","modified":1587722949454},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"82e34d28f8a1169b20b60101d5bb0446deba3514","modified":1587722949468},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1587722949469},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1587723354499},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1587723354499},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1587723354499},{"_id":"public/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1587723354499},{"_id":"public/js/local-search.js","hash":"d6673063958127a03881dab2f0376a47f5e08a88","modified":1587723354499},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1587723354499},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1587723354499},{"_id":"source/_posts/cv/methods.md","hash":"6231f507b1abb19b237e97eb10f700a7dde66be4","modified":1610953444578},{"_id":"source/_posts/dip/hist_equal.md","hash":"a9f0baf96144bfc1ba7b2eb6e9e60160c6702179","modified":1592906820778},{"_id":"source/_posts/pytorch/DL-env.md","hash":"b04e6156d52bb9933207b3c378f3c49198d12de2","modified":1612833755061},{"_id":"public/2020/06/23/dip/hist_equal/index.html","hash":"3731102d5930e57a3a8c54f3a62b080849dcb5ad","modified":1613701256929},{"_id":"public/2020/05/27/cv/methods/index.html","hash":"3e886ebb3300f5f2beb38d1de556901fd67b2c3b","modified":1613701256929},{"_id":"public/archives/2020/05/index.html","hash":"9b1d2276d81b344daa504edc1c63e3d7cf6574bf","modified":1613701256929},{"_id":"public/archives/2020/06/index.html","hash":"d7a3f18604dfd9dbec405caa1fc346c6114451d4","modified":1613701256929},{"_id":"public/2019/09/09/pytorch/DL-env/index.html","hash":"fb444461dc494832c74bab68425efbd6507539f1","modified":1613701256929},{"_id":"source/_posts/dl/x_ent_loss.md","hash":"f5f90c92361d932a64dc5289a6604260bd17b911","modified":1610443992331},{"_id":"source/_posts/pytorch/loss_1.md","hash":"f4a0f9b0cd71fce68c3ee614ed6f0bcbc02d252c","modified":1610683175833},{"_id":"source/_posts/pytorch/loss_2.md","hash":"785349448098fa76b3e729e00b5d6981904e2138","modified":1610694731771},{"_id":"source/_posts/pytorch/tricks_1.md","hash":"91b028b768243c380c1cc37347e64f8b067e1d0c","modified":1610090539103},{"_id":"source/_posts/dl/tricks_1.md","hash":"4c919d9ba90cbf8015fd380364e0fd04ca103a20","modified":1610104264076},{"_id":"public/2021/01/08/dl/tricks_1/index.html","hash":"d4585a50567ed8d3c9f67a30a8de818425ef6823","modified":1613701256929},{"_id":"public/2021/01/08/pytorch/tricks_1/index.html","hash":"2e3300fda5b25070479c44eab49637cf0b87286c","modified":1613701256929},{"_id":"public/tags/PyTorch/page/2/index.html","hash":"b15d0bbf209c45f6746a5849d4b24018b1753792","modified":1613701256929},{"_id":"public/tags/Deep-Learning/index.html","hash":"781997a9fc75cb1009b8feb84422b209e2fc2743","modified":1613701256929},{"_id":"public/archives/2021/index.html","hash":"327ac2890cd1ef07d751c8b08d092309a4bc5b81","modified":1613701256929},{"_id":"public/2021/01/13/pytorch/loss_2/index.html","hash":"39dc8152ce58cab40497e29f1e52308b2d85694f","modified":1613701256929},{"_id":"public/2021/01/12/pytorch/loss_1/index.html","hash":"5af84957f47d2f37356574e4d791f80bbf3679fd","modified":1613701256929},{"_id":"public/2021/01/12/dl/x_ent_loss/index.html","hash":"413191de02d1d9fa3d33ab606beb4ef996600648","modified":1613701256929},{"_id":"public/archives/2021/01/index.html","hash":"5cac1ff0ce6af8c08683599f2c567c82a683f36f","modified":1613701256929},{"_id":"source/_posts/TODO.md","hash":"8bbf8547be7d27a6d05f2b5ea7108d041d079112","modified":1612245762916},{"_id":"source/_posts/dl/dilated_conv.md","hash":"97ddc83c1ff05f612c8703ef21c8996834a4eeb9","modified":1613701129892},{"_id":"source/_posts/dl/receptive_field.md","hash":"978ea27e7345a1a9a6c3040f6f0c3b487a792831","modified":1613701138909},{"_id":"source/_posts/obj_det/faster_rcnn.md","hash":"9be66a2010eb3e334418ff9018309794d28cadf2","modified":1613614834871},{"_id":"source/_posts/tools/Hexo-Sync.md","hash":"97a36a5bf1def5bd2551416f7e7792b7c751ef95","modified":1613614805795},{"_id":"source/_posts/tools/shell.md","hash":"6d20e0f0806672623340ccd6d59bc0093e2ad75b","modified":1611398435878},{"_id":"source/_posts/img_cls/resnet.md","hash":"a0ac8f32dc5d2e21f91e626288f5e68d2c9e0301","modified":1611821556889},{"_id":"source/images/img_cls/resnet_4.png","hash":"853e6a7da14895d60a5d894abf4f081730b5a927","modified":1611196664381},{"_id":"source/images/img_cls/resnet_2.png","hash":"533b0bfdf322cb80e914589bae69b20e98f25dcd","modified":1611192784028},{"_id":"source/images/img_cls/resnet_1.png","hash":"5454e8ca5934d124bc10177771e1ad55999ac973","modified":1611192807710},{"_id":"source/images/img_cls/resnet_3.png","hash":"d8b027e159cacbf69af5f6829dea83fb1641af3e","modified":1611192738286},{"_id":"public/2021/02/19/dl/dilated_conv/index.html","hash":"b931d3c94422413abbf0908029d484f9c49e751b","modified":1613701256929},{"_id":"public/2021/02/18/obj_det/faster_rcnn/index.html","hash":"7a9e3f9c5a1cafe7ffce67e901a811447ecf8cd8","modified":1613701256929},{"_id":"public/2021/02/02/TODO/index.html","hash":"044feb33d99bf6313f04dcb1d244576f6758df6f","modified":1613701256929},{"_id":"public/2021/01/23/tools/shell/index.html","hash":"130a4021fdcd1ca64ce78961fb4e64bb7c92e488","modified":1613701256929},{"_id":"public/archives/2021/02/index.html","hash":"ed2d8d425a43ae86c1b76a27db249d7a8347ecac","modified":1613701256929},{"_id":"public/tags/Deep-Learning-CNN/index.html","hash":"d7b7a59f59414ad13113c78e491330204ecf6bbc","modified":1613701256929},{"_id":"public/tags/CNN-Deep-Learning/index.html","hash":"424e880849121f88b72bcf8fc3cce97f246e0551","modified":1613701256929},{"_id":"public/tags/img-cls/index.html","hash":"10d205f330563d03ac1f2d56c02c916f854a048c","modified":1613701256929},{"_id":"public/2021/02/18/dl/receptive_field/index.html","hash":"b170d4d21893d195a1e825be575015170d9788d8","modified":1613701256929},{"_id":"public/2021/01/19/img_cls/resnet/index.html","hash":"39b83a79a215612c5cbb428795b2819afe7c085e","modified":1613701256929},{"_id":"public/2019/06/13/tools/Hexo-Sync/index.html","hash":"b91f889bb6c84e294c0c662a4bbc55d17451dba3","modified":1613701256929},{"_id":"public/page/6/index.html","hash":"cc21a8af746b26a5d6fa08adfa0cfd4e3dd75443","modified":1613701256929},{"_id":"public/images/img_cls/resnet_2.png","hash":"533b0bfdf322cb80e914589bae69b20e98f25dcd","modified":1613701256929},{"_id":"public/images/img_cls/resnet_4.png","hash":"853e6a7da14895d60a5d894abf4f081730b5a927","modified":1613701256929},{"_id":"public/images/img_cls/resnet_3.png","hash":"d8b027e159cacbf69af5f6829dea83fb1641af3e","modified":1613701256929},{"_id":"public/images/img_cls/resnet_1.png","hash":"5454e8ca5934d124bc10177771e1ad55999ac973","modified":1613701256929}],"Category":[{"name":"DL Framework","_id":"ck9dzcivu001sgga64qfs77vd"},{"name":"math","_id":"ck9dzcjgh002vgga6gmby7446"}],"Data":[],"Page":[{"title":"about","date":"2019-07-23T07:12:21.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2019-07-23 15:12:21\n---\n","updated":"2020-04-20T05:45:52.303Z","path":"about/index.html","comments":1,"layout":"page","_id":"ck9dzchvm0001gga6388hele3","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"分类","date":"2019-06-14T13:23:43.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2019-06-14 21:23:43\ntype: categories\n---\n","updated":"2020-04-20T05:45:52.304Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ck9dzcio8000qgga67evffqk0","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","date":"2019-06-14T13:23:22.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2019-06-14 21:23:22\ntype: tags\n---\n","updated":"2020-04-20T05:45:52.521Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ck9dzciol000sgga6b7tl2sol","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"CGAN/DCGAN","date":"2019-07-29T09:00:43.000Z","mathjax":true,"_content":"# CGAN\n论文 [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n<!-- more -->\n在 [GAN](2019/07/23/GAN) 中我们知道 GAN 经过训练，生成器 G 可以根据一个随机噪声输入生成与训练集样本非常相似的样本（判别器 D 无法判别），但是 G 生成样本的标签是无法控制的，以 mnist 数据集为例，给 G 一个随机噪声输入，G 生成的样本图像可能表示数字 1，也可能是其他数字，GAN 无法控制，GAN 只能做到 G 生成样本图像很逼近真实样本图像。然而，使用额外信息来限制模型则可以控制数据生成过程，这个额外信息可以是分类标签或是其他形式的数据，于是本文的 CGAN 应运而生。\n\n## Conditional Adversarial Nets\nGAN 中的 G 和 D 均使用额外信息 y 进行条件限制，则得到 CGAN。额外信息 y 可是是分类标签或者其他形式的数据。以 mnist 训练集为例，通常选择图像的分类标签作为额外信息 y。\n\n预先已知的输入噪声 z 和 图像分类标签 y 合并一起作为 G 的输入（这是本文所用的最简单的方法，这种处理方式可以很容易地使用传统的 GAN 网络而不需要重新设计网络）。训练样本数据 x 以及对应的图像分类标签 y 合并到一起作为 D 的输入。（G 和 D 的结构可以与 GAN 中保持一致，也可以将部分 fc 替换为 conv/deconv）\n\n训练目标函数为，\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$\n\n图 1 为 CGAN 过程示意图，\n![](/images/CGAN_fig1.png)\n\n这里引用一个代码片段来进行说明\n```python\nimport tensorflow as tf\ny_dim=10    # one-hot vector for mnist-label\nz_dim=100   # length of noise input vector\ny=tf.placeholder(tf.float32, shape=[None,y_dim], name='label')\nx=tf.placeholder(tf.float32, shape=[None,28,28,1], name='real_img')\nz=tf.placeholder(tf.float32, shape=[None,z_dim], name='noise')\n\n# G 的输入由 noise 与 label 合并，单个输入 vector 长度由原来的 100 变成 110\nx_for_g=tf.concat([z,y], axis=1)    # [batch_size, 100+10]\n# 然后与 GAN 中 G 的处理相同\n\n# D 的输入由 real_img 与 label 合并\nnew_y=tf.reshape(y,[batch_size,1,1,y_dim])\nnew_y=new_y*tf.ones([batch_size,28,28,y_dim])   # [batch_size,28,28,10]\nx_for_d=tf.concat([x,new_y],axis=-1)    # [batch_size,28,28,1+10]\n# 然后与 GAN 中 D 的处理相同\n```\n\n## 实验\n### Unimodal\n使用 mnist 数据集，分类标签 y 使用长度为 10 的 one-hot 向量。CGAN 的结构和训练方法介绍略，这部分可以查看原文。图 2 显示了生成样本，每一行使用一个标签作为模型的限制条件。\n![](/images/CGAN_fig2.png)\n\n### Multimodal\n对应一到多映射，即每个图像可以有多个不同的标签。例如 Flickr 数据集，包含图像和对应的 UGM（user-generated metadata）。UGM 通常更具有描述性，并且语义上与人类使用自然语言描述图像更为接近，而不仅仅是标记图中的目标。不同的用户可能使用不同的词汇来描述相同的概念，因此使用一个高效的方法来规范化这些标签显得尤其重要。概念词嵌入（word embedding）在此情况下非常有用，因为语义相似的词其词向量非常接近。\n\n根据图像特征，我们可以使用 CGAN 生成 tag-vectors 以进行对图像自动打标签。使用 AlexNet 在 ImageNet 上训练网络，网络的最后一个 fc 层输出单元为 4096 个，这个输出作为最终的图像表示。为了得到词表示，我们从 YFCC100M 数据集的 metadata 中收集 user-tags，title 和 descriptions 作为文本预料，经过预处理和文本清洗，使用 skip-gram 模型进行训练，得到长度为 200 的词向量，我们忽略词频低于 200 的词，最终得到的词典大小为 247465。生成器 G 生成样本为 tag 特征向量，额外信息 y 为图像特征（上述的 4096 向量）。\n\n实验使用 MIR Flickr 25000 数据集，使用上述卷积模型和语言模型（AlexNet，skip-gram）分布抽取图像特征和 tag 特征。数据集中前 15000 的样本作为训练集。训练阶段，数据集中没有 tag 的图像被忽略掉，而如果图像拥有多个 tag，那么对于每个 tag 均分别使用一次这个图像。\n\nevaluation 阶段，对于每个图像生成 100 个样本（tag 特征向量），然后对每个生成样本，使用余弦相似度计算词典中与样本最接近的 20 个词，然后再所有 100 个样本中（我理解的是在 2000 个词中）选择 top 10 最常见的词作为图像的 tags。由于这部分实验没有看到源码，故其余部分的介绍略过，详情可参考原论文。\n\n# DCGAN\n论文 [Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n\n这篇文章主要是将卷积层、BN 以及 ReLU 引入 GAN 网络，没有官方代码，但是 github 上有很多实现，都非常简单易懂，例如 [DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)。","source":"_posts/CGAN.md","raw":"---\ntitle: CGAN/DCGAN\ndate: 2019-07-29 17:00:43\ntags: GAN\nmathjax: true\n---\n# CGAN\n论文 [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n<!-- more -->\n在 [GAN](2019/07/23/GAN) 中我们知道 GAN 经过训练，生成器 G 可以根据一个随机噪声输入生成与训练集样本非常相似的样本（判别器 D 无法判别），但是 G 生成样本的标签是无法控制的，以 mnist 数据集为例，给 G 一个随机噪声输入，G 生成的样本图像可能表示数字 1，也可能是其他数字，GAN 无法控制，GAN 只能做到 G 生成样本图像很逼近真实样本图像。然而，使用额外信息来限制模型则可以控制数据生成过程，这个额外信息可以是分类标签或是其他形式的数据，于是本文的 CGAN 应运而生。\n\n## Conditional Adversarial Nets\nGAN 中的 G 和 D 均使用额外信息 y 进行条件限制，则得到 CGAN。额外信息 y 可是是分类标签或者其他形式的数据。以 mnist 训练集为例，通常选择图像的分类标签作为额外信息 y。\n\n预先已知的输入噪声 z 和 图像分类标签 y 合并一起作为 G 的输入（这是本文所用的最简单的方法，这种处理方式可以很容易地使用传统的 GAN 网络而不需要重新设计网络）。训练样本数据 x 以及对应的图像分类标签 y 合并到一起作为 D 的输入。（G 和 D 的结构可以与 GAN 中保持一致，也可以将部分 fc 替换为 conv/deconv）\n\n训练目标函数为，\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$\n\n图 1 为 CGAN 过程示意图，\n![](/images/CGAN_fig1.png)\n\n这里引用一个代码片段来进行说明\n```python\nimport tensorflow as tf\ny_dim=10    # one-hot vector for mnist-label\nz_dim=100   # length of noise input vector\ny=tf.placeholder(tf.float32, shape=[None,y_dim], name='label')\nx=tf.placeholder(tf.float32, shape=[None,28,28,1], name='real_img')\nz=tf.placeholder(tf.float32, shape=[None,z_dim], name='noise')\n\n# G 的输入由 noise 与 label 合并，单个输入 vector 长度由原来的 100 变成 110\nx_for_g=tf.concat([z,y], axis=1)    # [batch_size, 100+10]\n# 然后与 GAN 中 G 的处理相同\n\n# D 的输入由 real_img 与 label 合并\nnew_y=tf.reshape(y,[batch_size,1,1,y_dim])\nnew_y=new_y*tf.ones([batch_size,28,28,y_dim])   # [batch_size,28,28,10]\nx_for_d=tf.concat([x,new_y],axis=-1)    # [batch_size,28,28,1+10]\n# 然后与 GAN 中 D 的处理相同\n```\n\n## 实验\n### Unimodal\n使用 mnist 数据集，分类标签 y 使用长度为 10 的 one-hot 向量。CGAN 的结构和训练方法介绍略，这部分可以查看原文。图 2 显示了生成样本，每一行使用一个标签作为模型的限制条件。\n![](/images/CGAN_fig2.png)\n\n### Multimodal\n对应一到多映射，即每个图像可以有多个不同的标签。例如 Flickr 数据集，包含图像和对应的 UGM（user-generated metadata）。UGM 通常更具有描述性，并且语义上与人类使用自然语言描述图像更为接近，而不仅仅是标记图中的目标。不同的用户可能使用不同的词汇来描述相同的概念，因此使用一个高效的方法来规范化这些标签显得尤其重要。概念词嵌入（word embedding）在此情况下非常有用，因为语义相似的词其词向量非常接近。\n\n根据图像特征，我们可以使用 CGAN 生成 tag-vectors 以进行对图像自动打标签。使用 AlexNet 在 ImageNet 上训练网络，网络的最后一个 fc 层输出单元为 4096 个，这个输出作为最终的图像表示。为了得到词表示，我们从 YFCC100M 数据集的 metadata 中收集 user-tags，title 和 descriptions 作为文本预料，经过预处理和文本清洗，使用 skip-gram 模型进行训练，得到长度为 200 的词向量，我们忽略词频低于 200 的词，最终得到的词典大小为 247465。生成器 G 生成样本为 tag 特征向量，额外信息 y 为图像特征（上述的 4096 向量）。\n\n实验使用 MIR Flickr 25000 数据集，使用上述卷积模型和语言模型（AlexNet，skip-gram）分布抽取图像特征和 tag 特征。数据集中前 15000 的样本作为训练集。训练阶段，数据集中没有 tag 的图像被忽略掉，而如果图像拥有多个 tag，那么对于每个 tag 均分别使用一次这个图像。\n\nevaluation 阶段，对于每个图像生成 100 个样本（tag 特征向量），然后对每个生成样本，使用余弦相似度计算词典中与样本最接近的 20 个词，然后再所有 100 个样本中（我理解的是在 2000 个词中）选择 top 10 最常见的词作为图像的 tags。由于这部分实验没有看到源码，故其余部分的介绍略过，详情可参考原论文。\n\n# DCGAN\n论文 [Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n\n这篇文章主要是将卷积层、BN 以及 ReLU 引入 GAN 网络，没有官方代码，但是 github 上有很多实现，都非常简单易懂，例如 [DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)。","slug":"CGAN","published":1,"updated":"2020-04-24T10:35:47.246Z","_id":"ck9dzchum0000gga6hn0q5b6g","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"CGAN\"><a href=\"#CGAN\" class=\"headerlink\" title=\"CGAN\"></a>CGAN</h1><p>论文 <a href=\"https://arxiv.org/abs/1411.1784\" target=\"_blank\" rel=\"noopener\">Conditional Generative Adversarial Nets</a></p>\n<a id=\"more\"></a>\n<p>在 <a href=\"2019/07/23/GAN\">GAN</a> 中我们知道 GAN 经过训练，生成器 G 可以根据一个随机噪声输入生成与训练集样本非常相似的样本（判别器 D 无法判别），但是 G 生成样本的标签是无法控制的，以 mnist 数据集为例，给 G 一个随机噪声输入，G 生成的样本图像可能表示数字 1，也可能是其他数字，GAN 无法控制，GAN 只能做到 G 生成样本图像很逼近真实样本图像。然而，使用额外信息来限制模型则可以控制数据生成过程，这个额外信息可以是分类标签或是其他形式的数据，于是本文的 CGAN 应运而生。</p>\n<h2 id=\"Conditional-Adversarial-Nets\"><a href=\"#Conditional-Adversarial-Nets\" class=\"headerlink\" title=\"Conditional Adversarial Nets\"></a>Conditional Adversarial Nets</h2><p>GAN 中的 G 和 D 均使用额外信息 y 进行条件限制，则得到 CGAN。额外信息 y 可是是分类标签或者其他形式的数据。以 mnist 训练集为例，通常选择图像的分类标签作为额外信息 y。</p>\n<p>预先已知的输入噪声 z 和 图像分类标签 y 合并一起作为 G 的输入（这是本文所用的最简单的方法，这种处理方式可以很容易地使用传统的 GAN 网络而不需要重新设计网络）。训练样本数据 x 以及对应的图像分类标签 y 合并到一起作为 D 的输入。（G 和 D 的结构可以与 GAN 中保持一致，也可以将部分 fc 替换为 conv/deconv）</p>\n<p>训练目标函数为，<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$</p>\n<p>图 1 为 CGAN 过程示意图，<br><img src=\"/images/CGAN_fig1.png\" alt=\"\"></p>\n<p>这里引用一个代码片段来进行说明</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\">y_dim=<span class=\"number\">10</span>    <span class=\"comment\"># one-hot vector for mnist-label</span></span><br><span class=\"line\">z_dim=<span class=\"number\">100</span>   <span class=\"comment\"># length of noise input vector</span></span><br><span class=\"line\">y=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,y_dim], name=<span class=\"string\">'label'</span>)</span><br><span class=\"line\">x=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>], name=<span class=\"string\">'real_img'</span>)</span><br><span class=\"line\">z=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,z_dim], name=<span class=\"string\">'noise'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># G 的输入由 noise 与 label 合并，单个输入 vector 长度由原来的 100 变成 110</span></span><br><span class=\"line\">x_for_g=tf.concat([z,y], axis=<span class=\"number\">1</span>)    <span class=\"comment\"># [batch_size, 100+10]</span></span><br><span class=\"line\"><span class=\"comment\"># 然后与 GAN 中 G 的处理相同</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># D 的输入由 real_img 与 label 合并</span></span><br><span class=\"line\">new_y=tf.reshape(y,[batch_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>,y_dim])</span><br><span class=\"line\">new_y=new_y*tf.ones([batch_size,<span class=\"number\">28</span>,<span class=\"number\">28</span>,y_dim])   <span class=\"comment\"># [batch_size,28,28,10]</span></span><br><span class=\"line\">x_for_d=tf.concat([x,new_y],axis=<span class=\"number\">-1</span>)    <span class=\"comment\"># [batch_size,28,28,1+10]</span></span><br><span class=\"line\"><span class=\"comment\"># 然后与 GAN 中 D 的处理相同</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><h3 id=\"Unimodal\"><a href=\"#Unimodal\" class=\"headerlink\" title=\"Unimodal\"></a>Unimodal</h3><p>使用 mnist 数据集，分类标签 y 使用长度为 10 的 one-hot 向量。CGAN 的结构和训练方法介绍略，这部分可以查看原文。图 2 显示了生成样本，每一行使用一个标签作为模型的限制条件。<br><img src=\"/images/CGAN_fig2.png\" alt=\"\"></p>\n<h3 id=\"Multimodal\"><a href=\"#Multimodal\" class=\"headerlink\" title=\"Multimodal\"></a>Multimodal</h3><p>对应一到多映射，即每个图像可以有多个不同的标签。例如 Flickr 数据集，包含图像和对应的 UGM（user-generated metadata）。UGM 通常更具有描述性，并且语义上与人类使用自然语言描述图像更为接近，而不仅仅是标记图中的目标。不同的用户可能使用不同的词汇来描述相同的概念，因此使用一个高效的方法来规范化这些标签显得尤其重要。概念词嵌入（word embedding）在此情况下非常有用，因为语义相似的词其词向量非常接近。</p>\n<p>根据图像特征，我们可以使用 CGAN 生成 tag-vectors 以进行对图像自动打标签。使用 AlexNet 在 ImageNet 上训练网络，网络的最后一个 fc 层输出单元为 4096 个，这个输出作为最终的图像表示。为了得到词表示，我们从 YFCC100M 数据集的 metadata 中收集 user-tags，title 和 descriptions 作为文本预料，经过预处理和文本清洗，使用 skip-gram 模型进行训练，得到长度为 200 的词向量，我们忽略词频低于 200 的词，最终得到的词典大小为 247465。生成器 G 生成样本为 tag 特征向量，额外信息 y 为图像特征（上述的 4096 向量）。</p>\n<p>实验使用 MIR Flickr 25000 数据集，使用上述卷积模型和语言模型（AlexNet，skip-gram）分布抽取图像特征和 tag 特征。数据集中前 15000 的样本作为训练集。训练阶段，数据集中没有 tag 的图像被忽略掉，而如果图像拥有多个 tag，那么对于每个 tag 均分别使用一次这个图像。</p>\n<p>evaluation 阶段，对于每个图像生成 100 个样本（tag 特征向量），然后对每个生成样本，使用余弦相似度计算词典中与样本最接近的 20 个词，然后再所有 100 个样本中（我理解的是在 2000 个词中）选择 top 10 最常见的词作为图像的 tags。由于这部分实验没有看到源码，故其余部分的介绍略过，详情可参考原论文。</p>\n<h1 id=\"DCGAN\"><a href=\"#DCGAN\" class=\"headerlink\" title=\"DCGAN\"></a>DCGAN</h1><p>论文 <a href=\"https://arxiv.org/abs/1511.06434\" target=\"_blank\" rel=\"noopener\">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks</a></p>\n<p>这篇文章主要是将卷积层、BN 以及 ReLU 引入 GAN 网络，没有官方代码，但是 github 上有很多实现，都非常简单易懂，例如 <a href=\"https://github.com/carpedm20/DCGAN-tensorflow\" target=\"_blank\" rel=\"noopener\">DCGAN-tensorflow</a>。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"CGAN\"><a href=\"#CGAN\" class=\"headerlink\" title=\"CGAN\"></a>CGAN</h1><p>论文 <a href=\"https://arxiv.org/abs/1411.1784\" target=\"_blank\" rel=\"noopener\">Conditional Generative Adversarial Nets</a></p>","more":"<p>在 <a href=\"2019/07/23/GAN\">GAN</a> 中我们知道 GAN 经过训练，生成器 G 可以根据一个随机噪声输入生成与训练集样本非常相似的样本（判别器 D 无法判别），但是 G 生成样本的标签是无法控制的，以 mnist 数据集为例，给 G 一个随机噪声输入，G 生成的样本图像可能表示数字 1，也可能是其他数字，GAN 无法控制，GAN 只能做到 G 生成样本图像很逼近真实样本图像。然而，使用额外信息来限制模型则可以控制数据生成过程，这个额外信息可以是分类标签或是其他形式的数据，于是本文的 CGAN 应运而生。</p>\n<h2 id=\"Conditional-Adversarial-Nets\"><a href=\"#Conditional-Adversarial-Nets\" class=\"headerlink\" title=\"Conditional Adversarial Nets\"></a>Conditional Adversarial Nets</h2><p>GAN 中的 G 和 D 均使用额外信息 y 进行条件限制，则得到 CGAN。额外信息 y 可是是分类标签或者其他形式的数据。以 mnist 训练集为例，通常选择图像的分类标签作为额外信息 y。</p>\n<p>预先已知的输入噪声 z 和 图像分类标签 y 合并一起作为 G 的输入（这是本文所用的最简单的方法，这种处理方式可以很容易地使用传统的 GAN 网络而不需要重新设计网络）。训练样本数据 x 以及对应的图像分类标签 y 合并到一起作为 D 的输入。（G 和 D 的结构可以与 GAN 中保持一致，也可以将部分 fc 替换为 conv/deconv）</p>\n<p>训练目标函数为，<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$</p>\n<p>图 1 为 CGAN 过程示意图，<br><img src=\"/images/CGAN_fig1.png\" alt=\"\"></p>\n<p>这里引用一个代码片段来进行说明</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\">y_dim=<span class=\"number\">10</span>    <span class=\"comment\"># one-hot vector for mnist-label</span></span><br><span class=\"line\">z_dim=<span class=\"number\">100</span>   <span class=\"comment\"># length of noise input vector</span></span><br><span class=\"line\">y=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,y_dim], name=<span class=\"string\">'label'</span>)</span><br><span class=\"line\">x=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>], name=<span class=\"string\">'real_img'</span>)</span><br><span class=\"line\">z=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,z_dim], name=<span class=\"string\">'noise'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># G 的输入由 noise 与 label 合并，单个输入 vector 长度由原来的 100 变成 110</span></span><br><span class=\"line\">x_for_g=tf.concat([z,y], axis=<span class=\"number\">1</span>)    <span class=\"comment\"># [batch_size, 100+10]</span></span><br><span class=\"line\"><span class=\"comment\"># 然后与 GAN 中 G 的处理相同</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># D 的输入由 real_img 与 label 合并</span></span><br><span class=\"line\">new_y=tf.reshape(y,[batch_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>,y_dim])</span><br><span class=\"line\">new_y=new_y*tf.ones([batch_size,<span class=\"number\">28</span>,<span class=\"number\">28</span>,y_dim])   <span class=\"comment\"># [batch_size,28,28,10]</span></span><br><span class=\"line\">x_for_d=tf.concat([x,new_y],axis=<span class=\"number\">-1</span>)    <span class=\"comment\"># [batch_size,28,28,1+10]</span></span><br><span class=\"line\"><span class=\"comment\"># 然后与 GAN 中 D 的处理相同</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><h3 id=\"Unimodal\"><a href=\"#Unimodal\" class=\"headerlink\" title=\"Unimodal\"></a>Unimodal</h3><p>使用 mnist 数据集，分类标签 y 使用长度为 10 的 one-hot 向量。CGAN 的结构和训练方法介绍略，这部分可以查看原文。图 2 显示了生成样本，每一行使用一个标签作为模型的限制条件。<br><img src=\"/images/CGAN_fig2.png\" alt=\"\"></p>\n<h3 id=\"Multimodal\"><a href=\"#Multimodal\" class=\"headerlink\" title=\"Multimodal\"></a>Multimodal</h3><p>对应一到多映射，即每个图像可以有多个不同的标签。例如 Flickr 数据集，包含图像和对应的 UGM（user-generated metadata）。UGM 通常更具有描述性，并且语义上与人类使用自然语言描述图像更为接近，而不仅仅是标记图中的目标。不同的用户可能使用不同的词汇来描述相同的概念，因此使用一个高效的方法来规范化这些标签显得尤其重要。概念词嵌入（word embedding）在此情况下非常有用，因为语义相似的词其词向量非常接近。</p>\n<p>根据图像特征，我们可以使用 CGAN 生成 tag-vectors 以进行对图像自动打标签。使用 AlexNet 在 ImageNet 上训练网络，网络的最后一个 fc 层输出单元为 4096 个，这个输出作为最终的图像表示。为了得到词表示，我们从 YFCC100M 数据集的 metadata 中收集 user-tags，title 和 descriptions 作为文本预料，经过预处理和文本清洗，使用 skip-gram 模型进行训练，得到长度为 200 的词向量，我们忽略词频低于 200 的词，最终得到的词典大小为 247465。生成器 G 生成样本为 tag 特征向量，额外信息 y 为图像特征（上述的 4096 向量）。</p>\n<p>实验使用 MIR Flickr 25000 数据集，使用上述卷积模型和语言模型（AlexNet，skip-gram）分布抽取图像特征和 tag 特征。数据集中前 15000 的样本作为训练集。训练阶段，数据集中没有 tag 的图像被忽略掉，而如果图像拥有多个 tag，那么对于每个 tag 均分别使用一次这个图像。</p>\n<p>evaluation 阶段，对于每个图像生成 100 个样本（tag 特征向量），然后对每个生成样本，使用余弦相似度计算词典中与样本最接近的 20 个词，然后再所有 100 个样本中（我理解的是在 2000 个词中）选择 top 10 最常见的词作为图像的 tags。由于这部分实验没有看到源码，故其余部分的介绍略过，详情可参考原论文。</p>\n<h1 id=\"DCGAN\"><a href=\"#DCGAN\" class=\"headerlink\" title=\"DCGAN\"></a>DCGAN</h1><p>论文 <a href=\"https://arxiv.org/abs/1511.06434\" target=\"_blank\" rel=\"noopener\">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks</a></p>\n<p>这篇文章主要是将卷积层、BN 以及 ReLU 引入 GAN 网络，没有官方代码，但是 github 上有很多实现，都非常简单易懂，例如 <a href=\"https://github.com/carpedm20/DCGAN-tensorflow\" target=\"_blank\" rel=\"noopener\">DCGAN-tensorflow</a>。</p>"},{"title":"数字图像处理（二）","date":"2019-12-07T03:08:24.000Z","mathjax":true,"_content":"\n> 参考教材《数字图像处理》(Gonzalez)\n\n# 1. 空间滤波\n使用空间滤波器（也称空间掩模，核，窗口）直接作用于图像，得到当前位置的像素值，通过平移得到其他位置的像素值。熟悉深度学习中的卷积操作的话，不难理解这个概念。\n<!-- more -->\n## 1.1 平滑空间滤波\n用于模糊和降噪（通常是模糊后再阈值过滤）。例如以下线性滤波器，\n1. 均值滤波\n2. 加权均值滤波\n\n又或者统计排序等非线性滤波器，\n1. 中值滤波\n   \n   中值就是统计里面的排序后位于中间的值。中值滤波提供降噪的同时，对图像的模糊程度要低\n\n## 1.2 锐化空间滤波\n前面平滑处理使用求和平均，求和可看作积分，锐化操作则相反，通过空间微分实现，目的是突出灰度过渡部分。\n对于一维函数 $f(x)$，一阶微分为\n$$\\frac {\\partial f} {\\partial x} = f(x+1) - f(x)$$\n二维函数 $f(x,y)$ 类似，分别沿两个轴微分。二阶微分为，\n$$\\frac {\\partial^2 f} {\\partial x^2} = f'(x) - f'(x-1) = f(x+1) + f(x-1) - 2f(x)$$\n\n以下为一些图像锐化增强的方法。\n### 1.2.1 拉普拉斯算子\n$$\\nabla^2 f = \\frac {\\partial^2 f} {\\partial x^2} + \\frac {\\partial^2 f} {\\partial y^2} $$\n\n又\n$$\\frac {\\partial^2 f} {\\partial x^2} = f(x+1,y)+f(x-1,y) - 2f(x,y)\n\\\\\\\\ \\frac {\\partial^2 f} {\\partial y^2} = f(x,y+1)+f(x,y-1) - 2f(x,y)$$\n故\n$$\\nabla^2 f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$\n\n还可以增加对角线方向的微分项，$f(x \\pm 1,y \\pm 1)$，以及 4 个 $-f(x,y)$，\n\n当然，以上我们还可以将微分乘以 -1，这表示微分的方向反过来，但是其增强效果是跟上面等效的。\n\n通常拉普拉斯算子增强得到将边线和突变点叠加到暗色背景中的图像，所以在叠加原图像，可以恢复背景并保持拉普拉斯锐化结果，如下：\n$$g(x,y)=f(x,y)+c\\left[ \\nabla^2 f(x,y) \\right]$$\n\n使用拉普拉斯算子后的图像可能存在负的像素值，此时可将负值转换为 0，超过 255 的值转换为 255（假设为 8 bit 灰度），但是这种处理方法显然过于草率，一种更好的方法是，记拉普拉斯图像为 `f`，然后\n$$f_m = f-\\min(f)\n\\\\\\\\ f_s=(L-1)[f_m/\\max(f_m)]$$\n如此就能保证像素值位于 $[0,L-1]$ 之间。如果叠加原图像，则可能不需要做如此标定。\n\n\n### 1.2.2 非锐化掩蔽和高提升滤波\n操作步骤：\n1. 模糊原图像\n2. 原图像减模糊图像（差为模板）\n3. 将模板加到原图像上\n   \n$$g_{mask}(x,y) = f(x,y) - \\overline f(x,y)\n\\\\\\\\ g(x,y)=f(x,y) + k \\cdot g_{mask}(x,y)$$\n\n### 1.2.3 梯度\n二维图像 $f(x,y)$ 的梯度为\n$$\\nabla f =\\begin{bmatrix} g_x \\\\\\\\ g_y \\end{bmatrix}= \\begin{bmatrix} \\frac {\\partial f} {\\partial x} \\\\\\\\ \\frac {\\partial f} {\\partial x} \\end{bmatrix}$$\n这是一个二维列向量，幅值为\n$$M(x,y) = \\sqrt {g_x^2 + g_y^2}$$\n\n此为梯度图像，与原图像大小相同。有时候使用绝对值来近似，\n$$M(x,y)=|g_x|+|g_y|$$\n\n将此滤波写成 $3 \\times 3$ 的滤波模板，记一个 $3 \\times 3$ 邻域像素值为，\n$$\\mathbf z=\\begin{bmatrix} z_1 & z_2 & z_3 \\\\ z_4 & z_5 & z_6 \\\\z_7 & z_8 & z_9 \\end{bmatrix}$$\n中心为 $z_5$，一阶微分为\n$$g_x=z_8-z_5, \\quad g_y = z_6-z_5$$\n\n早期的数字图像处理中， Roberts 提出使用交叉差分，\n$$g_x=z_9- z_5, \\quad g_y = z_8-z_6$$\n\n以上 `x,y` 方向哪个水平哪个垂直，在计算梯度幅值时其实是无所谓的，因为滤波模板在旋转 90° 整数倍时是各向同性的。\n\n__sobel 算子__\n\n$\\mathbf w_x=\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}$,  $\\mathbf w_y=\\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}$\n\n于是，\n\n~~$$g_x = \\mathbf w_x \\ast \\mathbf z, \\qquad g_x = \\mathbf w_x \\ast \\mathbf z$$~~\n$$g_x = \\mathbf w_x \\odot \\mathbf z, \\qquad g_x = \\mathbf w_x \\odot \\mathbf z$$\n\nsobel 算子常用于边缘检测。\n\n## 1.3 混合空间增强\n使用前述多种增加方法\n\n## 1.4 基于模糊技术的灰度变换\n\n模糊集合是一个由 `z` 值和相应隶属度函数组成的序对，\n$$A = \\{z, \\mu_A(z)|z \\in Z, \\ \\mu_A(z) \\in (0,1]\\}$$\n其中 $Z$ 为元素 `z` 的取值空间，隶属度函数的值域为 $[0,1]$。\n\n__空集：__ $\\mu_A(z) = 0$\n\n__相等：__ $\\mu_A(z) = \\mu_B(z), \\ \\forall z$\n\n__补集：__ $\\mu_{\\overline A}(z) = 1- \\mu_A(z)$\n\n__子集：__ $\\mu_A(z) \\le \\mu_B(z) \\Rightarrow A \\subseteq B$\n\n__并集：__ $\\mu_U(z)=\\max [\\mu_A(z), \\mu_B(z)]$\n\n__交集：__ $\\mu_I(z) = \\min [\\mu_A(z), \\mu_B(z)]$\n\n","source":"_posts/DIP-2.md","raw":"---\ntitle: 数字图像处理（二）\ndate: 2019-12-07 11:08:24\ntags: DIP\nmathjax: true\n---\n\n> 参考教材《数字图像处理》(Gonzalez)\n\n# 1. 空间滤波\n使用空间滤波器（也称空间掩模，核，窗口）直接作用于图像，得到当前位置的像素值，通过平移得到其他位置的像素值。熟悉深度学习中的卷积操作的话，不难理解这个概念。\n<!-- more -->\n## 1.1 平滑空间滤波\n用于模糊和降噪（通常是模糊后再阈值过滤）。例如以下线性滤波器，\n1. 均值滤波\n2. 加权均值滤波\n\n又或者统计排序等非线性滤波器，\n1. 中值滤波\n   \n   中值就是统计里面的排序后位于中间的值。中值滤波提供降噪的同时，对图像的模糊程度要低\n\n## 1.2 锐化空间滤波\n前面平滑处理使用求和平均，求和可看作积分，锐化操作则相反，通过空间微分实现，目的是突出灰度过渡部分。\n对于一维函数 $f(x)$，一阶微分为\n$$\\frac {\\partial f} {\\partial x} = f(x+1) - f(x)$$\n二维函数 $f(x,y)$ 类似，分别沿两个轴微分。二阶微分为，\n$$\\frac {\\partial^2 f} {\\partial x^2} = f'(x) - f'(x-1) = f(x+1) + f(x-1) - 2f(x)$$\n\n以下为一些图像锐化增强的方法。\n### 1.2.1 拉普拉斯算子\n$$\\nabla^2 f = \\frac {\\partial^2 f} {\\partial x^2} + \\frac {\\partial^2 f} {\\partial y^2} $$\n\n又\n$$\\frac {\\partial^2 f} {\\partial x^2} = f(x+1,y)+f(x-1,y) - 2f(x,y)\n\\\\\\\\ \\frac {\\partial^2 f} {\\partial y^2} = f(x,y+1)+f(x,y-1) - 2f(x,y)$$\n故\n$$\\nabla^2 f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$\n\n还可以增加对角线方向的微分项，$f(x \\pm 1,y \\pm 1)$，以及 4 个 $-f(x,y)$，\n\n当然，以上我们还可以将微分乘以 -1，这表示微分的方向反过来，但是其增强效果是跟上面等效的。\n\n通常拉普拉斯算子增强得到将边线和突变点叠加到暗色背景中的图像，所以在叠加原图像，可以恢复背景并保持拉普拉斯锐化结果，如下：\n$$g(x,y)=f(x,y)+c\\left[ \\nabla^2 f(x,y) \\right]$$\n\n使用拉普拉斯算子后的图像可能存在负的像素值，此时可将负值转换为 0，超过 255 的值转换为 255（假设为 8 bit 灰度），但是这种处理方法显然过于草率，一种更好的方法是，记拉普拉斯图像为 `f`，然后\n$$f_m = f-\\min(f)\n\\\\\\\\ f_s=(L-1)[f_m/\\max(f_m)]$$\n如此就能保证像素值位于 $[0,L-1]$ 之间。如果叠加原图像，则可能不需要做如此标定。\n\n\n### 1.2.2 非锐化掩蔽和高提升滤波\n操作步骤：\n1. 模糊原图像\n2. 原图像减模糊图像（差为模板）\n3. 将模板加到原图像上\n   \n$$g_{mask}(x,y) = f(x,y) - \\overline f(x,y)\n\\\\\\\\ g(x,y)=f(x,y) + k \\cdot g_{mask}(x,y)$$\n\n### 1.2.3 梯度\n二维图像 $f(x,y)$ 的梯度为\n$$\\nabla f =\\begin{bmatrix} g_x \\\\\\\\ g_y \\end{bmatrix}= \\begin{bmatrix} \\frac {\\partial f} {\\partial x} \\\\\\\\ \\frac {\\partial f} {\\partial x} \\end{bmatrix}$$\n这是一个二维列向量，幅值为\n$$M(x,y) = \\sqrt {g_x^2 + g_y^2}$$\n\n此为梯度图像，与原图像大小相同。有时候使用绝对值来近似，\n$$M(x,y)=|g_x|+|g_y|$$\n\n将此滤波写成 $3 \\times 3$ 的滤波模板，记一个 $3 \\times 3$ 邻域像素值为，\n$$\\mathbf z=\\begin{bmatrix} z_1 & z_2 & z_3 \\\\ z_4 & z_5 & z_6 \\\\z_7 & z_8 & z_9 \\end{bmatrix}$$\n中心为 $z_5$，一阶微分为\n$$g_x=z_8-z_5, \\quad g_y = z_6-z_5$$\n\n早期的数字图像处理中， Roberts 提出使用交叉差分，\n$$g_x=z_9- z_5, \\quad g_y = z_8-z_6$$\n\n以上 `x,y` 方向哪个水平哪个垂直，在计算梯度幅值时其实是无所谓的，因为滤波模板在旋转 90° 整数倍时是各向同性的。\n\n__sobel 算子__\n\n$\\mathbf w_x=\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}$,  $\\mathbf w_y=\\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}$\n\n于是，\n\n~~$$g_x = \\mathbf w_x \\ast \\mathbf z, \\qquad g_x = \\mathbf w_x \\ast \\mathbf z$$~~\n$$g_x = \\mathbf w_x \\odot \\mathbf z, \\qquad g_x = \\mathbf w_x \\odot \\mathbf z$$\n\nsobel 算子常用于边缘检测。\n\n## 1.3 混合空间增强\n使用前述多种增加方法\n\n## 1.4 基于模糊技术的灰度变换\n\n模糊集合是一个由 `z` 值和相应隶属度函数组成的序对，\n$$A = \\{z, \\mu_A(z)|z \\in Z, \\ \\mu_A(z) \\in (0,1]\\}$$\n其中 $Z$ 为元素 `z` 的取值空间，隶属度函数的值域为 $[0,1]$。\n\n__空集：__ $\\mu_A(z) = 0$\n\n__相等：__ $\\mu_A(z) = \\mu_B(z), \\ \\forall z$\n\n__补集：__ $\\mu_{\\overline A}(z) = 1- \\mu_A(z)$\n\n__子集：__ $\\mu_A(z) \\le \\mu_B(z) \\Rightarrow A \\subseteq B$\n\n__并集：__ $\\mu_U(z)=\\max [\\mu_A(z), \\mu_B(z)]$\n\n__交集：__ $\\mu_I(z) = \\min [\\mu_A(z), \\mu_B(z)]$\n\n","slug":"DIP-2","published":1,"updated":"2020-04-24T10:37:04.704Z","_id":"ck9dzchvr0002gga6f6le5fyl","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>参考教材《数字图像处理》(Gonzalez)</p>\n</blockquote>\n<h1 id=\"1-空间滤波\"><a href=\"#1-空间滤波\" class=\"headerlink\" title=\"1. 空间滤波\"></a>1. 空间滤波</h1><p>使用空间滤波器（也称空间掩模，核，窗口）直接作用于图像，得到当前位置的像素值，通过平移得到其他位置的像素值。熟悉深度学习中的卷积操作的话，不难理解这个概念。</p>\n<a id=\"more\"></a>\n<h2 id=\"1-1-平滑空间滤波\"><a href=\"#1-1-平滑空间滤波\" class=\"headerlink\" title=\"1.1 平滑空间滤波\"></a>1.1 平滑空间滤波</h2><p>用于模糊和降噪（通常是模糊后再阈值过滤）。例如以下线性滤波器，</p>\n<ol>\n<li>均值滤波</li>\n<li>加权均值滤波</li>\n</ol>\n<p>又或者统计排序等非线性滤波器，</p>\n<ol>\n<li><p>中值滤波</p>\n<p>中值就是统计里面的排序后位于中间的值。中值滤波提供降噪的同时，对图像的模糊程度要低</p>\n</li>\n</ol>\n<h2 id=\"1-2-锐化空间滤波\"><a href=\"#1-2-锐化空间滤波\" class=\"headerlink\" title=\"1.2 锐化空间滤波\"></a>1.2 锐化空间滤波</h2><p>前面平滑处理使用求和平均，求和可看作积分，锐化操作则相反，通过空间微分实现，目的是突出灰度过渡部分。<br>对于一维函数 $f(x)$，一阶微分为<br>$$\\frac {\\partial f} {\\partial x} = f(x+1) - f(x)$$<br>二维函数 $f(x,y)$ 类似，分别沿两个轴微分。二阶微分为，<br>$$\\frac {\\partial^2 f} {\\partial x^2} = f’(x) - f’(x-1) = f(x+1) + f(x-1) - 2f(x)$$</p>\n<p>以下为一些图像锐化增强的方法。</p>\n<h3 id=\"1-2-1-拉普拉斯算子\"><a href=\"#1-2-1-拉普拉斯算子\" class=\"headerlink\" title=\"1.2.1 拉普拉斯算子\"></a>1.2.1 拉普拉斯算子</h3><p>$$\\nabla^2 f = \\frac {\\partial^2 f} {\\partial x^2} + \\frac {\\partial^2 f} {\\partial y^2} $$</p>\n<p>又<br>$$\\frac {\\partial^2 f} {\\partial x^2} = f(x+1,y)+f(x-1,y) - 2f(x,y)<br>\\\\ \\frac {\\partial^2 f} {\\partial y^2} = f(x,y+1)+f(x,y-1) - 2f(x,y)$$<br>故<br>$$\\nabla^2 f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$</p>\n<p>还可以增加对角线方向的微分项，$f(x \\pm 1,y \\pm 1)$，以及 4 个 $-f(x,y)$，</p>\n<p>当然，以上我们还可以将微分乘以 -1，这表示微分的方向反过来，但是其增强效果是跟上面等效的。</p>\n<p>通常拉普拉斯算子增强得到将边线和突变点叠加到暗色背景中的图像，所以在叠加原图像，可以恢复背景并保持拉普拉斯锐化结果，如下：<br>$$g(x,y)=f(x,y)+c\\left[ \\nabla^2 f(x,y) \\right]$$</p>\n<p>使用拉普拉斯算子后的图像可能存在负的像素值，此时可将负值转换为 0，超过 255 的值转换为 255（假设为 8 bit 灰度），但是这种处理方法显然过于草率，一种更好的方法是，记拉普拉斯图像为 <code>f</code>，然后<br>$$f_m = f-\\min(f)<br>\\\\ f_s=(L-1)[f_m/\\max(f_m)]$$<br>如此就能保证像素值位于 $[0,L-1]$ 之间。如果叠加原图像，则可能不需要做如此标定。</p>\n<h3 id=\"1-2-2-非锐化掩蔽和高提升滤波\"><a href=\"#1-2-2-非锐化掩蔽和高提升滤波\" class=\"headerlink\" title=\"1.2.2 非锐化掩蔽和高提升滤波\"></a>1.2.2 非锐化掩蔽和高提升滤波</h3><p>操作步骤：</p>\n<ol>\n<li>模糊原图像</li>\n<li>原图像减模糊图像（差为模板）</li>\n<li>将模板加到原图像上</li>\n</ol>\n<p>$$g_{mask}(x,y) = f(x,y) - \\overline f(x,y)<br>\\\\ g(x,y)=f(x,y) + k \\cdot g_{mask}(x,y)$$</p>\n<h3 id=\"1-2-3-梯度\"><a href=\"#1-2-3-梯度\" class=\"headerlink\" title=\"1.2.3 梯度\"></a>1.2.3 梯度</h3><p>二维图像 $f(x,y)$ 的梯度为<br>$$\\nabla f =\\begin{bmatrix} g_x \\\\ g_y \\end{bmatrix}= \\begin{bmatrix} \\frac {\\partial f} {\\partial x} \\\\ \\frac {\\partial f} {\\partial x} \\end{bmatrix}$$<br>这是一个二维列向量，幅值为<br>$$M(x,y) = \\sqrt {g_x^2 + g_y^2}$$</p>\n<p>此为梯度图像，与原图像大小相同。有时候使用绝对值来近似，<br>$$M(x,y)=|g_x|+|g_y|$$</p>\n<p>将此滤波写成 $3 \\times 3$ 的滤波模板，记一个 $3 \\times 3$ 邻域像素值为，<br>$$\\mathbf z=\\begin{bmatrix} z_1 &amp; z_2 &amp; z_3 \\ z_4 &amp; z_5 &amp; z_6 \\z_7 &amp; z_8 &amp; z_9 \\end{bmatrix}$$<br>中心为 $z_5$，一阶微分为<br>$$g_x=z_8-z_5, \\quad g_y = z_6-z_5$$</p>\n<p>早期的数字图像处理中， Roberts 提出使用交叉差分，<br>$$g_x=z_9- z_5, \\quad g_y = z_8-z_6$$</p>\n<p>以上 <code>x,y</code> 方向哪个水平哪个垂直，在计算梯度幅值时其实是无所谓的，因为滤波模板在旋转 90° 整数倍时是各向同性的。</p>\n<p><strong>sobel 算子</strong></p>\n<p>$\\mathbf w_x=\\begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 1 \\end{bmatrix}$,  $\\mathbf w_y=\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \\end{bmatrix}$</p>\n<p>于是，</p>\n<p><del>$$g_x = \\mathbf w_x \\ast \\mathbf z, \\qquad g_x = \\mathbf w_x \\ast \\mathbf z$$</del><br>$$g_x = \\mathbf w_x \\odot \\mathbf z, \\qquad g_x = \\mathbf w_x \\odot \\mathbf z$$</p>\n<p>sobel 算子常用于边缘检测。</p>\n<h2 id=\"1-3-混合空间增强\"><a href=\"#1-3-混合空间增强\" class=\"headerlink\" title=\"1.3 混合空间增强\"></a>1.3 混合空间增强</h2><p>使用前述多种增加方法</p>\n<h2 id=\"1-4-基于模糊技术的灰度变换\"><a href=\"#1-4-基于模糊技术的灰度变换\" class=\"headerlink\" title=\"1.4 基于模糊技术的灰度变换\"></a>1.4 基于模糊技术的灰度变换</h2><p>模糊集合是一个由 <code>z</code> 值和相应隶属度函数组成的序对，<br>$$A = {z, \\mu_A(z)|z \\in Z, \\ \\mu_A(z) \\in (0,1]}$$<br>其中 $Z$ 为元素 <code>z</code> 的取值空间，隶属度函数的值域为 $[0,1]$。</p>\n<p><strong>空集：</strong> $\\mu_A(z) = 0$</p>\n<p><strong>相等：</strong> $\\mu_A(z) = \\mu_B(z), \\ \\forall z$</p>\n<p><strong>补集：</strong> $\\mu_{\\overline A}(z) = 1- \\mu_A(z)$</p>\n<p><strong>子集：</strong> $\\mu_A(z) \\le \\mu_B(z) \\Rightarrow A \\subseteq B$</p>\n<p><strong>并集：</strong> $\\mu_U(z)=\\max [\\mu_A(z), \\mu_B(z)]$</p>\n<p><strong>交集：</strong> $\\mu_I(z) = \\min [\\mu_A(z), \\mu_B(z)]$</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>参考教材《数字图像处理》(Gonzalez)</p>\n</blockquote>\n<h1 id=\"1-空间滤波\"><a href=\"#1-空间滤波\" class=\"headerlink\" title=\"1. 空间滤波\"></a>1. 空间滤波</h1><p>使用空间滤波器（也称空间掩模，核，窗口）直接作用于图像，得到当前位置的像素值，通过平移得到其他位置的像素值。熟悉深度学习中的卷积操作的话，不难理解这个概念。</p>","more":"<h2 id=\"1-1-平滑空间滤波\"><a href=\"#1-1-平滑空间滤波\" class=\"headerlink\" title=\"1.1 平滑空间滤波\"></a>1.1 平滑空间滤波</h2><p>用于模糊和降噪（通常是模糊后再阈值过滤）。例如以下线性滤波器，</p>\n<ol>\n<li>均值滤波</li>\n<li>加权均值滤波</li>\n</ol>\n<p>又或者统计排序等非线性滤波器，</p>\n<ol>\n<li><p>中值滤波</p>\n<p>中值就是统计里面的排序后位于中间的值。中值滤波提供降噪的同时，对图像的模糊程度要低</p>\n</li>\n</ol>\n<h2 id=\"1-2-锐化空间滤波\"><a href=\"#1-2-锐化空间滤波\" class=\"headerlink\" title=\"1.2 锐化空间滤波\"></a>1.2 锐化空间滤波</h2><p>前面平滑处理使用求和平均，求和可看作积分，锐化操作则相反，通过空间微分实现，目的是突出灰度过渡部分。<br>对于一维函数 $f(x)$，一阶微分为<br>$$\\frac {\\partial f} {\\partial x} = f(x+1) - f(x)$$<br>二维函数 $f(x,y)$ 类似，分别沿两个轴微分。二阶微分为，<br>$$\\frac {\\partial^2 f} {\\partial x^2} = f’(x) - f’(x-1) = f(x+1) + f(x-1) - 2f(x)$$</p>\n<p>以下为一些图像锐化增强的方法。</p>\n<h3 id=\"1-2-1-拉普拉斯算子\"><a href=\"#1-2-1-拉普拉斯算子\" class=\"headerlink\" title=\"1.2.1 拉普拉斯算子\"></a>1.2.1 拉普拉斯算子</h3><p>$$\\nabla^2 f = \\frac {\\partial^2 f} {\\partial x^2} + \\frac {\\partial^2 f} {\\partial y^2} $$</p>\n<p>又<br>$$\\frac {\\partial^2 f} {\\partial x^2} = f(x+1,y)+f(x-1,y) - 2f(x,y)<br>\\\\ \\frac {\\partial^2 f} {\\partial y^2} = f(x,y+1)+f(x,y-1) - 2f(x,y)$$<br>故<br>$$\\nabla^2 f = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$</p>\n<p>还可以增加对角线方向的微分项，$f(x \\pm 1,y \\pm 1)$，以及 4 个 $-f(x,y)$，</p>\n<p>当然，以上我们还可以将微分乘以 -1，这表示微分的方向反过来，但是其增强效果是跟上面等效的。</p>\n<p>通常拉普拉斯算子增强得到将边线和突变点叠加到暗色背景中的图像，所以在叠加原图像，可以恢复背景并保持拉普拉斯锐化结果，如下：<br>$$g(x,y)=f(x,y)+c\\left[ \\nabla^2 f(x,y) \\right]$$</p>\n<p>使用拉普拉斯算子后的图像可能存在负的像素值，此时可将负值转换为 0，超过 255 的值转换为 255（假设为 8 bit 灰度），但是这种处理方法显然过于草率，一种更好的方法是，记拉普拉斯图像为 <code>f</code>，然后<br>$$f_m = f-\\min(f)<br>\\\\ f_s=(L-1)[f_m/\\max(f_m)]$$<br>如此就能保证像素值位于 $[0,L-1]$ 之间。如果叠加原图像，则可能不需要做如此标定。</p>\n<h3 id=\"1-2-2-非锐化掩蔽和高提升滤波\"><a href=\"#1-2-2-非锐化掩蔽和高提升滤波\" class=\"headerlink\" title=\"1.2.2 非锐化掩蔽和高提升滤波\"></a>1.2.2 非锐化掩蔽和高提升滤波</h3><p>操作步骤：</p>\n<ol>\n<li>模糊原图像</li>\n<li>原图像减模糊图像（差为模板）</li>\n<li>将模板加到原图像上</li>\n</ol>\n<p>$$g_{mask}(x,y) = f(x,y) - \\overline f(x,y)<br>\\\\ g(x,y)=f(x,y) + k \\cdot g_{mask}(x,y)$$</p>\n<h3 id=\"1-2-3-梯度\"><a href=\"#1-2-3-梯度\" class=\"headerlink\" title=\"1.2.3 梯度\"></a>1.2.3 梯度</h3><p>二维图像 $f(x,y)$ 的梯度为<br>$$\\nabla f =\\begin{bmatrix} g_x \\\\ g_y \\end{bmatrix}= \\begin{bmatrix} \\frac {\\partial f} {\\partial x} \\\\ \\frac {\\partial f} {\\partial x} \\end{bmatrix}$$<br>这是一个二维列向量，幅值为<br>$$M(x,y) = \\sqrt {g_x^2 + g_y^2}$$</p>\n<p>此为梯度图像，与原图像大小相同。有时候使用绝对值来近似，<br>$$M(x,y)=|g_x|+|g_y|$$</p>\n<p>将此滤波写成 $3 \\times 3$ 的滤波模板，记一个 $3 \\times 3$ 邻域像素值为，<br>$$\\mathbf z=\\begin{bmatrix} z_1 &amp; z_2 &amp; z_3 \\ z_4 &amp; z_5 &amp; z_6 \\z_7 &amp; z_8 &amp; z_9 \\end{bmatrix}$$<br>中心为 $z_5$，一阶微分为<br>$$g_x=z_8-z_5, \\quad g_y = z_6-z_5$$</p>\n<p>早期的数字图像处理中， Roberts 提出使用交叉差分，<br>$$g_x=z_9- z_5, \\quad g_y = z_8-z_6$$</p>\n<p>以上 <code>x,y</code> 方向哪个水平哪个垂直，在计算梯度幅值时其实是无所谓的，因为滤波模板在旋转 90° 整数倍时是各向同性的。</p>\n<p><strong>sobel 算子</strong></p>\n<p>$\\mathbf w_x=\\begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 1 \\end{bmatrix}$,  $\\mathbf w_y=\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \\end{bmatrix}$</p>\n<p>于是，</p>\n<p><del>$$g_x = \\mathbf w_x \\ast \\mathbf z, \\qquad g_x = \\mathbf w_x \\ast \\mathbf z$$</del><br>$$g_x = \\mathbf w_x \\odot \\mathbf z, \\qquad g_x = \\mathbf w_x \\odot \\mathbf z$$</p>\n<p>sobel 算子常用于边缘检测。</p>\n<h2 id=\"1-3-混合空间增强\"><a href=\"#1-3-混合空间增强\" class=\"headerlink\" title=\"1.3 混合空间增强\"></a>1.3 混合空间增强</h2><p>使用前述多种增加方法</p>\n<h2 id=\"1-4-基于模糊技术的灰度变换\"><a href=\"#1-4-基于模糊技术的灰度变换\" class=\"headerlink\" title=\"1.4 基于模糊技术的灰度变换\"></a>1.4 基于模糊技术的灰度变换</h2><p>模糊集合是一个由 <code>z</code> 值和相应隶属度函数组成的序对，<br>$$A = {z, \\mu_A(z)|z \\in Z, \\ \\mu_A(z) \\in (0,1]}$$<br>其中 $Z$ 为元素 <code>z</code> 的取值空间，隶属度函数的值域为 $[0,1]$。</p>\n<p><strong>空集：</strong> $\\mu_A(z) = 0$</p>\n<p><strong>相等：</strong> $\\mu_A(z) = \\mu_B(z), \\ \\forall z$</p>\n<p><strong>补集：</strong> $\\mu_{\\overline A}(z) = 1- \\mu_A(z)$</p>\n<p><strong>子集：</strong> $\\mu_A(z) \\le \\mu_B(z) \\Rightarrow A \\subseteq B$</p>\n<p><strong>并集：</strong> $\\mu_U(z)=\\max [\\mu_A(z), \\mu_B(z)]$</p>\n<p><strong>交集：</strong> $\\mu_I(z) = \\min [\\mu_A(z), \\mu_B(z)]$</p>"},{"title":"DetNet","date":"2019-07-17T02:05:50.000Z","mathjax":true,"_content":"论文 [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n<!-- more -->\n本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。\n\nDetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。\n\n# DetNet\n如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：\n![](/images/DetNet_fig1.png)<center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center>\n\n1. 网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。\n2. 大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。\n3. 小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。\n\nDetNet 经过如下设计可解决以上问题：\n1. 直接为目标检测量身定制 stage 的数量\n2. 即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。\n\n## DetNet 设计\n使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，\n\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1 & 64 \\\\\\\\ 3 \\times 3 & 64 \\\\\\\\ 1 \\times 1 & 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1 & 128 \\\\\\\\ 3 \\times 3 & 128 \\\\\\\\ 1 \\times 1 & 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1 & 256 \\\\\\\\ 3 \\times 3 & 256 \\\\\\\\ 1 \\times 1 & 1024\\end{bmatrix} \\times 6$|\n\n从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：\n![](/images/DetNet_fig2.png)<center>fig 2. DetNet 的结构细节</center>\n\n1. 从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。\n2. 从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。\n3. bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64->128->256->512）。\n\nDetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。\n\n# 实验\n实验和结果分析，略\n","source":"_posts/DetNet.md","raw":"---\ntitle: DetNet\ndate: 2019-07-17 10:05:50\ntags: object detection\nmathjax: true\n---\n论文 [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n<!-- more -->\n本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。\n\nDetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。\n\n# DetNet\n如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：\n![](/images/DetNet_fig1.png)<center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center>\n\n1. 网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。\n2. 大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。\n3. 小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。\n\nDetNet 经过如下设计可解决以上问题：\n1. 直接为目标检测量身定制 stage 的数量\n2. 即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。\n\n## DetNet 设计\n使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，\n\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1 & 64 \\\\\\\\ 3 \\times 3 & 64 \\\\\\\\ 1 \\times 1 & 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1 & 128 \\\\\\\\ 3 \\times 3 & 128 \\\\\\\\ 1 \\times 1 & 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1 & 256 \\\\\\\\ 3 \\times 3 & 256 \\\\\\\\ 1 \\times 1 & 1024\\end{bmatrix} \\times 6$|\n\n从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：\n![](/images/DetNet_fig2.png)<center>fig 2. DetNet 的结构细节</center>\n\n1. 从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。\n2. 从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。\n3. bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64->128->256->512）。\n\nDetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。\n\n# 实验\n实验和结果分析，略\n","slug":"DetNet","published":1,"updated":"2020-04-24T10:36:48.330Z","_id":"ck9dzchwc0005gga6hlb6eg33","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>\n<a id=\"more\"></a>\n<p>本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。</p>\n<p>DetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。</p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p>如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：<br><img src=\"/images/DetNet_fig1.png\" alt=\"\"><center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center></p>\n<ol>\n<li>网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。</li>\n<li>大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。</li>\n<li>小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。</li>\n</ol>\n<p>DetNet 经过如下设计可解决以上问题：</p>\n<ol>\n<li>直接为目标检测量身定制 stage 的数量</li>\n<li>即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。</li>\n</ol>\n<h2 id=\"DetNet-设计\"><a href=\"#DetNet-设计\" class=\"headerlink\" title=\"DetNet 设计\"></a>DetNet 设计</h2><p>使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">ResNet</th>\n<th align=\"center\">output size</th>\n<th align=\"center\">50-layer</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">conv1</td>\n<td align=\"center\">112x112</td>\n<td align=\"center\">7x7,64, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">maxpool</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">3x3, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">conv2_x</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 64 \\\\ 3 \\times 3 &amp; 64 \\\\ 1 \\times 1 &amp; 256\\end{bmatrix} \\times 3$</td>\n</tr>\n<tr>\n<td align=\"center\">conv3_x</td>\n<td align=\"center\">28x28</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 128 \\\\ 3 \\times 3 &amp; 128 \\\\ 1 \\times 1 &amp; 512\\end{bmatrix} \\times 4$</td>\n</tr>\n<tr>\n<td align=\"center\">conv4_x</td>\n<td align=\"center\">14x14</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 256 \\\\ 3 \\times 3 &amp; 256 \\\\ 1 \\times 1 &amp; 1024\\end{bmatrix} \\times 6$</td>\n</tr>\n</tbody></table>\n<p>从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：<br><img src=\"/images/DetNet_fig2.png\" alt=\"\"><center>fig 2. DetNet 的结构细节</center></p>\n<ol>\n<li>从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。</li>\n<li>从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。</li>\n<li>bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64-&gt;128-&gt;256-&gt;512）。</li>\n</ol>\n<p>DetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验和结果分析，略</p>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>","more":"<p>本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。</p>\n<p>DetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。</p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p>如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：<br><img src=\"/images/DetNet_fig1.png\" alt=\"\"><center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center></p>\n<ol>\n<li>网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。</li>\n<li>大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。</li>\n<li>小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。</li>\n</ol>\n<p>DetNet 经过如下设计可解决以上问题：</p>\n<ol>\n<li>直接为目标检测量身定制 stage 的数量</li>\n<li>即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。</li>\n</ol>\n<h2 id=\"DetNet-设计\"><a href=\"#DetNet-设计\" class=\"headerlink\" title=\"DetNet 设计\"></a>DetNet 设计</h2><p>使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">ResNet</th>\n<th align=\"center\">output size</th>\n<th align=\"center\">50-layer</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">conv1</td>\n<td align=\"center\">112x112</td>\n<td align=\"center\">7x7,64, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">maxpool</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">3x3, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">conv2_x</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 64 \\\\ 3 \\times 3 &amp; 64 \\\\ 1 \\times 1 &amp; 256\\end{bmatrix} \\times 3$</td>\n</tr>\n<tr>\n<td align=\"center\">conv3_x</td>\n<td align=\"center\">28x28</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 128 \\\\ 3 \\times 3 &amp; 128 \\\\ 1 \\times 1 &amp; 512\\end{bmatrix} \\times 4$</td>\n</tr>\n<tr>\n<td align=\"center\">conv4_x</td>\n<td align=\"center\">14x14</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 256 \\\\ 3 \\times 3 &amp; 256 \\\\ 1 \\times 1 &amp; 1024\\end{bmatrix} \\times 6$</td>\n</tr>\n</tbody></table>\n<p>从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：<br><img src=\"/images/DetNet_fig2.png\" alt=\"\"><center>fig 2. DetNet 的结构细节</center></p>\n<ol>\n<li>从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。</li>\n<li>从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。</li>\n<li>bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64-&gt;128-&gt;256-&gt;512）。</li>\n</ol>\n<p>DetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验和结果分析，略</p>"},{"title":"RepPoints","date":"2019-07-17T08:05:41.000Z","mathjax":true,"_content":"论文 [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n<!-- more -->\n大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。\n\n如图 1，\n![](/images/RepPoints_fig1.png)\n\nRepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。~~RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。~~\n\n# RepPoints\n## BBox 表示\n目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。\n\n坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n回归损失使用 smooth L1 损失。\n\n## RepPoints\n前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，\n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n其中 n 为采样点数量，本文设置为 9。\n\nRepPoints 的坐标调整可表示为，\n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n其中 $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。\n\n__RepPoints 变换到 gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，\n1. Min-max function  \n   $\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。\n\n__RepPoints 的学习__ 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。\n\n# RPDet\n类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，\n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector) 结构如图 2，\n![](/images/RepPoints_fig2.png)\n\n其中 N 表示 RepPoints 的数量。\n\n采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。\n\n得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。\n\n## 与可变形 RoI pooling 的关系\n可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。\n\n# 实验\n略\n\n# 结论\n本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。","source":"_posts/RepPoints.md","raw":"---\ntitle: RepPoints\ndate: 2019-07-17 16:05:41\ntags: object detection\nmathjax: true\n---\n论文 [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n<!-- more -->\n大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。\n\n如图 1，\n![](/images/RepPoints_fig1.png)\n\nRepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。~~RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。~~\n\n# RepPoints\n## BBox 表示\n目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。\n\n坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n回归损失使用 smooth L1 损失。\n\n## RepPoints\n前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，\n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n其中 n 为采样点数量，本文设置为 9。\n\nRepPoints 的坐标调整可表示为，\n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n其中 $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。\n\n__RepPoints 变换到 gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，\n1. Min-max function  \n   $\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。\n\n__RepPoints 的学习__ 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。\n\n# RPDet\n类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，\n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector) 结构如图 2，\n![](/images/RepPoints_fig2.png)\n\n其中 N 表示 RepPoints 的数量。\n\n采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。\n\n得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。\n\n## 与可变形 RoI pooling 的关系\n可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。\n\n# 实验\n略\n\n# 结论\n本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。","slug":"RepPoints","published":1,"updated":"2020-04-24T10:36:17.184Z","_id":"ck9dzchwk0006gga6e8788jqj","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>\n<a id=\"more\"></a>\n<p>大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。</p>\n<p>如图 1，<br><img src=\"/images/RepPoints_fig1.png\" alt=\"\"></p>\n<p>RepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。<del>RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-表示\"><a href=\"#BBox-表示\" class=\"headerlink\" title=\"BBox 表示\"></a>BBox 表示</h2><p>目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             …<br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p>开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。</p>\n<p>坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p>记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p>回归损失使用 smooth L1 损失。</p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，<br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br>其中 n 为采样点数量，本文设置为 9。</p>\n<p>RepPoints 的坐标调整可表示为，<br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br>其中 ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。</p>\n<p><strong>RepPoints 变换到 gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。</p>\n</li>\n</ol>\n<p><strong>RepPoints 的学习</strong> 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。</p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，</p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2) … -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector) 结构如图 2，<br><img src=\"/images/RepPoints_fig2.png\" alt=\"\"></p>\n<p>其中 N 表示 RepPoints 的数量。</p>\n<p>采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。</p>\n<p>得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。</p>\n<h2 id=\"与可变形-RoI-pooling-的关系\"><a href=\"#与可变形-RoI-pooling-的关系\" class=\"headerlink\" title=\"与可变形 RoI pooling 的关系\"></a>与可变形 RoI pooling 的关系</h2><p>可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>略</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。</p>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>","more":"<p>大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。</p>\n<p>如图 1，<br><img src=\"/images/RepPoints_fig1.png\" alt=\"\"></p>\n<p>RepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。<del>RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-表示\"><a href=\"#BBox-表示\" class=\"headerlink\" title=\"BBox 表示\"></a>BBox 表示</h2><p>目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             …<br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p>开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。</p>\n<p>坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p>记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p>回归损失使用 smooth L1 损失。</p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，<br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br>其中 n 为采样点数量，本文设置为 9。</p>\n<p>RepPoints 的坐标调整可表示为，<br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br>其中 ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。</p>\n<p><strong>RepPoints 变换到 gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。</p>\n</li>\n</ol>\n<p><strong>RepPoints 的学习</strong> 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。</p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，</p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2) … -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector) 结构如图 2，<br><img src=\"/images/RepPoints_fig2.png\" alt=\"\"></p>\n<p>其中 N 表示 RepPoints 的数量。</p>\n<p>采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。</p>\n<p>得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。</p>\n<h2 id=\"与可变形-RoI-pooling-的关系\"><a href=\"#与可变形-RoI-pooling-的关系\" class=\"headerlink\" title=\"与可变形 RoI pooling 的关系\"></a>与可变形 RoI pooling 的关系</h2><p>可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>略</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。</p>"},{"title":"WGAN","date":"2019-07-25T12:24:41.000Z","mathjax":true,"_content":"论文 [Wasserstein GAN](https://arxiv.org/abs/1701.07875)","source":"_posts/WGAN.md","raw":"---\ntitle: WGAN\ndate: 2019-07-25 20:24:41\ntags: GAN\nmathjax: true\n---\n论文 [Wasserstein GAN](https://arxiv.org/abs/1701.07875)","slug":"WGAN","published":1,"updated":"2020-04-20T05:45:52.256Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9dzchwq0009gga6daloav3p","content":"<p>论文 <a href=\"https://arxiv.org/abs/1701.07875\" target=\"_blank\" rel=\"noopener\">Wasserstein GAN</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1701.07875\" target=\"_blank\" rel=\"noopener\">Wasserstein GAN</a></p>\n"},{"title":"CV 中的常用方法总结","date":"2019-06-24T09:33:22.000Z","mathjax":true,"_content":"总结 CV 中的一些概念和操作（并不局限于 CV）。\n<!-- more -->\n# RF\n第 k layer 上的感受野大小为\n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。\n# NMS\n以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：\n1. proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内\n2. 过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）\n3. 按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）\n4. 非极大抑制 NMS\n5. 按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）\n\nNMS 过程如下：\n1. 对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中\n2. 找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，`K.append(I[0])`。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）\n3. 重复过程 2，直到当前 I 为空 \n4. K 中保存了 NMS 之后的 proposals 的列表下标值\n\n# Soft-NMS\nNMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：\n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   * 分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，\n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n   可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，\n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n   在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。\n\n# Deconvolution\n在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。\n(to be continued...)","source":"_posts/cv-mtds.md","raw":"---\ntitle: CV 中的常用方法总结\ndate: 2019-06-24 17:33:22\ntags: CV\nmathjax: true\n---\n总结 CV 中的一些概念和操作（并不局限于 CV）。\n<!-- more -->\n# RF\n第 k layer 上的感受野大小为\n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。\n# NMS\n以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：\n1. proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内\n2. 过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）\n3. 按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）\n4. 非极大抑制 NMS\n5. 按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）\n\nNMS 过程如下：\n1. 对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中\n2. 找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，`K.append(I[0])`。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）\n3. 重复过程 2，直到当前 I 为空 \n4. K 中保存了 NMS 之后的 proposals 的列表下标值\n\n# Soft-NMS\nNMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：\n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   * 分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，\n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n   可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，\n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n   在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。\n\n# Deconvolution\n在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。\n(to be continued...)","slug":"cv-mtds","published":1,"updated":"2020-04-24T10:35:35.883Z","_id":"ck9dzchwx000agga66fq8463i","comments":1,"layout":"post","photos":[],"link":"","content":"<p>总结 CV 中的一些概念和操作（并不局限于 CV）。</p>\n<a id=\"more\"></a>\n<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p>第 k layer 上的感受野大小为<br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p>以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：</p>\n<ol>\n<li>proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内</li>\n<li>过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）</li>\n<li>按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）</li>\n<li>非极大抑制 NMS</li>\n<li>按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）</li>\n</ol>\n<p>NMS 过程如下：</p>\n<ol>\n<li>对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中</li>\n<li>找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，<code>K.append(I[0])</code>。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）</li>\n<li>重复过程 2，直到当前 I 为空 </li>\n<li>K 中保存了 NMS 之后的 proposals 的列表下标值</li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：</p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,…,b_N}, \\mathcal S = {s_1,…,S_N}, N_t, m$</li>\n<li>分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p>现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，</p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br>可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，<br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br>在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。</p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p>在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。<br>(to be continued…)</p>\n","site":{"data":{}},"excerpt":"<p>总结 CV 中的一些概念和操作（并不局限于 CV）。</p>","more":"<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p>第 k layer 上的感受野大小为<br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p>以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：</p>\n<ol>\n<li>proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内</li>\n<li>过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）</li>\n<li>按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）</li>\n<li>非极大抑制 NMS</li>\n<li>按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）</li>\n</ol>\n<p>NMS 过程如下：</p>\n<ol>\n<li>对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中</li>\n<li>找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，<code>K.append(I[0])</code>。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）</li>\n<li>重复过程 2，直到当前 I 为空 </li>\n<li>K 中保存了 NMS 之后的 proposals 的列表下标值</li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：</p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,…,b_N}, \\mathcal S = {s_1,…,S_N}, N_t, m$</li>\n<li>分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p>现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，</p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br>可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，<br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br>在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。</p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p>在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。<br>(to be continued…)</p>"},{"title":"cpp-aux-tools","date":"2019-07-11T11:16:23.000Z","_content":"来看一个 c++ 程序片段\n<!-- more -->\n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n编译\n```\ngcc test.cpp -o test.o\n```\n查看 f 的 low-level assembler 名称（name mangling），\n```\nnm test.o | grep f\n// 输出\n// 000000000000008b T _Z4fi\n```\n逆过程为\n```\nc++filt -n _Z4fi\n// 输出\n// f(int)\n```\n\n反汇编\n```\nobjdump -d test.o\n```\n更多 option 可查看 `objdump --help`。\n\n查看头文件搜索路径\n```\ngcc -xc++ -E -v -\n```\n\n查看链接库依赖\n```\nldd test.o\n```\n\n设置动态库、头文件搜索目录的相关环境变量\n```\nexport LD_LIBRARY_PATH=/xx/xx:$LD_LIBRARY_PATH\nexport CPLUS_INCLUDE_PATH=/xx/xx:$CPLUS_INCLUDE_PATH\n```","source":"_posts/cpp-aux-tools.md","raw":"---\ntitle: cpp-aux-tools\ndate: 2019-07-11 19:16:23\ntags: c++\n---\n来看一个 c++ 程序片段\n<!-- more -->\n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n编译\n```\ngcc test.cpp -o test.o\n```\n查看 f 的 low-level assembler 名称（name mangling），\n```\nnm test.o | grep f\n// 输出\n// 000000000000008b T _Z4fi\n```\n逆过程为\n```\nc++filt -n _Z4fi\n// 输出\n// f(int)\n```\n\n反汇编\n```\nobjdump -d test.o\n```\n更多 option 可查看 `objdump --help`。\n\n查看头文件搜索路径\n```\ngcc -xc++ -E -v -\n```\n\n查看链接库依赖\n```\nldd test.o\n```\n\n设置动态库、头文件搜索目录的相关环境变量\n```\nexport LD_LIBRARY_PATH=/xx/xx:$LD_LIBRARY_PATH\nexport CPLUS_INCLUDE_PATH=/xx/xx:$CPLUS_INCLUDE_PATH\n```","slug":"cpp-aux-tools","published":1,"updated":"2020-07-21T09:12:50.733Z","_id":"ck9dzchx5000dgga67s1f5l05","comments":1,"layout":"post","photos":[],"link":"","content":"<p>来看一个 c++ 程序片段</p>\n<a id=\"more\"></a>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n<p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n<p>查看 f 的 low-level assembler 名称（name mangling），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">&#x2F;&#x2F; 输出</span><br><span class=\"line\">&#x2F;&#x2F; 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n<p>逆过程为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">&#x2F;&#x2F; 输出</span><br><span class=\"line\">&#x2F;&#x2F; f(int)</span><br></pre></td></tr></table></figure>\n\n<p>反汇编</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">objdump -d test.o</span><br></pre></td></tr></table></figure>\n<p>更多 option 可查看 <code>objdump --help</code>。</p>\n<p>查看头文件搜索路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc -xc++ -E -v -</span><br></pre></td></tr></table></figure>\n\n<p>查看链接库依赖</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ldd test.o</span><br></pre></td></tr></table></figure>\n\n<p>设置动态库、头文件搜索目录的相关环境变量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export LD_LIBRARY_PATH&#x3D;&#x2F;xx&#x2F;xx:$LD_LIBRARY_PATH</span><br><span class=\"line\">export CPLUS_INCLUDE_PATH&#x3D;&#x2F;xx&#x2F;xx:$CPLUS_INCLUDE_PATH</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>来看一个 c++ 程序片段</p>","more":"<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n<p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n<p>查看 f 的 low-level assembler 名称（name mangling），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">&#x2F;&#x2F; 输出</span><br><span class=\"line\">&#x2F;&#x2F; 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n<p>逆过程为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">&#x2F;&#x2F; 输出</span><br><span class=\"line\">&#x2F;&#x2F; f(int)</span><br></pre></td></tr></table></figure>\n\n<p>反汇编</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">objdump -d test.o</span><br></pre></td></tr></table></figure>\n<p>更多 option 可查看 <code>objdump --help</code>。</p>\n<p>查看头文件搜索路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc -xc++ -E -v -</span><br></pre></td></tr></table></figure>\n\n<p>查看链接库依赖</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ldd test.o</span><br></pre></td></tr></table></figure>\n\n<p>设置动态库、头文件搜索目录的相关环境变量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export LD_LIBRARY_PATH&#x3D;&#x2F;xx&#x2F;xx:$LD_LIBRARY_PATH</span><br><span class=\"line\">export CPLUS_INCLUDE_PATH&#x3D;&#x2F;xx&#x2F;xx:$CPLUS_INCLUDE_PATH</span><br></pre></td></tr></table></figure>"},{"title":"loss","date":"2019-07-16T09:32:26.000Z","mathjax":true,"_content":"总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）\n<!-- more -->\n# Cross-Entropy Loss\n交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,...,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,...,t_C)$，其中\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n于是交叉熵损失为\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in \\{0,1\\}$，\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n为方便起见，记\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n于是，\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\n如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n其中 $(1-p_t)^{\\gamma}$ 称为调节因子。\n\nFocal loss 的性质：\n1. $p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大\n2. $p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小\n\n# MSE\n均方误差为\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。\n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n缺点：当 $|Y_i-\\hat Y_i|>1$ 时，误差会被放大很多，导致模型训练不稳定。\n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n缺点：当 $|Y_i-\\hat Y_i|<1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。\n## Smooth L1 Loss\n结合以上两点，得到 Smooth L1 损失，\n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。\n\n","source":"_posts/loss.md","raw":"---\ntitle: loss\ndate: 2019-07-16 17:32:26\ntags: CV\nmathjax: true\n---\n总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）\n<!-- more -->\n# Cross-Entropy Loss\n交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,...,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,...,t_C)$，其中\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n于是交叉熵损失为\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in \\{0,1\\}$，\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n为方便起见，记\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n于是，\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\n如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n其中 $(1-p_t)^{\\gamma}$ 称为调节因子。\n\nFocal loss 的性质：\n1. $p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大\n2. $p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小\n\n# MSE\n均方误差为\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。\n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n缺点：当 $|Y_i-\\hat Y_i|>1$ 时，误差会被放大很多，导致模型训练不稳定。\n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n缺点：当 $|Y_i-\\hat Y_i|<1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。\n## Smooth L1 Loss\n结合以上两点，得到 Smooth L1 损失，\n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。\n\n","slug":"loss","published":1,"updated":"2020-04-24T10:36:36.189Z","_id":"ck9dzchx8000fgga62vo6ej0e","comments":1,"layout":"post","photos":[],"link":"","content":"<p>总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）</p>\n<a id=\"more\"></a>\n<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,…,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,…,t_C)$，其中<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br>于是交叉熵损失为<br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p>特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in {0,1}$，<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br>为方便起见，记<br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br>于是，<br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br>其中 $(1-p_t)^{\\gamma}$ 称为调节因子。</p>\n<p>Focal loss 的性质：</p>\n<ol>\n<li>$p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大</li>\n<li>$p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小</li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p>均方误差为<br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br>表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。</p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br>缺点：当 $|Y_i-\\hat Y_i|&gt;1$ 时，误差会被放大很多，导致模型训练不稳定。</p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br>缺点：当 $|Y_i-\\hat Y_i|&lt;1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。</p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p>结合以上两点，得到 Smooth L1 损失，<br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p>机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。</p>\n","site":{"data":{}},"excerpt":"<p>总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）</p>","more":"<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,…,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,…,t_C)$，其中<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br>于是交叉熵损失为<br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p>特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in {0,1}$，<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br>为方便起见，记<br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br>于是，<br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br>其中 $(1-p_t)^{\\gamma}$ 称为调节因子。</p>\n<p>Focal loss 的性质：</p>\n<ol>\n<li>$p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大</li>\n<li>$p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小</li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p>均方误差为<br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br>表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。</p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br>缺点：当 $|Y_i-\\hat Y_i|&gt;1$ 时，误差会被放大很多，导致模型训练不稳定。</p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br>缺点：当 $|Y_i-\\hat Y_i|&lt;1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。</p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p>结合以上两点，得到 Smooth L1 损失，<br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p>机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。</p>"},{"title":"BBox-Reg-Uncertainty","date":"2019-06-28T01:23:16.000Z","mathjax":true,"_content":"论文：[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n<!-- more -->\n# 简介\n大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center>\n\n当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 __回归 loss 较大__。\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center>\n\n为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 __回归 loss 较小__。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为\n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n这是离散分布的情况，对于连续分布则为，\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n注意，此时 p(x) 和 q(x) 表示概率密度而非概率。\n显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。\n\n使用 KL loss 学习 bbox 回归有以下三个优点：\n1. 可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小\n2. 学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题\n3. 学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用\n\n\n\n我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。\n\n# 方法\n## BBox 参数化\n基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，\n![](/images/BBox-reg_fig3.png)\n\n令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ 为：\n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为$\\{x_1,y_1,x_2,y_2\\}$。\n\n我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，\n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。\n\n~~（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）~~\n\ngt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，\n$$P_D(x)=\\delta(x-x_g)$$\n其中 $x_g$ 是 gt box 位置 x 坐标。\n\n## 使用 KL Loss 的 BBox 回归\n最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，\n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。\n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。\n\n如图 4，\n![](/images/BBox-reg_fig4.png)\n\n当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，\n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。\n\n当 $|x_g - x_e| > 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，\n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。\n\n## Variance Voting\n得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU>0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，\n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes\n\n$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量\n\n$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵\n\n$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整\n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 [CV 中的常用方法总结](/2019/06/24/cv-mtds)。\n\n算法 1 中，对于当前得分最高的检测 box，记为 b， $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU>0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：\n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。\n\n# 实验\n实验介绍及结果分析略，请阅读原文以获得更详细的信息。\n\n# 结论\n大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。\n\n从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。","source":"_posts/BBox-Reg-Uncertainty.md","raw":"---\ntitle: BBox-Reg-Uncertainty\ndate: 2019-06-28 09:23:16\ntags: object detection\nmathjax: true\n---\n论文：[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n<!-- more -->\n# 简介\n大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center>\n\n当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 __回归 loss 较大__。\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center>\n\n为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 __回归 loss 较小__。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为\n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n这是离散分布的情况，对于连续分布则为，\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n注意，此时 p(x) 和 q(x) 表示概率密度而非概率。\n显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。\n\n使用 KL loss 学习 bbox 回归有以下三个优点：\n1. 可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小\n2. 学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题\n3. 学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用\n\n\n\n我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。\n\n# 方法\n## BBox 参数化\n基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，\n![](/images/BBox-reg_fig3.png)\n\n令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ 为：\n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为$\\{x_1,y_1,x_2,y_2\\}$。\n\n我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，\n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。\n\n~~（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）~~\n\ngt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，\n$$P_D(x)=\\delta(x-x_g)$$\n其中 $x_g$ 是 gt box 位置 x 坐标。\n\n## 使用 KL Loss 的 BBox 回归\n最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，\n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。\n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。\n\n如图 4，\n![](/images/BBox-reg_fig4.png)\n\n当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，\n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。\n\n当 $|x_g - x_e| > 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，\n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。\n\n## Variance Voting\n得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU>0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，\n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes\n\n$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量\n\n$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵\n\n$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整\n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 [CV 中的常用方法总结](/2019/06/24/cv-mtds)。\n\n算法 1 中，对于当前得分最高的检测 box，记为 b， $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU>0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：\n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。\n\n# 实验\n实验介绍及结果分析略，请阅读原文以获得更详细的信息。\n\n# 结论\n大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。\n\n从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。","slug":"BBox-Reg-Uncertainty","published":1,"updated":"2020-04-24T10:35:17.063Z","_id":"ck9dzcio3000pgga64t9ca2tj","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，<br><img src=\"/images/BBox-reg_fig1.png\" alt=\"\"> <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center></p>\n<p>当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 <strong>回归 loss 较大</strong>。<br><img src=\"/images/BBox-reg_fig2.png\" alt=\"\"> <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center></p>\n<p>为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 <strong>回归 loss 较小</strong>。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为<br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br>这是离散分布的情况，对于连续分布则为，<br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br>注意，此时 p(x) 和 q(x) 表示概率密度而非概率。<br>显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。</p>\n<p>使用 KL loss 学习 bbox 回归有以下三个优点：</p>\n<ol>\n<li>可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小</li>\n<li>学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题</li>\n<li>学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用</li>\n</ol>\n<p>我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"BBox-参数化\"><a href=\"#BBox-参数化\" class=\"headerlink\" title=\"BBox 参数化\"></a>BBox 参数化</h2><p>基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，<br><img src=\"/images/BBox-reg_fig3.png\" alt=\"\"></p>\n<p>令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 ${t_i| i=x_1,y_1,x_2,y_2}$ 为：</p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p>其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为${x_1,y_1,x_2,y_2}$。</p>\n<p>我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，<br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br>其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。</p>\n<p><del>（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）</del></p>\n<p>gt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，<br>$$P_D(x)=\\delta(x-x_g)$$<br>其中 $x_g$ 是 gt box 位置 x 坐标。</p>\n<h2 id=\"使用-KL-Loss-的-BBox-回归\"><a href=\"#使用-KL-Loss-的-BBox-回归\" class=\"headerlink\" title=\"使用 KL Loss 的 BBox 回归\"></a>使用 KL Loss 的 BBox 回归</h2><p>最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，<br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br>其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。<br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。</p>\n<p>如图 4，<br><img src=\"/images/BBox-reg_fig4.png\" alt=\"\"></p>\n<p>当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br>当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br>损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，<br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p>由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br>反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。</p>\n<p>当 $|x_g - x_e| &gt; 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，<br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p>根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p>得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU&gt;0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，</p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes</p>\n<p>$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量</p>\n<p>$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵</p>\n<p>$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整</p>\n<p>$\\mathcal B={b_1,…,b_N}, \\ \\mathcal S={s_1,…,s_N}, \\ \\mathcal C={\\sigma_1^2,…,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color='gree'>$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p>我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 <a href=\"/2019/06/24/cv-mtds\">CV 中的常用方法总结</a>。</p>\n<p>算法 1 中，对于当前得分最高的检测 box，记为 b， ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU&gt;0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：<br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br>上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍及结果分析略，请阅读原文以获得更详细的信息。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。</p>\n<p>从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。</p>\n","site":{"data":{}},"excerpt":"<p>论文：<a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，<br><img src=\"/images/BBox-reg_fig1.png\" alt=\"\"> <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center></p>\n<p>当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 <strong>回归 loss 较大</strong>。<br><img src=\"/images/BBox-reg_fig2.png\" alt=\"\"> <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center></p>\n<p>为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 <strong>回归 loss 较小</strong>。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为<br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br>这是离散分布的情况，对于连续分布则为，<br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br>注意，此时 p(x) 和 q(x) 表示概率密度而非概率。<br>显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。</p>\n<p>使用 KL loss 学习 bbox 回归有以下三个优点：</p>\n<ol>\n<li>可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小</li>\n<li>学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题</li>\n<li>学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用</li>\n</ol>\n<p>我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"BBox-参数化\"><a href=\"#BBox-参数化\" class=\"headerlink\" title=\"BBox 参数化\"></a>BBox 参数化</h2><p>基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，<br><img src=\"/images/BBox-reg_fig3.png\" alt=\"\"></p>\n<p>令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 ${t_i| i=x_1,y_1,x_2,y_2}$ 为：</p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p>其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为${x_1,y_1,x_2,y_2}$。</p>\n<p>我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，<br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br>其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。</p>\n<p><del>（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）</del></p>\n<p>gt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，<br>$$P_D(x)=\\delta(x-x_g)$$<br>其中 $x_g$ 是 gt box 位置 x 坐标。</p>\n<h2 id=\"使用-KL-Loss-的-BBox-回归\"><a href=\"#使用-KL-Loss-的-BBox-回归\" class=\"headerlink\" title=\"使用 KL Loss 的 BBox 回归\"></a>使用 KL Loss 的 BBox 回归</h2><p>最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，<br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br>其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。<br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。</p>\n<p>如图 4，<br><img src=\"/images/BBox-reg_fig4.png\" alt=\"\"></p>\n<p>当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br>当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br>损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，<br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p>由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br>反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。</p>\n<p>当 $|x_g - x_e| &gt; 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，<br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p>根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p>得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU&gt;0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，</p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes</p>\n<p>$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量</p>\n<p>$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵</p>\n<p>$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整</p>\n<p>$\\mathcal B={b_1,…,b_N}, \\ \\mathcal S={s_1,…,s_N}, \\ \\mathcal C={\\sigma_1^2,…,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color='gree'>$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p>我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 <a href=\"/2019/06/24/cv-mtds\">CV 中的常用方法总结</a>。</p>\n<p>算法 1 中，对于当前得分最高的检测 box，记为 b， ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU&gt;0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：<br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br>上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍及结果分析略，请阅读原文以获得更详细的信息。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。</p>\n<p>从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。</p>"},{"title":"DSOD","date":"2019-07-08T01:14:40.000Z","mathjax":true,"_content":"论文 [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n<!-- more -->\n# Introduction\n近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：\n1. 有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。\n2. 学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。\n3. 领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。\n\n于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。\n\n# DSOD\n## 结构\nDSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。\n![](/images/DSOD_fig1.png)<center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center>\n\n整个 DSOD 网络结构如表 1 所示。\n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD 结构 </center>\n\nDSOD 设计原则如下：\n### 无 Proposal\n我们调查了如下三类 SOTA 的目标检测器：\n1. R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。\n2. Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals\n3. YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。\n\n发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。\n\n于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。\n\n### 深度监督\n中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。\n\n### Stem Block\nStem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。\n\n### 密集预测结构\n图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。\n\n在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。\n\n# Experiments\n实验部分略，可阅读原文以获取详细信息。\n\n# Conclusion\n提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。","source":"_posts/DSOD.md","raw":"---\ntitle: DSOD\ndate: 2019-07-08 09:14:40\ntags: object detection\nmathjax: true\n---\n论文 [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n<!-- more -->\n# Introduction\n近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：\n1. 有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。\n2. 学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。\n3. 领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。\n\n于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。\n\n# DSOD\n## 结构\nDSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。\n![](/images/DSOD_fig1.png)<center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center>\n\n整个 DSOD 网络结构如表 1 所示。\n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD 结构 </center>\n\nDSOD 设计原则如下：\n### 无 Proposal\n我们调查了如下三类 SOTA 的目标检测器：\n1. R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。\n2. Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals\n3. YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。\n\n发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。\n\n于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。\n\n### 深度监督\n中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。\n\n### Stem Block\nStem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。\n\n### 密集预测结构\n图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。\n\n在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。\n\n# Experiments\n实验部分略，可阅读原文以获取详细信息。\n\n# Conclusion\n提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。","slug":"DSOD","published":1,"updated":"2020-04-24T10:37:33.289Z","_id":"ck9dzcioe000rgga66xyghfdd","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>\n<a id=\"more\"></a>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：</p>\n<ol>\n<li>有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。</li>\n<li>学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。</li>\n<li>领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。</li>\n</ol>\n<p>于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。</p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>DSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。<br><img src=\"/images/DSOD_fig1.png\" alt=\"\"><center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center></p>\n<p>整个 DSOD 网络结构如表 1 所示。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD 结构 </center>\n\n<p>DSOD 设计原则如下：</p>\n<h3 id=\"无-Proposal\"><a href=\"#无-Proposal\" class=\"headerlink\" title=\"无 Proposal\"></a>无 Proposal</h3><p>我们调查了如下三类 SOTA 的目标检测器：</p>\n<ol>\n<li>R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。</li>\n<li>Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals</li>\n<li>YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。</li>\n</ol>\n<p>发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。</p>\n<p>于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。</p>\n<h3 id=\"深度监督\"><a href=\"#深度监督\" class=\"headerlink\" title=\"深度监督\"></a>深度监督</h3><p>中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。</p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。</p>\n<h3 id=\"密集预测结构\"><a href=\"#密集预测结构\" class=\"headerlink\" title=\"密集预测结构\"></a>密集预测结构</h3><p>图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。</p>\n<p>在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，可阅读原文以获取详细信息。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。</p>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：</p>\n<ol>\n<li>有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。</li>\n<li>学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。</li>\n<li>领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。</li>\n</ol>\n<p>于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。</p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>DSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。<br><img src=\"/images/DSOD_fig1.png\" alt=\"\"><center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center></p>\n<p>整个 DSOD 网络结构如表 1 所示。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD 结构 </center>\n\n<p>DSOD 设计原则如下：</p>\n<h3 id=\"无-Proposal\"><a href=\"#无-Proposal\" class=\"headerlink\" title=\"无 Proposal\"></a>无 Proposal</h3><p>我们调查了如下三类 SOTA 的目标检测器：</p>\n<ol>\n<li>R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。</li>\n<li>Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals</li>\n<li>YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。</li>\n</ol>\n<p>发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。</p>\n<p>于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。</p>\n<h3 id=\"深度监督\"><a href=\"#深度监督\" class=\"headerlink\" title=\"深度监督\"></a>深度监督</h3><p>中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。</p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。</p>\n<h3 id=\"密集预测结构\"><a href=\"#密集预测结构\" class=\"headerlink\" title=\"密集预测结构\"></a>密集预测结构</h3><p>图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。</p>\n<p>在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，可阅读原文以获取详细信息。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。</p>"},{"title":"数字图像处理（一）","date":"2019-12-05T03:37:39.000Z","mathjax":true,"_content":"\n> 根据教材《数字图像处理》(Gonzalez)\n# 1. 坐标变换\n## 1.1 仿射变换\n特点：\n1. 平直性：原图中的直线段在仿射变换后的图像中依然是直线段。\n2. 平行性：原图中平行的两个线段在仿射变换后的图中依然是平行的\n<!-- more -->   \n一般形式：\n$$(x,y)=\\mathbf T[(v,w)]$$\n其中 $(v,w)$ 是原图中的某点坐标，$(x,y)$ 是变换后图像中对应点的坐标\n$$\\begin{bmatrix}x & y & 1 \\end{bmatrix}=\\begin{bmatrix}v & w & 1 \\end{bmatrix}\\mathbf T=\\begin{bmatrix}v & w & 1 \\end{bmatrix}\\begin{bmatrix}t_{11} & t_{12} & 0 \\\\ t_{21} & t_{22} & 0 \\\\ t_{31} & t_{32} & 1\\end{bmatrix}$$\n\n$$\\begin{cases}x=t_{11} v + t_{21} w + t_{31} \\\\\ny=t_{12} v + t_{22} w + t_{32} \\end{cases} \\qquad(1)$$\n\n反向映射：\n$$(v,w)=\\mathbf T^{-1} [(x,y)]$$\n\n### 1.1.1 恒等变换\n$$\\mathbf T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0& 1&0 \\\\ 0&0&1\\end{bmatrix}$$\n### 1.1.2 尺度变换\n$$\\mathbf T = \\begin{bmatrix} c_x & 0 & 0 \\\\ 0& c_y&0 \\\\ 0&0&1\\end{bmatrix}$$\nx 方向变为原来的 $c_x$ 倍，y 方向变为原来的 $c_y$ 倍。\n### 1.1.3 旋转变换\n$$\\mathbf T = \\begin{bmatrix} cos \\theta & sin \\theta & 0 \\\\ -sin \\theta& cos \\theta&0 \\\\ 0&0&1\\end{bmatrix}$$\n其中 $\\theta$ 为（图像绕左上角顶点）顺时针旋转角度。\n### 1.1.4 平移变换\n$$\\mathbf T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0& 1&0 \\\\ t_x&t_y&1\\end{bmatrix}$$\n### 1.1.5 偏移变换\n又称错切（shear）变换，分水平错切和垂直错切两种情况。水平错切：将每一点水平移动，移动长度和该点的纵坐标成比例，\n$$(v,w) \\rightarrow (v+mw,w)$$\n$m>0$ 时向右移动，$m<0$ 时向左移动。\n类似地可以得到垂直方向变换为，\n$$(v,w) \\rightarrow (v, vn+w)$$\n于是错切变换矩阵为\n$$\\mathbf T=\\begin{bmatrix} 1 & n & 0 \\\\ m & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$$\n\n## 1.2 透视变换\n透视变换是二维 $(x,y)$ 到三维 $(x,y,z)$，然后再到二维 $(x',y')$ 的映射，所以也称为投影变换。\n\n事实上，二维可看作三维上的一个平面 $(x,y,1)$，前面的仿射变换则是将这个平面上的点 $(x,y,1)$ 经过 $3 \\times 3$ 变换矩阵后依然位于这个平面内 $(x',y',1)$，而透视变换则是先变换到三维空间任意点上，然后从三维空间在变换到 $(x',y',1)$，故不难想象变换矩阵具有如下形式，\n$$[x,y,1]=[v,w,1]T=[v,w,1]\\begin{bmatrix} t_{11} & t_{12} & t_{13} \\\\t_{21} & t_{22} & t_{23} \\\\t_{31} & t_{32} & t_{33} \\end{bmatrix}$$\n其中 $t_{13}v+t_{23}w+t_{33}=1$。从变换矩阵也可以看出，仿射变换是一种特殊的透视变换。\n\n正如下一节将要讲到的，需要知道输入图像和输出图像中分别 4 个约束点，才能确定透视变换的矩阵参数，这是因为 4 个约束点提供 8 个方程，加上自身的约束方程，共 9 个方程从而解出 9 个参数，可参考 opencv 的 python 教程中关于[透视变换的例子](https://docs.opencv.org/4.1.2/da/d6e/tutorial_py_geometric_transformations.html)。\n\n\n# 2. 图像配准\n有时候知道输入图像和经过某种变换后的输出图像，要求变换函数，这就是图像配准。主要方法是使用约束点（控制点），根据输入图像和输出图像上的一组 n 个约束点来估计变换函数，例如已知是仿射变换，那么只需要一组 3 个约束点（6个方程解6个参数），根据式 $(1)$ 即可求出参数；如果变换函数模型是双线性模型，那么有\n$$x=c_1v+ c_2w+c_3vw + c_4 \\\\ y=c_5v+c_6w+c_7vw+c_8$$\n即，需要 4 个约束点得到变换函数参数。\n\n还有其他更加复杂的策略，略。\n\n# 3. 图像内插\n假设要求变换后图像中 $(x,y)$ 的像素值，根据反向映射得到原图对应点位置为 $(i+u,j+v)$（整数+小数的形式），$0\\le u,v < 1$\n## 3.1 最近邻内插\n对 $u,v$ 分别采用四舍五入，得到原来图像对应像素位置 $(\\lfloor i+u+\\frac 1 2\\rfloor,\\lfloor j+v+\\frac 1 2 \\rfloor)$\n## 3.2 双线性内插\n使用 4 个最近邻即 $(i,j), (i+1,j),(i,j+1),(i+1,j+1)$ 根据下式进行确定，\n$$f(i+u,j+v)=(1-u)(1-v)f(i,j)+(1-u)v f(i,j+1)+u(1-v)f(i+1,j)+uvf(i+1,j+1)$$\n\n## 3.2 双三次内插\n使用 16 个最近邻点，一种确定权重因子的方法是使用 BiCubic 函数，\n$$W(x)=\\begin{cases} (a+2)|x|^3 - (a+3)|x|^2+1 & |x| \\le 1 \\\\ a|x|^3-5a|x|^2+8a|x|-4a & 1 <|x|<2 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n其中， $x$ 是水平（垂直）方向上的距离，a 通常取 $a=-0.5$。\n\n为了表示方便，坐标使用 $(x,y)$ 而非前面的 $(i,j)$ 表示。图像上目标点 $(x,y)$ 的 $4\\times4$ 邻域的点 $(x_i,y_j), \\ i,j=0,1,2,3$，按下式进行双三次插值，\n$$f(x,y)=\\sum_{i=0}^3\\sum_{j=0}^3 f(x_i,y_j) W(x-x_i) W(y-y_j)$$\n\n# 4. 灰度变换\n这里仅讨论部分灰度变换的方法。\n## 4.1 直方图均衡\n假设灰度范围为 $[0,L-1]$，变换形式为\n$$s=T(r)$$\n表示将灰度 `r` 变换为 `s`。\n这里假定变换函数单调增（若非特别说明，不一定是严格单调增），否则灰度变换后产生认为缺陷。\n\n我们可以将 `r` 看作输入图像的表示灰度的随机变量，`s` 为输出图像的表示灰度的随机变量，令 $p_r(r), \\ p_s(s)$ 分别表示 `r` 和 `s` 的概率密度函数，那么\n$$p_s(s)=p_r(r) \\frac {dr} {ds} \\qquad(2)$$\n\n其中 $T(r)=s, \\ r_1 \\le r \\le r_2$，非严格单调增时 $r_1<r_2$，严格单调增时 $r_1=r_2$。\n\n使用如下变换函数来实现直方图均衡，\n$$s=T(r)=(L-1)\\int_0^r p_r(w) dw \\qquad(3)$$\n其中 积分项是归一化的，所以增加 $(L-1)$ 因子将灰度放大到合适的范围内。\n\n对 (3) 式求导，\n$$\\frac {ds} {dr} = \\frac {dT(r)} {dr} = (L-1) \\frac d {dr} \\left[\\int_0^r p_r(w)dw \\right] = (L-1)p_r(r) \\qquad(4)$$\n\n将 (4) 式代入 (2) 式，\n$$p_s(s)=p_r(r) \\frac {dr} {ds} = \\frac 1 {L-1} \\qquad(5)$$\n\n这说明， 随机变量 `s` 是均匀分布的。\n\n数字图像处理中常采用离散化处理，此时变换函数为，\n$$s=T(r)=(L-1) \\sum_{0}^r p_r(r) \\qquad(3')$$\n其中 $p_r(r)=n_r/N$，$n_r$ 为灰度 `r` 的像素数量，`N` 为图像总像素数量。\n此时 \n$$1=s - (s-1) = (L-1)\\sum_0^{r_2}p_r(r) - (L-1)\\sum_0^{r_1}p_r(r)=(L-1)\\sum_{r_1}^{r_2}p_r(r)$$\n于是，\n$$p_s(s) = \\sum_{r_1}^{r_2}p_r(r)=\\frac 1 {L-1}$$\n注意，上式中由于 $r$ 取不到 $r_1$，故可能会出现近似等于 $1/(L-1)$。\n\n## 4.2 直方图匹配\n指定输出图像的直方图的形状。令 `r`  和 `z` 分别表示输入和输出图像的灰度随机变量，`s` 为一个均匀分布的灰度随机变量，有\n$$s=T(r)=(L-1)\\int_0^r p_r(r) dw \\qquad(6)\n\\\\\\\\ s=G(z)=(L-1)\\int_0^z p_z(t) dt \\qquad(7)$$\n\n于是，\n$$z=G^{-1}(s)$$\n此时要求变换函数 $G$ 是严格单调增，否则不存在反函数。\n\n由于输入图像给定，容易计算出 $p_r(r)$，而指定输出图像的直方图形状，即 $p_z(z)$ 已知，根据 (7) 式可得 $G$（每个 z 到 s 的映射），于是直方图匹配步骤如下：\n1. 计算 $p_r(r)$，然后计算 `s`\n2. 根据式 (7) 计算 $G(z)$\n3. 求反函数 $G^{-1}(s)$，根据 `s` 计算出 `z`\n\n实际处理过程为：对输入图像做直方图均衡得到 `s` 灰度的图，然后对此图中每个像素执行反映射 $z=G^{-1}(s)$（s 到 z 的映射），得到最终输出图像。\n\n## 4.3 局部直方图处理\n对每个位置的邻域计算直方图，然后进行均衡化或者应用其他匹配变换函数，用于修改这个邻域中心的灰度，然后平移邻域（一个像素位置或者移到另一个非重叠区域）。\n\n## 4.4 直方图统计\n令 `r` 为表示灰度的离散随机变量，归一化的直方图为 $p(r)$，应用概率相关的知识，`r` 的 n 阶（中心）矩定义为，\n$$\\mu_n(r)=\\sum_{i=0}^{L-1} (r_i-m)^n p(r_i)$$\n其中 `m` 是期望（或称平均灰度）$m=\\sum_0^{L-1} r_i p(r_i)$\n\n二阶矩（灰度方差）为\n$$u_2(r)=\\sum_{i=0}^{L-1}(r_i-m)^2 p(r_i)$$\n通常用 $\\sigma^2$ 表示，单独列出二阶矩是因为比较重要。\n\n实际给定一个图像时，可以直接计算样本均值和样本方差，无需计算直方图，\n$$m=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y)\n\\\\\\\\ \\sigma^2=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} [f(x,y)-m]^2$$\n以上方差计算有时候使用 $MN-1$ 作为分母，以获得一个无偏估计，实际中，使用 $MN$ 作为分母的偏差可忽略不计。\n\n考虑点 $(x,y)$ 的邻域 $S_{xy}$，仿照上面全局统计量可写出局部均值和局部方差如下，\n$$m_{s_{xy}}=\\sum_{i=0}^{L-1}r_i p_{s_{xy}} (r_i)\n\\\\\\\\ \\sigma_{s_{xy}}^2 = \\sum _{i=0}^{L-1} (r_i - m_{S_{xy}})^2 p_{s_{xy}}(r_i)$$\n\n有时候只需要对图像上暗区进行增强，亮区保持不变，所以需要先判断当前邻域属于暗区还是亮区，记全局均值为 $m_G$，如果 $m_{s_{xy}} \\le k_0 m_G$，其中 $0< k_0 < 1.0$，那么属于暗区，对其进行局部增强。\n\n如果想增强低对比度的局部区域，那么判断方法为 $\\sigma_{s_{xy}} \\le k_2 \\sigma_G$。当然一般还会设置一个阈值下限，例如对比度为 0 的恒定区域，其实是没必要增强的，所以 $k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G$。对于满足增强条件的点而言，增强操作可以是将像素值乘以一个常数 `E` ，这样这个点相对于图像上其他不需要增强的点，像素得以增大（或减小）。\n\n总结增强方法如下，\n$$g(x,y)=\\begin{cases} E \\cdot f(x,y) & m_{s_{xy}} \\le k_0 m_G, \\ k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G \n\\\\\\\\ f(x,y) & \\text{otherwise} \\end{cases}$$","source":"_posts/DIP-1.md","raw":"---\ntitle: 数字图像处理（一）\ndate: 2019-12-05 11:37:39\ntags: DIP\nmathjax: true\n---\n\n> 根据教材《数字图像处理》(Gonzalez)\n# 1. 坐标变换\n## 1.1 仿射变换\n特点：\n1. 平直性：原图中的直线段在仿射变换后的图像中依然是直线段。\n2. 平行性：原图中平行的两个线段在仿射变换后的图中依然是平行的\n<!-- more -->   \n一般形式：\n$$(x,y)=\\mathbf T[(v,w)]$$\n其中 $(v,w)$ 是原图中的某点坐标，$(x,y)$ 是变换后图像中对应点的坐标\n$$\\begin{bmatrix}x & y & 1 \\end{bmatrix}=\\begin{bmatrix}v & w & 1 \\end{bmatrix}\\mathbf T=\\begin{bmatrix}v & w & 1 \\end{bmatrix}\\begin{bmatrix}t_{11} & t_{12} & 0 \\\\ t_{21} & t_{22} & 0 \\\\ t_{31} & t_{32} & 1\\end{bmatrix}$$\n\n$$\\begin{cases}x=t_{11} v + t_{21} w + t_{31} \\\\\ny=t_{12} v + t_{22} w + t_{32} \\end{cases} \\qquad(1)$$\n\n反向映射：\n$$(v,w)=\\mathbf T^{-1} [(x,y)]$$\n\n### 1.1.1 恒等变换\n$$\\mathbf T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0& 1&0 \\\\ 0&0&1\\end{bmatrix}$$\n### 1.1.2 尺度变换\n$$\\mathbf T = \\begin{bmatrix} c_x & 0 & 0 \\\\ 0& c_y&0 \\\\ 0&0&1\\end{bmatrix}$$\nx 方向变为原来的 $c_x$ 倍，y 方向变为原来的 $c_y$ 倍。\n### 1.1.3 旋转变换\n$$\\mathbf T = \\begin{bmatrix} cos \\theta & sin \\theta & 0 \\\\ -sin \\theta& cos \\theta&0 \\\\ 0&0&1\\end{bmatrix}$$\n其中 $\\theta$ 为（图像绕左上角顶点）顺时针旋转角度。\n### 1.1.4 平移变换\n$$\\mathbf T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0& 1&0 \\\\ t_x&t_y&1\\end{bmatrix}$$\n### 1.1.5 偏移变换\n又称错切（shear）变换，分水平错切和垂直错切两种情况。水平错切：将每一点水平移动，移动长度和该点的纵坐标成比例，\n$$(v,w) \\rightarrow (v+mw,w)$$\n$m>0$ 时向右移动，$m<0$ 时向左移动。\n类似地可以得到垂直方向变换为，\n$$(v,w) \\rightarrow (v, vn+w)$$\n于是错切变换矩阵为\n$$\\mathbf T=\\begin{bmatrix} 1 & n & 0 \\\\ m & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$$\n\n## 1.2 透视变换\n透视变换是二维 $(x,y)$ 到三维 $(x,y,z)$，然后再到二维 $(x',y')$ 的映射，所以也称为投影变换。\n\n事实上，二维可看作三维上的一个平面 $(x,y,1)$，前面的仿射变换则是将这个平面上的点 $(x,y,1)$ 经过 $3 \\times 3$ 变换矩阵后依然位于这个平面内 $(x',y',1)$，而透视变换则是先变换到三维空间任意点上，然后从三维空间在变换到 $(x',y',1)$，故不难想象变换矩阵具有如下形式，\n$$[x,y,1]=[v,w,1]T=[v,w,1]\\begin{bmatrix} t_{11} & t_{12} & t_{13} \\\\t_{21} & t_{22} & t_{23} \\\\t_{31} & t_{32} & t_{33} \\end{bmatrix}$$\n其中 $t_{13}v+t_{23}w+t_{33}=1$。从变换矩阵也可以看出，仿射变换是一种特殊的透视变换。\n\n正如下一节将要讲到的，需要知道输入图像和输出图像中分别 4 个约束点，才能确定透视变换的矩阵参数，这是因为 4 个约束点提供 8 个方程，加上自身的约束方程，共 9 个方程从而解出 9 个参数，可参考 opencv 的 python 教程中关于[透视变换的例子](https://docs.opencv.org/4.1.2/da/d6e/tutorial_py_geometric_transformations.html)。\n\n\n# 2. 图像配准\n有时候知道输入图像和经过某种变换后的输出图像，要求变换函数，这就是图像配准。主要方法是使用约束点（控制点），根据输入图像和输出图像上的一组 n 个约束点来估计变换函数，例如已知是仿射变换，那么只需要一组 3 个约束点（6个方程解6个参数），根据式 $(1)$ 即可求出参数；如果变换函数模型是双线性模型，那么有\n$$x=c_1v+ c_2w+c_3vw + c_4 \\\\ y=c_5v+c_6w+c_7vw+c_8$$\n即，需要 4 个约束点得到变换函数参数。\n\n还有其他更加复杂的策略，略。\n\n# 3. 图像内插\n假设要求变换后图像中 $(x,y)$ 的像素值，根据反向映射得到原图对应点位置为 $(i+u,j+v)$（整数+小数的形式），$0\\le u,v < 1$\n## 3.1 最近邻内插\n对 $u,v$ 分别采用四舍五入，得到原来图像对应像素位置 $(\\lfloor i+u+\\frac 1 2\\rfloor,\\lfloor j+v+\\frac 1 2 \\rfloor)$\n## 3.2 双线性内插\n使用 4 个最近邻即 $(i,j), (i+1,j),(i,j+1),(i+1,j+1)$ 根据下式进行确定，\n$$f(i+u,j+v)=(1-u)(1-v)f(i,j)+(1-u)v f(i,j+1)+u(1-v)f(i+1,j)+uvf(i+1,j+1)$$\n\n## 3.2 双三次内插\n使用 16 个最近邻点，一种确定权重因子的方法是使用 BiCubic 函数，\n$$W(x)=\\begin{cases} (a+2)|x|^3 - (a+3)|x|^2+1 & |x| \\le 1 \\\\ a|x|^3-5a|x|^2+8a|x|-4a & 1 <|x|<2 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n其中， $x$ 是水平（垂直）方向上的距离，a 通常取 $a=-0.5$。\n\n为了表示方便，坐标使用 $(x,y)$ 而非前面的 $(i,j)$ 表示。图像上目标点 $(x,y)$ 的 $4\\times4$ 邻域的点 $(x_i,y_j), \\ i,j=0,1,2,3$，按下式进行双三次插值，\n$$f(x,y)=\\sum_{i=0}^3\\sum_{j=0}^3 f(x_i,y_j) W(x-x_i) W(y-y_j)$$\n\n# 4. 灰度变换\n这里仅讨论部分灰度变换的方法。\n## 4.1 直方图均衡\n假设灰度范围为 $[0,L-1]$，变换形式为\n$$s=T(r)$$\n表示将灰度 `r` 变换为 `s`。\n这里假定变换函数单调增（若非特别说明，不一定是严格单调增），否则灰度变换后产生认为缺陷。\n\n我们可以将 `r` 看作输入图像的表示灰度的随机变量，`s` 为输出图像的表示灰度的随机变量，令 $p_r(r), \\ p_s(s)$ 分别表示 `r` 和 `s` 的概率密度函数，那么\n$$p_s(s)=p_r(r) \\frac {dr} {ds} \\qquad(2)$$\n\n其中 $T(r)=s, \\ r_1 \\le r \\le r_2$，非严格单调增时 $r_1<r_2$，严格单调增时 $r_1=r_2$。\n\n使用如下变换函数来实现直方图均衡，\n$$s=T(r)=(L-1)\\int_0^r p_r(w) dw \\qquad(3)$$\n其中 积分项是归一化的，所以增加 $(L-1)$ 因子将灰度放大到合适的范围内。\n\n对 (3) 式求导，\n$$\\frac {ds} {dr} = \\frac {dT(r)} {dr} = (L-1) \\frac d {dr} \\left[\\int_0^r p_r(w)dw \\right] = (L-1)p_r(r) \\qquad(4)$$\n\n将 (4) 式代入 (2) 式，\n$$p_s(s)=p_r(r) \\frac {dr} {ds} = \\frac 1 {L-1} \\qquad(5)$$\n\n这说明， 随机变量 `s` 是均匀分布的。\n\n数字图像处理中常采用离散化处理，此时变换函数为，\n$$s=T(r)=(L-1) \\sum_{0}^r p_r(r) \\qquad(3')$$\n其中 $p_r(r)=n_r/N$，$n_r$ 为灰度 `r` 的像素数量，`N` 为图像总像素数量。\n此时 \n$$1=s - (s-1) = (L-1)\\sum_0^{r_2}p_r(r) - (L-1)\\sum_0^{r_1}p_r(r)=(L-1)\\sum_{r_1}^{r_2}p_r(r)$$\n于是，\n$$p_s(s) = \\sum_{r_1}^{r_2}p_r(r)=\\frac 1 {L-1}$$\n注意，上式中由于 $r$ 取不到 $r_1$，故可能会出现近似等于 $1/(L-1)$。\n\n## 4.2 直方图匹配\n指定输出图像的直方图的形状。令 `r`  和 `z` 分别表示输入和输出图像的灰度随机变量，`s` 为一个均匀分布的灰度随机变量，有\n$$s=T(r)=(L-1)\\int_0^r p_r(r) dw \\qquad(6)\n\\\\\\\\ s=G(z)=(L-1)\\int_0^z p_z(t) dt \\qquad(7)$$\n\n于是，\n$$z=G^{-1}(s)$$\n此时要求变换函数 $G$ 是严格单调增，否则不存在反函数。\n\n由于输入图像给定，容易计算出 $p_r(r)$，而指定输出图像的直方图形状，即 $p_z(z)$ 已知，根据 (7) 式可得 $G$（每个 z 到 s 的映射），于是直方图匹配步骤如下：\n1. 计算 $p_r(r)$，然后计算 `s`\n2. 根据式 (7) 计算 $G(z)$\n3. 求反函数 $G^{-1}(s)$，根据 `s` 计算出 `z`\n\n实际处理过程为：对输入图像做直方图均衡得到 `s` 灰度的图，然后对此图中每个像素执行反映射 $z=G^{-1}(s)$（s 到 z 的映射），得到最终输出图像。\n\n## 4.3 局部直方图处理\n对每个位置的邻域计算直方图，然后进行均衡化或者应用其他匹配变换函数，用于修改这个邻域中心的灰度，然后平移邻域（一个像素位置或者移到另一个非重叠区域）。\n\n## 4.4 直方图统计\n令 `r` 为表示灰度的离散随机变量，归一化的直方图为 $p(r)$，应用概率相关的知识，`r` 的 n 阶（中心）矩定义为，\n$$\\mu_n(r)=\\sum_{i=0}^{L-1} (r_i-m)^n p(r_i)$$\n其中 `m` 是期望（或称平均灰度）$m=\\sum_0^{L-1} r_i p(r_i)$\n\n二阶矩（灰度方差）为\n$$u_2(r)=\\sum_{i=0}^{L-1}(r_i-m)^2 p(r_i)$$\n通常用 $\\sigma^2$ 表示，单独列出二阶矩是因为比较重要。\n\n实际给定一个图像时，可以直接计算样本均值和样本方差，无需计算直方图，\n$$m=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y)\n\\\\\\\\ \\sigma^2=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} [f(x,y)-m]^2$$\n以上方差计算有时候使用 $MN-1$ 作为分母，以获得一个无偏估计，实际中，使用 $MN$ 作为分母的偏差可忽略不计。\n\n考虑点 $(x,y)$ 的邻域 $S_{xy}$，仿照上面全局统计量可写出局部均值和局部方差如下，\n$$m_{s_{xy}}=\\sum_{i=0}^{L-1}r_i p_{s_{xy}} (r_i)\n\\\\\\\\ \\sigma_{s_{xy}}^2 = \\sum _{i=0}^{L-1} (r_i - m_{S_{xy}})^2 p_{s_{xy}}(r_i)$$\n\n有时候只需要对图像上暗区进行增强，亮区保持不变，所以需要先判断当前邻域属于暗区还是亮区，记全局均值为 $m_G$，如果 $m_{s_{xy}} \\le k_0 m_G$，其中 $0< k_0 < 1.0$，那么属于暗区，对其进行局部增强。\n\n如果想增强低对比度的局部区域，那么判断方法为 $\\sigma_{s_{xy}} \\le k_2 \\sigma_G$。当然一般还会设置一个阈值下限，例如对比度为 0 的恒定区域，其实是没必要增强的，所以 $k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G$。对于满足增强条件的点而言，增强操作可以是将像素值乘以一个常数 `E` ，这样这个点相对于图像上其他不需要增强的点，像素得以增大（或减小）。\n\n总结增强方法如下，\n$$g(x,y)=\\begin{cases} E \\cdot f(x,y) & m_{s_{xy}} \\le k_0 m_G, \\ k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G \n\\\\\\\\ f(x,y) & \\text{otherwise} \\end{cases}$$","slug":"DIP-1","published":1,"updated":"2020-04-24T10:36:56.327Z","_id":"ck9dzcip5000ugga6gc1t0r5m","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>根据教材《数字图像处理》(Gonzalez)</p>\n</blockquote>\n<h1 id=\"1-坐标变换\"><a href=\"#1-坐标变换\" class=\"headerlink\" title=\"1. 坐标变换\"></a>1. 坐标变换</h1><h2 id=\"1-1-仿射变换\"><a href=\"#1-1-仿射变换\" class=\"headerlink\" title=\"1.1 仿射变换\"></a>1.1 仿射变换</h2><p>特点：</p>\n<ol>\n<li>平直性：原图中的直线段在仿射变换后的图像中依然是直线段。</li>\n<li>平行性：原图中平行的两个线段在仿射变换后的图中依然是平行的<a id=\"more\"></a>   \n一般形式：<br>$$(x,y)=\\mathbf T[(v,w)]$$<br>其中 $(v,w)$ 是原图中的某点坐标，$(x,y)$ 是变换后图像中对应点的坐标<br>$$\\begin{bmatrix}x &amp; y &amp; 1 \\end{bmatrix}=\\begin{bmatrix}v &amp; w &amp; 1 \\end{bmatrix}\\mathbf T=\\begin{bmatrix}v &amp; w &amp; 1 \\end{bmatrix}\\begin{bmatrix}t_{11} &amp; t_{12} &amp; 0 \\ t_{21} &amp; t_{22} &amp; 0 \\ t_{31} &amp; t_{32} &amp; 1\\end{bmatrix}$$</li>\n</ol>\n<p>$$\\begin{cases}x=t_{11} v + t_{21} w + t_{31} \\<br>y=t_{12} v + t_{22} w + t_{32} \\end{cases} \\qquad(1)$$</p>\n<p>反向映射：<br>$$(v,w)=\\mathbf T^{-1} [(x,y)]$$</p>\n<h3 id=\"1-1-1-恒等变换\"><a href=\"#1-1-1-恒等变换\" class=\"headerlink\" title=\"1.1.1 恒等变换\"></a>1.1.1 恒等变换</h3><p>$$\\mathbf T = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0&amp; 1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$</p>\n<h3 id=\"1-1-2-尺度变换\"><a href=\"#1-1-2-尺度变换\" class=\"headerlink\" title=\"1.1.2 尺度变换\"></a>1.1.2 尺度变换</h3><p>$$\\mathbf T = \\begin{bmatrix} c_x &amp; 0 &amp; 0 \\ 0&amp; c_y&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$<br>x 方向变为原来的 $c_x$ 倍，y 方向变为原来的 $c_y$ 倍。</p>\n<h3 id=\"1-1-3-旋转变换\"><a href=\"#1-1-3-旋转变换\" class=\"headerlink\" title=\"1.1.3 旋转变换\"></a>1.1.3 旋转变换</h3><p>$$\\mathbf T = \\begin{bmatrix} cos \\theta &amp; sin \\theta &amp; 0 \\ -sin \\theta&amp; cos \\theta&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$<br>其中 $\\theta$ 为（图像绕左上角顶点）顺时针旋转角度。</p>\n<h3 id=\"1-1-4-平移变换\"><a href=\"#1-1-4-平移变换\" class=\"headerlink\" title=\"1.1.4 平移变换\"></a>1.1.4 平移变换</h3><p>$$\\mathbf T = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0&amp; 1&amp;0 \\ t_x&amp;t_y&amp;1\\end{bmatrix}$$</p>\n<h3 id=\"1-1-5-偏移变换\"><a href=\"#1-1-5-偏移变换\" class=\"headerlink\" title=\"1.1.5 偏移变换\"></a>1.1.5 偏移变换</h3><p>又称错切（shear）变换，分水平错切和垂直错切两种情况。水平错切：将每一点水平移动，移动长度和该点的纵坐标成比例，<br>$$(v,w) \\rightarrow (v+mw,w)$$<br>$m&gt;0$ 时向右移动，$m&lt;0$ 时向左移动。<br>类似地可以得到垂直方向变换为，<br>$$(v,w) \\rightarrow (v, vn+w)$$<br>于是错切变换矩阵为<br>$$\\mathbf T=\\begin{bmatrix} 1 &amp; n &amp; 0 \\ m &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\\end{bmatrix}$$</p>\n<h2 id=\"1-2-透视变换\"><a href=\"#1-2-透视变换\" class=\"headerlink\" title=\"1.2 透视变换\"></a>1.2 透视变换</h2><p>透视变换是二维 $(x,y)$ 到三维 $(x,y,z)$，然后再到二维 $(x’,y’)$ 的映射，所以也称为投影变换。</p>\n<p>事实上，二维可看作三维上的一个平面 $(x,y,1)$，前面的仿射变换则是将这个平面上的点 $(x,y,1)$ 经过 $3 \\times 3$ 变换矩阵后依然位于这个平面内 $(x’,y’,1)$，而透视变换则是先变换到三维空间任意点上，然后从三维空间在变换到 $(x’,y’,1)$，故不难想象变换矩阵具有如下形式，<br>$$[x,y,1]=[v,w,1]T=[v,w,1]\\begin{bmatrix} t_{11} &amp; t_{12} &amp; t_{13} \\t_{21} &amp; t_{22} &amp; t_{23} \\t_{31} &amp; t_{32} &amp; t_{33} \\end{bmatrix}$$<br>其中 $t_{13}v+t_{23}w+t_{33}=1$。从变换矩阵也可以看出，仿射变换是一种特殊的透视变换。</p>\n<p>正如下一节将要讲到的，需要知道输入图像和输出图像中分别 4 个约束点，才能确定透视变换的矩阵参数，这是因为 4 个约束点提供 8 个方程，加上自身的约束方程，共 9 个方程从而解出 9 个参数，可参考 opencv 的 python 教程中关于<a href=\"https://docs.opencv.org/4.1.2/da/d6e/tutorial_py_geometric_transformations.html\" target=\"_blank\" rel=\"noopener\">透视变换的例子</a>。</p>\n<h1 id=\"2-图像配准\"><a href=\"#2-图像配准\" class=\"headerlink\" title=\"2. 图像配准\"></a>2. 图像配准</h1><p>有时候知道输入图像和经过某种变换后的输出图像，要求变换函数，这就是图像配准。主要方法是使用约束点（控制点），根据输入图像和输出图像上的一组 n 个约束点来估计变换函数，例如已知是仿射变换，那么只需要一组 3 个约束点（6个方程解6个参数），根据式 $(1)$ 即可求出参数；如果变换函数模型是双线性模型，那么有<br>$$x=c_1v+ c_2w+c_3vw + c_4 \\ y=c_5v+c_6w+c_7vw+c_8$$<br>即，需要 4 个约束点得到变换函数参数。</p>\n<p>还有其他更加复杂的策略，略。</p>\n<h1 id=\"3-图像内插\"><a href=\"#3-图像内插\" class=\"headerlink\" title=\"3. 图像内插\"></a>3. 图像内插</h1><p>假设要求变换后图像中 $(x,y)$ 的像素值，根据反向映射得到原图对应点位置为 $(i+u,j+v)$（整数+小数的形式），$0\\le u,v &lt; 1$</p>\n<h2 id=\"3-1-最近邻内插\"><a href=\"#3-1-最近邻内插\" class=\"headerlink\" title=\"3.1 最近邻内插\"></a>3.1 最近邻内插</h2><p>对 $u,v$ 分别采用四舍五入，得到原来图像对应像素位置 $(\\lfloor i+u+\\frac 1 2\\rfloor,\\lfloor j+v+\\frac 1 2 \\rfloor)$</p>\n<h2 id=\"3-2-双线性内插\"><a href=\"#3-2-双线性内插\" class=\"headerlink\" title=\"3.2 双线性内插\"></a>3.2 双线性内插</h2><p>使用 4 个最近邻即 $(i,j), (i+1,j),(i,j+1),(i+1,j+1)$ 根据下式进行确定，<br>$$f(i+u,j+v)=(1-u)(1-v)f(i,j)+(1-u)v f(i,j+1)+u(1-v)f(i+1,j)+uvf(i+1,j+1)$$</p>\n<h2 id=\"3-2-双三次内插\"><a href=\"#3-2-双三次内插\" class=\"headerlink\" title=\"3.2 双三次内插\"></a>3.2 双三次内插</h2><p>使用 16 个最近邻点，一种确定权重因子的方法是使用 BiCubic 函数，<br>$$W(x)=\\begin{cases} (a+2)|x|^3 - (a+3)|x|^2+1 &amp; |x| \\le 1 \\ a|x|^3-5a|x|^2+8a|x|-4a &amp; 1 &lt;|x|&lt;2 \\ 0 &amp; \\text{otherwise} \\end{cases}$$<br>其中， $x$ 是水平（垂直）方向上的距离，a 通常取 $a=-0.5$。</p>\n<p>为了表示方便，坐标使用 $(x,y)$ 而非前面的 $(i,j)$ 表示。图像上目标点 $(x,y)$ 的 $4\\times4$ 邻域的点 $(x_i,y_j), \\ i,j=0,1,2,3$，按下式进行双三次插值，<br>$$f(x,y)=\\sum_{i=0}^3\\sum_{j=0}^3 f(x_i,y_j) W(x-x_i) W(y-y_j)$$</p>\n<h1 id=\"4-灰度变换\"><a href=\"#4-灰度变换\" class=\"headerlink\" title=\"4. 灰度变换\"></a>4. 灰度变换</h1><p>这里仅讨论部分灰度变换的方法。</p>\n<h2 id=\"4-1-直方图均衡\"><a href=\"#4-1-直方图均衡\" class=\"headerlink\" title=\"4.1 直方图均衡\"></a>4.1 直方图均衡</h2><p>假设灰度范围为 $[0,L-1]$，变换形式为<br>$$s=T(r)$$<br>表示将灰度 <code>r</code> 变换为 <code>s</code>。<br>这里假定变换函数单调增（若非特别说明，不一定是严格单调增），否则灰度变换后产生认为缺陷。</p>\n<p>我们可以将 <code>r</code> 看作输入图像的表示灰度的随机变量，<code>s</code> 为输出图像的表示灰度的随机变量，令 $p_r(r), \\ p_s(s)$ 分别表示 <code>r</code> 和 <code>s</code> 的概率密度函数，那么<br>$$p_s(s)=p_r(r) \\frac {dr} {ds} \\qquad(2)$$</p>\n<p>其中 $T(r)=s, \\ r_1 \\le r \\le r_2$，非严格单调增时 $r_1&lt;r_2$，严格单调增时 $r_1=r_2$。</p>\n<p>使用如下变换函数来实现直方图均衡，<br>$$s=T(r)=(L-1)\\int_0^r p_r(w) dw \\qquad(3)$$<br>其中 积分项是归一化的，所以增加 $(L-1)$ 因子将灰度放大到合适的范围内。</p>\n<p>对 (3) 式求导，<br>$$\\frac {ds} {dr} = \\frac {dT(r)} {dr} = (L-1) \\frac d {dr} \\left[\\int_0^r p_r(w)dw \\right] = (L-1)p_r(r) \\qquad(4)$$</p>\n<p>将 (4) 式代入 (2) 式，<br>$$p_s(s)=p_r(r) \\frac {dr} {ds} = \\frac 1 {L-1} \\qquad(5)$$</p>\n<p>这说明， 随机变量 <code>s</code> 是均匀分布的。</p>\n<p>数字图像处理中常采用离散化处理，此时变换函数为，<br>$$s=T(r)=(L-1) \\sum_{0}^r p_r(r) \\qquad(3’)$$<br>其中 $p_r(r)=n_r/N$，$n_r$ 为灰度 <code>r</code> 的像素数量，<code>N</code> 为图像总像素数量。<br>此时<br>$$1=s - (s-1) = (L-1)\\sum_0^{r_2}p_r(r) - (L-1)\\sum_0^{r_1}p_r(r)=(L-1)\\sum_{r_1}^{r_2}p_r(r)$$<br>于是，<br>$$p_s(s) = \\sum_{r_1}^{r_2}p_r(r)=\\frac 1 {L-1}$$<br>注意，上式中由于 $r$ 取不到 $r_1$，故可能会出现近似等于 $1/(L-1)$。</p>\n<h2 id=\"4-2-直方图匹配\"><a href=\"#4-2-直方图匹配\" class=\"headerlink\" title=\"4.2 直方图匹配\"></a>4.2 直方图匹配</h2><p>指定输出图像的直方图的形状。令 <code>r</code>  和 <code>z</code> 分别表示输入和输出图像的灰度随机变量，<code>s</code> 为一个均匀分布的灰度随机变量，有<br>$$s=T(r)=(L-1)\\int_0^r p_r(r) dw \\qquad(6)<br>\\\\ s=G(z)=(L-1)\\int_0^z p_z(t) dt \\qquad(7)$$</p>\n<p>于是，<br>$$z=G^{-1}(s)$$<br>此时要求变换函数 $G$ 是严格单调增，否则不存在反函数。</p>\n<p>由于输入图像给定，容易计算出 $p_r(r)$，而指定输出图像的直方图形状，即 $p_z(z)$ 已知，根据 (7) 式可得 $G$（每个 z 到 s 的映射），于是直方图匹配步骤如下：</p>\n<ol>\n<li>计算 $p_r(r)$，然后计算 <code>s</code></li>\n<li>根据式 (7) 计算 $G(z)$</li>\n<li>求反函数 $G^{-1}(s)$，根据 <code>s</code> 计算出 <code>z</code></li>\n</ol>\n<p>实际处理过程为：对输入图像做直方图均衡得到 <code>s</code> 灰度的图，然后对此图中每个像素执行反映射 $z=G^{-1}(s)$（s 到 z 的映射），得到最终输出图像。</p>\n<h2 id=\"4-3-局部直方图处理\"><a href=\"#4-3-局部直方图处理\" class=\"headerlink\" title=\"4.3 局部直方图处理\"></a>4.3 局部直方图处理</h2><p>对每个位置的邻域计算直方图，然后进行均衡化或者应用其他匹配变换函数，用于修改这个邻域中心的灰度，然后平移邻域（一个像素位置或者移到另一个非重叠区域）。</p>\n<h2 id=\"4-4-直方图统计\"><a href=\"#4-4-直方图统计\" class=\"headerlink\" title=\"4.4 直方图统计\"></a>4.4 直方图统计</h2><p>令 <code>r</code> 为表示灰度的离散随机变量，归一化的直方图为 $p(r)$，应用概率相关的知识，<code>r</code> 的 n 阶（中心）矩定义为，<br>$$\\mu_n(r)=\\sum_{i=0}^{L-1} (r_i-m)^n p(r_i)$$<br>其中 <code>m</code> 是期望（或称平均灰度）$m=\\sum_0^{L-1} r_i p(r_i)$</p>\n<p>二阶矩（灰度方差）为<br>$$u_2(r)=\\sum_{i=0}^{L-1}(r_i-m)^2 p(r_i)$$<br>通常用 $\\sigma^2$ 表示，单独列出二阶矩是因为比较重要。</p>\n<p>实际给定一个图像时，可以直接计算样本均值和样本方差，无需计算直方图，<br>$$m=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y)<br>\\\\ \\sigma^2=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} [f(x,y)-m]^2$$<br>以上方差计算有时候使用 $MN-1$ 作为分母，以获得一个无偏估计，实际中，使用 $MN$ 作为分母的偏差可忽略不计。</p>\n<p>考虑点 $(x,y)$ 的邻域 $S_{xy}$，仿照上面全局统计量可写出局部均值和局部方差如下，<br>$$m_{s_{xy}}=\\sum_{i=0}^{L-1}r_i p_{s_{xy}} (r_i)<br>\\\\ \\sigma_{s_{xy}}^2 = \\sum <em>{i=0}^{L-1} (r_i - m</em>{S_{xy}})^2 p_{s_{xy}}(r_i)$$</p>\n<p>有时候只需要对图像上暗区进行增强，亮区保持不变，所以需要先判断当前邻域属于暗区还是亮区，记全局均值为 $m_G$，如果 $m_{s_{xy}} \\le k_0 m_G$，其中 $0&lt; k_0 &lt; 1.0$，那么属于暗区，对其进行局部增强。</p>\n<p>如果想增强低对比度的局部区域，那么判断方法为 $\\sigma_{s_{xy}} \\le k_2 \\sigma_G$。当然一般还会设置一个阈值下限，例如对比度为 0 的恒定区域，其实是没必要增强的，所以 $k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G$。对于满足增强条件的点而言，增强操作可以是将像素值乘以一个常数 <code>E</code> ，这样这个点相对于图像上其他不需要增强的点，像素得以增大（或减小）。</p>\n<p>总结增强方法如下，<br>$$g(x,y)=\\begin{cases} E \\cdot f(x,y) &amp; m_{s_{xy}} \\le k_0 m_G, \\ k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G<br>\\\\ f(x,y) &amp; \\text{otherwise} \\end{cases}$$</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>根据教材《数字图像处理》(Gonzalez)</p>\n</blockquote>\n<h1 id=\"1-坐标变换\"><a href=\"#1-坐标变换\" class=\"headerlink\" title=\"1. 坐标变换\"></a>1. 坐标变换</h1><h2 id=\"1-1-仿射变换\"><a href=\"#1-1-仿射变换\" class=\"headerlink\" title=\"1.1 仿射变换\"></a>1.1 仿射变换</h2><p>特点：</p>\n<ol>\n<li>平直性：原图中的直线段在仿射变换后的图像中依然是直线段。</li>\n<li>平行性：原图中平行的两个线段在仿射变换后的图中依然是平行的","more":"一般形式：<br>$$(x,y)=\\mathbf T[(v,w)]$$<br>其中 $(v,w)$ 是原图中的某点坐标，$(x,y)$ 是变换后图像中对应点的坐标<br>$$\\begin{bmatrix}x &amp; y &amp; 1 \\end{bmatrix}=\\begin{bmatrix}v &amp; w &amp; 1 \\end{bmatrix}\\mathbf T=\\begin{bmatrix}v &amp; w &amp; 1 \\end{bmatrix}\\begin{bmatrix}t_{11} &amp; t_{12} &amp; 0 \\ t_{21} &amp; t_{22} &amp; 0 \\ t_{31} &amp; t_{32} &amp; 1\\end{bmatrix}$$</li>\n</ol>\n<p>$$\\begin{cases}x=t_{11} v + t_{21} w + t_{31} \\<br>y=t_{12} v + t_{22} w + t_{32} \\end{cases} \\qquad(1)$$</p>\n<p>反向映射：<br>$$(v,w)=\\mathbf T^{-1} [(x,y)]$$</p>\n<h3 id=\"1-1-1-恒等变换\"><a href=\"#1-1-1-恒等变换\" class=\"headerlink\" title=\"1.1.1 恒等变换\"></a>1.1.1 恒等变换</h3><p>$$\\mathbf T = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0&amp; 1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$</p>\n<h3 id=\"1-1-2-尺度变换\"><a href=\"#1-1-2-尺度变换\" class=\"headerlink\" title=\"1.1.2 尺度变换\"></a>1.1.2 尺度变换</h3><p>$$\\mathbf T = \\begin{bmatrix} c_x &amp; 0 &amp; 0 \\ 0&amp; c_y&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$<br>x 方向变为原来的 $c_x$ 倍，y 方向变为原来的 $c_y$ 倍。</p>\n<h3 id=\"1-1-3-旋转变换\"><a href=\"#1-1-3-旋转变换\" class=\"headerlink\" title=\"1.1.3 旋转变换\"></a>1.1.3 旋转变换</h3><p>$$\\mathbf T = \\begin{bmatrix} cos \\theta &amp; sin \\theta &amp; 0 \\ -sin \\theta&amp; cos \\theta&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$$<br>其中 $\\theta$ 为（图像绕左上角顶点）顺时针旋转角度。</p>\n<h3 id=\"1-1-4-平移变换\"><a href=\"#1-1-4-平移变换\" class=\"headerlink\" title=\"1.1.4 平移变换\"></a>1.1.4 平移变换</h3><p>$$\\mathbf T = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0&amp; 1&amp;0 \\ t_x&amp;t_y&amp;1\\end{bmatrix}$$</p>\n<h3 id=\"1-1-5-偏移变换\"><a href=\"#1-1-5-偏移变换\" class=\"headerlink\" title=\"1.1.5 偏移变换\"></a>1.1.5 偏移变换</h3><p>又称错切（shear）变换，分水平错切和垂直错切两种情况。水平错切：将每一点水平移动，移动长度和该点的纵坐标成比例，<br>$$(v,w) \\rightarrow (v+mw,w)$$<br>$m&gt;0$ 时向右移动，$m&lt;0$ 时向左移动。<br>类似地可以得到垂直方向变换为，<br>$$(v,w) \\rightarrow (v, vn+w)$$<br>于是错切变换矩阵为<br>$$\\mathbf T=\\begin{bmatrix} 1 &amp; n &amp; 0 \\ m &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\\end{bmatrix}$$</p>\n<h2 id=\"1-2-透视变换\"><a href=\"#1-2-透视变换\" class=\"headerlink\" title=\"1.2 透视变换\"></a>1.2 透视变换</h2><p>透视变换是二维 $(x,y)$ 到三维 $(x,y,z)$，然后再到二维 $(x’,y’)$ 的映射，所以也称为投影变换。</p>\n<p>事实上，二维可看作三维上的一个平面 $(x,y,1)$，前面的仿射变换则是将这个平面上的点 $(x,y,1)$ 经过 $3 \\times 3$ 变换矩阵后依然位于这个平面内 $(x’,y’,1)$，而透视变换则是先变换到三维空间任意点上，然后从三维空间在变换到 $(x’,y’,1)$，故不难想象变换矩阵具有如下形式，<br>$$[x,y,1]=[v,w,1]T=[v,w,1]\\begin{bmatrix} t_{11} &amp; t_{12} &amp; t_{13} \\t_{21} &amp; t_{22} &amp; t_{23} \\t_{31} &amp; t_{32} &amp; t_{33} \\end{bmatrix}$$<br>其中 $t_{13}v+t_{23}w+t_{33}=1$。从变换矩阵也可以看出，仿射变换是一种特殊的透视变换。</p>\n<p>正如下一节将要讲到的，需要知道输入图像和输出图像中分别 4 个约束点，才能确定透视变换的矩阵参数，这是因为 4 个约束点提供 8 个方程，加上自身的约束方程，共 9 个方程从而解出 9 个参数，可参考 opencv 的 python 教程中关于<a href=\"https://docs.opencv.org/4.1.2/da/d6e/tutorial_py_geometric_transformations.html\" target=\"_blank\" rel=\"noopener\">透视变换的例子</a>。</p>\n<h1 id=\"2-图像配准\"><a href=\"#2-图像配准\" class=\"headerlink\" title=\"2. 图像配准\"></a>2. 图像配准</h1><p>有时候知道输入图像和经过某种变换后的输出图像，要求变换函数，这就是图像配准。主要方法是使用约束点（控制点），根据输入图像和输出图像上的一组 n 个约束点来估计变换函数，例如已知是仿射变换，那么只需要一组 3 个约束点（6个方程解6个参数），根据式 $(1)$ 即可求出参数；如果变换函数模型是双线性模型，那么有<br>$$x=c_1v+ c_2w+c_3vw + c_4 \\ y=c_5v+c_6w+c_7vw+c_8$$<br>即，需要 4 个约束点得到变换函数参数。</p>\n<p>还有其他更加复杂的策略，略。</p>\n<h1 id=\"3-图像内插\"><a href=\"#3-图像内插\" class=\"headerlink\" title=\"3. 图像内插\"></a>3. 图像内插</h1><p>假设要求变换后图像中 $(x,y)$ 的像素值，根据反向映射得到原图对应点位置为 $(i+u,j+v)$（整数+小数的形式），$0\\le u,v &lt; 1$</p>\n<h2 id=\"3-1-最近邻内插\"><a href=\"#3-1-最近邻内插\" class=\"headerlink\" title=\"3.1 最近邻内插\"></a>3.1 最近邻内插</h2><p>对 $u,v$ 分别采用四舍五入，得到原来图像对应像素位置 $(\\lfloor i+u+\\frac 1 2\\rfloor,\\lfloor j+v+\\frac 1 2 \\rfloor)$</p>\n<h2 id=\"3-2-双线性内插\"><a href=\"#3-2-双线性内插\" class=\"headerlink\" title=\"3.2 双线性内插\"></a>3.2 双线性内插</h2><p>使用 4 个最近邻即 $(i,j), (i+1,j),(i,j+1),(i+1,j+1)$ 根据下式进行确定，<br>$$f(i+u,j+v)=(1-u)(1-v)f(i,j)+(1-u)v f(i,j+1)+u(1-v)f(i+1,j)+uvf(i+1,j+1)$$</p>\n<h2 id=\"3-2-双三次内插\"><a href=\"#3-2-双三次内插\" class=\"headerlink\" title=\"3.2 双三次内插\"></a>3.2 双三次内插</h2><p>使用 16 个最近邻点，一种确定权重因子的方法是使用 BiCubic 函数，<br>$$W(x)=\\begin{cases} (a+2)|x|^3 - (a+3)|x|^2+1 &amp; |x| \\le 1 \\ a|x|^3-5a|x|^2+8a|x|-4a &amp; 1 &lt;|x|&lt;2 \\ 0 &amp; \\text{otherwise} \\end{cases}$$<br>其中， $x$ 是水平（垂直）方向上的距离，a 通常取 $a=-0.5$。</p>\n<p>为了表示方便，坐标使用 $(x,y)$ 而非前面的 $(i,j)$ 表示。图像上目标点 $(x,y)$ 的 $4\\times4$ 邻域的点 $(x_i,y_j), \\ i,j=0,1,2,3$，按下式进行双三次插值，<br>$$f(x,y)=\\sum_{i=0}^3\\sum_{j=0}^3 f(x_i,y_j) W(x-x_i) W(y-y_j)$$</p>\n<h1 id=\"4-灰度变换\"><a href=\"#4-灰度变换\" class=\"headerlink\" title=\"4. 灰度变换\"></a>4. 灰度变换</h1><p>这里仅讨论部分灰度变换的方法。</p>\n<h2 id=\"4-1-直方图均衡\"><a href=\"#4-1-直方图均衡\" class=\"headerlink\" title=\"4.1 直方图均衡\"></a>4.1 直方图均衡</h2><p>假设灰度范围为 $[0,L-1]$，变换形式为<br>$$s=T(r)$$<br>表示将灰度 <code>r</code> 变换为 <code>s</code>。<br>这里假定变换函数单调增（若非特别说明，不一定是严格单调增），否则灰度变换后产生认为缺陷。</p>\n<p>我们可以将 <code>r</code> 看作输入图像的表示灰度的随机变量，<code>s</code> 为输出图像的表示灰度的随机变量，令 $p_r(r), \\ p_s(s)$ 分别表示 <code>r</code> 和 <code>s</code> 的概率密度函数，那么<br>$$p_s(s)=p_r(r) \\frac {dr} {ds} \\qquad(2)$$</p>\n<p>其中 $T(r)=s, \\ r_1 \\le r \\le r_2$，非严格单调增时 $r_1&lt;r_2$，严格单调增时 $r_1=r_2$。</p>\n<p>使用如下变换函数来实现直方图均衡，<br>$$s=T(r)=(L-1)\\int_0^r p_r(w) dw \\qquad(3)$$<br>其中 积分项是归一化的，所以增加 $(L-1)$ 因子将灰度放大到合适的范围内。</p>\n<p>对 (3) 式求导，<br>$$\\frac {ds} {dr} = \\frac {dT(r)} {dr} = (L-1) \\frac d {dr} \\left[\\int_0^r p_r(w)dw \\right] = (L-1)p_r(r) \\qquad(4)$$</p>\n<p>将 (4) 式代入 (2) 式，<br>$$p_s(s)=p_r(r) \\frac {dr} {ds} = \\frac 1 {L-1} \\qquad(5)$$</p>\n<p>这说明， 随机变量 <code>s</code> 是均匀分布的。</p>\n<p>数字图像处理中常采用离散化处理，此时变换函数为，<br>$$s=T(r)=(L-1) \\sum_{0}^r p_r(r) \\qquad(3’)$$<br>其中 $p_r(r)=n_r/N$，$n_r$ 为灰度 <code>r</code> 的像素数量，<code>N</code> 为图像总像素数量。<br>此时<br>$$1=s - (s-1) = (L-1)\\sum_0^{r_2}p_r(r) - (L-1)\\sum_0^{r_1}p_r(r)=(L-1)\\sum_{r_1}^{r_2}p_r(r)$$<br>于是，<br>$$p_s(s) = \\sum_{r_1}^{r_2}p_r(r)=\\frac 1 {L-1}$$<br>注意，上式中由于 $r$ 取不到 $r_1$，故可能会出现近似等于 $1/(L-1)$。</p>\n<h2 id=\"4-2-直方图匹配\"><a href=\"#4-2-直方图匹配\" class=\"headerlink\" title=\"4.2 直方图匹配\"></a>4.2 直方图匹配</h2><p>指定输出图像的直方图的形状。令 <code>r</code>  和 <code>z</code> 分别表示输入和输出图像的灰度随机变量，<code>s</code> 为一个均匀分布的灰度随机变量，有<br>$$s=T(r)=(L-1)\\int_0^r p_r(r) dw \\qquad(6)<br>\\\\ s=G(z)=(L-1)\\int_0^z p_z(t) dt \\qquad(7)$$</p>\n<p>于是，<br>$$z=G^{-1}(s)$$<br>此时要求变换函数 $G$ 是严格单调增，否则不存在反函数。</p>\n<p>由于输入图像给定，容易计算出 $p_r(r)$，而指定输出图像的直方图形状，即 $p_z(z)$ 已知，根据 (7) 式可得 $G$（每个 z 到 s 的映射），于是直方图匹配步骤如下：</p>\n<ol>\n<li>计算 $p_r(r)$，然后计算 <code>s</code></li>\n<li>根据式 (7) 计算 $G(z)$</li>\n<li>求反函数 $G^{-1}(s)$，根据 <code>s</code> 计算出 <code>z</code></li>\n</ol>\n<p>实际处理过程为：对输入图像做直方图均衡得到 <code>s</code> 灰度的图，然后对此图中每个像素执行反映射 $z=G^{-1}(s)$（s 到 z 的映射），得到最终输出图像。</p>\n<h2 id=\"4-3-局部直方图处理\"><a href=\"#4-3-局部直方图处理\" class=\"headerlink\" title=\"4.3 局部直方图处理\"></a>4.3 局部直方图处理</h2><p>对每个位置的邻域计算直方图，然后进行均衡化或者应用其他匹配变换函数，用于修改这个邻域中心的灰度，然后平移邻域（一个像素位置或者移到另一个非重叠区域）。</p>\n<h2 id=\"4-4-直方图统计\"><a href=\"#4-4-直方图统计\" class=\"headerlink\" title=\"4.4 直方图统计\"></a>4.4 直方图统计</h2><p>令 <code>r</code> 为表示灰度的离散随机变量，归一化的直方图为 $p(r)$，应用概率相关的知识，<code>r</code> 的 n 阶（中心）矩定义为，<br>$$\\mu_n(r)=\\sum_{i=0}^{L-1} (r_i-m)^n p(r_i)$$<br>其中 <code>m</code> 是期望（或称平均灰度）$m=\\sum_0^{L-1} r_i p(r_i)$</p>\n<p>二阶矩（灰度方差）为<br>$$u_2(r)=\\sum_{i=0}^{L-1}(r_i-m)^2 p(r_i)$$<br>通常用 $\\sigma^2$ 表示，单独列出二阶矩是因为比较重要。</p>\n<p>实际给定一个图像时，可以直接计算样本均值和样本方差，无需计算直方图，<br>$$m=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y)<br>\\\\ \\sigma^2=\\frac 1 {MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} [f(x,y)-m]^2$$<br>以上方差计算有时候使用 $MN-1$ 作为分母，以获得一个无偏估计，实际中，使用 $MN$ 作为分母的偏差可忽略不计。</p>\n<p>考虑点 $(x,y)$ 的邻域 $S_{xy}$，仿照上面全局统计量可写出局部均值和局部方差如下，<br>$$m_{s_{xy}}=\\sum_{i=0}^{L-1}r_i p_{s_{xy}} (r_i)<br>\\\\ \\sigma_{s_{xy}}^2 = \\sum <em>{i=0}^{L-1} (r_i - m</em>{S_{xy}})^2 p_{s_{xy}}(r_i)$$</p>\n<p>有时候只需要对图像上暗区进行增强，亮区保持不变，所以需要先判断当前邻域属于暗区还是亮区，记全局均值为 $m_G$，如果 $m_{s_{xy}} \\le k_0 m_G$，其中 $0&lt; k_0 &lt; 1.0$，那么属于暗区，对其进行局部增强。</p>\n<p>如果想增强低对比度的局部区域，那么判断方法为 $\\sigma_{s_{xy}} \\le k_2 \\sigma_G$。当然一般还会设置一个阈值下限，例如对比度为 0 的恒定区域，其实是没必要增强的，所以 $k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G$。对于满足增强条件的点而言，增强操作可以是将像素值乘以一个常数 <code>E</code> ，这样这个点相对于图像上其他不需要增强的点，像素得以增大（或减小）。</p>\n<p>总结增强方法如下，<br>$$g(x,y)=\\begin{cases} E \\cdot f(x,y) &amp; m_{s_{xy}} \\le k_0 m_G, \\ k_1 \\sigma_G \\le \\sigma_{s_{xy}} \\le k_2 \\sigma_G<br>\\\\ f(x,y) &amp; \\text{otherwise} \\end{cases}$$</p>"},{"title":"FSAF","date":"2019-06-27T01:14:42.000Z","mathjax":true,"_content":"论文：[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n<!-- more -->\n目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：\n1. 启发式导向的特征选择\n2. 基于 overlap 选取 anchor \n\n第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。\n\n![](/images/FSAF_fig2.png)\n\n本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center>\n\n对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。\n\n# FSAF 模块\n我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：\n1. 如何创建 anchor-free 分支\n2. 如何生成 anchor-free 分支的监督信号（GT target）\n3. 如何为每个实例动态选择 feature level\n4. 如何联合训练/测试 anchor-free 分支和 anchor-based 分支\n\n## 网络框架\n图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 $\\{P_l|l\\in [3,7]\\}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。\n\n基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，\n\n![](/images/FSAF_fig4.png)<center>Fig 4 具有 FSAF 的 RetinaNet 框架</center>\n\n这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。\n\n## Ground-truth and Loss\n给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n（到这里就发现与 [GA-RPN](/2019/06/25/GA-RPN) 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）\n\n图 5 是一个 car 实例的 GT 生成（GT target）的例子\n\n![](/images/FSAF_fig5.png)\n\n__分类输出：__ 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：\n- 位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域\n- 位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域\n- 如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略\n\n这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。\n\n如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。\n\n__Box 回归输出：__ 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 [FCOS](FCOS) 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为\n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n具体可参考 UnitBox。\n\nInference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。\n\n## 在线特征选择\nFSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。\n\n给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。\n\n图 6 显示了在线特征选择的过程。\n\n![](/images/FSAF_fig6.png) <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center>\n\n首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。\n\n我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l'}$，其中\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。\n\n## Joint Inference and Training\n当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。\n\n__Inference:__ FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。\n\n__初始化：__ backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。\n\n__优化：__ 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。\n\n# 实验\n实验介绍以及结果分析略，请阅读原文以获取详细信息。\n\n#总结\n本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。","source":"_posts/FSAF.md","raw":"---\ntitle: FSAF\ndate: 2019-06-27 09:14:42\ntags: object detection\nmathjax: true\n---\n论文：[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n<!-- more -->\n目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：\n1. 启发式导向的特征选择\n2. 基于 overlap 选取 anchor \n\n第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。\n\n![](/images/FSAF_fig2.png)\n\n本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center>\n\n对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。\n\n# FSAF 模块\n我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：\n1. 如何创建 anchor-free 分支\n2. 如何生成 anchor-free 分支的监督信号（GT target）\n3. 如何为每个实例动态选择 feature level\n4. 如何联合训练/测试 anchor-free 分支和 anchor-based 分支\n\n## 网络框架\n图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 $\\{P_l|l\\in [3,7]\\}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。\n\n基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，\n\n![](/images/FSAF_fig4.png)<center>Fig 4 具有 FSAF 的 RetinaNet 框架</center>\n\n这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。\n\n## Ground-truth and Loss\n给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n（到这里就发现与 [GA-RPN](/2019/06/25/GA-RPN) 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）\n\n图 5 是一个 car 实例的 GT 生成（GT target）的例子\n\n![](/images/FSAF_fig5.png)\n\n__分类输出：__ 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：\n- 位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域\n- 位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域\n- 如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略\n\n这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。\n\n如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。\n\n__Box 回归输出：__ 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 [FCOS](FCOS) 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为\n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n具体可参考 UnitBox。\n\nInference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。\n\n## 在线特征选择\nFSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。\n\n给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。\n\n图 6 显示了在线特征选择的过程。\n\n![](/images/FSAF_fig6.png) <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center>\n\n首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。\n\n我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l'}$，其中\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。\n\n## Joint Inference and Training\n当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。\n\n__Inference:__ FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。\n\n__初始化：__ backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。\n\n__优化：__ 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。\n\n# 实验\n实验介绍以及结果分析略，请阅读原文以获取详细信息。\n\n#总结\n本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。","slug":"FSAF","published":1,"updated":"2020-04-24T10:37:38.161Z","_id":"ck9dzcip9000wgga69soc6hs3","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>\n<a id=\"more\"></a>\n<p>目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：</p>\n<ol>\n<li>启发式导向的特征选择</li>\n<li>基于 overlap 选取 anchor </li>\n</ol>\n<p>第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。</p>\n<p><img src=\"/images/FSAF_fig2.png\" alt=\"\"></p>\n<p>本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt=\"\"> <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center></p>\n<p>对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。</p>\n<h1 id=\"FSAF-模块\"><a href=\"#FSAF-模块\" class=\"headerlink\" title=\"FSAF 模块\"></a>FSAF 模块</h1><p>我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：</p>\n<ol>\n<li>如何创建 anchor-free 分支</li>\n<li>如何生成 anchor-free 分支的监督信号（GT target）</li>\n<li>如何为每个实例动态选择 feature level</li>\n<li>如何联合训练/测试 anchor-free 分支和 anchor-based 分支</li>\n</ol>\n<h2 id=\"网络框架\"><a href=\"#网络框架\" class=\"headerlink\" title=\"网络框架\"></a>网络框架</h2><p>图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 ${P_l|l\\in [3,7]}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。</p>\n<p>基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt=\"\"><center>Fig 4 具有 FSAF 的 RetinaNet 框架</center></p>\n<p>这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p>给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br>（到这里就发现与 <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a> 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）</p>\n<p>图 5 是一个 car 实例的 GT 生成（GT target）的例子</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt=\"\"></p>\n<p><strong>分类输出：</strong> 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：</p>\n<ul>\n<li>位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域</li>\n<li>位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域</li>\n<li>如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略</li>\n</ul>\n<p>这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。</p>\n<p>如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。</p>\n<p><strong>Box 回归输出：</strong> 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br>其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 <a href=\"FCOS\">FCOS</a> 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为<br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br>具体可参考 UnitBox。</p>\n<p>Inference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。</p>\n<h2 id=\"在线特征选择\"><a href=\"#在线特征选择\" class=\"headerlink\" title=\"在线特征选择\"></a>在线特征选择</h2><p>FSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。</p>\n<p>给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。</p>\n<p>图 6 显示了在线特征选择的过程。</p>\n<p><img src=\"/images/FSAF_fig6.png\" alt=\"\"> <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center></p>\n<p>首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br>对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。</p>\n<p>我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l’}$，其中<br>$$l’ = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，<br>$$ \\sqrt{wh}/2^{l’} \\approx 224/2^{l_0}$$<br>可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。</p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p>当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。</p>\n<p><strong>Inference:</strong> FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。</p>\n<p><strong>初始化：</strong> backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。</p>\n<p><strong>优化：</strong> 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍以及结果分析略，请阅读原文以获取详细信息。</p>\n<p>#总结<br>本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。</p>\n","site":{"data":{}},"excerpt":"<p>论文：<a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>","more":"<p>目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：</p>\n<ol>\n<li>启发式导向的特征选择</li>\n<li>基于 overlap 选取 anchor </li>\n</ol>\n<p>第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。</p>\n<p><img src=\"/images/FSAF_fig2.png\" alt=\"\"></p>\n<p>本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt=\"\"> <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center></p>\n<p>对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。</p>\n<h1 id=\"FSAF-模块\"><a href=\"#FSAF-模块\" class=\"headerlink\" title=\"FSAF 模块\"></a>FSAF 模块</h1><p>我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：</p>\n<ol>\n<li>如何创建 anchor-free 分支</li>\n<li>如何生成 anchor-free 分支的监督信号（GT target）</li>\n<li>如何为每个实例动态选择 feature level</li>\n<li>如何联合训练/测试 anchor-free 分支和 anchor-based 分支</li>\n</ol>\n<h2 id=\"网络框架\"><a href=\"#网络框架\" class=\"headerlink\" title=\"网络框架\"></a>网络框架</h2><p>图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 ${P_l|l\\in [3,7]}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。</p>\n<p>基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt=\"\"><center>Fig 4 具有 FSAF 的 RetinaNet 框架</center></p>\n<p>这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p>给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br>（到这里就发现与 <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a> 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）</p>\n<p>图 5 是一个 car 实例的 GT 生成（GT target）的例子</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt=\"\"></p>\n<p><strong>分类输出：</strong> 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：</p>\n<ul>\n<li>位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域</li>\n<li>位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域</li>\n<li>如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略</li>\n</ul>\n<p>这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。</p>\n<p>如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。</p>\n<p><strong>Box 回归输出：</strong> 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br>其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 <a href=\"FCOS\">FCOS</a> 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为<br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br>具体可参考 UnitBox。</p>\n<p>Inference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。</p>\n<h2 id=\"在线特征选择\"><a href=\"#在线特征选择\" class=\"headerlink\" title=\"在线特征选择\"></a>在线特征选择</h2><p>FSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。</p>\n<p>给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。</p>\n<p>图 6 显示了在线特征选择的过程。</p>\n<p><img src=\"/images/FSAF_fig6.png\" alt=\"\"> <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center></p>\n<p>首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br>对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。</p>\n<p>我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l’}$，其中<br>$$l’ = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，<br>$$ \\sqrt{wh}/2^{l’} \\approx 224/2^{l_0}$$<br>可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。</p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p>当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。</p>\n<p><strong>Inference:</strong> FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。</p>\n<p><strong>初始化：</strong> backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。</p>\n<p><strong>优化：</strong> 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍以及结果分析略，请阅读原文以获取详细信息。</p>\n<p>#总结<br>本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。</p>"},{"title":"DeRPN","date":"2019-07-15T07:04:18.000Z","mathjax":true,"_content":"论文 [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n<!-- more -->\ntwo-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，\n![](/images/DeRPN_fig1.png)\n\nDeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。\n\n# 方法论\n## 建模\n我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：\n$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$\n其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。\n\n显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，\n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\n其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。\n### 匹配复杂度\n假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。\n\n## 维度分解\n### Anchor strings\nRPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 $\\{a_n\\}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。\n\n图 2 为 DeRPN 网络，\n![](/images/DeRPN_fig2.png) <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center>\n\n如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，\n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 $\\{a_n\\}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。\n\n上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。\n\n上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，\n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n剩下的就不多说了。\n\n忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛\n\n### Label assignment\n对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。\n\n### Consistent network\nDeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 $\\{a_n\\}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。\n\n### Scale-sensitive loss function\n目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\n这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^*$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^*$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，\n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，\n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# 维度合并\nDeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。\n\n__像素级别的合并算法__ 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$，每个组合后的 bbox 的概率使用调和平均计算得到，\n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。\n\n类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。\n\n# 实验\n请阅读原文，略。\n\n# 结论\n1. 介绍了 DeRPN，将目标的宽和高两个维度进行分解\n2. 使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没","source":"_posts/DeRPN.md","raw":"---\ntitle: DeRPN\ndate: 2019-07-15 15:04:18\ntags: object detection\nmathjax: true\n---\n论文 [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n<!-- more -->\ntwo-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，\n![](/images/DeRPN_fig1.png)\n\nDeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。\n\n# 方法论\n## 建模\n我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：\n$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$\n其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。\n\n显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，\n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\n其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。\n### 匹配复杂度\n假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。\n\n## 维度分解\n### Anchor strings\nRPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 $\\{a_n\\}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。\n\n图 2 为 DeRPN 网络，\n![](/images/DeRPN_fig2.png) <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center>\n\n如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，\n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 $\\{a_n\\}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。\n\n上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。\n\n上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，\n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n剩下的就不多说了。\n\n忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛\n\n### Label assignment\n对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。\n\n### Consistent network\nDeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 $\\{a_n\\}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。\n\n### Scale-sensitive loss function\n目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\n这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^*$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^*$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，\n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，\n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# 维度合并\nDeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。\n\n__像素级别的合并算法__ 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$，每个组合后的 bbox 的概率使用调和平均计算得到，\n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。\n\n类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。\n\n# 实验\n请阅读原文，略。\n\n# 结论\n1. 介绍了 DeRPN，将目标的宽和高两个维度进行分解\n2. 使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没","slug":"DeRPN","published":1,"updated":"2020-04-24T10:35:49.189Z","_id":"ck9dzcipi000ygga64z7kh24x","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>\n<a id=\"more\"></a>\n<p>two-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，<br><img src=\"/images/DeRPN_fig1.png\" alt=\"\"></p>\n<p>DeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。</p>\n<h1 id=\"方法论\"><a href=\"#方法论\" class=\"headerlink\" title=\"方法论\"></a>方法论</h1><h2 id=\"建模\"><a href=\"#建模\" class=\"headerlink\" title=\"建模\"></a>建模</h2><p>我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：<br>$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$<br>其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。</p>\n<p>显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br>相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，<br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。</p>\n<h3 id=\"匹配复杂度\"><a href=\"#匹配复杂度\" class=\"headerlink\" title=\"匹配复杂度\"></a>匹配复杂度</h3><p>假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。</p>\n<h2 id=\"维度分解\"><a href=\"#维度分解\" class=\"headerlink\" title=\"维度分解\"></a>维度分解</h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 ${a_n}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。</p>\n<p>图 2 为 DeRPN 网络，<br><img src=\"/images/DeRPN_fig2.png\" alt=\"\"> <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center></p>\n<p>如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，<br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,…,N) \\quad(9)$$<br>$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 ${a_n}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。</p>\n<p>上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。</p>\n<p>上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，<br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br>剩下的就不多说了。</p>\n<p>忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛</p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p>对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。</p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 ${a_n}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。</p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p>目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，<br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,…,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,…,M} \\quad (12)$$</p>\n<p>这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^<em>$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^</em>$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，<br>$$L_{cls}(p_i,p_i^<em>)=- p_i^*\\log p_i-(1-p_i^</em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^<em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j</em>})$$</p>\n<p>预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，<br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"维度合并\"><a href=\"#维度合并\" class=\"headerlink\" title=\"维度合并\"></a>维度合并</h1><p>DeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。</p>\n<p><strong>像素级别的合并算法</strong> 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w={(x,y^{(k)},w,h^{(k)}}$，每个组合后的 bbox 的概率使用调和平均计算得到，<br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br>其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。</p>\n<p>类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h={(x^{(k)},y,w^{(k)},h}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>请阅读原文，略。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><ol>\n<li>介绍了 DeRPN，将目标的宽和高两个维度进行分解</li>\n<li>使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>","more":"<p>two-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，<br><img src=\"/images/DeRPN_fig1.png\" alt=\"\"></p>\n<p>DeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。</p>\n<h1 id=\"方法论\"><a href=\"#方法论\" class=\"headerlink\" title=\"方法论\"></a>方法论</h1><h2 id=\"建模\"><a href=\"#建模\" class=\"headerlink\" title=\"建模\"></a>建模</h2><p>我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：<br>$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$<br>其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。</p>\n<p>显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br>相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，<br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。</p>\n<h3 id=\"匹配复杂度\"><a href=\"#匹配复杂度\" class=\"headerlink\" title=\"匹配复杂度\"></a>匹配复杂度</h3><p>假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。</p>\n<h2 id=\"维度分解\"><a href=\"#维度分解\" class=\"headerlink\" title=\"维度分解\"></a>维度分解</h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 ${a_n}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。</p>\n<p>图 2 为 DeRPN 网络，<br><img src=\"/images/DeRPN_fig2.png\" alt=\"\"> <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center></p>\n<p>如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，<br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,…,N) \\quad(9)$$<br>$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 ${a_n}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。</p>\n<p>上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。</p>\n<p>上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，<br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br>剩下的就不多说了。</p>\n<p>忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛</p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p>对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。</p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 ${a_n}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。</p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p>目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，<br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,…,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,…,M} \\quad (12)$$</p>\n<p>这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^<em>$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^</em>$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，<br>$$L_{cls}(p_i,p_i^<em>)=- p_i^*\\log p_i-(1-p_i^</em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^<em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j</em>})$$</p>\n<p>预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，<br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"维度合并\"><a href=\"#维度合并\" class=\"headerlink\" title=\"维度合并\"></a>维度合并</h1><p>DeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。</p>\n<p><strong>像素级别的合并算法</strong> 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w={(x,y^{(k)},w,h^{(k)}}$，每个组合后的 bbox 的概率使用调和平均计算得到，<br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br>其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。</p>\n<p>类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h={(x^{(k)},y,w^{(k)},h}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>请阅读原文，略。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><ol>\n<li>介绍了 DeRPN，将目标的宽和高两个维度进行分解</li>\n<li>使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没</li>\n</ol>"},{"title":"GA-RPN","date":"2019-06-25T09:01:57.000Z","mathjax":true,"_content":"论文：[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n<!-- more -->\n目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。\n\n合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：\n1. anchor 中心应与 feature map 的像素点对准\n2. 感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致\n\n滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：\n1. 对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。\n2. 为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量\n\n本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：\n1. 根据观察，image 中的目标位置不是均匀分布的\n2. 目标的 scale 与目标位置和 image 内容相关\n   \n于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：\n1. 确定可能包含目标的子区域\n2. 根据子区域位置确定其 shape\n\n可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。\n\n使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。\n\n# Guided Anchoring\n目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：\n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n这种因式分解基于如下两点：\n1. 给定 image，目标仅位于某些特定区域\n2. shape 即 scale 和 aspect ratio 与 anchor 的位置有关\n   \n根据以上公式，我们的 anchor 生成模块如图 1，\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center>\n\n给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。\n\n由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。\n\n## Anchor 位置预测\nanchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。\n\n使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。\n\n预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。\n\n## Anchor shape 预测\n确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。\n\n由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，\n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。\n\n以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。\n\n## Anchor-Guided 特征适配\n传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，\n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。\n\n## 训练\n### 联合目标函数\n本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：\n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g',y_g',w_g',h_g')$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：\n1. 中心区域 \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）\n\n2. 忽略区域\n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$，其中$\\sigma_2 > \\sigma_1$，IR 内的像素点被标记为 `ignore`，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 `[0.3,0.7]` 范围则标记为 -1，标记为 -1 的 anchor 不参与训练\n\n3. 外围区域\n   \n   $OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）\n\n由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。\n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n分两步来决定最佳 shape target：\n1. 将 anchor 与某个 gt box 匹配起来\n2. 预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box\n\nFaster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。\n\n但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：\n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。\n\n总结一下以上过程：\n1. 选择 9 组 (w,h)\n2. 给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)\n3. 最大 vIoU 的那个 gt box 与此位置 anchor 相匹配\n4. shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失\n\n那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？\n\n当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。\n\n## 使用高质量 proposals\n得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，\n![](/images/GA-RPN_fig3.png) <center>Fig 3 不同 IoU 下的 proposals 数量</center>\n\n比起 RPN，GA-RPN 有如下两个明显优点：\n1. 正例 proposals 数量更多\n2. 高 IoU 处两者的 proposals 数量比例更明显\n\n在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。\n\n除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。\n\n![](/images/GA-RPN_fig4.png)\n\n# 实验\n实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。\n\n# 结论\n提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。","source":"_posts/GA-RPN.md","raw":"---\ntitle: GA-RPN\ndate: 2019-06-25 17:01:57\ntags: object detection\nmathjax: true\n---\n论文：[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n<!-- more -->\n目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。\n\n合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：\n1. anchor 中心应与 feature map 的像素点对准\n2. 感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致\n\n滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：\n1. 对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。\n2. 为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量\n\n本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：\n1. 根据观察，image 中的目标位置不是均匀分布的\n2. 目标的 scale 与目标位置和 image 内容相关\n   \n于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：\n1. 确定可能包含目标的子区域\n2. 根据子区域位置确定其 shape\n\n可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。\n\n使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。\n\n# Guided Anchoring\n目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：\n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n这种因式分解基于如下两点：\n1. 给定 image，目标仅位于某些特定区域\n2. shape 即 scale 和 aspect ratio 与 anchor 的位置有关\n   \n根据以上公式，我们的 anchor 生成模块如图 1，\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center>\n\n给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。\n\n由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。\n\n## Anchor 位置预测\nanchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。\n\n使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。\n\n预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。\n\n## Anchor shape 预测\n确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。\n\n由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，\n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。\n\n以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。\n\n## Anchor-Guided 特征适配\n传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，\n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。\n\n## 训练\n### 联合目标函数\n本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：\n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g',y_g',w_g',h_g')$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：\n1. 中心区域 \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）\n\n2. 忽略区域\n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$，其中$\\sigma_2 > \\sigma_1$，IR 内的像素点被标记为 `ignore`，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 `[0.3,0.7]` 范围则标记为 -1，标记为 -1 的 anchor 不参与训练\n\n3. 外围区域\n   \n   $OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）\n\n由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。\n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n分两步来决定最佳 shape target：\n1. 将 anchor 与某个 gt box 匹配起来\n2. 预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box\n\nFaster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。\n\n但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：\n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。\n\n总结一下以上过程：\n1. 选择 9 组 (w,h)\n2. 给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)\n3. 最大 vIoU 的那个 gt box 与此位置 anchor 相匹配\n4. shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失\n\n那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？\n\n当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。\n\n## 使用高质量 proposals\n得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，\n![](/images/GA-RPN_fig3.png) <center>Fig 3 不同 IoU 下的 proposals 数量</center>\n\n比起 RPN，GA-RPN 有如下两个明显优点：\n1. 正例 proposals 数量更多\n2. 高 IoU 处两者的 proposals 数量比例更明显\n\n在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。\n\n除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。\n\n![](/images/GA-RPN_fig4.png)\n\n# 实验\n实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。\n\n# 结论\n提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。","slug":"GA-RPN","published":1,"updated":"2020-04-24T10:37:42.094Z","_id":"ck9dzcipm0010gga66f5f2gtr","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>\n<a id=\"more\"></a>\n<p>目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。</p>\n<p>合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：</p>\n<ol>\n<li>anchor 中心应与 feature map 的像素点对准</li>\n<li>感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致</li>\n</ol>\n<p>滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：</p>\n<ol>\n<li>对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。</li>\n<li>为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量</li>\n</ol>\n<p>本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：</p>\n<ol>\n<li>根据观察，image 中的目标位置不是均匀分布的</li>\n<li>目标的 scale 与目标位置和 image 内容相关</li>\n</ol>\n<p>于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：</p>\n<ol>\n<li>确定可能包含目标的子区域</li>\n<li>根据子区域位置确定其 shape</li>\n</ol>\n<p>可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。</p>\n<p>使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p>目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：<br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br>这种因式分解基于如下两点：</p>\n<ol>\n<li>给定 image，目标仅位于某些特定区域</li>\n<li>shape 即 scale 和 aspect ratio 与 anchor 的位置有关</li>\n</ol>\n<p>根据以上公式，我们的 anchor 生成模块如图 1，</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt=\"\"> <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center></p>\n<p>给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。</p>\n<p>由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。</p>\n<h2 id=\"Anchor-位置预测\"><a href=\"#Anchor-位置预测\" class=\"headerlink\" title=\"Anchor 位置预测\"></a>Anchor 位置预测</h2><p>anchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。</p>\n<p>使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。</p>\n<p>预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。</p>\n<h2 id=\"Anchor-shape-预测\"><a href=\"#Anchor-shape-预测\" class=\"headerlink\" title=\"Anchor shape 预测\"></a>Anchor shape 预测</h2><p>确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。</p>\n<p>由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，<br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br>于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。</p>\n<p>以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。</p>\n<h2 id=\"Anchor-Guided-特征适配\"><a href=\"#Anchor-Guided-特征适配\" class=\"headerlink\" title=\"Anchor-Guided 特征适配\"></a>Anchor-Guided 特征适配</h2><p>传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，<br>$$\\mathbf f_i’= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><h3 id=\"联合目标函数\"><a href=\"#联合目标函数\" class=\"headerlink\" title=\"联合目标函数\"></a>联合目标函数</h3><p>本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：<br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p>为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g’,y_g’,w_g’,h_g’)$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：</p>\n<ol>\n<li><p>中心区域 </p>\n<p>$CR=\\mathcal R(x_g’,y_g’,\\sigma_1 w_g’, \\sigma_1 h_g’)$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）</p>\n</li>\n<li><p>忽略区域</p>\n<p>$IR=\\mathcal R(x_g’,y_g’,\\sigma_2 w_g’, \\sigma_2 h_g’) \\setminus CR$，其中$\\sigma_2 &gt; \\sigma_1$，IR 内的像素点被标记为 <code>ignore</code>，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 <code>[0.3,0.7]</code> 范围则标记为 -1，标记为 -1 的 anchor 不参与训练</p>\n</li>\n<li><p>外围区域</p>\n<p>$OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）</p>\n</li>\n</ol>\n<p>由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。<br><img src=\"/images/GA-RPN_fig2.png\" alt=\"\"></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p>分两步来决定最佳 shape target：</p>\n<ol>\n<li>将 anchor 与某个 gt box 匹配起来</li>\n<li>预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box</li>\n</ol>\n<p>Faster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。</p>\n<p>但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br>对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：<br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br>其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。</p>\n<p>总结一下以上过程：</p>\n<ol>\n<li>选择 9 组 (w,h)</li>\n<li>给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)</li>\n<li>最大 vIoU 的那个 gt box 与此位置 anchor 相匹配</li>\n<li>shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失</li>\n</ol>\n<p>那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？</p>\n<p>当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。</p>\n<h2 id=\"使用高质量-proposals\"><a href=\"#使用高质量-proposals\" class=\"headerlink\" title=\"使用高质量 proposals\"></a>使用高质量 proposals</h2><p>得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，<br><img src=\"/images/GA-RPN_fig3.png\" alt=\"\"> <center>Fig 3 不同 IoU 下的 proposals 数量</center></p>\n<p>比起 RPN，GA-RPN 有如下两个明显优点：</p>\n<ol>\n<li>正例 proposals 数量更多</li>\n<li>高 IoU 处两者的 proposals 数量比例更明显</li>\n</ol>\n<p>在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。</p>\n<p>除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt=\"\"></p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。</p>\n","site":{"data":{}},"excerpt":"<p>论文：<a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>","more":"<p>目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。</p>\n<p>合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：</p>\n<ol>\n<li>anchor 中心应与 feature map 的像素点对准</li>\n<li>感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致</li>\n</ol>\n<p>滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：</p>\n<ol>\n<li>对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。</li>\n<li>为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量</li>\n</ol>\n<p>本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：</p>\n<ol>\n<li>根据观察，image 中的目标位置不是均匀分布的</li>\n<li>目标的 scale 与目标位置和 image 内容相关</li>\n</ol>\n<p>于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：</p>\n<ol>\n<li>确定可能包含目标的子区域</li>\n<li>根据子区域位置确定其 shape</li>\n</ol>\n<p>可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。</p>\n<p>使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p>目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：<br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br>这种因式分解基于如下两点：</p>\n<ol>\n<li>给定 image，目标仅位于某些特定区域</li>\n<li>shape 即 scale 和 aspect ratio 与 anchor 的位置有关</li>\n</ol>\n<p>根据以上公式，我们的 anchor 生成模块如图 1，</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt=\"\"> <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center></p>\n<p>给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。</p>\n<p>由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。</p>\n<h2 id=\"Anchor-位置预测\"><a href=\"#Anchor-位置预测\" class=\"headerlink\" title=\"Anchor 位置预测\"></a>Anchor 位置预测</h2><p>anchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。</p>\n<p>使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。</p>\n<p>预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。</p>\n<h2 id=\"Anchor-shape-预测\"><a href=\"#Anchor-shape-预测\" class=\"headerlink\" title=\"Anchor shape 预测\"></a>Anchor shape 预测</h2><p>确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。</p>\n<p>由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，<br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br>于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。</p>\n<p>以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。</p>\n<h2 id=\"Anchor-Guided-特征适配\"><a href=\"#Anchor-Guided-特征适配\" class=\"headerlink\" title=\"Anchor-Guided 特征适配\"></a>Anchor-Guided 特征适配</h2><p>传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，<br>$$\\mathbf f_i’= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><h3 id=\"联合目标函数\"><a href=\"#联合目标函数\" class=\"headerlink\" title=\"联合目标函数\"></a>联合目标函数</h3><p>本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：<br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p>为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g’,y_g’,w_g’,h_g’)$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：</p>\n<ol>\n<li><p>中心区域 </p>\n<p>$CR=\\mathcal R(x_g’,y_g’,\\sigma_1 w_g’, \\sigma_1 h_g’)$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）</p>\n</li>\n<li><p>忽略区域</p>\n<p>$IR=\\mathcal R(x_g’,y_g’,\\sigma_2 w_g’, \\sigma_2 h_g’) \\setminus CR$，其中$\\sigma_2 &gt; \\sigma_1$，IR 内的像素点被标记为 <code>ignore</code>，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 <code>[0.3,0.7]</code> 范围则标记为 -1，标记为 -1 的 anchor 不参与训练</p>\n</li>\n<li><p>外围区域</p>\n<p>$OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）</p>\n</li>\n</ol>\n<p>由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。<br><img src=\"/images/GA-RPN_fig2.png\" alt=\"\"></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p>分两步来决定最佳 shape target：</p>\n<ol>\n<li>将 anchor 与某个 gt box 匹配起来</li>\n<li>预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box</li>\n</ol>\n<p>Faster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。</p>\n<p>但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br>对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：<br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br>其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。</p>\n<p>总结一下以上过程：</p>\n<ol>\n<li>选择 9 组 (w,h)</li>\n<li>给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)</li>\n<li>最大 vIoU 的那个 gt box 与此位置 anchor 相匹配</li>\n<li>shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失</li>\n</ol>\n<p>那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？</p>\n<p>当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。</p>\n<h2 id=\"使用高质量-proposals\"><a href=\"#使用高质量-proposals\" class=\"headerlink\" title=\"使用高质量 proposals\"></a>使用高质量 proposals</h2><p>得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，<br><img src=\"/images/GA-RPN_fig3.png\" alt=\"\"> <center>Fig 3 不同 IoU 下的 proposals 数量</center></p>\n<p>比起 RPN，GA-RPN 有如下两个明显优点：</p>\n<ol>\n<li>正例 proposals 数量更多</li>\n<li>高 IoU 处两者的 proposals 数量比例更明显</li>\n</ol>\n<p>在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。</p>\n<p>除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt=\"\"></p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。</p>"},{"title":"Grid-RCNN","date":"2019-07-19T09:44:19.000Z","mathjax":true,"_content":"# Grid R-CNN\n论文 [Grid R-CNN](https://arxiv.org/abs/1811.12030)\n\n源码 [Grid R-CNN](https://github.com/STVIR/Grid-R-CNN)\n<!-- more -->\n## 1. 简介\n目标检测器包含两个分支：分类和回归，其中回归分支通常由 conv 和 fc 层组成，因为最终的输出单元数量固定为 4，表示 x,y,w,h 的坐标偏差（或者固定为 4*(C+1)，表示坐标偏差的预测是分类相关的），而 fc 层适用输出单元数量固定的场景。\n![](/images/Grid-RCNN_fig1.png)\n\n这篇文章介绍了 Grid R-CNN，主要是修改了类 R-CNN 目标检测器的回归分支，与传统类 R-CNN 的区别主要如图 1 所示，即，使用 grid points 表示目标定位：在得到 RoI 特征后，使用全卷积 FCN 得到特征图，然后再通过某种方法得到 grid points。此修改有如下优点：\n1. FCN 保留了 RoI 的空间信息，二维（显然，之前的方法都是展平成了一维向量），所以可准确的得到 grid points\n2. 根据 grid points 可以更加精确的获得目标定位。以前的方法都是通过左上角和右下角坐标确定目标 bbox，这种方法显然较为粗糙，因为目标形状的任意性，角点可能位于目标形状之外的背景上，角点处的局部特征并不能反映目标的特征（ExtremeNet 中也提过 CornerNet 的这个问题），而 grid points 采用 3x3 的点作为监督，由于点数增多，可以降低其中某些不准确点对位置预测的负面影响。如图 1(b)，右上角的坐标不准确，但是中上（top-middle）点可以帮助校正最终的目标位置。\n\n利用 grid points 数量多这个优势，文中还提出一种信息融合方法，为每个 grid point 设计独立的一组 feature maps：对某个 grid point，聚集其邻近 grid point 的 feature maps 并进行融合成，作为当前这个 grid point 的 feature map，这个合成的 feature map 则用于预测当前 grid point 的位置。这种空间信息互补的方法可以使得位置预测更加准确。\n\n## 2. Grid R-CNN\n网络的整体结构如图 2 所示，基于 region proposals，使用 RoIAlign 抽取出 RoI 特征，然后分为两支，其中一支用于目标分类预测，另一支经过 FCN 得到 feature maps，然后经过 sigmoid 得到概率热图（probability heatmap），注意不是传统的坐标偏差，heatmaps 可由 target map 监督，从这个概率热图上我们可以通过某种方法得到 grid points（下文 2.1 一节会介绍这种方法），再经过上述的空间信息融合方法（2.2 一节会介绍），最后得到准确的目标定位。接下来我们将整个问题分而治之进行介绍。\n![](/images/Grid-RCNN_fig2.png)\n\n### 2.1 Grid Guided Localization\n一般地， NxN 的 grid points 位列 bbox 之上。以上文提到的 3x3 的 grid points 为例，共四个角点，四条边的中点，以及 bbox 的中心点。\n\n我们简单地描述一下坐标预测分支：使用 RoIAlign 抽取 RoI 得到空间尺度固定为 14x14 的 feature，然后跟着是 8 个 3x3 的空洞卷积，目的是为了增大感受野，接着是两个 2x 反卷积，得到空间尺度为 56x56 的 feature，其通道数为 NxN，然后应用 pixel-wise 的 sigmoid 函数，得到 NxN 个概率热图 heatmap，注意 heatmap 与 feature map 两者是不同的，每个热图对应一个 grid point，且对应一个 supervision map，在这个 supervision map 上以 target grid point 为中心的可以组成十字型的 5 个像素点标记为正 $t_i=1$，其他像素位置处表示为负 $t_i=0$（target grid point 在 supervision map 上的位置在 2.3 一节介绍），然后使用 binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$进行优化，其中 $p_i$ 表示某个 pixel 处的概率。\n\nInference 阶段，在每个 heatmap 上选择最高置信度的点，记为 $(H_x,H_y)$，经过简单的映射计算可以得到其在原始 image 平面上的坐标，记为 $(I_x,I_y)$，\n$$I_x=P_x+ \\frac {H_x}{w_o} w_p\n\\\\\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$\n其中 $(P_x,P_y)$ 为 region proposal 的左上角在原始 image 平面上的坐标，$(w_p,h_p)$ 表示 proposal 的宽高，$(w_o,h_o)$ 表示输出 heatmap 的宽高。 __注意：__ heatmap 与 proposal 尺度不一定相同！！！   \n有了预测的 grid points 之后就可以得到目标 bbox 的四个边的坐标，记为 $B=(x_l,y_u,x_r,y_b)$。第 j 个 grid point 记为 $g_j$，其坐标为 $(x_j,y_j)$，其概率为 $p_j$（第 j 个 heatmap 上置信度最高值），定义 $E_i$ 为位于第 i 个边（左上右下，其中一个）上的 grid points 的下标集合，即，如果 $g_j$ 位于 第 i 个边上，那么 $j \\in E_i$，于是根据 grid points 集合可以计算 B，采用加权平均，如下，\n$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j\n\\\\\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$\n\n### 2.2 Grid Points Feature Fusion\n由 FCN 得到得 heatmap 保留了 RoI 的空间信息，所以 grid points 之间存在某种联系，他们之间可以互相校正位置，通过一种空间信息融合方法得以实现。以 3x3 grid points 为例，要校正左上角 point，我们可以利用其他 grid points 对应的 feature maps 上左上角区域的特征。\n\n为了区分不同 grid points 的 feature maps，使用 NxN 不同的 filters 从最后的 feature map 上分别为这些 grid points 抽取特征，于是每个 feature map 与其对应的 grid point 有特殊的联系，称第 i 个 point 对应的 feature map 为 $F_i$。\n\n对于每个 grid point，与其 L1 距离为 1 的其他 points 参与融合过程，称这些 points 为 source points，记为 $S_i$，对于 $S_i$ 中的 point j，其 feature maps $F_j$ 经过三个连续的 5x5 卷积，记此过程为 $T_{j \\rightarrow i}$，$S_i$ 中所有 source points 经过此过程后得到的 features 再与 $F_i$ 进行融合得到 $F_i'$，融合采用简单的求和操作，\n$$F_i'=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$\n![](/images/Grid-RCNN_fig3.png)\n\n如图 3(a) 是左上角 point 的融合示意图。有了融合后的特征 $F_i'$ 后，还可进行二次融合，类似的，对 source point 的 $F_i'$ 特征应用变换函数 $T_{j \\rightarrow i}^+$（几个连续的卷积），其参数不与第一个融合的过程共享，第二次融合后的特征记为 $F_i''$，用于输出最终的 heatmap，并从中根据前述方法取最大置信度得到 grid point 位置预测。第二次融合使得 L1 距离小于等于 2 的 points 的 features 对当前 grid point 均有贡献，如图 3(b)。\n\n__注意：__ 融合前后的两组 feature maps 均通过 sigmoid 计算得到两组 heatmaps，然后分别与 supervision maps 计算出前后两处 loss（作为位置预测损失），损失计算如上面所述均采用 Binary Cross-Entropy Loss。\n\n### 2.3 Extended Region Mapping\nGrid 预测模块输出的 heatmap 是固定尺度的，各 pixel 位置处的概率表示可能是 grid point 的概率。对 RoI 特征应用的是全卷积 FCN，所以空间信息得以保留，输出 heatmap 自然就对应 region proposal 在原 image 上的空间区域，但是，region proposal 不一定完全包含目标，这意味着 gt grid point 可能位于 region proposal 之外，导致无法在 supervision map 上标记 target grid point，这种缺失造成训练阶段训练样本的利用率低的问题，同时在 inference 阶段，简单地在 heatmap 上选择置信度最大的 pixel 作为 grid point，由于 region proposal 没有完全包含目标，可能会选择出一个错误的 grid point 位置，从而导致预测存在偏差。很多场合下有超过一半的 gt grid points 都没有被 region proposals 覆盖到，如图 4，proposal（最小白框）比 gt box 小，9 个 grid points 中有 7 个将会位于输出 heatmap 之外。\n\n解决上述问题的一个最自然的方法是增大 proposal，从而确保大部分 gt grid points 还是会被 proposal 包含进来，但是这同时也会包含 背景或其他目标 的特征，这些特征对于预测当前目标是冗余的，甚至是有害的。作者实验显示，简单的增大 proposal 确实没什么好处，在小目标检测上是有害的。作者采用的解决方法是修改输出 heatmap 和原 image 上 region 之间的关系：当给定 proposal 时，RoI 特征依然是从 feature map 上相同 region 中获取，此处不需要增大 proposal；但是重新定义输出 heatmap 所代表的区域为 region 的两倍大，这样大多数场合下，所有的 grid points 都被包含进来了，如图 4 中虚线框所示，但是 supervision map 与输出 heatmap 尺度相同的，这相当于将 gt box 缩小一倍然后映射到 supervision map 上从而进行标记，标记方法前文已经提到过，在以 gt grid point 为中心的十字型 pixels，即 gt grid point 和位于其上下左右四个最近邻 pixels，共 5 个 pixels 标记为正。\n![](/images/Grid-RCNN_fig4.png)\n\n扩展的区域映射使用公式表达如下，\n$$I_x'=P_x+\\frac {4H_x-w_o}{2w_o}w_p\n\\\\\\\\ I_y'=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$\n\n以图 4 为例，推导一下上式的由来。  \n记 heatmap 对应到原 image 上两倍 region proposal 大小的区域为 R'，类似于 $(I_x,I_y)$ 地有，\n$$I_x'=P_x'+\\frac {H_x}{w_o} 2 w_p, \\quad I_y'=P_y'+\\frac {H_y}{h_o} 2 h_p$$\n其中 $(P_x',P_y')$ 是 R' 的左上角在原 image 上的坐标，其与 proposal 的左上角坐标具有关系，\n$$2(x_c-P_x)=x_c-P_x'=w_p, \\quad 2(y_c-P_y)=y_c-P_y'=h_p$$\n其中 $(x_c,y_c)$ 是 proposal 与 R' 共同的中心在原 image 上的坐标，综合上两式即可证明。\n\n### 2.4 Implementation Details\n实现细节和实验分析略，请阅读原文。\n\n# Grid R-CNN Plus\n论文 [Grid R-CNN Plus: Faster and Better](https://arxiv.org/abs/1906.05688)\n\n前面讲到将 Grid R-CNN 模块引入到 two-stage 目标检测器中可以显著提升 mAP，然而 Grid R-CNN 的计算量较大导致 inference 时间较长，从而使其难以广泛应用，所以这篇文章提出 Grid R-CNN Plus，通过一些有效的改进使其成为更好更快的目标检测器。\n\n大概介绍一下 Grid R-CNN Plus：对于每个 grid point，Grid R-CNN 均使用相同的表示区域来生成 supervision map，显然这是低效的，例如，左上 grid point 不会出现在其对应 supervision map 的右部和底部，所以在 Grid R-CNN Plus 中，仅监督最有可能的 1/4 区域，这样就降低了 grid 分支中 feature maps 的尺度。另外，特征融合阶段的卷积层数量也减少了。如此，降低计算损耗的同时，由于更加专注 grid point 的表示区域（原来区域中最有可能的 1/4），grid point 定位也更加准确。\n\n在采用策略，归一化方法，NMS 策略和超参数上也进行了分析和优化。\n\n## 回顾 Grid R-CNN\n图 1 是 Grid R-CNN 的框架结构示意图。与一般 two-stage 检测器类似，Grid R-CNN 包含 RPN 和 R-CNN。基于 region propopsals，使用 RoIAlign 从 CNN backbone 的输出 feature maps 上 RoI 对应区域抽取特征，这个 RoI 特征用于预测分类和 bbox 定位。Grid R-CNN 与一般类 R-CNN 的不同之处在于使用 grid points 代替了坐标偏差。Grid 预测分支使用 FCN 结构输出保留空间信息的 heatmaps，从这些 heatmaps 中可以分别定位目标的 grid points。\n![](/images/Grid-RCNN-Plus_fig1.png)\n\nGrid R-CNN 使用 8 个 3x3 的卷积和两个 2x 反卷积得到 heatmap，这种分支结构比较重量级。降低计算量的一种方法是：首先从 RPN 中选择 1000 的 proposals，然后送入分类分支得到分类得分后，进行 NMS 之后然后选择其中 top 100 的 proposals，然后再送入 grid 分支，以降低计算耗时。\n\n为了提高准确性，还使用了特征融合机制和扩展区域映射，其中特征融合利用了 grid points 空间相关性互相进行校正，扩展区域映射则解决了 gt grid points 位于 proposal 之外的痛点。\n\n## Grid R-CNN Plus\n### Grid Point 专用表示区域\n因为只有 IoU > 0.5 的 proposals（正例）才可能会被选择送入 Grid 分支，所以 supervision map 上 gt grid point 被限制在一个较小的区域，如图 2 所示是各个 grid point 的 gt label 分布，以 3x3 grid points 为例说明，左上角 point 的 gt label 只可能出现在 supervision map 的左上角区域，所以，如果所有 grid points 的表示区域均相同（相同的 scale 和 center），那么大多数 pixel 的输出均不会被激活，这是很低效的，于是就有了 grid point 专用表示区域。\n![](/images/Grid-RCNN-Plus_fig2.png)\n\n原来每个 grid point 的表示区域的尺度是 56x56，现在降为 28x28，是原来区域中最有可能的 1/4，这样输出 heatmap 尺度降为一半。Grid point 专用表示区域也可以看作一种规范化过程，即，从原来的有偏分布到现在的规范化分布，如图 2 中，上左 point 和 中右 point 的 gt label 分布均以区域中心为中心。\n\n### Light Grid Head\n由于输出 heatmap 尺度变为一半，我们同时将 grid 分支中其中 features 的分辨率也降为一半（例如从 14x14 降为 7x7），这降低了计算损耗。具体而言，使用 RoIAlign 从 RoI 中抽取固定大小 14x14 的特征之后，使用一个 3x3 stride=2 的卷积层，使得特征 size 降为 7x7，在这之后，使用 7 个 3x3 stride=1 的卷积层，每层的输出特征大小均为 7x7，最终特征分为 N 组（默认为 9）每组特征对应一个 grid point。我们显式地将特征与 grid points 一一起来，然后使用 2 个 2x 反卷积生成 28x28 的 heatmaps。\n\ngrid point 专用表示区域使得不同的 grid points 的特征之间更加联系紧密，所以不需要很多的卷积层来弥补不同 grid points 特征之间的差异，所以 Grid R-CNN Plus 仅使用一个 5x5 depth-wise 的卷积，depth-wise 表示一个卷积核负责一个通道（二维平面上的卷积），而 Grid R-CNN 则使用三个 5x5 的普通卷积。这个改进也使得 grid 分支更加轻量。\n\n### 跨越图像的采样策略\ngrid branch 仅使用正例（IoU > 0.5 的 positive proposals）进行训练，那么不同训练批次如果具有不同数量的正例，会对最终性能产品影响，例如，某些图像只有极少数正例，而其他图像可能会包含成百上千的正例，这种情况会导致 grid 分支的特征分布不稳定。所以在 Grid R-CNN Plus 中采用跨越单个图像的采样策略：当某个图像中正例数量较小时，可以使用其他图像来填充正例的空缺。具体操作为，将原来单个图像采样 96 个正例改为每两个图像采样 192 个正例。这种改进使得训练更加稳定，性能也得到提升。\n\n### 仅一次 NMS\nGrid R-CNN 中，proposals 经过分类分支得到分类得分，然后使用 IoU 阈值 0.5 进行非极大抑制 NMS，之后取分类得分 top 125 的 proposals 送入 grid 分支进行定位预测，然后再次进行 NMS 以得到更加精确的结果。然而 NMS 是非常耗费计算量的，实验显示即使较少数量的 proposals，在 80 个分类（COCO 分类数量）上的 NMS 也很慢，所以 Grid R-CNN Plus 中移除了第二次 NMS。\n\n## 实验\n实验略。","source":"_posts/Grid-RCNN.md","raw":"---\ntitle: Grid-RCNN\ndate: 2019-07-19 17:44:19\ntags: object detection\nmathjax: true\n---\n# Grid R-CNN\n论文 [Grid R-CNN](https://arxiv.org/abs/1811.12030)\n\n源码 [Grid R-CNN](https://github.com/STVIR/Grid-R-CNN)\n<!-- more -->\n## 1. 简介\n目标检测器包含两个分支：分类和回归，其中回归分支通常由 conv 和 fc 层组成，因为最终的输出单元数量固定为 4，表示 x,y,w,h 的坐标偏差（或者固定为 4*(C+1)，表示坐标偏差的预测是分类相关的），而 fc 层适用输出单元数量固定的场景。\n![](/images/Grid-RCNN_fig1.png)\n\n这篇文章介绍了 Grid R-CNN，主要是修改了类 R-CNN 目标检测器的回归分支，与传统类 R-CNN 的区别主要如图 1 所示，即，使用 grid points 表示目标定位：在得到 RoI 特征后，使用全卷积 FCN 得到特征图，然后再通过某种方法得到 grid points。此修改有如下优点：\n1. FCN 保留了 RoI 的空间信息，二维（显然，之前的方法都是展平成了一维向量），所以可准确的得到 grid points\n2. 根据 grid points 可以更加精确的获得目标定位。以前的方法都是通过左上角和右下角坐标确定目标 bbox，这种方法显然较为粗糙，因为目标形状的任意性，角点可能位于目标形状之外的背景上，角点处的局部特征并不能反映目标的特征（ExtremeNet 中也提过 CornerNet 的这个问题），而 grid points 采用 3x3 的点作为监督，由于点数增多，可以降低其中某些不准确点对位置预测的负面影响。如图 1(b)，右上角的坐标不准确，但是中上（top-middle）点可以帮助校正最终的目标位置。\n\n利用 grid points 数量多这个优势，文中还提出一种信息融合方法，为每个 grid point 设计独立的一组 feature maps：对某个 grid point，聚集其邻近 grid point 的 feature maps 并进行融合成，作为当前这个 grid point 的 feature map，这个合成的 feature map 则用于预测当前 grid point 的位置。这种空间信息互补的方法可以使得位置预测更加准确。\n\n## 2. Grid R-CNN\n网络的整体结构如图 2 所示，基于 region proposals，使用 RoIAlign 抽取出 RoI 特征，然后分为两支，其中一支用于目标分类预测，另一支经过 FCN 得到 feature maps，然后经过 sigmoid 得到概率热图（probability heatmap），注意不是传统的坐标偏差，heatmaps 可由 target map 监督，从这个概率热图上我们可以通过某种方法得到 grid points（下文 2.1 一节会介绍这种方法），再经过上述的空间信息融合方法（2.2 一节会介绍），最后得到准确的目标定位。接下来我们将整个问题分而治之进行介绍。\n![](/images/Grid-RCNN_fig2.png)\n\n### 2.1 Grid Guided Localization\n一般地， NxN 的 grid points 位列 bbox 之上。以上文提到的 3x3 的 grid points 为例，共四个角点，四条边的中点，以及 bbox 的中心点。\n\n我们简单地描述一下坐标预测分支：使用 RoIAlign 抽取 RoI 得到空间尺度固定为 14x14 的 feature，然后跟着是 8 个 3x3 的空洞卷积，目的是为了增大感受野，接着是两个 2x 反卷积，得到空间尺度为 56x56 的 feature，其通道数为 NxN，然后应用 pixel-wise 的 sigmoid 函数，得到 NxN 个概率热图 heatmap，注意 heatmap 与 feature map 两者是不同的，每个热图对应一个 grid point，且对应一个 supervision map，在这个 supervision map 上以 target grid point 为中心的可以组成十字型的 5 个像素点标记为正 $t_i=1$，其他像素位置处表示为负 $t_i=0$（target grid point 在 supervision map 上的位置在 2.3 一节介绍），然后使用 binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$进行优化，其中 $p_i$ 表示某个 pixel 处的概率。\n\nInference 阶段，在每个 heatmap 上选择最高置信度的点，记为 $(H_x,H_y)$，经过简单的映射计算可以得到其在原始 image 平面上的坐标，记为 $(I_x,I_y)$，\n$$I_x=P_x+ \\frac {H_x}{w_o} w_p\n\\\\\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$\n其中 $(P_x,P_y)$ 为 region proposal 的左上角在原始 image 平面上的坐标，$(w_p,h_p)$ 表示 proposal 的宽高，$(w_o,h_o)$ 表示输出 heatmap 的宽高。 __注意：__ heatmap 与 proposal 尺度不一定相同！！！   \n有了预测的 grid points 之后就可以得到目标 bbox 的四个边的坐标，记为 $B=(x_l,y_u,x_r,y_b)$。第 j 个 grid point 记为 $g_j$，其坐标为 $(x_j,y_j)$，其概率为 $p_j$（第 j 个 heatmap 上置信度最高值），定义 $E_i$ 为位于第 i 个边（左上右下，其中一个）上的 grid points 的下标集合，即，如果 $g_j$ 位于 第 i 个边上，那么 $j \\in E_i$，于是根据 grid points 集合可以计算 B，采用加权平均，如下，\n$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j\n\\\\\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$\n\n### 2.2 Grid Points Feature Fusion\n由 FCN 得到得 heatmap 保留了 RoI 的空间信息，所以 grid points 之间存在某种联系，他们之间可以互相校正位置，通过一种空间信息融合方法得以实现。以 3x3 grid points 为例，要校正左上角 point，我们可以利用其他 grid points 对应的 feature maps 上左上角区域的特征。\n\n为了区分不同 grid points 的 feature maps，使用 NxN 不同的 filters 从最后的 feature map 上分别为这些 grid points 抽取特征，于是每个 feature map 与其对应的 grid point 有特殊的联系，称第 i 个 point 对应的 feature map 为 $F_i$。\n\n对于每个 grid point，与其 L1 距离为 1 的其他 points 参与融合过程，称这些 points 为 source points，记为 $S_i$，对于 $S_i$ 中的 point j，其 feature maps $F_j$ 经过三个连续的 5x5 卷积，记此过程为 $T_{j \\rightarrow i}$，$S_i$ 中所有 source points 经过此过程后得到的 features 再与 $F_i$ 进行融合得到 $F_i'$，融合采用简单的求和操作，\n$$F_i'=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$\n![](/images/Grid-RCNN_fig3.png)\n\n如图 3(a) 是左上角 point 的融合示意图。有了融合后的特征 $F_i'$ 后，还可进行二次融合，类似的，对 source point 的 $F_i'$ 特征应用变换函数 $T_{j \\rightarrow i}^+$（几个连续的卷积），其参数不与第一个融合的过程共享，第二次融合后的特征记为 $F_i''$，用于输出最终的 heatmap，并从中根据前述方法取最大置信度得到 grid point 位置预测。第二次融合使得 L1 距离小于等于 2 的 points 的 features 对当前 grid point 均有贡献，如图 3(b)。\n\n__注意：__ 融合前后的两组 feature maps 均通过 sigmoid 计算得到两组 heatmaps，然后分别与 supervision maps 计算出前后两处 loss（作为位置预测损失），损失计算如上面所述均采用 Binary Cross-Entropy Loss。\n\n### 2.3 Extended Region Mapping\nGrid 预测模块输出的 heatmap 是固定尺度的，各 pixel 位置处的概率表示可能是 grid point 的概率。对 RoI 特征应用的是全卷积 FCN，所以空间信息得以保留，输出 heatmap 自然就对应 region proposal 在原 image 上的空间区域，但是，region proposal 不一定完全包含目标，这意味着 gt grid point 可能位于 region proposal 之外，导致无法在 supervision map 上标记 target grid point，这种缺失造成训练阶段训练样本的利用率低的问题，同时在 inference 阶段，简单地在 heatmap 上选择置信度最大的 pixel 作为 grid point，由于 region proposal 没有完全包含目标，可能会选择出一个错误的 grid point 位置，从而导致预测存在偏差。很多场合下有超过一半的 gt grid points 都没有被 region proposals 覆盖到，如图 4，proposal（最小白框）比 gt box 小，9 个 grid points 中有 7 个将会位于输出 heatmap 之外。\n\n解决上述问题的一个最自然的方法是增大 proposal，从而确保大部分 gt grid points 还是会被 proposal 包含进来，但是这同时也会包含 背景或其他目标 的特征，这些特征对于预测当前目标是冗余的，甚至是有害的。作者实验显示，简单的增大 proposal 确实没什么好处，在小目标检测上是有害的。作者采用的解决方法是修改输出 heatmap 和原 image 上 region 之间的关系：当给定 proposal 时，RoI 特征依然是从 feature map 上相同 region 中获取，此处不需要增大 proposal；但是重新定义输出 heatmap 所代表的区域为 region 的两倍大，这样大多数场合下，所有的 grid points 都被包含进来了，如图 4 中虚线框所示，但是 supervision map 与输出 heatmap 尺度相同的，这相当于将 gt box 缩小一倍然后映射到 supervision map 上从而进行标记，标记方法前文已经提到过，在以 gt grid point 为中心的十字型 pixels，即 gt grid point 和位于其上下左右四个最近邻 pixels，共 5 个 pixels 标记为正。\n![](/images/Grid-RCNN_fig4.png)\n\n扩展的区域映射使用公式表达如下，\n$$I_x'=P_x+\\frac {4H_x-w_o}{2w_o}w_p\n\\\\\\\\ I_y'=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$\n\n以图 4 为例，推导一下上式的由来。  \n记 heatmap 对应到原 image 上两倍 region proposal 大小的区域为 R'，类似于 $(I_x,I_y)$ 地有，\n$$I_x'=P_x'+\\frac {H_x}{w_o} 2 w_p, \\quad I_y'=P_y'+\\frac {H_y}{h_o} 2 h_p$$\n其中 $(P_x',P_y')$ 是 R' 的左上角在原 image 上的坐标，其与 proposal 的左上角坐标具有关系，\n$$2(x_c-P_x)=x_c-P_x'=w_p, \\quad 2(y_c-P_y)=y_c-P_y'=h_p$$\n其中 $(x_c,y_c)$ 是 proposal 与 R' 共同的中心在原 image 上的坐标，综合上两式即可证明。\n\n### 2.4 Implementation Details\n实现细节和实验分析略，请阅读原文。\n\n# Grid R-CNN Plus\n论文 [Grid R-CNN Plus: Faster and Better](https://arxiv.org/abs/1906.05688)\n\n前面讲到将 Grid R-CNN 模块引入到 two-stage 目标检测器中可以显著提升 mAP，然而 Grid R-CNN 的计算量较大导致 inference 时间较长，从而使其难以广泛应用，所以这篇文章提出 Grid R-CNN Plus，通过一些有效的改进使其成为更好更快的目标检测器。\n\n大概介绍一下 Grid R-CNN Plus：对于每个 grid point，Grid R-CNN 均使用相同的表示区域来生成 supervision map，显然这是低效的，例如，左上 grid point 不会出现在其对应 supervision map 的右部和底部，所以在 Grid R-CNN Plus 中，仅监督最有可能的 1/4 区域，这样就降低了 grid 分支中 feature maps 的尺度。另外，特征融合阶段的卷积层数量也减少了。如此，降低计算损耗的同时，由于更加专注 grid point 的表示区域（原来区域中最有可能的 1/4），grid point 定位也更加准确。\n\n在采用策略，归一化方法，NMS 策略和超参数上也进行了分析和优化。\n\n## 回顾 Grid R-CNN\n图 1 是 Grid R-CNN 的框架结构示意图。与一般 two-stage 检测器类似，Grid R-CNN 包含 RPN 和 R-CNN。基于 region propopsals，使用 RoIAlign 从 CNN backbone 的输出 feature maps 上 RoI 对应区域抽取特征，这个 RoI 特征用于预测分类和 bbox 定位。Grid R-CNN 与一般类 R-CNN 的不同之处在于使用 grid points 代替了坐标偏差。Grid 预测分支使用 FCN 结构输出保留空间信息的 heatmaps，从这些 heatmaps 中可以分别定位目标的 grid points。\n![](/images/Grid-RCNN-Plus_fig1.png)\n\nGrid R-CNN 使用 8 个 3x3 的卷积和两个 2x 反卷积得到 heatmap，这种分支结构比较重量级。降低计算量的一种方法是：首先从 RPN 中选择 1000 的 proposals，然后送入分类分支得到分类得分后，进行 NMS 之后然后选择其中 top 100 的 proposals，然后再送入 grid 分支，以降低计算耗时。\n\n为了提高准确性，还使用了特征融合机制和扩展区域映射，其中特征融合利用了 grid points 空间相关性互相进行校正，扩展区域映射则解决了 gt grid points 位于 proposal 之外的痛点。\n\n## Grid R-CNN Plus\n### Grid Point 专用表示区域\n因为只有 IoU > 0.5 的 proposals（正例）才可能会被选择送入 Grid 分支，所以 supervision map 上 gt grid point 被限制在一个较小的区域，如图 2 所示是各个 grid point 的 gt label 分布，以 3x3 grid points 为例说明，左上角 point 的 gt label 只可能出现在 supervision map 的左上角区域，所以，如果所有 grid points 的表示区域均相同（相同的 scale 和 center），那么大多数 pixel 的输出均不会被激活，这是很低效的，于是就有了 grid point 专用表示区域。\n![](/images/Grid-RCNN-Plus_fig2.png)\n\n原来每个 grid point 的表示区域的尺度是 56x56，现在降为 28x28，是原来区域中最有可能的 1/4，这样输出 heatmap 尺度降为一半。Grid point 专用表示区域也可以看作一种规范化过程，即，从原来的有偏分布到现在的规范化分布，如图 2 中，上左 point 和 中右 point 的 gt label 分布均以区域中心为中心。\n\n### Light Grid Head\n由于输出 heatmap 尺度变为一半，我们同时将 grid 分支中其中 features 的分辨率也降为一半（例如从 14x14 降为 7x7），这降低了计算损耗。具体而言，使用 RoIAlign 从 RoI 中抽取固定大小 14x14 的特征之后，使用一个 3x3 stride=2 的卷积层，使得特征 size 降为 7x7，在这之后，使用 7 个 3x3 stride=1 的卷积层，每层的输出特征大小均为 7x7，最终特征分为 N 组（默认为 9）每组特征对应一个 grid point。我们显式地将特征与 grid points 一一起来，然后使用 2 个 2x 反卷积生成 28x28 的 heatmaps。\n\ngrid point 专用表示区域使得不同的 grid points 的特征之间更加联系紧密，所以不需要很多的卷积层来弥补不同 grid points 特征之间的差异，所以 Grid R-CNN Plus 仅使用一个 5x5 depth-wise 的卷积，depth-wise 表示一个卷积核负责一个通道（二维平面上的卷积），而 Grid R-CNN 则使用三个 5x5 的普通卷积。这个改进也使得 grid 分支更加轻量。\n\n### 跨越图像的采样策略\ngrid branch 仅使用正例（IoU > 0.5 的 positive proposals）进行训练，那么不同训练批次如果具有不同数量的正例，会对最终性能产品影响，例如，某些图像只有极少数正例，而其他图像可能会包含成百上千的正例，这种情况会导致 grid 分支的特征分布不稳定。所以在 Grid R-CNN Plus 中采用跨越单个图像的采样策略：当某个图像中正例数量较小时，可以使用其他图像来填充正例的空缺。具体操作为，将原来单个图像采样 96 个正例改为每两个图像采样 192 个正例。这种改进使得训练更加稳定，性能也得到提升。\n\n### 仅一次 NMS\nGrid R-CNN 中，proposals 经过分类分支得到分类得分，然后使用 IoU 阈值 0.5 进行非极大抑制 NMS，之后取分类得分 top 125 的 proposals 送入 grid 分支进行定位预测，然后再次进行 NMS 以得到更加精确的结果。然而 NMS 是非常耗费计算量的，实验显示即使较少数量的 proposals，在 80 个分类（COCO 分类数量）上的 NMS 也很慢，所以 Grid R-CNN Plus 中移除了第二次 NMS。\n\n## 实验\n实验略。","slug":"Grid-RCNN","published":1,"updated":"2020-04-24T10:38:03.391Z","_id":"ck9dzcips0012gga6cg30bkya","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Grid-R-CNN\"><a href=\"#Grid-R-CNN\" class=\"headerlink\" title=\"Grid R-CNN\"></a>Grid R-CNN</h1><p>论文 <a href=\"https://arxiv.org/abs/1811.12030\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<p>源码 <a href=\"https://github.com/STVIR/Grid-R-CNN\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<a id=\"more\"></a>\n<h2 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1. 简介\"></a>1. 简介</h2><p>目标检测器包含两个分支：分类和回归，其中回归分支通常由 conv 和 fc 层组成，因为最终的输出单元数量固定为 4，表示 x,y,w,h 的坐标偏差（或者固定为 4*(C+1)，表示坐标偏差的预测是分类相关的），而 fc 层适用输出单元数量固定的场景。<br><img src=\"/images/Grid-RCNN_fig1.png\" alt=\"\"></p>\n<p>这篇文章介绍了 Grid R-CNN，主要是修改了类 R-CNN 目标检测器的回归分支，与传统类 R-CNN 的区别主要如图 1 所示，即，使用 grid points 表示目标定位：在得到 RoI 特征后，使用全卷积 FCN 得到特征图，然后再通过某种方法得到 grid points。此修改有如下优点：</p>\n<ol>\n<li>FCN 保留了 RoI 的空间信息，二维（显然，之前的方法都是展平成了一维向量），所以可准确的得到 grid points</li>\n<li>根据 grid points 可以更加精确的获得目标定位。以前的方法都是通过左上角和右下角坐标确定目标 bbox，这种方法显然较为粗糙，因为目标形状的任意性，角点可能位于目标形状之外的背景上，角点处的局部特征并不能反映目标的特征（ExtremeNet 中也提过 CornerNet 的这个问题），而 grid points 采用 3x3 的点作为监督，由于点数增多，可以降低其中某些不准确点对位置预测的负面影响。如图 1(b)，右上角的坐标不准确，但是中上（top-middle）点可以帮助校正最终的目标位置。</li>\n</ol>\n<p>利用 grid points 数量多这个优势，文中还提出一种信息融合方法，为每个 grid point 设计独立的一组 feature maps：对某个 grid point，聚集其邻近 grid point 的 feature maps 并进行融合成，作为当前这个 grid point 的 feature map，这个合成的 feature map 则用于预测当前 grid point 的位置。这种空间信息互补的方法可以使得位置预测更加准确。</p>\n<h2 id=\"2-Grid-R-CNN\"><a href=\"#2-Grid-R-CNN\" class=\"headerlink\" title=\"2. Grid R-CNN\"></a>2. Grid R-CNN</h2><p>网络的整体结构如图 2 所示，基于 region proposals，使用 RoIAlign 抽取出 RoI 特征，然后分为两支，其中一支用于目标分类预测，另一支经过 FCN 得到 feature maps，然后经过 sigmoid 得到概率热图（probability heatmap），注意不是传统的坐标偏差，heatmaps 可由 target map 监督，从这个概率热图上我们可以通过某种方法得到 grid points（下文 2.1 一节会介绍这种方法），再经过上述的空间信息融合方法（2.2 一节会介绍），最后得到准确的目标定位。接下来我们将整个问题分而治之进行介绍。<br><img src=\"/images/Grid-RCNN_fig2.png\" alt=\"\"></p>\n<h3 id=\"2-1-Grid-Guided-Localization\"><a href=\"#2-1-Grid-Guided-Localization\" class=\"headerlink\" title=\"2.1 Grid Guided Localization\"></a>2.1 Grid Guided Localization</h3><p>一般地， NxN 的 grid points 位列 bbox 之上。以上文提到的 3x3 的 grid points 为例，共四个角点，四条边的中点，以及 bbox 的中心点。</p>\n<p>我们简单地描述一下坐标预测分支：使用 RoIAlign 抽取 RoI 得到空间尺度固定为 14x14 的 feature，然后跟着是 8 个 3x3 的空洞卷积，目的是为了增大感受野，接着是两个 2x 反卷积，得到空间尺度为 56x56 的 feature，其通道数为 NxN，然后应用 pixel-wise 的 sigmoid 函数，得到 NxN 个概率热图 heatmap，注意 heatmap 与 feature map 两者是不同的，每个热图对应一个 grid point，且对应一个 supervision map，在这个 supervision map 上以 target grid point 为中心的可以组成十字型的 5 个像素点标记为正 $t_i=1$，其他像素位置处表示为负 $t_i=0$（target grid point 在 supervision map 上的位置在 2.3 一节介绍），然后使用 binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$进行优化，其中 $p_i$ 表示某个 pixel 处的概率。</p>\n<p>Inference 阶段，在每个 heatmap 上选择最高置信度的点，记为 $(H_x,H_y)$，经过简单的映射计算可以得到其在原始 image 平面上的坐标，记为 $(I_x,I_y)$，<br>$$I_x=P_x+ \\frac {H_x}{w_o} w_p<br>\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$<br>其中 $(P_x,P_y)$ 为 region proposal 的左上角在原始 image 平面上的坐标，$(w_p,h_p)$ 表示 proposal 的宽高，$(w_o,h_o)$ 表示输出 heatmap 的宽高。 <strong>注意：</strong> heatmap 与 proposal 尺度不一定相同！！！<br>有了预测的 grid points 之后就可以得到目标 bbox 的四个边的坐标，记为 $B=(x_l,y_u,x_r,y_b)$。第 j 个 grid point 记为 $g_j$，其坐标为 $(x_j,y_j)$，其概率为 $p_j$（第 j 个 heatmap 上置信度最高值），定义 $E_i$ 为位于第 i 个边（左上右下，其中一个）上的 grid points 的下标集合，即，如果 $g_j$ 位于 第 i 个边上，那么 $j \\in E_i$，于是根据 grid points 集合可以计算 B，采用加权平均，如下，<br>$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j<br>\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$</p>\n<h3 id=\"2-2-Grid-Points-Feature-Fusion\"><a href=\"#2-2-Grid-Points-Feature-Fusion\" class=\"headerlink\" title=\"2.2 Grid Points Feature Fusion\"></a>2.2 Grid Points Feature Fusion</h3><p>由 FCN 得到得 heatmap 保留了 RoI 的空间信息，所以 grid points 之间存在某种联系，他们之间可以互相校正位置，通过一种空间信息融合方法得以实现。以 3x3 grid points 为例，要校正左上角 point，我们可以利用其他 grid points 对应的 feature maps 上左上角区域的特征。</p>\n<p>为了区分不同 grid points 的 feature maps，使用 NxN 不同的 filters 从最后的 feature map 上分别为这些 grid points 抽取特征，于是每个 feature map 与其对应的 grid point 有特殊的联系，称第 i 个 point 对应的 feature map 为 $F_i$。</p>\n<p>对于每个 grid point，与其 L1 距离为 1 的其他 points 参与融合过程，称这些 points 为 source points，记为 $S_i$，对于 $S_i$ 中的 point j，其 feature maps $F_j$ 经过三个连续的 5x5 卷积，记此过程为 $T_{j \\rightarrow i}$，$S_i$ 中所有 source points 经过此过程后得到的 features 再与 $F_i$ 进行融合得到 $F_i’$，融合采用简单的求和操作，<br>$$F_i’=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$<br><img src=\"/images/Grid-RCNN_fig3.png\" alt=\"\"></p>\n<p>如图 3(a) 是左上角 point 的融合示意图。有了融合后的特征 $F_i’$ 后，还可进行二次融合，类似的，对 source point 的 $F_i’$ 特征应用变换函数 $T_{j \\rightarrow i}^+$（几个连续的卷积），其参数不与第一个融合的过程共享，第二次融合后的特征记为 $F_i’’$，用于输出最终的 heatmap，并从中根据前述方法取最大置信度得到 grid point 位置预测。第二次融合使得 L1 距离小于等于 2 的 points 的 features 对当前 grid point 均有贡献，如图 3(b)。</p>\n<p><strong>注意：</strong> 融合前后的两组 feature maps 均通过 sigmoid 计算得到两组 heatmaps，然后分别与 supervision maps 计算出前后两处 loss（作为位置预测损失），损失计算如上面所述均采用 Binary Cross-Entropy Loss。</p>\n<h3 id=\"2-3-Extended-Region-Mapping\"><a href=\"#2-3-Extended-Region-Mapping\" class=\"headerlink\" title=\"2.3 Extended Region Mapping\"></a>2.3 Extended Region Mapping</h3><p>Grid 预测模块输出的 heatmap 是固定尺度的，各 pixel 位置处的概率表示可能是 grid point 的概率。对 RoI 特征应用的是全卷积 FCN，所以空间信息得以保留，输出 heatmap 自然就对应 region proposal 在原 image 上的空间区域，但是，region proposal 不一定完全包含目标，这意味着 gt grid point 可能位于 region proposal 之外，导致无法在 supervision map 上标记 target grid point，这种缺失造成训练阶段训练样本的利用率低的问题，同时在 inference 阶段，简单地在 heatmap 上选择置信度最大的 pixel 作为 grid point，由于 region proposal 没有完全包含目标，可能会选择出一个错误的 grid point 位置，从而导致预测存在偏差。很多场合下有超过一半的 gt grid points 都没有被 region proposals 覆盖到，如图 4，proposal（最小白框）比 gt box 小，9 个 grid points 中有 7 个将会位于输出 heatmap 之外。</p>\n<p>解决上述问题的一个最自然的方法是增大 proposal，从而确保大部分 gt grid points 还是会被 proposal 包含进来，但是这同时也会包含 背景或其他目标 的特征，这些特征对于预测当前目标是冗余的，甚至是有害的。作者实验显示，简单的增大 proposal 确实没什么好处，在小目标检测上是有害的。作者采用的解决方法是修改输出 heatmap 和原 image 上 region 之间的关系：当给定 proposal 时，RoI 特征依然是从 feature map 上相同 region 中获取，此处不需要增大 proposal；但是重新定义输出 heatmap 所代表的区域为 region 的两倍大，这样大多数场合下，所有的 grid points 都被包含进来了，如图 4 中虚线框所示，但是 supervision map 与输出 heatmap 尺度相同的，这相当于将 gt box 缩小一倍然后映射到 supervision map 上从而进行标记，标记方法前文已经提到过，在以 gt grid point 为中心的十字型 pixels，即 gt grid point 和位于其上下左右四个最近邻 pixels，共 5 个 pixels 标记为正。<br><img src=\"/images/Grid-RCNN_fig4.png\" alt=\"\"></p>\n<p>扩展的区域映射使用公式表达如下，<br>$$I_x’=P_x+\\frac {4H_x-w_o}{2w_o}w_p<br>\\\\ I_y’=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$</p>\n<p>以图 4 为例，推导一下上式的由来。<br>记 heatmap 对应到原 image 上两倍 region proposal 大小的区域为 R’，类似于 $(I_x,I_y)$ 地有，<br>$$I_x’=P_x’+\\frac {H_x}{w_o} 2 w_p, \\quad I_y’=P_y’+\\frac {H_y}{h_o} 2 h_p$$<br>其中 $(P_x’,P_y’)$ 是 R’ 的左上角在原 image 上的坐标，其与 proposal 的左上角坐标具有关系，<br>$$2(x_c-P_x)=x_c-P_x’=w_p, \\quad 2(y_c-P_y)=y_c-P_y’=h_p$$<br>其中 $(x_c,y_c)$ 是 proposal 与 R’ 共同的中心在原 image 上的坐标，综合上两式即可证明。</p>\n<h3 id=\"2-4-Implementation-Details\"><a href=\"#2-4-Implementation-Details\" class=\"headerlink\" title=\"2.4 Implementation Details\"></a>2.4 Implementation Details</h3><p>实现细节和实验分析略，请阅读原文。</p>\n<h1 id=\"Grid-R-CNN-Plus\"><a href=\"#Grid-R-CNN-Plus\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h1><p>论文 <a href=\"https://arxiv.org/abs/1906.05688\" target=\"_blank\" rel=\"noopener\">Grid R-CNN Plus: Faster and Better</a></p>\n<p>前面讲到将 Grid R-CNN 模块引入到 two-stage 目标检测器中可以显著提升 mAP，然而 Grid R-CNN 的计算量较大导致 inference 时间较长，从而使其难以广泛应用，所以这篇文章提出 Grid R-CNN Plus，通过一些有效的改进使其成为更好更快的目标检测器。</p>\n<p>大概介绍一下 Grid R-CNN Plus：对于每个 grid point，Grid R-CNN 均使用相同的表示区域来生成 supervision map，显然这是低效的，例如，左上 grid point 不会出现在其对应 supervision map 的右部和底部，所以在 Grid R-CNN Plus 中，仅监督最有可能的 1/4 区域，这样就降低了 grid 分支中 feature maps 的尺度。另外，特征融合阶段的卷积层数量也减少了。如此，降低计算损耗的同时，由于更加专注 grid point 的表示区域（原来区域中最有可能的 1/4），grid point 定位也更加准确。</p>\n<p>在采用策略，归一化方法，NMS 策略和超参数上也进行了分析和优化。</p>\n<h2 id=\"回顾-Grid-R-CNN\"><a href=\"#回顾-Grid-R-CNN\" class=\"headerlink\" title=\"回顾 Grid R-CNN\"></a>回顾 Grid R-CNN</h2><p>图 1 是 Grid R-CNN 的框架结构示意图。与一般 two-stage 检测器类似，Grid R-CNN 包含 RPN 和 R-CNN。基于 region propopsals，使用 RoIAlign 从 CNN backbone 的输出 feature maps 上 RoI 对应区域抽取特征，这个 RoI 特征用于预测分类和 bbox 定位。Grid R-CNN 与一般类 R-CNN 的不同之处在于使用 grid points 代替了坐标偏差。Grid 预测分支使用 FCN 结构输出保留空间信息的 heatmaps，从这些 heatmaps 中可以分别定位目标的 grid points。<br><img src=\"/images/Grid-RCNN-Plus_fig1.png\" alt=\"\"></p>\n<p>Grid R-CNN 使用 8 个 3x3 的卷积和两个 2x 反卷积得到 heatmap，这种分支结构比较重量级。降低计算量的一种方法是：首先从 RPN 中选择 1000 的 proposals，然后送入分类分支得到分类得分后，进行 NMS 之后然后选择其中 top 100 的 proposals，然后再送入 grid 分支，以降低计算耗时。</p>\n<p>为了提高准确性，还使用了特征融合机制和扩展区域映射，其中特征融合利用了 grid points 空间相关性互相进行校正，扩展区域映射则解决了 gt grid points 位于 proposal 之外的痛点。</p>\n<h2 id=\"Grid-R-CNN-Plus-1\"><a href=\"#Grid-R-CNN-Plus-1\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h2><h3 id=\"Grid-Point-专用表示区域\"><a href=\"#Grid-Point-专用表示区域\" class=\"headerlink\" title=\"Grid Point 专用表示区域\"></a>Grid Point 专用表示区域</h3><p>因为只有 IoU &gt; 0.5 的 proposals（正例）才可能会被选择送入 Grid 分支，所以 supervision map 上 gt grid point 被限制在一个较小的区域，如图 2 所示是各个 grid point 的 gt label 分布，以 3x3 grid points 为例说明，左上角 point 的 gt label 只可能出现在 supervision map 的左上角区域，所以，如果所有 grid points 的表示区域均相同（相同的 scale 和 center），那么大多数 pixel 的输出均不会被激活，这是很低效的，于是就有了 grid point 专用表示区域。<br><img src=\"/images/Grid-RCNN-Plus_fig2.png\" alt=\"\"></p>\n<p>原来每个 grid point 的表示区域的尺度是 56x56，现在降为 28x28，是原来区域中最有可能的 1/4，这样输出 heatmap 尺度降为一半。Grid point 专用表示区域也可以看作一种规范化过程，即，从原来的有偏分布到现在的规范化分布，如图 2 中，上左 point 和 中右 point 的 gt label 分布均以区域中心为中心。</p>\n<h3 id=\"Light-Grid-Head\"><a href=\"#Light-Grid-Head\" class=\"headerlink\" title=\"Light Grid Head\"></a>Light Grid Head</h3><p>由于输出 heatmap 尺度变为一半，我们同时将 grid 分支中其中 features 的分辨率也降为一半（例如从 14x14 降为 7x7），这降低了计算损耗。具体而言，使用 RoIAlign 从 RoI 中抽取固定大小 14x14 的特征之后，使用一个 3x3 stride=2 的卷积层，使得特征 size 降为 7x7，在这之后，使用 7 个 3x3 stride=1 的卷积层，每层的输出特征大小均为 7x7，最终特征分为 N 组（默认为 9）每组特征对应一个 grid point。我们显式地将特征与 grid points 一一起来，然后使用 2 个 2x 反卷积生成 28x28 的 heatmaps。</p>\n<p>grid point 专用表示区域使得不同的 grid points 的特征之间更加联系紧密，所以不需要很多的卷积层来弥补不同 grid points 特征之间的差异，所以 Grid R-CNN Plus 仅使用一个 5x5 depth-wise 的卷积，depth-wise 表示一个卷积核负责一个通道（二维平面上的卷积），而 Grid R-CNN 则使用三个 5x5 的普通卷积。这个改进也使得 grid 分支更加轻量。</p>\n<h3 id=\"跨越图像的采样策略\"><a href=\"#跨越图像的采样策略\" class=\"headerlink\" title=\"跨越图像的采样策略\"></a>跨越图像的采样策略</h3><p>grid branch 仅使用正例（IoU &gt; 0.5 的 positive proposals）进行训练，那么不同训练批次如果具有不同数量的正例，会对最终性能产品影响，例如，某些图像只有极少数正例，而其他图像可能会包含成百上千的正例，这种情况会导致 grid 分支的特征分布不稳定。所以在 Grid R-CNN Plus 中采用跨越单个图像的采样策略：当某个图像中正例数量较小时，可以使用其他图像来填充正例的空缺。具体操作为，将原来单个图像采样 96 个正例改为每两个图像采样 192 个正例。这种改进使得训练更加稳定，性能也得到提升。</p>\n<h3 id=\"仅一次-NMS\"><a href=\"#仅一次-NMS\" class=\"headerlink\" title=\"仅一次 NMS\"></a>仅一次 NMS</h3><p>Grid R-CNN 中，proposals 经过分类分支得到分类得分，然后使用 IoU 阈值 0.5 进行非极大抑制 NMS，之后取分类得分 top 125 的 proposals 送入 grid 分支进行定位预测，然后再次进行 NMS 以得到更加精确的结果。然而 NMS 是非常耗费计算量的，实验显示即使较少数量的 proposals，在 80 个分类（COCO 分类数量）上的 NMS 也很慢，所以 Grid R-CNN Plus 中移除了第二次 NMS。</p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验略。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Grid-R-CNN\"><a href=\"#Grid-R-CNN\" class=\"headerlink\" title=\"Grid R-CNN\"></a>Grid R-CNN</h1><p>论文 <a href=\"https://arxiv.org/abs/1811.12030\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<p>源码 <a href=\"https://github.com/STVIR/Grid-R-CNN\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>","more":"<h2 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1. 简介\"></a>1. 简介</h2><p>目标检测器包含两个分支：分类和回归，其中回归分支通常由 conv 和 fc 层组成，因为最终的输出单元数量固定为 4，表示 x,y,w,h 的坐标偏差（或者固定为 4*(C+1)，表示坐标偏差的预测是分类相关的），而 fc 层适用输出单元数量固定的场景。<br><img src=\"/images/Grid-RCNN_fig1.png\" alt=\"\"></p>\n<p>这篇文章介绍了 Grid R-CNN，主要是修改了类 R-CNN 目标检测器的回归分支，与传统类 R-CNN 的区别主要如图 1 所示，即，使用 grid points 表示目标定位：在得到 RoI 特征后，使用全卷积 FCN 得到特征图，然后再通过某种方法得到 grid points。此修改有如下优点：</p>\n<ol>\n<li>FCN 保留了 RoI 的空间信息，二维（显然，之前的方法都是展平成了一维向量），所以可准确的得到 grid points</li>\n<li>根据 grid points 可以更加精确的获得目标定位。以前的方法都是通过左上角和右下角坐标确定目标 bbox，这种方法显然较为粗糙，因为目标形状的任意性，角点可能位于目标形状之外的背景上，角点处的局部特征并不能反映目标的特征（ExtremeNet 中也提过 CornerNet 的这个问题），而 grid points 采用 3x3 的点作为监督，由于点数增多，可以降低其中某些不准确点对位置预测的负面影响。如图 1(b)，右上角的坐标不准确，但是中上（top-middle）点可以帮助校正最终的目标位置。</li>\n</ol>\n<p>利用 grid points 数量多这个优势，文中还提出一种信息融合方法，为每个 grid point 设计独立的一组 feature maps：对某个 grid point，聚集其邻近 grid point 的 feature maps 并进行融合成，作为当前这个 grid point 的 feature map，这个合成的 feature map 则用于预测当前 grid point 的位置。这种空间信息互补的方法可以使得位置预测更加准确。</p>\n<h2 id=\"2-Grid-R-CNN\"><a href=\"#2-Grid-R-CNN\" class=\"headerlink\" title=\"2. Grid R-CNN\"></a>2. Grid R-CNN</h2><p>网络的整体结构如图 2 所示，基于 region proposals，使用 RoIAlign 抽取出 RoI 特征，然后分为两支，其中一支用于目标分类预测，另一支经过 FCN 得到 feature maps，然后经过 sigmoid 得到概率热图（probability heatmap），注意不是传统的坐标偏差，heatmaps 可由 target map 监督，从这个概率热图上我们可以通过某种方法得到 grid points（下文 2.1 一节会介绍这种方法），再经过上述的空间信息融合方法（2.2 一节会介绍），最后得到准确的目标定位。接下来我们将整个问题分而治之进行介绍。<br><img src=\"/images/Grid-RCNN_fig2.png\" alt=\"\"></p>\n<h3 id=\"2-1-Grid-Guided-Localization\"><a href=\"#2-1-Grid-Guided-Localization\" class=\"headerlink\" title=\"2.1 Grid Guided Localization\"></a>2.1 Grid Guided Localization</h3><p>一般地， NxN 的 grid points 位列 bbox 之上。以上文提到的 3x3 的 grid points 为例，共四个角点，四条边的中点，以及 bbox 的中心点。</p>\n<p>我们简单地描述一下坐标预测分支：使用 RoIAlign 抽取 RoI 得到空间尺度固定为 14x14 的 feature，然后跟着是 8 个 3x3 的空洞卷积，目的是为了增大感受野，接着是两个 2x 反卷积，得到空间尺度为 56x56 的 feature，其通道数为 NxN，然后应用 pixel-wise 的 sigmoid 函数，得到 NxN 个概率热图 heatmap，注意 heatmap 与 feature map 两者是不同的，每个热图对应一个 grid point，且对应一个 supervision map，在这个 supervision map 上以 target grid point 为中心的可以组成十字型的 5 个像素点标记为正 $t_i=1$，其他像素位置处表示为负 $t_i=0$（target grid point 在 supervision map 上的位置在 2.3 一节介绍），然后使用 binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$进行优化，其中 $p_i$ 表示某个 pixel 处的概率。</p>\n<p>Inference 阶段，在每个 heatmap 上选择最高置信度的点，记为 $(H_x,H_y)$，经过简单的映射计算可以得到其在原始 image 平面上的坐标，记为 $(I_x,I_y)$，<br>$$I_x=P_x+ \\frac {H_x}{w_o} w_p<br>\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$<br>其中 $(P_x,P_y)$ 为 region proposal 的左上角在原始 image 平面上的坐标，$(w_p,h_p)$ 表示 proposal 的宽高，$(w_o,h_o)$ 表示输出 heatmap 的宽高。 <strong>注意：</strong> heatmap 与 proposal 尺度不一定相同！！！<br>有了预测的 grid points 之后就可以得到目标 bbox 的四个边的坐标，记为 $B=(x_l,y_u,x_r,y_b)$。第 j 个 grid point 记为 $g_j$，其坐标为 $(x_j,y_j)$，其概率为 $p_j$（第 j 个 heatmap 上置信度最高值），定义 $E_i$ 为位于第 i 个边（左上右下，其中一个）上的 grid points 的下标集合，即，如果 $g_j$ 位于 第 i 个边上，那么 $j \\in E_i$，于是根据 grid points 集合可以计算 B，采用加权平均，如下，<br>$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j<br>\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$</p>\n<h3 id=\"2-2-Grid-Points-Feature-Fusion\"><a href=\"#2-2-Grid-Points-Feature-Fusion\" class=\"headerlink\" title=\"2.2 Grid Points Feature Fusion\"></a>2.2 Grid Points Feature Fusion</h3><p>由 FCN 得到得 heatmap 保留了 RoI 的空间信息，所以 grid points 之间存在某种联系，他们之间可以互相校正位置，通过一种空间信息融合方法得以实现。以 3x3 grid points 为例，要校正左上角 point，我们可以利用其他 grid points 对应的 feature maps 上左上角区域的特征。</p>\n<p>为了区分不同 grid points 的 feature maps，使用 NxN 不同的 filters 从最后的 feature map 上分别为这些 grid points 抽取特征，于是每个 feature map 与其对应的 grid point 有特殊的联系，称第 i 个 point 对应的 feature map 为 $F_i$。</p>\n<p>对于每个 grid point，与其 L1 距离为 1 的其他 points 参与融合过程，称这些 points 为 source points，记为 $S_i$，对于 $S_i$ 中的 point j，其 feature maps $F_j$ 经过三个连续的 5x5 卷积，记此过程为 $T_{j \\rightarrow i}$，$S_i$ 中所有 source points 经过此过程后得到的 features 再与 $F_i$ 进行融合得到 $F_i’$，融合采用简单的求和操作，<br>$$F_i’=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$<br><img src=\"/images/Grid-RCNN_fig3.png\" alt=\"\"></p>\n<p>如图 3(a) 是左上角 point 的融合示意图。有了融合后的特征 $F_i’$ 后，还可进行二次融合，类似的，对 source point 的 $F_i’$ 特征应用变换函数 $T_{j \\rightarrow i}^+$（几个连续的卷积），其参数不与第一个融合的过程共享，第二次融合后的特征记为 $F_i’’$，用于输出最终的 heatmap，并从中根据前述方法取最大置信度得到 grid point 位置预测。第二次融合使得 L1 距离小于等于 2 的 points 的 features 对当前 grid point 均有贡献，如图 3(b)。</p>\n<p><strong>注意：</strong> 融合前后的两组 feature maps 均通过 sigmoid 计算得到两组 heatmaps，然后分别与 supervision maps 计算出前后两处 loss（作为位置预测损失），损失计算如上面所述均采用 Binary Cross-Entropy Loss。</p>\n<h3 id=\"2-3-Extended-Region-Mapping\"><a href=\"#2-3-Extended-Region-Mapping\" class=\"headerlink\" title=\"2.3 Extended Region Mapping\"></a>2.3 Extended Region Mapping</h3><p>Grid 预测模块输出的 heatmap 是固定尺度的，各 pixel 位置处的概率表示可能是 grid point 的概率。对 RoI 特征应用的是全卷积 FCN，所以空间信息得以保留，输出 heatmap 自然就对应 region proposal 在原 image 上的空间区域，但是，region proposal 不一定完全包含目标，这意味着 gt grid point 可能位于 region proposal 之外，导致无法在 supervision map 上标记 target grid point，这种缺失造成训练阶段训练样本的利用率低的问题，同时在 inference 阶段，简单地在 heatmap 上选择置信度最大的 pixel 作为 grid point，由于 region proposal 没有完全包含目标，可能会选择出一个错误的 grid point 位置，从而导致预测存在偏差。很多场合下有超过一半的 gt grid points 都没有被 region proposals 覆盖到，如图 4，proposal（最小白框）比 gt box 小，9 个 grid points 中有 7 个将会位于输出 heatmap 之外。</p>\n<p>解决上述问题的一个最自然的方法是增大 proposal，从而确保大部分 gt grid points 还是会被 proposal 包含进来，但是这同时也会包含 背景或其他目标 的特征，这些特征对于预测当前目标是冗余的，甚至是有害的。作者实验显示，简单的增大 proposal 确实没什么好处，在小目标检测上是有害的。作者采用的解决方法是修改输出 heatmap 和原 image 上 region 之间的关系：当给定 proposal 时，RoI 特征依然是从 feature map 上相同 region 中获取，此处不需要增大 proposal；但是重新定义输出 heatmap 所代表的区域为 region 的两倍大，这样大多数场合下，所有的 grid points 都被包含进来了，如图 4 中虚线框所示，但是 supervision map 与输出 heatmap 尺度相同的，这相当于将 gt box 缩小一倍然后映射到 supervision map 上从而进行标记，标记方法前文已经提到过，在以 gt grid point 为中心的十字型 pixels，即 gt grid point 和位于其上下左右四个最近邻 pixels，共 5 个 pixels 标记为正。<br><img src=\"/images/Grid-RCNN_fig4.png\" alt=\"\"></p>\n<p>扩展的区域映射使用公式表达如下，<br>$$I_x’=P_x+\\frac {4H_x-w_o}{2w_o}w_p<br>\\\\ I_y’=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$</p>\n<p>以图 4 为例，推导一下上式的由来。<br>记 heatmap 对应到原 image 上两倍 region proposal 大小的区域为 R’，类似于 $(I_x,I_y)$ 地有，<br>$$I_x’=P_x’+\\frac {H_x}{w_o} 2 w_p, \\quad I_y’=P_y’+\\frac {H_y}{h_o} 2 h_p$$<br>其中 $(P_x’,P_y’)$ 是 R’ 的左上角在原 image 上的坐标，其与 proposal 的左上角坐标具有关系，<br>$$2(x_c-P_x)=x_c-P_x’=w_p, \\quad 2(y_c-P_y)=y_c-P_y’=h_p$$<br>其中 $(x_c,y_c)$ 是 proposal 与 R’ 共同的中心在原 image 上的坐标，综合上两式即可证明。</p>\n<h3 id=\"2-4-Implementation-Details\"><a href=\"#2-4-Implementation-Details\" class=\"headerlink\" title=\"2.4 Implementation Details\"></a>2.4 Implementation Details</h3><p>实现细节和实验分析略，请阅读原文。</p>\n<h1 id=\"Grid-R-CNN-Plus\"><a href=\"#Grid-R-CNN-Plus\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h1><p>论文 <a href=\"https://arxiv.org/abs/1906.05688\" target=\"_blank\" rel=\"noopener\">Grid R-CNN Plus: Faster and Better</a></p>\n<p>前面讲到将 Grid R-CNN 模块引入到 two-stage 目标检测器中可以显著提升 mAP，然而 Grid R-CNN 的计算量较大导致 inference 时间较长，从而使其难以广泛应用，所以这篇文章提出 Grid R-CNN Plus，通过一些有效的改进使其成为更好更快的目标检测器。</p>\n<p>大概介绍一下 Grid R-CNN Plus：对于每个 grid point，Grid R-CNN 均使用相同的表示区域来生成 supervision map，显然这是低效的，例如，左上 grid point 不会出现在其对应 supervision map 的右部和底部，所以在 Grid R-CNN Plus 中，仅监督最有可能的 1/4 区域，这样就降低了 grid 分支中 feature maps 的尺度。另外，特征融合阶段的卷积层数量也减少了。如此，降低计算损耗的同时，由于更加专注 grid point 的表示区域（原来区域中最有可能的 1/4），grid point 定位也更加准确。</p>\n<p>在采用策略，归一化方法，NMS 策略和超参数上也进行了分析和优化。</p>\n<h2 id=\"回顾-Grid-R-CNN\"><a href=\"#回顾-Grid-R-CNN\" class=\"headerlink\" title=\"回顾 Grid R-CNN\"></a>回顾 Grid R-CNN</h2><p>图 1 是 Grid R-CNN 的框架结构示意图。与一般 two-stage 检测器类似，Grid R-CNN 包含 RPN 和 R-CNN。基于 region propopsals，使用 RoIAlign 从 CNN backbone 的输出 feature maps 上 RoI 对应区域抽取特征，这个 RoI 特征用于预测分类和 bbox 定位。Grid R-CNN 与一般类 R-CNN 的不同之处在于使用 grid points 代替了坐标偏差。Grid 预测分支使用 FCN 结构输出保留空间信息的 heatmaps，从这些 heatmaps 中可以分别定位目标的 grid points。<br><img src=\"/images/Grid-RCNN-Plus_fig1.png\" alt=\"\"></p>\n<p>Grid R-CNN 使用 8 个 3x3 的卷积和两个 2x 反卷积得到 heatmap，这种分支结构比较重量级。降低计算量的一种方法是：首先从 RPN 中选择 1000 的 proposals，然后送入分类分支得到分类得分后，进行 NMS 之后然后选择其中 top 100 的 proposals，然后再送入 grid 分支，以降低计算耗时。</p>\n<p>为了提高准确性，还使用了特征融合机制和扩展区域映射，其中特征融合利用了 grid points 空间相关性互相进行校正，扩展区域映射则解决了 gt grid points 位于 proposal 之外的痛点。</p>\n<h2 id=\"Grid-R-CNN-Plus-1\"><a href=\"#Grid-R-CNN-Plus-1\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h2><h3 id=\"Grid-Point-专用表示区域\"><a href=\"#Grid-Point-专用表示区域\" class=\"headerlink\" title=\"Grid Point 专用表示区域\"></a>Grid Point 专用表示区域</h3><p>因为只有 IoU &gt; 0.5 的 proposals（正例）才可能会被选择送入 Grid 分支，所以 supervision map 上 gt grid point 被限制在一个较小的区域，如图 2 所示是各个 grid point 的 gt label 分布，以 3x3 grid points 为例说明，左上角 point 的 gt label 只可能出现在 supervision map 的左上角区域，所以，如果所有 grid points 的表示区域均相同（相同的 scale 和 center），那么大多数 pixel 的输出均不会被激活，这是很低效的，于是就有了 grid point 专用表示区域。<br><img src=\"/images/Grid-RCNN-Plus_fig2.png\" alt=\"\"></p>\n<p>原来每个 grid point 的表示区域的尺度是 56x56，现在降为 28x28，是原来区域中最有可能的 1/4，这样输出 heatmap 尺度降为一半。Grid point 专用表示区域也可以看作一种规范化过程，即，从原来的有偏分布到现在的规范化分布，如图 2 中，上左 point 和 中右 point 的 gt label 分布均以区域中心为中心。</p>\n<h3 id=\"Light-Grid-Head\"><a href=\"#Light-Grid-Head\" class=\"headerlink\" title=\"Light Grid Head\"></a>Light Grid Head</h3><p>由于输出 heatmap 尺度变为一半，我们同时将 grid 分支中其中 features 的分辨率也降为一半（例如从 14x14 降为 7x7），这降低了计算损耗。具体而言，使用 RoIAlign 从 RoI 中抽取固定大小 14x14 的特征之后，使用一个 3x3 stride=2 的卷积层，使得特征 size 降为 7x7，在这之后，使用 7 个 3x3 stride=1 的卷积层，每层的输出特征大小均为 7x7，最终特征分为 N 组（默认为 9）每组特征对应一个 grid point。我们显式地将特征与 grid points 一一起来，然后使用 2 个 2x 反卷积生成 28x28 的 heatmaps。</p>\n<p>grid point 专用表示区域使得不同的 grid points 的特征之间更加联系紧密，所以不需要很多的卷积层来弥补不同 grid points 特征之间的差异，所以 Grid R-CNN Plus 仅使用一个 5x5 depth-wise 的卷积，depth-wise 表示一个卷积核负责一个通道（二维平面上的卷积），而 Grid R-CNN 则使用三个 5x5 的普通卷积。这个改进也使得 grid 分支更加轻量。</p>\n<h3 id=\"跨越图像的采样策略\"><a href=\"#跨越图像的采样策略\" class=\"headerlink\" title=\"跨越图像的采样策略\"></a>跨越图像的采样策略</h3><p>grid branch 仅使用正例（IoU &gt; 0.5 的 positive proposals）进行训练，那么不同训练批次如果具有不同数量的正例，会对最终性能产品影响，例如，某些图像只有极少数正例，而其他图像可能会包含成百上千的正例，这种情况会导致 grid 分支的特征分布不稳定。所以在 Grid R-CNN Plus 中采用跨越单个图像的采样策略：当某个图像中正例数量较小时，可以使用其他图像来填充正例的空缺。具体操作为，将原来单个图像采样 96 个正例改为每两个图像采样 192 个正例。这种改进使得训练更加稳定，性能也得到提升。</p>\n<h3 id=\"仅一次-NMS\"><a href=\"#仅一次-NMS\" class=\"headerlink\" title=\"仅一次 NMS\"></a>仅一次 NMS</h3><p>Grid R-CNN 中，proposals 经过分类分支得到分类得分，然后使用 IoU 阈值 0.5 进行非极大抑制 NMS，之后取分类得分 top 125 的 proposals 送入 grid 分支进行定位预测，然后再次进行 NMS 以得到更加精确的结果。然而 NMS 是非常耗费计算量的，实验显示即使较少数量的 proposals，在 80 个分类（COCO 分类数量）上的 NMS 也很慢，所以 Grid R-CNN Plus 中移除了第二次 NMS。</p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验略。</p>"},{"title":"ImprovedGAN","date":"2019-08-01T07:48:46.000Z","mathjax":true,"_content":"标题 [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n\n源码 [improved_gan](https://github.com/openai/improved_gan)\n<!-- more -->\n# 简介\nGAN 是基于博弈论学习生成模型的一类方法总称。GAN 目的是训练一个生成网络其生成样本的分布可以拟合真实数据分布。虽然 [DCGAN](2019/07/23/GAN) 在 GAN 中引入 conv+BN+ReLU 在一定程度上改善了生成器，但是我们认为 GAN 这个零和博弈问题具有高维参数且非凸，需要达到 Nash 均衡才是最佳解，而传统的基于目标函数梯度下降方法目的并非用于寻找 Nash 均衡。本文提出了以下改进方法：\n1. 特征匹配\n2. 小批量特征\n3. 虚拟批归一化\n\n# GAN 训练收敛\n训练 GAN 意味着寻找二人非合作博弈中的 Nash 均衡，每个玩家希望最小化自己的损失函数，即生成器损失 $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$ 和判别器损失 $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$，Nash 均衡指 $J^{(D)}$ 关于 $\\theta^{(D)}$ 最小，同时 $J^{(G)}$ 关于 $\\theta^{(G)}$ 最小。寻找 Nash 均衡点是一个比较困难的问题，虽然某些特殊情况下有算法可以解决，但是由于这里损失函数非凸且参数维度很高，那些方法均不适用。以上 Nash 均衡的说明让我们从直觉上认为需要同时最小化 G 和 D 的损失。但是很不幸，更新 $\\theta^{(D)}$ 降低 $J^{(D)}$ 却增大 $J^{(G)}$，更新 $\\theta^{(G)}$ 以降低 $J^{(G)}$ 但是会增大 $J^{(D)}$。这就是导致梯度下降法难以收敛（往往是在一个轨道上一直反复，而不会到达最佳点）。例如一个玩家根据 x 来最小化 xy，另一个玩家根据 y 来最小化 -xy，梯度下降法更新会使得 x 和 y 值构成的点在一个椭圆上往复，而不会收敛到 x=y=0。本文介绍以下方法启发式的促使更新达到收敛。\n\n## 特征匹配\n特征匹配使用新的生成器损失函数以解决 GAN 训练不稳定问题。新的目标函数不是直接最大化 D 的输出（G 原本的目标是让 D 对生成样本有越大越好的输出），而是让 G 生成的样本能够匹配真实数据的统计量，这是一种更直接的思想。具体而言，训练 G 以匹配特征的期望值，这个特征来自于 D 的网络中间层。令 $\\mathbf {f(x)}$ 表示 D 网络中间层的激活响应，即前面所指的特征，那么 G 的新目标函数为\n$$\\|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))\\|_2^2$$\nG 的训练目标就是最小化这个目标损失。\n\n## 小批量判别\nGAN 训练失败的原因之一是生成器训练时总是会陷入一组参数无法逃脱，我这里称其为“陷入点”，当临近“陷入点”时，生成器的输出点总是很相似，而这些相似的点会让判别器总是指向一个差不多的方向，由于判别器 __独立处理__ 每个样本，这些样本对应的梯度相互之间无法合作，缺乏一种反馈机制去通知生成器让其输出相互之间尽可能不相似，生成器所有的输出都向同一个点竞争，这个点是为了让判别器判别为真实的数据，所以结果就是生成器陷入一组模型参数无法自拔，陷入之后，判别器通过学习又能够将这个点判别为来自生成器，但是梯度 __无法区分__ 各个不同的输出，于是判别器的梯度会一直在空间中将生成器产生的这个“陷入点”推来推去，导致算法无法收敛。一种显然的解决办法是让判别器不独立处理每个样本，而是一次能看到多个样本的合并，这就是小批量判别方法。\n\n现在我们的实验建模瞄准于区分生成器的各个相互靠得很近得样本。小批量中样本之间接近程度按如下方法计算：  \n令 $\\mathbf {f(x_i)} \\in \\Bbb R^A$ 表示输入 $\\mathbf x_i$ 对应的特征向量，这个特征由 D 网络中间层产生，然后将特征向量乘以一个张量 $T \\in \\Bbb R^{A \\times B \\times C}$，结果是一个矩阵 $M_i \\in \\Bbb R^{B \\times C}$，对于输入样本编号 $i \\in \\{1,...,n\\}$，得到对应的矩阵 $\\{M_i |i=1,...,n\\}$，计算两两矩阵的各行向量之间的 L1 距离，然后应用负指数函数，\n$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-\\|M_{i,b}-M_{j,b}\\| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in \\{1,...,n\\}, \\quad b \\in \\{1,...,B\\}$$\n\n其中下标 b 表示 row index。如图 1，minibatch layer 中样本 $\\mathbf x_i$ 对应的输出定义为，\n$$\\begin{aligned} &o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R\n\\\\\\\\ &o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,...o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B\n\\\\\\\\ &o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$\n\n然后，将 minibatch layer 的输出 $o(\\mathbf x_i)$ 与 minibatch layer 的输入 $\\mathbf {f(x_i)}$ concatenate 起来，作为 D 的下一 layer 的输入。对生成样本和训练数据分别计算 minibatch layer 特征。\n![](/images/ImprovedGAN_fig1.png)\n\n## 历史平均\n修改每个玩家（G 和 D）的损失使得包含 $\\|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]\\|^2$，其中 $\\theta[i]$ 是历史时期 i 的参数值。\n\n## 单边标注平滑\nLabel 平滑，就是将分类器的 target 值由 0 和 1 替换为一个平滑的值如 0.9 或 0.1。我们将正例 target 替换为 $\\alpha$，负例 target 替换为 $\\beta$，那么最佳判别器变为\n$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$\n\n- 当 $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\alpha$\n- 当 $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\beta$\n\n当然我们也可以按 [GAN](2019/7/23/GAN) 中那样推导 $D^{\\ast}$，推导过程这里略过，只是此时目标变为\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$\n\n这里约定正例 target 大于负例 target，即 $\\alpha > \\beta$，由 (1) 式，可知 D 输出范围为 $\\beta < D(x) < \\alpha$。\n\n由于分子中出现 $p_{model}$，那么当 $p_{data} \\rightarrow 0$，且 $p_{model}$ 足够大时，来自 $p_{model}$ 的错误样本将得不到促使向真实数据靠近的激励，所以只对正例 label 平滑处理为 $\\alpha$，负例 label 依然为 0。\n\n## 虚拟批归一化\nDCGAN 中使用了批归一化 BN 使得网络优化更加有效，但是也会带来问题，比如一个输入 $\\mathbf x$，其对应的输出高度依赖于同一 minibatch 中的其他输入 $\\mathbf x'$。为了避免这个问题，本文使用了虚拟批归一化 VBN，每个样本输入 $\\mathbf x$ 的归一化过程基于 reference batch 中样本的统计量以及 $\\mathbf x$ 自身，reference batch 是在训练初期选定并固定不变，reference batch 使用统计量进行归一化。由于 VBN 计算强度较高，故只在 G 网络中使用。\n\n# 图像质量评估\nGAN 的性能评估最直接的方法就是人类观察员判断，缺点是难以公平公正。本文提出了一个自动评估方法：应用 Inception 模型到每个生成样本上，以获得条件 label 分布 $p(y|\\mathbf x)$，那些包含有意义目标的图像的条件 label 分布 $p(y|\\mathbf x)$ 应该具有较低的信息熵，也就是说，具有较低的不确定性，这意味着，对于给定的输入 $\\mathbf x$（包含有意义目标的图像），Inception 模型每次输出值 y （比如图像分类 c）比较稳定变化很小。但是我们又希望生成模型能够生成各种不同的图像，即对于不同的噪声输入 z，G 能够生成各种不同的图像，分别以这些不同的图像作为输入， Inception 模型的输出也尽可能不同（不确定性较大），这说明 $\\int p(y|\\mathbf x=G(z)) dz$ 应该具有较大的信息熵。结合以上这两点要求，性能指标为这两个分布 KL 散度的期望，\n$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)\\|p(y)) ]$$\n\n应用指数函数仅仅是为了便于比较值的大小。\n\n# 半监督学习\n考虑一个标准分类器，输入为 $\\mathbf x$，共有 K 种类别，输出为长度 K 的向量 $[l_1,...,l_K]$，表示每个类别的得分，通过 softmax 得到对应的概率：\n$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$\n\n在监督学习中，此模型的训练是最小化交叉熵（或最大化 log 似然函数）。\n\n增加来自生成器 G 的样本到数据集中，可以实现标准分类器的半监督学习，G 生成样本标记类别 y=K+1，分类器的输出维度改为 K+1，利用 $p_{model}(y=K+1|\\mathbf x)$ 判断输入 $\\mathbf x$ 是生成样本的概率，与 GAN 中的 $1-D(\\mathbf x)$ 是对应的。也可以使用未标注数据进行学习，对于来自 K 个类别的真实数据，需要最大化 $\\log p_{model}(y \\in \\{1,...,K\\}|\\mathbf x)$（log 似然函数），假设数据集中一半是真实数据，一半是生成数据，那么分类器训练的损失函数为，\n$$\\begin{aligned} &L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}\n\\\\\\\\ &L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y <K+1)\n\\\\\\\\ &L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$\n\n其中求期望实际上是经验期望也就是均值损失。其中 $L_{unsupervised}$ 就是标准 GAN 的 objective，在 $L_{unsupervised}$ 中作替换 $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$，就更明显了,于是有\n$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$\n\n最小化 $L_{supervised}$ 和 $L_{unsupervised}$ 的最优解是满足 $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$ 以及 $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$，其中 $c(\\mathbf x)$ 是待定的系数函数。训练 G 以近似真实的数据分布，一种训练方法是最小化 GAN objective，使用这里的分类器作为判别器 D，这种方法引入了 G 和分类器之间的相互作用，经验表明，在半监督学习中，使用特征匹配 GAN 可以很好的优化 G。\n\n分类器输出维度为 K+1 是过参数化的，由于输出向量中每个元素值均减去同一个值 $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$，对 softmax 的值并不影响，所以可固定 $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$，于是 $L_{supervised}$ 变为具有 K 个类别的原始分类器的标准监督损失，此时判别器 D 为 $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$，其中 $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$。","source":"_posts/ImprovedGAN.md","raw":"---\ntitle: ImprovedGAN\ndate: 2019-08-01 15:48:46\ntags: GAN\nmathjax: true\n---\n标题 [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n\n源码 [improved_gan](https://github.com/openai/improved_gan)\n<!-- more -->\n# 简介\nGAN 是基于博弈论学习生成模型的一类方法总称。GAN 目的是训练一个生成网络其生成样本的分布可以拟合真实数据分布。虽然 [DCGAN](2019/07/23/GAN) 在 GAN 中引入 conv+BN+ReLU 在一定程度上改善了生成器，但是我们认为 GAN 这个零和博弈问题具有高维参数且非凸，需要达到 Nash 均衡才是最佳解，而传统的基于目标函数梯度下降方法目的并非用于寻找 Nash 均衡。本文提出了以下改进方法：\n1. 特征匹配\n2. 小批量特征\n3. 虚拟批归一化\n\n# GAN 训练收敛\n训练 GAN 意味着寻找二人非合作博弈中的 Nash 均衡，每个玩家希望最小化自己的损失函数，即生成器损失 $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$ 和判别器损失 $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$，Nash 均衡指 $J^{(D)}$ 关于 $\\theta^{(D)}$ 最小，同时 $J^{(G)}$ 关于 $\\theta^{(G)}$ 最小。寻找 Nash 均衡点是一个比较困难的问题，虽然某些特殊情况下有算法可以解决，但是由于这里损失函数非凸且参数维度很高，那些方法均不适用。以上 Nash 均衡的说明让我们从直觉上认为需要同时最小化 G 和 D 的损失。但是很不幸，更新 $\\theta^{(D)}$ 降低 $J^{(D)}$ 却增大 $J^{(G)}$，更新 $\\theta^{(G)}$ 以降低 $J^{(G)}$ 但是会增大 $J^{(D)}$。这就是导致梯度下降法难以收敛（往往是在一个轨道上一直反复，而不会到达最佳点）。例如一个玩家根据 x 来最小化 xy，另一个玩家根据 y 来最小化 -xy，梯度下降法更新会使得 x 和 y 值构成的点在一个椭圆上往复，而不会收敛到 x=y=0。本文介绍以下方法启发式的促使更新达到收敛。\n\n## 特征匹配\n特征匹配使用新的生成器损失函数以解决 GAN 训练不稳定问题。新的目标函数不是直接最大化 D 的输出（G 原本的目标是让 D 对生成样本有越大越好的输出），而是让 G 生成的样本能够匹配真实数据的统计量，这是一种更直接的思想。具体而言，训练 G 以匹配特征的期望值，这个特征来自于 D 的网络中间层。令 $\\mathbf {f(x)}$ 表示 D 网络中间层的激活响应，即前面所指的特征，那么 G 的新目标函数为\n$$\\|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))\\|_2^2$$\nG 的训练目标就是最小化这个目标损失。\n\n## 小批量判别\nGAN 训练失败的原因之一是生成器训练时总是会陷入一组参数无法逃脱，我这里称其为“陷入点”，当临近“陷入点”时，生成器的输出点总是很相似，而这些相似的点会让判别器总是指向一个差不多的方向，由于判别器 __独立处理__ 每个样本，这些样本对应的梯度相互之间无法合作，缺乏一种反馈机制去通知生成器让其输出相互之间尽可能不相似，生成器所有的输出都向同一个点竞争，这个点是为了让判别器判别为真实的数据，所以结果就是生成器陷入一组模型参数无法自拔，陷入之后，判别器通过学习又能够将这个点判别为来自生成器，但是梯度 __无法区分__ 各个不同的输出，于是判别器的梯度会一直在空间中将生成器产生的这个“陷入点”推来推去，导致算法无法收敛。一种显然的解决办法是让判别器不独立处理每个样本，而是一次能看到多个样本的合并，这就是小批量判别方法。\n\n现在我们的实验建模瞄准于区分生成器的各个相互靠得很近得样本。小批量中样本之间接近程度按如下方法计算：  \n令 $\\mathbf {f(x_i)} \\in \\Bbb R^A$ 表示输入 $\\mathbf x_i$ 对应的特征向量，这个特征由 D 网络中间层产生，然后将特征向量乘以一个张量 $T \\in \\Bbb R^{A \\times B \\times C}$，结果是一个矩阵 $M_i \\in \\Bbb R^{B \\times C}$，对于输入样本编号 $i \\in \\{1,...,n\\}$，得到对应的矩阵 $\\{M_i |i=1,...,n\\}$，计算两两矩阵的各行向量之间的 L1 距离，然后应用负指数函数，\n$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-\\|M_{i,b}-M_{j,b}\\| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in \\{1,...,n\\}, \\quad b \\in \\{1,...,B\\}$$\n\n其中下标 b 表示 row index。如图 1，minibatch layer 中样本 $\\mathbf x_i$ 对应的输出定义为，\n$$\\begin{aligned} &o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R\n\\\\\\\\ &o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,...o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B\n\\\\\\\\ &o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$\n\n然后，将 minibatch layer 的输出 $o(\\mathbf x_i)$ 与 minibatch layer 的输入 $\\mathbf {f(x_i)}$ concatenate 起来，作为 D 的下一 layer 的输入。对生成样本和训练数据分别计算 minibatch layer 特征。\n![](/images/ImprovedGAN_fig1.png)\n\n## 历史平均\n修改每个玩家（G 和 D）的损失使得包含 $\\|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]\\|^2$，其中 $\\theta[i]$ 是历史时期 i 的参数值。\n\n## 单边标注平滑\nLabel 平滑，就是将分类器的 target 值由 0 和 1 替换为一个平滑的值如 0.9 或 0.1。我们将正例 target 替换为 $\\alpha$，负例 target 替换为 $\\beta$，那么最佳判别器变为\n$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$\n\n- 当 $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\alpha$\n- 当 $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\beta$\n\n当然我们也可以按 [GAN](2019/7/23/GAN) 中那样推导 $D^{\\ast}$，推导过程这里略过，只是此时目标变为\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$\n\n这里约定正例 target 大于负例 target，即 $\\alpha > \\beta$，由 (1) 式，可知 D 输出范围为 $\\beta < D(x) < \\alpha$。\n\n由于分子中出现 $p_{model}$，那么当 $p_{data} \\rightarrow 0$，且 $p_{model}$ 足够大时，来自 $p_{model}$ 的错误样本将得不到促使向真实数据靠近的激励，所以只对正例 label 平滑处理为 $\\alpha$，负例 label 依然为 0。\n\n## 虚拟批归一化\nDCGAN 中使用了批归一化 BN 使得网络优化更加有效，但是也会带来问题，比如一个输入 $\\mathbf x$，其对应的输出高度依赖于同一 minibatch 中的其他输入 $\\mathbf x'$。为了避免这个问题，本文使用了虚拟批归一化 VBN，每个样本输入 $\\mathbf x$ 的归一化过程基于 reference batch 中样本的统计量以及 $\\mathbf x$ 自身，reference batch 是在训练初期选定并固定不变，reference batch 使用统计量进行归一化。由于 VBN 计算强度较高，故只在 G 网络中使用。\n\n# 图像质量评估\nGAN 的性能评估最直接的方法就是人类观察员判断，缺点是难以公平公正。本文提出了一个自动评估方法：应用 Inception 模型到每个生成样本上，以获得条件 label 分布 $p(y|\\mathbf x)$，那些包含有意义目标的图像的条件 label 分布 $p(y|\\mathbf x)$ 应该具有较低的信息熵，也就是说，具有较低的不确定性，这意味着，对于给定的输入 $\\mathbf x$（包含有意义目标的图像），Inception 模型每次输出值 y （比如图像分类 c）比较稳定变化很小。但是我们又希望生成模型能够生成各种不同的图像，即对于不同的噪声输入 z，G 能够生成各种不同的图像，分别以这些不同的图像作为输入， Inception 模型的输出也尽可能不同（不确定性较大），这说明 $\\int p(y|\\mathbf x=G(z)) dz$ 应该具有较大的信息熵。结合以上这两点要求，性能指标为这两个分布 KL 散度的期望，\n$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)\\|p(y)) ]$$\n\n应用指数函数仅仅是为了便于比较值的大小。\n\n# 半监督学习\n考虑一个标准分类器，输入为 $\\mathbf x$，共有 K 种类别，输出为长度 K 的向量 $[l_1,...,l_K]$，表示每个类别的得分，通过 softmax 得到对应的概率：\n$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$\n\n在监督学习中，此模型的训练是最小化交叉熵（或最大化 log 似然函数）。\n\n增加来自生成器 G 的样本到数据集中，可以实现标准分类器的半监督学习，G 生成样本标记类别 y=K+1，分类器的输出维度改为 K+1，利用 $p_{model}(y=K+1|\\mathbf x)$ 判断输入 $\\mathbf x$ 是生成样本的概率，与 GAN 中的 $1-D(\\mathbf x)$ 是对应的。也可以使用未标注数据进行学习，对于来自 K 个类别的真实数据，需要最大化 $\\log p_{model}(y \\in \\{1,...,K\\}|\\mathbf x)$（log 似然函数），假设数据集中一半是真实数据，一半是生成数据，那么分类器训练的损失函数为，\n$$\\begin{aligned} &L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}\n\\\\\\\\ &L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y <K+1)\n\\\\\\\\ &L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$\n\n其中求期望实际上是经验期望也就是均值损失。其中 $L_{unsupervised}$ 就是标准 GAN 的 objective，在 $L_{unsupervised}$ 中作替换 $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$，就更明显了,于是有\n$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$\n\n最小化 $L_{supervised}$ 和 $L_{unsupervised}$ 的最优解是满足 $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$ 以及 $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$，其中 $c(\\mathbf x)$ 是待定的系数函数。训练 G 以近似真实的数据分布，一种训练方法是最小化 GAN objective，使用这里的分类器作为判别器 D，这种方法引入了 G 和分类器之间的相互作用，经验表明，在半监督学习中，使用特征匹配 GAN 可以很好的优化 G。\n\n分类器输出维度为 K+1 是过参数化的，由于输出向量中每个元素值均减去同一个值 $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$，对 softmax 的值并不影响，所以可固定 $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$，于是 $L_{supervised}$ 变为具有 K 个类别的原始分类器的标准监督损失，此时判别器 D 为 $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$，其中 $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$。","slug":"ImprovedGAN","published":1,"updated":"2020-04-24T10:38:18.844Z","_id":"ck9dzcipw0014gga6fg3vbir8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>标题 <a href=\"https://arxiv.org/abs/1606.03498\" target=\"_blank\" rel=\"noopener\">Improved Techniques for Training GANs</a></p>\n<p>源码 <a href=\"https://github.com/openai/improved_gan\" target=\"_blank\" rel=\"noopener\">improved_gan</a></p>\n<a id=\"more\"></a>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>GAN 是基于博弈论学习生成模型的一类方法总称。GAN 目的是训练一个生成网络其生成样本的分布可以拟合真实数据分布。虽然 <a href=\"2019/07/23/GAN\">DCGAN</a> 在 GAN 中引入 conv+BN+ReLU 在一定程度上改善了生成器，但是我们认为 GAN 这个零和博弈问题具有高维参数且非凸，需要达到 Nash 均衡才是最佳解，而传统的基于目标函数梯度下降方法目的并非用于寻找 Nash 均衡。本文提出了以下改进方法：</p>\n<ol>\n<li>特征匹配</li>\n<li>小批量特征</li>\n<li>虚拟批归一化</li>\n</ol>\n<h1 id=\"GAN-训练收敛\"><a href=\"#GAN-训练收敛\" class=\"headerlink\" title=\"GAN 训练收敛\"></a>GAN 训练收敛</h1><p>训练 GAN 意味着寻找二人非合作博弈中的 Nash 均衡，每个玩家希望最小化自己的损失函数，即生成器损失 $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$ 和判别器损失 $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$，Nash 均衡指 $J^{(D)}$ 关于 $\\theta^{(D)}$ 最小，同时 $J^{(G)}$ 关于 $\\theta^{(G)}$ 最小。寻找 Nash 均衡点是一个比较困难的问题，虽然某些特殊情况下有算法可以解决，但是由于这里损失函数非凸且参数维度很高，那些方法均不适用。以上 Nash 均衡的说明让我们从直觉上认为需要同时最小化 G 和 D 的损失。但是很不幸，更新 $\\theta^{(D)}$ 降低 $J^{(D)}$ 却增大 $J^{(G)}$，更新 $\\theta^{(G)}$ 以降低 $J^{(G)}$ 但是会增大 $J^{(D)}$。这就是导致梯度下降法难以收敛（往往是在一个轨道上一直反复，而不会到达最佳点）。例如一个玩家根据 x 来最小化 xy，另一个玩家根据 y 来最小化 -xy，梯度下降法更新会使得 x 和 y 值构成的点在一个椭圆上往复，而不会收敛到 x=y=0。本文介绍以下方法启发式的促使更新达到收敛。</p>\n<h2 id=\"特征匹配\"><a href=\"#特征匹配\" class=\"headerlink\" title=\"特征匹配\"></a>特征匹配</h2><p>特征匹配使用新的生成器损失函数以解决 GAN 训练不稳定问题。新的目标函数不是直接最大化 D 的输出（G 原本的目标是让 D 对生成样本有越大越好的输出），而是让 G 生成的样本能够匹配真实数据的统计量，这是一种更直接的思想。具体而言，训练 G 以匹配特征的期望值，这个特征来自于 D 的网络中间层。令 $\\mathbf {f(x)}$ 表示 D 网络中间层的激活响应，即前面所指的特征，那么 G 的新目标函数为<br>$$|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))|_2^2$$<br>G 的训练目标就是最小化这个目标损失。</p>\n<h2 id=\"小批量判别\"><a href=\"#小批量判别\" class=\"headerlink\" title=\"小批量判别\"></a>小批量判别</h2><p>GAN 训练失败的原因之一是生成器训练时总是会陷入一组参数无法逃脱，我这里称其为“陷入点”，当临近“陷入点”时，生成器的输出点总是很相似，而这些相似的点会让判别器总是指向一个差不多的方向，由于判别器 <strong>独立处理</strong> 每个样本，这些样本对应的梯度相互之间无法合作，缺乏一种反馈机制去通知生成器让其输出相互之间尽可能不相似，生成器所有的输出都向同一个点竞争，这个点是为了让判别器判别为真实的数据，所以结果就是生成器陷入一组模型参数无法自拔，陷入之后，判别器通过学习又能够将这个点判别为来自生成器，但是梯度 <strong>无法区分</strong> 各个不同的输出，于是判别器的梯度会一直在空间中将生成器产生的这个“陷入点”推来推去，导致算法无法收敛。一种显然的解决办法是让判别器不独立处理每个样本，而是一次能看到多个样本的合并，这就是小批量判别方法。</p>\n<p>现在我们的实验建模瞄准于区分生成器的各个相互靠得很近得样本。小批量中样本之间接近程度按如下方法计算：<br>令 $\\mathbf {f(x_i)} \\in \\Bbb R^A$ 表示输入 $\\mathbf x_i$ 对应的特征向量，这个特征由 D 网络中间层产生，然后将特征向量乘以一个张量 $T \\in \\Bbb R^{A \\times B \\times C}$，结果是一个矩阵 $M_i \\in \\Bbb R^{B \\times C}$，对于输入样本编号 $i \\in {1,…,n}$，得到对应的矩阵 ${M_i |i=1,…,n}$，计算两两矩阵的各行向量之间的 L1 距离，然后应用负指数函数，<br>$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-|M_{i,b}-M_{j,b}| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in {1,…,n}, \\quad b \\in {1,…,B}$$</p>\n<p>其中下标 b 表示 row index。如图 1，minibatch layer 中样本 $\\mathbf x_i$ 对应的输出定义为，<br>$$\\begin{aligned} &amp;o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R<br>\\\\ &amp;o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,…o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B<br>\\\\ &amp;o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$</p>\n<p>然后，将 minibatch layer 的输出 $o(\\mathbf x_i)$ 与 minibatch layer 的输入 $\\mathbf {f(x_i)}$ concatenate 起来，作为 D 的下一 layer 的输入。对生成样本和训练数据分别计算 minibatch layer 特征。<br><img src=\"/images/ImprovedGAN_fig1.png\" alt=\"\"></p>\n<h2 id=\"历史平均\"><a href=\"#历史平均\" class=\"headerlink\" title=\"历史平均\"></a>历史平均</h2><p>修改每个玩家（G 和 D）的损失使得包含 $|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]|^2$，其中 $\\theta[i]$ 是历史时期 i 的参数值。</p>\n<h2 id=\"单边标注平滑\"><a href=\"#单边标注平滑\" class=\"headerlink\" title=\"单边标注平滑\"></a>单边标注平滑</h2><p>Label 平滑，就是将分类器的 target 值由 0 和 1 替换为一个平滑的值如 0.9 或 0.1。我们将正例 target 替换为 $\\alpha$，负例 target 替换为 $\\beta$，那么最佳判别器变为<br>$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$</p>\n<ul>\n<li>当 $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\alpha$</li>\n<li>当 $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\beta$</li>\n</ul>\n<p>当然我们也可以按 <a href=\"2019/7/23/GAN\">GAN</a> 中那样推导 $D^{\\ast}$，推导过程这里略过，只是此时目标变为<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$</p>\n<p>这里约定正例 target 大于负例 target，即 $\\alpha &gt; \\beta$，由 (1) 式，可知 D 输出范围为 $\\beta &lt; D(x) &lt; \\alpha$。</p>\n<p>由于分子中出现 $p_{model}$，那么当 $p_{data} \\rightarrow 0$，且 $p_{model}$ 足够大时，来自 $p_{model}$ 的错误样本将得不到促使向真实数据靠近的激励，所以只对正例 label 平滑处理为 $\\alpha$，负例 label 依然为 0。</p>\n<h2 id=\"虚拟批归一化\"><a href=\"#虚拟批归一化\" class=\"headerlink\" title=\"虚拟批归一化\"></a>虚拟批归一化</h2><p>DCGAN 中使用了批归一化 BN 使得网络优化更加有效，但是也会带来问题，比如一个输入 $\\mathbf x$，其对应的输出高度依赖于同一 minibatch 中的其他输入 $\\mathbf x’$。为了避免这个问题，本文使用了虚拟批归一化 VBN，每个样本输入 $\\mathbf x$ 的归一化过程基于 reference batch 中样本的统计量以及 $\\mathbf x$ 自身，reference batch 是在训练初期选定并固定不变，reference batch 使用统计量进行归一化。由于 VBN 计算强度较高，故只在 G 网络中使用。</p>\n<h1 id=\"图像质量评估\"><a href=\"#图像质量评估\" class=\"headerlink\" title=\"图像质量评估\"></a>图像质量评估</h1><p>GAN 的性能评估最直接的方法就是人类观察员判断，缺点是难以公平公正。本文提出了一个自动评估方法：应用 Inception 模型到每个生成样本上，以获得条件 label 分布 $p(y|\\mathbf x)$，那些包含有意义目标的图像的条件 label 分布 $p(y|\\mathbf x)$ 应该具有较低的信息熵，也就是说，具有较低的不确定性，这意味着，对于给定的输入 $\\mathbf x$（包含有意义目标的图像），Inception 模型每次输出值 y （比如图像分类 c）比较稳定变化很小。但是我们又希望生成模型能够生成各种不同的图像，即对于不同的噪声输入 z，G 能够生成各种不同的图像，分别以这些不同的图像作为输入， Inception 模型的输出也尽可能不同（不确定性较大），这说明 $\\int p(y|\\mathbf x=G(z)) dz$ 应该具有较大的信息熵。结合以上这两点要求，性能指标为这两个分布 KL 散度的期望，<br>$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)|p(y)) ]$$</p>\n<p>应用指数函数仅仅是为了便于比较值的大小。</p>\n<h1 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h1><p>考虑一个标准分类器，输入为 $\\mathbf x$，共有 K 种类别，输出为长度 K 的向量 $[l_1,…,l_K]$，表示每个类别的得分，通过 softmax 得到对应的概率：<br>$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$</p>\n<p>在监督学习中，此模型的训练是最小化交叉熵（或最大化 log 似然函数）。</p>\n<p>增加来自生成器 G 的样本到数据集中，可以实现标准分类器的半监督学习，G 生成样本标记类别 y=K+1，分类器的输出维度改为 K+1，利用 $p_{model}(y=K+1|\\mathbf x)$ 判断输入 $\\mathbf x$ 是生成样本的概率，与 GAN 中的 $1-D(\\mathbf x)$ 是对应的。也可以使用未标注数据进行学习，对于来自 K 个类别的真实数据，需要最大化 $\\log p_{model}(y \\in {1,…,K}|\\mathbf x)$（log 似然函数），假设数据集中一半是真实数据，一半是生成数据，那么分类器训练的损失函数为，<br>$$\\begin{aligned} &amp;L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}<br>\\\\ &amp;L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y &lt;K+1)<br>\\\\ &amp;L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$</p>\n<p>其中求期望实际上是经验期望也就是均值损失。其中 $L_{unsupervised}$ 就是标准 GAN 的 objective，在 $L_{unsupervised}$ 中作替换 $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$，就更明显了,于是有<br>$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$</p>\n<p>最小化 $L_{supervised}$ 和 $L_{unsupervised}$ 的最优解是满足 $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$ 以及 $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$，其中 $c(\\mathbf x)$ 是待定的系数函数。训练 G 以近似真实的数据分布，一种训练方法是最小化 GAN objective，使用这里的分类器作为判别器 D，这种方法引入了 G 和分类器之间的相互作用，经验表明，在半监督学习中，使用特征匹配 GAN 可以很好的优化 G。</p>\n<p>分类器输出维度为 K+1 是过参数化的，由于输出向量中每个元素值均减去同一个值 $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$，对 softmax 的值并不影响，所以可固定 $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$，于是 $L_{supervised}$ 变为具有 K 个类别的原始分类器的标准监督损失，此时判别器 D 为 $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$，其中 $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$。</p>\n","site":{"data":{}},"excerpt":"<p>标题 <a href=\"https://arxiv.org/abs/1606.03498\" target=\"_blank\" rel=\"noopener\">Improved Techniques for Training GANs</a></p>\n<p>源码 <a href=\"https://github.com/openai/improved_gan\" target=\"_blank\" rel=\"noopener\">improved_gan</a></p>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>GAN 是基于博弈论学习生成模型的一类方法总称。GAN 目的是训练一个生成网络其生成样本的分布可以拟合真实数据分布。虽然 <a href=\"2019/07/23/GAN\">DCGAN</a> 在 GAN 中引入 conv+BN+ReLU 在一定程度上改善了生成器，但是我们认为 GAN 这个零和博弈问题具有高维参数且非凸，需要达到 Nash 均衡才是最佳解，而传统的基于目标函数梯度下降方法目的并非用于寻找 Nash 均衡。本文提出了以下改进方法：</p>\n<ol>\n<li>特征匹配</li>\n<li>小批量特征</li>\n<li>虚拟批归一化</li>\n</ol>\n<h1 id=\"GAN-训练收敛\"><a href=\"#GAN-训练收敛\" class=\"headerlink\" title=\"GAN 训练收敛\"></a>GAN 训练收敛</h1><p>训练 GAN 意味着寻找二人非合作博弈中的 Nash 均衡，每个玩家希望最小化自己的损失函数，即生成器损失 $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$ 和判别器损失 $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$，Nash 均衡指 $J^{(D)}$ 关于 $\\theta^{(D)}$ 最小，同时 $J^{(G)}$ 关于 $\\theta^{(G)}$ 最小。寻找 Nash 均衡点是一个比较困难的问题，虽然某些特殊情况下有算法可以解决，但是由于这里损失函数非凸且参数维度很高，那些方法均不适用。以上 Nash 均衡的说明让我们从直觉上认为需要同时最小化 G 和 D 的损失。但是很不幸，更新 $\\theta^{(D)}$ 降低 $J^{(D)}$ 却增大 $J^{(G)}$，更新 $\\theta^{(G)}$ 以降低 $J^{(G)}$ 但是会增大 $J^{(D)}$。这就是导致梯度下降法难以收敛（往往是在一个轨道上一直反复，而不会到达最佳点）。例如一个玩家根据 x 来最小化 xy，另一个玩家根据 y 来最小化 -xy，梯度下降法更新会使得 x 和 y 值构成的点在一个椭圆上往复，而不会收敛到 x=y=0。本文介绍以下方法启发式的促使更新达到收敛。</p>\n<h2 id=\"特征匹配\"><a href=\"#特征匹配\" class=\"headerlink\" title=\"特征匹配\"></a>特征匹配</h2><p>特征匹配使用新的生成器损失函数以解决 GAN 训练不稳定问题。新的目标函数不是直接最大化 D 的输出（G 原本的目标是让 D 对生成样本有越大越好的输出），而是让 G 生成的样本能够匹配真实数据的统计量，这是一种更直接的思想。具体而言，训练 G 以匹配特征的期望值，这个特征来自于 D 的网络中间层。令 $\\mathbf {f(x)}$ 表示 D 网络中间层的激活响应，即前面所指的特征，那么 G 的新目标函数为<br>$$|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))|_2^2$$<br>G 的训练目标就是最小化这个目标损失。</p>\n<h2 id=\"小批量判别\"><a href=\"#小批量判别\" class=\"headerlink\" title=\"小批量判别\"></a>小批量判别</h2><p>GAN 训练失败的原因之一是生成器训练时总是会陷入一组参数无法逃脱，我这里称其为“陷入点”，当临近“陷入点”时，生成器的输出点总是很相似，而这些相似的点会让判别器总是指向一个差不多的方向，由于判别器 <strong>独立处理</strong> 每个样本，这些样本对应的梯度相互之间无法合作，缺乏一种反馈机制去通知生成器让其输出相互之间尽可能不相似，生成器所有的输出都向同一个点竞争，这个点是为了让判别器判别为真实的数据，所以结果就是生成器陷入一组模型参数无法自拔，陷入之后，判别器通过学习又能够将这个点判别为来自生成器，但是梯度 <strong>无法区分</strong> 各个不同的输出，于是判别器的梯度会一直在空间中将生成器产生的这个“陷入点”推来推去，导致算法无法收敛。一种显然的解决办法是让判别器不独立处理每个样本，而是一次能看到多个样本的合并，这就是小批量判别方法。</p>\n<p>现在我们的实验建模瞄准于区分生成器的各个相互靠得很近得样本。小批量中样本之间接近程度按如下方法计算：<br>令 $\\mathbf {f(x_i)} \\in \\Bbb R^A$ 表示输入 $\\mathbf x_i$ 对应的特征向量，这个特征由 D 网络中间层产生，然后将特征向量乘以一个张量 $T \\in \\Bbb R^{A \\times B \\times C}$，结果是一个矩阵 $M_i \\in \\Bbb R^{B \\times C}$，对于输入样本编号 $i \\in {1,…,n}$，得到对应的矩阵 ${M_i |i=1,…,n}$，计算两两矩阵的各行向量之间的 L1 距离，然后应用负指数函数，<br>$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-|M_{i,b}-M_{j,b}| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in {1,…,n}, \\quad b \\in {1,…,B}$$</p>\n<p>其中下标 b 表示 row index。如图 1，minibatch layer 中样本 $\\mathbf x_i$ 对应的输出定义为，<br>$$\\begin{aligned} &amp;o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R<br>\\\\ &amp;o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,…o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B<br>\\\\ &amp;o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$</p>\n<p>然后，将 minibatch layer 的输出 $o(\\mathbf x_i)$ 与 minibatch layer 的输入 $\\mathbf {f(x_i)}$ concatenate 起来，作为 D 的下一 layer 的输入。对生成样本和训练数据分别计算 minibatch layer 特征。<br><img src=\"/images/ImprovedGAN_fig1.png\" alt=\"\"></p>\n<h2 id=\"历史平均\"><a href=\"#历史平均\" class=\"headerlink\" title=\"历史平均\"></a>历史平均</h2><p>修改每个玩家（G 和 D）的损失使得包含 $|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]|^2$，其中 $\\theta[i]$ 是历史时期 i 的参数值。</p>\n<h2 id=\"单边标注平滑\"><a href=\"#单边标注平滑\" class=\"headerlink\" title=\"单边标注平滑\"></a>单边标注平滑</h2><p>Label 平滑，就是将分类器的 target 值由 0 和 1 替换为一个平滑的值如 0.9 或 0.1。我们将正例 target 替换为 $\\alpha$，负例 target 替换为 $\\beta$，那么最佳判别器变为<br>$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$</p>\n<ul>\n<li>当 $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\alpha$</li>\n<li>当 $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ 时，$D(\\mathbf x) \\rightarrow \\beta$</li>\n</ul>\n<p>当然我们也可以按 <a href=\"2019/7/23/GAN\">GAN</a> 中那样推导 $D^{\\ast}$，推导过程这里略过，只是此时目标变为<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$</p>\n<p>这里约定正例 target 大于负例 target，即 $\\alpha &gt; \\beta$，由 (1) 式，可知 D 输出范围为 $\\beta &lt; D(x) &lt; \\alpha$。</p>\n<p>由于分子中出现 $p_{model}$，那么当 $p_{data} \\rightarrow 0$，且 $p_{model}$ 足够大时，来自 $p_{model}$ 的错误样本将得不到促使向真实数据靠近的激励，所以只对正例 label 平滑处理为 $\\alpha$，负例 label 依然为 0。</p>\n<h2 id=\"虚拟批归一化\"><a href=\"#虚拟批归一化\" class=\"headerlink\" title=\"虚拟批归一化\"></a>虚拟批归一化</h2><p>DCGAN 中使用了批归一化 BN 使得网络优化更加有效，但是也会带来问题，比如一个输入 $\\mathbf x$，其对应的输出高度依赖于同一 minibatch 中的其他输入 $\\mathbf x’$。为了避免这个问题，本文使用了虚拟批归一化 VBN，每个样本输入 $\\mathbf x$ 的归一化过程基于 reference batch 中样本的统计量以及 $\\mathbf x$ 自身，reference batch 是在训练初期选定并固定不变，reference batch 使用统计量进行归一化。由于 VBN 计算强度较高，故只在 G 网络中使用。</p>\n<h1 id=\"图像质量评估\"><a href=\"#图像质量评估\" class=\"headerlink\" title=\"图像质量评估\"></a>图像质量评估</h1><p>GAN 的性能评估最直接的方法就是人类观察员判断，缺点是难以公平公正。本文提出了一个自动评估方法：应用 Inception 模型到每个生成样本上，以获得条件 label 分布 $p(y|\\mathbf x)$，那些包含有意义目标的图像的条件 label 分布 $p(y|\\mathbf x)$ 应该具有较低的信息熵，也就是说，具有较低的不确定性，这意味着，对于给定的输入 $\\mathbf x$（包含有意义目标的图像），Inception 模型每次输出值 y （比如图像分类 c）比较稳定变化很小。但是我们又希望生成模型能够生成各种不同的图像，即对于不同的噪声输入 z，G 能够生成各种不同的图像，分别以这些不同的图像作为输入， Inception 模型的输出也尽可能不同（不确定性较大），这说明 $\\int p(y|\\mathbf x=G(z)) dz$ 应该具有较大的信息熵。结合以上这两点要求，性能指标为这两个分布 KL 散度的期望，<br>$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)|p(y)) ]$$</p>\n<p>应用指数函数仅仅是为了便于比较值的大小。</p>\n<h1 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h1><p>考虑一个标准分类器，输入为 $\\mathbf x$，共有 K 种类别，输出为长度 K 的向量 $[l_1,…,l_K]$，表示每个类别的得分，通过 softmax 得到对应的概率：<br>$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$</p>\n<p>在监督学习中，此模型的训练是最小化交叉熵（或最大化 log 似然函数）。</p>\n<p>增加来自生成器 G 的样本到数据集中，可以实现标准分类器的半监督学习，G 生成样本标记类别 y=K+1，分类器的输出维度改为 K+1，利用 $p_{model}(y=K+1|\\mathbf x)$ 判断输入 $\\mathbf x$ 是生成样本的概率，与 GAN 中的 $1-D(\\mathbf x)$ 是对应的。也可以使用未标注数据进行学习，对于来自 K 个类别的真实数据，需要最大化 $\\log p_{model}(y \\in {1,…,K}|\\mathbf x)$（log 似然函数），假设数据集中一半是真实数据，一半是生成数据，那么分类器训练的损失函数为，<br>$$\\begin{aligned} &amp;L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}<br>\\\\ &amp;L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y &lt;K+1)<br>\\\\ &amp;L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$</p>\n<p>其中求期望实际上是经验期望也就是均值损失。其中 $L_{unsupervised}$ 就是标准 GAN 的 objective，在 $L_{unsupervised}$ 中作替换 $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$，就更明显了,于是有<br>$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$</p>\n<p>最小化 $L_{supervised}$ 和 $L_{unsupervised}$ 的最优解是满足 $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$ 以及 $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$，其中 $c(\\mathbf x)$ 是待定的系数函数。训练 G 以近似真实的数据分布，一种训练方法是最小化 GAN objective，使用这里的分类器作为判别器 D，这种方法引入了 G 和分类器之间的相互作用，经验表明，在半监督学习中，使用特征匹配 GAN 可以很好的优化 G。</p>\n<p>分类器输出维度为 K+1 是过参数化的，由于输出向量中每个元素值均减去同一个值 $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$，对 softmax 的值并不影响，所以可固定 $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$，于是 $L_{supervised}$ 变为具有 K 个类别的原始分类器的标准监督损失，此时判别器 D 为 $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$，其中 $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$。</p>"},{"title":"M2Det","date":"2019-06-28T09:59:08.000Z","mathjax":true,"_content":"论文：[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n<!-- more -->\n# Introduction\n我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，\n![](/images/M2Det_fig1.png)\n\nSSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：\n1. pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。\n2. pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。\n\n一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。\n\n本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，\n![](/images/M2Det_fig2.png)\n\n首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。\n\n为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。\n\n# Method\nM2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。\n\n## MLFPN\n如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：\n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。\n\n### FFM\nFFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 __对称__ 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center>\n\n### TUM\nTUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。\n\n### SFAM\nSFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center>\n\n第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为 \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n其中 $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，\n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$。\n\n### 网络配置\n分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。\n\nMLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为\n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。\n\n在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 [soft-NMS](/2019/06/24/cv-mtds) 进一步处理检测结果。\n\n# 实验\n实验略，请阅读原文以获取详细信息\n\n# 结论\n提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。","source":"_posts/M2Det.md","raw":"---\ntitle: M2Det\ndate: 2019-06-28 17:59:08\ntags: object detection\nmathjax: true\n---\n论文：[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n<!-- more -->\n# Introduction\n我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，\n![](/images/M2Det_fig1.png)\n\nSSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：\n1. pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。\n2. pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。\n\n一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。\n\n本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，\n![](/images/M2Det_fig2.png)\n\n首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。\n\n为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。\n\n# Method\nM2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。\n\n## MLFPN\n如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：\n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。\n\n### FFM\nFFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 __对称__ 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center>\n\n### TUM\nTUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。\n\n### SFAM\nSFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center>\n\n第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为 \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n其中 $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，\n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$。\n\n### 网络配置\n分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。\n\nMLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为\n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。\n\n在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 [soft-NMS](/2019/06/24/cv-mtds) 进一步处理检测结果。\n\n# 实验\n实验略，请阅读原文以获取详细信息\n\n# 结论\n提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。","slug":"M2Det","published":1,"updated":"2020-04-24T10:36:31.648Z","_id":"ck9dzciq40016gga60l432k8l","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>\n<a id=\"more\"></a>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，<br><img src=\"/images/M2Det_fig1.png\" alt=\"\"></p>\n<p>SSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：</p>\n<ol>\n<li>pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。</li>\n<li>pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。</li>\n</ol>\n<p>一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。</p>\n<p>本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，<br><img src=\"/images/M2Det_fig2.png\" alt=\"\"></p>\n<p>首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。</p>\n<p>为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。</p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。</p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p>如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：<br>$$[x_1^l,…x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,…L \\end{cases}$$<br>其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。</p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 <strong>对称</strong> 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。<br><img src=\"/images/M2Det_fig4.png\" alt=\"\"> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，<br><img src=\"/images/M2Det_fig3.png\" alt=\"\"><center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center></p>\n<p>第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为<br>$$\\mathbf X=[\\mathbf X_1,…,\\mathbf X_i]$$<br>其中 $\\mathbf X_i=Concat(x_i^1,…x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，<br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p>最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},…,\\tilde {\\mathbf X_i^C}]$。</p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h3><p>分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。</p>\n<p>MLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为<br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。</p>\n<p>在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> 进一步处理检测结果。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验略，请阅读原文以获取详细信息</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。</p>\n","site":{"data":{}},"excerpt":"<p>论文：<a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，<br><img src=\"/images/M2Det_fig1.png\" alt=\"\"></p>\n<p>SSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：</p>\n<ol>\n<li>pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。</li>\n<li>pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。</li>\n</ol>\n<p>一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。</p>\n<p>本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，<br><img src=\"/images/M2Det_fig2.png\" alt=\"\"></p>\n<p>首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。</p>\n<p>为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。</p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。</p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p>如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：<br>$$[x_1^l,…x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,…L \\end{cases}$$<br>其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。</p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 <strong>对称</strong> 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。<br><img src=\"/images/M2Det_fig4.png\" alt=\"\"> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，<br><img src=\"/images/M2Det_fig3.png\" alt=\"\"><center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center></p>\n<p>第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为<br>$$\\mathbf X=[\\mathbf X_1,…,\\mathbf X_i]$$<br>其中 $\\mathbf X_i=Concat(x_i^1,…x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，<br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p>最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},…,\\tilde {\\mathbf X_i^C}]$。</p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h3><p>分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。</p>\n<p>MLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为<br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。</p>\n<p>在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> 进一步处理检测结果。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验略，请阅读原文以获取详细信息</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。</p>"},{"title":"TridentNet","date":"2019-06-21T08:24:19.000Z","mathjax":true,"_content":"论文：[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n<!-- more -->\n代码：[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# 简介\n目标检测通常分为：\n1. one stage，如 YOLO, SSD\n2. two stage，如 Faster R-CNN, R-FCN\n\n这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：\n1. 生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时\n2. 利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD\n3. 2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。\n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。\n\n# 感受野\nbackbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。\n\n假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。\n\n实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 _conv4_ stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font>\n\n从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：\n1. 网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的\n2. 尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小\n\n# Trident 网络\nTridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。\n## 网络结构\n如图 2，\n![](/images/TridentNet_fig2.png)\n\n网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。\n\n__多分支块__ 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。\n\n以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。\n\n__分支间共享权重__ 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。\n参数共享优点有三：\n1. 降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数\n2. 对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）\n3. 因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。\n\n## scale-aware 训练机制\n根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。\n\n每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 `(w,h)`，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。\n\nscale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。\n\n## Inference 和近似\nInference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。\n\n__快速推断近似__ 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。\n\n# 实验\n实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。\n\n## 实现细节\n使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。\n\n性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。\n\n## 消融学习\n\n__TridentNet 组件__ Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。\n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n   如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。\n2. __Scale-aware__\n   Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。\n3. __Weight-sharing__\n   Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。\n\n__分支数量__ Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ 目标检测结果。ResNet-50，使用不同分支数量</font>\n\n其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。\n\n# 结论\n提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。","source":"_posts/TridentNet.md","raw":"---\ntitle: TridentNet\ndate: 2019-06-21 16:24:19\ntags: object detection\nmathjax: true\n---\n论文：[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n<!-- more -->\n代码：[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# 简介\n目标检测通常分为：\n1. one stage，如 YOLO, SSD\n2. two stage，如 Faster R-CNN, R-FCN\n\n这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：\n1. 生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时\n2. 利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD\n3. 2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。\n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。\n\n# 感受野\nbackbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。\n\n假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。\n\n实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 _conv4_ stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font>\n\n从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：\n1. 网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的\n2. 尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小\n\n# Trident 网络\nTridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。\n## 网络结构\n如图 2，\n![](/images/TridentNet_fig2.png)\n\n网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。\n\n__多分支块__ 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。\n\n以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。\n\n__分支间共享权重__ 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。\n参数共享优点有三：\n1. 降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数\n2. 对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）\n3. 因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。\n\n## scale-aware 训练机制\n根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。\n\n每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 `(w,h)`，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。\n\nscale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。\n\n## Inference 和近似\nInference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。\n\n__快速推断近似__ 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。\n\n# 实验\n实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。\n\n## 实现细节\n使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。\n\n性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。\n\n## 消融学习\n\n__TridentNet 组件__ Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。\n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n   如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。\n2. __Scale-aware__\n   Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。\n3. __Weight-sharing__\n   Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。\n\n__分支数量__ Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ 目标检测结果。ResNet-50，使用不同分支数量</font>\n\n其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。\n\n# 结论\n提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。","slug":"TridentNet","published":1,"updated":"2020-04-24T10:36:12.120Z","_id":"ck9dzciqf0018gga6gl2k56v5","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>\n<a id=\"more\"></a>\n<p>代码：<a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目标检测通常分为：</p>\n<ol>\n<li>one stage，如 YOLO, SSD</li>\n<li>two stage，如 Faster R-CNN, R-FCN</li>\n</ol>\n<p>这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：</p>\n<ol>\n<li>生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时</li>\n<li>利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD</li>\n<li>2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。</li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt=\"\"> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt=\"\"> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p>本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。</p>\n<h1 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h1><p>backbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。</p>\n<p>假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。</p>\n<p>实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 <em>conv4</em> stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font></p>\n<p>从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：</p>\n<ol>\n<li>网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的</li>\n<li>尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小</li>\n</ol>\n<h1 id=\"Trident-网络\"><a href=\"#Trident-网络\" class=\"headerlink\" title=\"Trident 网络\"></a>Trident 网络</h1><p>TridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。</p>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>如图 2，<br><img src=\"/images/TridentNet_fig2.png\" alt=\"\"></p>\n<p>网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。</p>\n<p><strong>多分支块</strong> 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。</p>\n<p>以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。</p>\n<p><strong>分支间共享权重</strong> 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。<br>参数共享优点有三：</p>\n<ol>\n<li>降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数</li>\n<li>对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）</li>\n<li>因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。</li>\n</ol>\n<h2 id=\"scale-aware-训练机制\"><a href=\"#scale-aware-训练机制\" class=\"headerlink\" title=\"scale-aware 训练机制\"></a>scale-aware 训练机制</h2><p>根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。</p>\n<p>每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 <code>(w,h)</code>，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。</p>\n<p>scale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。</p>\n<h2 id=\"Inference-和近似\"><a href=\"#Inference-和近似\" class=\"headerlink\" title=\"Inference 和近似\"></a>Inference 和近似</h2><p>Inference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。</p>\n<p><strong>快速推断近似</strong> 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。</p>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><p>使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。</p>\n<p>性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。</p>\n<h2 id=\"消融学习\"><a href=\"#消融学习\" class=\"headerlink\" title=\"消融学习\"></a>消融学习</h2><p><strong>TridentNet 组件</strong> Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。<br><img src=\"/images/TridentNet_fig3.png\" alt=\"\"></p>\n<ol>\n<li><strong>Multi-branch</strong><br>如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。</li>\n<li><strong>Scale-aware</strong><br>Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。</li>\n<li><strong>Weight-sharing</strong><br>Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。</li>\n</ol>\n<p><strong>分支数量</strong> Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=2> Table 3 COCO <em>minival</em> 目标检测结果。ResNet-50，使用不同分支数量</font></p>\n<p>其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。</p>\n","site":{"data":{}},"excerpt":"<p>论文：<a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>","more":"<p>代码：<a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目标检测通常分为：</p>\n<ol>\n<li>one stage，如 YOLO, SSD</li>\n<li>two stage，如 Faster R-CNN, R-FCN</li>\n</ol>\n<p>这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：</p>\n<ol>\n<li>生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时</li>\n<li>利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD</li>\n<li>2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。</li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt=\"\"> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt=\"\"> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p>本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。</p>\n<h1 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h1><p>backbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。</p>\n<p>假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。</p>\n<p>实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 <em>conv4</em> stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font></p>\n<p>从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：</p>\n<ol>\n<li>网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的</li>\n<li>尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小</li>\n</ol>\n<h1 id=\"Trident-网络\"><a href=\"#Trident-网络\" class=\"headerlink\" title=\"Trident 网络\"></a>Trident 网络</h1><p>TridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。</p>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>如图 2，<br><img src=\"/images/TridentNet_fig2.png\" alt=\"\"></p>\n<p>网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。</p>\n<p><strong>多分支块</strong> 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。</p>\n<p>以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。</p>\n<p><strong>分支间共享权重</strong> 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。<br>参数共享优点有三：</p>\n<ol>\n<li>降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数</li>\n<li>对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）</li>\n<li>因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。</li>\n</ol>\n<h2 id=\"scale-aware-训练机制\"><a href=\"#scale-aware-训练机制\" class=\"headerlink\" title=\"scale-aware 训练机制\"></a>scale-aware 训练机制</h2><p>根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。</p>\n<p>每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 <code>(w,h)</code>，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。</p>\n<p>scale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。</p>\n<h2 id=\"Inference-和近似\"><a href=\"#Inference-和近似\" class=\"headerlink\" title=\"Inference 和近似\"></a>Inference 和近似</h2><p>Inference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。</p>\n<p><strong>快速推断近似</strong> 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。</p>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><p>使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。</p>\n<p>性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。</p>\n<h2 id=\"消融学习\"><a href=\"#消融学习\" class=\"headerlink\" title=\"消融学习\"></a>消融学习</h2><p><strong>TridentNet 组件</strong> Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。<br><img src=\"/images/TridentNet_fig3.png\" alt=\"\"></p>\n<ol>\n<li><strong>Multi-branch</strong><br>如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。</li>\n<li><strong>Scale-aware</strong><br>Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。</li>\n<li><strong>Weight-sharing</strong><br>Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。</li>\n</ol>\n<p><strong>分支数量</strong> Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=2> Table 3 COCO <em>minival</em> 目标检测结果。ResNet-50，使用不同分支数量</font></p>\n<p>其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。</p>"},{"title":"mask-rcnn","date":"2019-07-08T09:39:57.000Z","mathjax":true,"_content":"论文 [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n<!-- more -->\n# Introduction\n这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  \n\nMask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。  \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  \n\n另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。\n\n# Mask R-CNN\nFaster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。\n\n__Faster R-CNN:__ 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。\n\n__Mask R-CNN:__ 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。\n\n训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，\n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。\n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。  \nmask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，\n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n其中 $f(\\cdot)$ 表示 sigmoid。\n\n__Mask Representation:__ 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。\n\n__RoIAlign:__ RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。\n\n可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，\n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI 的大小 和 空间 bin 的大小分别为\n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n对于第 (i,j) 个 bin，其位置为\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n其中 $0 \\le i<7, \\ 0\\le j<7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）\n\n两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，\n![](/images/mask-rcnn_fig3.png)\n\n特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。\n\n__Network Architecture:__ Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。\n\nBackbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。\n\n我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。\n\n对于 Network head，如图 4，\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。  \n图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。\n# Experiments\n实验部分略，请阅读原文。\n\n# Appendix\n有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i < R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，\n1. 计算第 i 个 box 的宽高  \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2. 将 mask map resize 到 box 宽高的大小  \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3. 将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4. 将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","source":"_posts/mask-rcnn.md","raw":"---\ntitle: mask-rcnn\ndate: 2019-07-08 17:39:57\ntags: object detection\nmathjax: true\n---\n论文 [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n<!-- more -->\n# Introduction\n这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  \n\nMask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。  \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  \n\n另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。\n\n# Mask R-CNN\nFaster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。\n\n__Faster R-CNN:__ 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。\n\n__Mask R-CNN:__ 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。\n\n训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，\n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。\n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。  \nmask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，\n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n其中 $f(\\cdot)$ 表示 sigmoid。\n\n__Mask Representation:__ 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。\n\n__RoIAlign:__ RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。\n\n可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，\n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI 的大小 和 空间 bin 的大小分别为\n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n对于第 (i,j) 个 bin，其位置为\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n其中 $0 \\le i<7, \\ 0\\le j<7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）\n\n两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，\n![](/images/mask-rcnn_fig3.png)\n\n特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。\n\n__Network Architecture:__ Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。\n\nBackbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。\n\n我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。\n\n对于 Network head，如图 4，\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。  \n图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。\n# Experiments\n实验部分略，请阅读原文。\n\n# Appendix\n有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i < R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，\n1. 计算第 i 个 box 的宽高  \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2. 将 mask map resize 到 box 宽高的大小  \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3. 将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4. 将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","slug":"mask-rcnn","published":1,"updated":"2020-04-24T10:36:21.610Z","_id":"ck9dzciqf001agga6bgobcaq5","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>\n<a id=\"more\"></a>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  </p>\n<p>Mask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。<br><img src=\"/images/mask-rcnn_fig1.png\" alt=\"\"></p>\n<p>Faster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  </p>\n<p>另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。</p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。</p>\n<p><strong>Faster R-CNN:</strong> 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。</p>\n<p><strong>Mask R-CNN:</strong> 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。</p>\n<p>训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，<br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br>上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。<br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br>上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。<br>mask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，<br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br>其中 $f(\\cdot)$ 表示 sigmoid。</p>\n<p><strong>Mask Representation:</strong> 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。</p>\n<p><strong>RoIAlign:</strong> RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。</p>\n<p>可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，<br>$$x_1’=[x_1/16] \\quad y_1’=[y_1/16]<br>\\quad x_2’=[x_2/16]<br>\\quad y_2’=[y_2/16]$$<br>RoI 的大小 和 空间 bin 的大小分别为<br>$$w’=x_2’-x_1’+1<br>\\quad h’=y_2’-y_1’+1<br>\\\\ w^b=w’/7 \\quad h^b=h’/7$$<br>对于第 (i,j) 个 bin，其位置为<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p>其中 $0 \\le i&lt;7, \\ 0\\le j&lt;7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）</p>\n<p>两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，<br><img src=\"/images/mask-rcnn_fig3.png\" alt=\"\"></p>\n<p>特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。</p>\n<p><strong>Network Architecture:</strong> Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。</p>\n<p>Backbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。</p>\n<p>我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。</p>\n<p>对于 Network head，如图 4，<br><img src=\"/images/mask-rcnn_fig4.png\" alt=\"\"></p>\n<p>ResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。<br>图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，请阅读原文。</p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p>有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i &lt; R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，</p>\n<ol>\n<li>计算第 i 个 box 的宽高<br>$w=x_2-x_1, \\ h=y_2-y_1$</li>\n<li>将 mask map resize 到 box 宽高的大小  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure></li>\n<li>将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></li>\n<li>将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  </p>\n<p>Mask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。<br><img src=\"/images/mask-rcnn_fig1.png\" alt=\"\"></p>\n<p>Faster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  </p>\n<p>另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。</p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。</p>\n<p><strong>Faster R-CNN:</strong> 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。</p>\n<p><strong>Mask R-CNN:</strong> 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。</p>\n<p>训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，<br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br>上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。<br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br>上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。<br>mask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，<br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br>其中 $f(\\cdot)$ 表示 sigmoid。</p>\n<p><strong>Mask Representation:</strong> 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。</p>\n<p><strong>RoIAlign:</strong> RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。</p>\n<p>可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，<br>$$x_1’=[x_1/16] \\quad y_1’=[y_1/16]<br>\\quad x_2’=[x_2/16]<br>\\quad y_2’=[y_2/16]$$<br>RoI 的大小 和 空间 bin 的大小分别为<br>$$w’=x_2’-x_1’+1<br>\\quad h’=y_2’-y_1’+1<br>\\\\ w^b=w’/7 \\quad h^b=h’/7$$<br>对于第 (i,j) 个 bin，其位置为<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p>其中 $0 \\le i&lt;7, \\ 0\\le j&lt;7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）</p>\n<p>两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，<br><img src=\"/images/mask-rcnn_fig3.png\" alt=\"\"></p>\n<p>特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。</p>\n<p><strong>Network Architecture:</strong> Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。</p>\n<p>Backbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。</p>\n<p>我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。</p>\n<p>对于 Network head，如图 4，<br><img src=\"/images/mask-rcnn_fig4.png\" alt=\"\"></p>\n<p>ResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。<br>图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，请阅读原文。</p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p>有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i &lt; R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，</p>\n<ol>\n<li>计算第 i 个 box 的宽高<br>$w=x_2-x_1, \\ h=y_2-y_1$</li>\n<li>将 mask map resize 到 box 宽高的大小  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure></li>\n<li>将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></li>\n<li>将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n</li>\n</ol>"},{"title":"libra-rcnn","date":"2019-07-03T12:07:44.000Z","mathjax":true,"_content":"论文 [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n<!-- more -->\n# Introduction\n当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：\n1. 所选 region 是否具有代表性\n2. 抽取的特征是否被充分利用\n3. 设计的目标函数是否最优\n\n如图 1，很多训练过程均存在以上三点问题，\n![](/images/libra-rcnn_fig1.png) <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center>\n\n由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：\n\n## 样本不平衡\n为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。\n\n## 特征不平衡\n深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，\n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n从上图可知，融合后的 a' 特征，其来自于 b,c 层的特征信息不是均衡的。\n\n## 优化目标不平衡\n目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。\n\n我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：\n1. IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例\n2. 均衡的 feature pyramid，使用 __相同深度__ 合并到一起的均衡语义特征进行强化\n3. 均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者\n\n# Methodology\nLibra R-CNN 结构如图 2，\n![](/images/libra-rcnn_fig2.png)\n\n其中所有组件的详细介绍如下。\n\n## IoU-balanced Sampling\n首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，\n![](/images/libra-rcnn_fig3.png)\n\n我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。\n\n受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为\n$$p=\\frac N M$$\n\n为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为\n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。\n\nIoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。\n\n### SOURCE CODE\n经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：\n1. 获取所有 proposals 的最大 IoU，记为 `max_overlaps`\n2. 获取所有 proposals 的最大 IoU 的最大值，`max_iou=max_overlaps.max()`\n3. 设置阈值下限 `floor_thr`，对 `(floor_thr, max_iou)` 范围内的 proposals 进行 IoU balanced sampling\n4. 设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 `(max_iou-floor_thr)/K`\n5. 对于第 k 个桶，计算对应的 IoU 范围，记为 `[sk,ek)`\n6. 获取第 k 个桶内的 proposals 的 index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7. 获取第 k 个桶内的负例的 index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set 为 floor_thr<iou<0.5  proposals 的 index\n   ```\n8. 从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例\n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling 正例采样过程：\n1. 获取正例 proposals 所对应的 gt 的 index，记为 `gt_inds`\n2. 将第 1 步的结果去重，`unique_gt_inds=gt_inds.unique()`，得到所有 gt 的 index\n   \n   为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中\n3. 所有 gt 的数量为 `num_gts=len(unique_gt_indx)`，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 `num_per_gt=N/num_gts` 个正例\n4. 对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 `num_per_gt` 个正例\n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码\n\n## Balanced Feature Pyramid\n使用 __相同深度__ 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。\n![](/images/libra-rcnn_fig4.png)\n\n### 获取均衡语义特征\n记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 $\\{C_2,C_3,C_4,C_5\\}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，\n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。\n\n### 精修均衡语义特征\n进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$ 可用于目标检测，检测网络结构与 FPN 的一致。\n\n## Balanced L1 Loss\n遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，\n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。\n\n由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,\n![](/images/libra-rcnn_fig5.png)<center>横坐标 regression error 为 |x|，参见下文中的说明</center>\n\n均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，\n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n相关的梯度满足，\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\n上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，\n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n其中，\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n于是 smooth L1 损失对应的梯度为，\n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|<1  的梯度（因为 |x|<1 表示样本损失较小），首先对于 smooth L1 损失在 |x|<1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 __上凹__ 的，满足这些特性的一组曲线其函数为，\n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，\n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n根据损失在 |x|=1 处连续，得到\n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有\n$$\\alpha \\ln (b+1)=\\gamma$$\n解得，\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n损失函数曲线如图 5(b) 所示。\n\n# Experiments\n实验部分略\n\n# Conclusion\n提出了 Libra R-CNN，包含三点：\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","source":"_posts/libra-rcnn.md","raw":"---\ntitle: libra-rcnn\ndate: 2019-07-03 20:07:44\ntags: object detection\nmathjax: true\n---\n论文 [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n<!-- more -->\n# Introduction\n当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：\n1. 所选 region 是否具有代表性\n2. 抽取的特征是否被充分利用\n3. 设计的目标函数是否最优\n\n如图 1，很多训练过程均存在以上三点问题，\n![](/images/libra-rcnn_fig1.png) <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center>\n\n由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：\n\n## 样本不平衡\n为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。\n\n## 特征不平衡\n深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，\n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n从上图可知，融合后的 a' 特征，其来自于 b,c 层的特征信息不是均衡的。\n\n## 优化目标不平衡\n目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。\n\n我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：\n1. IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例\n2. 均衡的 feature pyramid，使用 __相同深度__ 合并到一起的均衡语义特征进行强化\n3. 均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者\n\n# Methodology\nLibra R-CNN 结构如图 2，\n![](/images/libra-rcnn_fig2.png)\n\n其中所有组件的详细介绍如下。\n\n## IoU-balanced Sampling\n首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，\n![](/images/libra-rcnn_fig3.png)\n\n我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。\n\n受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为\n$$p=\\frac N M$$\n\n为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为\n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。\n\nIoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。\n\n### SOURCE CODE\n经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：\n1. 获取所有 proposals 的最大 IoU，记为 `max_overlaps`\n2. 获取所有 proposals 的最大 IoU 的最大值，`max_iou=max_overlaps.max()`\n3. 设置阈值下限 `floor_thr`，对 `(floor_thr, max_iou)` 范围内的 proposals 进行 IoU balanced sampling\n4. 设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 `(max_iou-floor_thr)/K`\n5. 对于第 k 个桶，计算对应的 IoU 范围，记为 `[sk,ek)`\n6. 获取第 k 个桶内的 proposals 的 index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7. 获取第 k 个桶内的负例的 index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set 为 floor_thr<iou<0.5  proposals 的 index\n   ```\n8. 从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例\n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling 正例采样过程：\n1. 获取正例 proposals 所对应的 gt 的 index，记为 `gt_inds`\n2. 将第 1 步的结果去重，`unique_gt_inds=gt_inds.unique()`，得到所有 gt 的 index\n   \n   为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中\n3. 所有 gt 的数量为 `num_gts=len(unique_gt_indx)`，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 `num_per_gt=N/num_gts` 个正例\n4. 对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 `num_per_gt` 个正例\n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码\n\n## Balanced Feature Pyramid\n使用 __相同深度__ 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。\n![](/images/libra-rcnn_fig4.png)\n\n### 获取均衡语义特征\n记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 $\\{C_2,C_3,C_4,C_5\\}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，\n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。\n\n### 精修均衡语义特征\n进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$ 可用于目标检测，检测网络结构与 FPN 的一致。\n\n## Balanced L1 Loss\n遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，\n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。\n\n由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,\n![](/images/libra-rcnn_fig5.png)<center>横坐标 regression error 为 |x|，参见下文中的说明</center>\n\n均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，\n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n相关的梯度满足，\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\n上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，\n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n其中，\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n于是 smooth L1 损失对应的梯度为，\n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|<1  的梯度（因为 |x|<1 表示样本损失较小），首先对于 smooth L1 损失在 |x|<1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 __上凹__ 的，满足这些特性的一组曲线其函数为，\n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，\n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n根据损失在 |x|=1 处连续，得到\n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有\n$$\\alpha \\ln (b+1)=\\gamma$$\n解得，\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n损失函数曲线如图 5(b) 所示。\n\n# Experiments\n实验部分略\n\n# Conclusion\n提出了 Libra R-CNN，包含三点：\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","slug":"libra-rcnn","published":1,"updated":"2020-04-24T10:36:41.422Z","_id":"ck9dzciqm001cgga6034132ht","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>\n<a id=\"more\"></a>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：</p>\n<ol>\n<li>所选 region 是否具有代表性</li>\n<li>抽取的特征是否被充分利用</li>\n<li>设计的目标函数是否最优</li>\n</ol>\n<p>如图 1，很多训练过程均存在以上三点问题，<br><img src=\"/images/libra-rcnn_fig1.png\" alt=\"\"> <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center></p>\n<p>由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：</p>\n<h2 id=\"样本不平衡\"><a href=\"#样本不平衡\" class=\"headerlink\" title=\"样本不平衡\"></a>样本不平衡</h2><p>为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。</p>\n<h2 id=\"特征不平衡\"><a href=\"#特征不平衡\" class=\"headerlink\" title=\"特征不平衡\"></a>特征不平衡</h2><p>深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，<br><img src=\"/images/libra-rcnn_figa.png\" alt=\"\"><center>FPN</center></p>\n<p>从上图可知，融合后的 a’ 特征，其来自于 b,c 层的特征信息不是均衡的。</p>\n<h2 id=\"优化目标不平衡\"><a href=\"#优化目标不平衡\" class=\"headerlink\" title=\"优化目标不平衡\"></a>优化目标不平衡</h2><p>目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。</p>\n<p>我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：</p>\n<ol>\n<li>IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例</li>\n<li>均衡的 feature pyramid，使用 <strong>相同深度</strong> 合并到一起的均衡语义特征进行强化</li>\n<li>均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者</li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN 结构如图 2，<br><img src=\"/images/libra-rcnn_fig2.png\" alt=\"\"></p>\n<p>其中所有组件的详细介绍如下。</p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p>首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，<br><img src=\"/images/libra-rcnn_fig3.png\" alt=\"\"></p>\n<p>我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。</p>\n<p>受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为<br>$$p=\\frac N M$$</p>\n<p>为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为<br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。</p>\n<p>IoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。</p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p>经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：</p>\n<ol>\n<li><p>获取所有 proposals 的最大 IoU，记为 <code>max_overlaps</code></p>\n</li>\n<li><p>获取所有 proposals 的最大 IoU 的最大值，<code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p>设置阈值下限 <code>floor_thr</code>，对 <code>(floor_thr, max_iou)</code> 范围内的 proposals 进行 IoU balanced sampling</p>\n</li>\n<li><p>设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p>对于第 k 个桶，计算对应的 IoU 范围，记为 <code>[sk,ek)</code></p>\n</li>\n<li><p>获取第 k 个桶内的 proposals 的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></li>\n<li><p>获取第 k 个桶内的负例的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set 为 floor_thr&lt;iou&lt;0.5  proposals 的 index</span></span><br></pre></td></tr></table></figure></li>\n<li><p>从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n<p>IoU balanced sampling 正例采样过程：</p>\n</li>\n<li><p>获取正例 proposals 所对应的 gt 的 index，记为 <code>gt_inds</code></p>\n</li>\n<li><p>将第 1 步的结果去重，<code>unique_gt_inds=gt_inds.unique()</code>，得到所有 gt 的 index</p>\n<p>为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中</p>\n</li>\n<li><p>所有 gt 的数量为 <code>num_gts=len(unique_gt_indx)</code>，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 <code>num_per_gt=N/num_gts</code> 个正例</p>\n</li>\n<li><p>对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 <code>num_per_gt</code> 个正例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n<p>以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码</p>\n</li>\n</ol>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p>使用 <strong>相同深度</strong> 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。<br><img src=\"/images/libra-rcnn_fig4.png\" alt=\"\"></p>\n<h3 id=\"获取均衡语义特征\"><a href=\"#获取均衡语义特征\" class=\"headerlink\" title=\"获取均衡语义特征\"></a>获取均衡语义特征</h3><p>记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 ${C_2,C_3,C_4,C_5}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，<br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p>这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。</p>\n<h3 id=\"精修均衡语义特征\"><a href=\"#精修均衡语义特征\" class=\"headerlink\" title=\"精修均衡语义特征\"></a>精修均衡语义特征</h3><p>进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$ 可用于目标检测，检测网络结构与 FPN 的一致。</p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p>遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，<br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br>上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。</p>\n<p>由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,<br><img src=\"/images/libra-rcnn_fig5.png\" alt=\"\"><center>横坐标 regression error 为 |x|，参见下文中的说明</center></p>\n<p>均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，<br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br>相关的梯度满足，<br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，<br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br>其中，<br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br>于是 smooth L1 损失对应的梯度为，<br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br>我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|&lt;1  的梯度（因为 |x|&lt;1 表示样本损失较小），首先对于 smooth L1 损失在 |x|&lt;1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 <strong>上凹</strong> 的，满足这些特性的一组曲线其函数为，<br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br>其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，<br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br>根据损失在 |x|=1 处连续，得到<br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br>由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有<br>$$\\alpha \\ln (b+1)=\\gamma$$<br>解得，<br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p>损失函数曲线如图 5(b) 所示。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出了 Libra R-CNN，包含三点：</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：</p>\n<ol>\n<li>所选 region 是否具有代表性</li>\n<li>抽取的特征是否被充分利用</li>\n<li>设计的目标函数是否最优</li>\n</ol>\n<p>如图 1，很多训练过程均存在以上三点问题，<br><img src=\"/images/libra-rcnn_fig1.png\" alt=\"\"> <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center></p>\n<p>由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：</p>\n<h2 id=\"样本不平衡\"><a href=\"#样本不平衡\" class=\"headerlink\" title=\"样本不平衡\"></a>样本不平衡</h2><p>为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。</p>\n<h2 id=\"特征不平衡\"><a href=\"#特征不平衡\" class=\"headerlink\" title=\"特征不平衡\"></a>特征不平衡</h2><p>深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，<br><img src=\"/images/libra-rcnn_figa.png\" alt=\"\"><center>FPN</center></p>\n<p>从上图可知，融合后的 a’ 特征，其来自于 b,c 层的特征信息不是均衡的。</p>\n<h2 id=\"优化目标不平衡\"><a href=\"#优化目标不平衡\" class=\"headerlink\" title=\"优化目标不平衡\"></a>优化目标不平衡</h2><p>目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。</p>\n<p>我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：</p>\n<ol>\n<li>IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例</li>\n<li>均衡的 feature pyramid，使用 <strong>相同深度</strong> 合并到一起的均衡语义特征进行强化</li>\n<li>均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者</li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN 结构如图 2，<br><img src=\"/images/libra-rcnn_fig2.png\" alt=\"\"></p>\n<p>其中所有组件的详细介绍如下。</p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p>首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，<br><img src=\"/images/libra-rcnn_fig3.png\" alt=\"\"></p>\n<p>我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。</p>\n<p>受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为<br>$$p=\\frac N M$$</p>\n<p>为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为<br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。</p>\n<p>IoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。</p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p>经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：</p>\n<ol>\n<li><p>获取所有 proposals 的最大 IoU，记为 <code>max_overlaps</code></p>\n</li>\n<li><p>获取所有 proposals 的最大 IoU 的最大值，<code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p>设置阈值下限 <code>floor_thr</code>，对 <code>(floor_thr, max_iou)</code> 范围内的 proposals 进行 IoU balanced sampling</p>\n</li>\n<li><p>设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p>对于第 k 个桶，计算对应的 IoU 范围，记为 <code>[sk,ek)</code></p>\n</li>\n<li><p>获取第 k 个桶内的 proposals 的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></li>\n<li><p>获取第 k 个桶内的负例的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set 为 floor_thr&lt;iou&lt;0.5  proposals 的 index</span></span><br></pre></td></tr></table></figure></li>\n<li><p>从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n<p>IoU balanced sampling 正例采样过程：</p>\n</li>\n<li><p>获取正例 proposals 所对应的 gt 的 index，记为 <code>gt_inds</code></p>\n</li>\n<li><p>将第 1 步的结果去重，<code>unique_gt_inds=gt_inds.unique()</code>，得到所有 gt 的 index</p>\n<p>为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中</p>\n</li>\n<li><p>所有 gt 的数量为 <code>num_gts=len(unique_gt_indx)</code>，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 <code>num_per_gt=N/num_gts</code> 个正例</p>\n</li>\n<li><p>对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 <code>num_per_gt</code> 个正例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n<p>以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码</p>\n</li>\n</ol>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p>使用 <strong>相同深度</strong> 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。<br><img src=\"/images/libra-rcnn_fig4.png\" alt=\"\"></p>\n<h3 id=\"获取均衡语义特征\"><a href=\"#获取均衡语义特征\" class=\"headerlink\" title=\"获取均衡语义特征\"></a>获取均衡语义特征</h3><p>记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 ${C_2,C_3,C_4,C_5}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，<br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p>这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。</p>\n<h3 id=\"精修均衡语义特征\"><a href=\"#精修均衡语义特征\" class=\"headerlink\" title=\"精修均衡语义特征\"></a>精修均衡语义特征</h3><p>进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$ 可用于目标检测，检测网络结构与 FPN 的一致。</p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p>遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，<br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br>上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。</p>\n<p>由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,<br><img src=\"/images/libra-rcnn_fig5.png\" alt=\"\"><center>横坐标 regression error 为 |x|，参见下文中的说明</center></p>\n<p>均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，<br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br>相关的梯度满足，<br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，<br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br>其中，<br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br>于是 smooth L1 损失对应的梯度为，<br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br>我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|&lt;1  的梯度（因为 |x|&lt;1 表示样本损失较小），首先对于 smooth L1 损失在 |x|&lt;1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 <strong>上凹</strong> 的，满足这些特性的一组曲线其函数为，<br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br>其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，<br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br>根据损失在 |x|=1 处连续，得到<br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br>由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有<br>$$\\alpha \\ln (b+1)=\\gamma$$<br>解得，<br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p>损失函数曲线如图 5(b) 所示。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出了 Libra R-CNN，包含三点：</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>"},{"title":"mAP","date":"2019-06-16T03:43:57.000Z","mathjax":true,"_content":"# mAP\n目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。\n-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。\n<!-- more -->\n## 相关概念\n0. Positive 表示检测结果\n1. True Positive (TP): IoU 大于等于阈值的检测 box\n2. False Positive (FP): IoU 小于阈值的检测 box\n3. Precision = TP/(TP+FP) = TP/(所有检测)\n4. Recall = TP/(TP+FN) = TP/(所有gt)\n\n由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在[项目](https://github.io/shajian/shajian.github.io)提 issue）：\n1. TP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP\n2. FP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n   如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN\n4. TN\n   \n   目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。\n\nVOC 使用阈值 `0.5`。\n## 指标\n### PR 曲线\n每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R' >= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自[stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)）。\n\n给定目标分类 \"Aeroplane\"，假设检测结果如下,\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\n（BB 表示检测结果所匹配 \"match\" 的 GT box）\n\n以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，\n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n稍作解释：\n\n1. rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14\n2. rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29\n3. rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29\n4. ...\n\n### AP\nVOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,...,1}，然后对 R' >= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R' >= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 __PR 曲线__ 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：\n\n#### 11-点插值\n取11个 R 值的 [0,1] 区间等分点计算平均正确率：\n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n其中，$\\rho(\\tilde r)$ 为计算得到的正确率。\n举个例子如图（完整例子请参考[这里](https://github.com/rafaelpadilla/Object-Detection-Metrics)），\n![](/images/mAP_fig1.png)\n\n蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### 所有点插值\nAP 计算式为，\n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。\n如下图，\n![](/images/mAP_fig2.png)\n\n蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，\n![](/images/mAP_fig3.png)\n\n于是计算 AP 为，\n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC 曲线\n## 相关概念\n1. TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)\n2. TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC 曲线\n\nROC 是常见的评价分类器的指标。\n\nROC 全称 [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)（以下很多内容均来自于这个维基百科词条）。\n\n根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。\n如下图所示，\n![](/images/mAP_fig4.png)\n\n图中 (0,0) 和 (1,1) 两点分别对应：\n1. 当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0\n2. 当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1\n\n实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。\n\n一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。\n\n## ROC 空间\n二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X>T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n两者均为阈值 T 的函数。\n\n1. TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率\n2. FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。\n\n下图表示某分类器的分类情况，\n![图 5](/images/mAP_fig5.png)\n\n横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。\n## AUC\n通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。\n\nAUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有\n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n于是，\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。\n\n最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n概率密度分别为 $f_1,f_0$。\n\n由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 < X_1$ 的概率为：\n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n与上面的计算式形式完全一样，证毕。","source":"_posts/mAP.md","raw":"---\ntitle: mAP\ndate: 2019-06-16 11:43:57\ntags: object detection\nmathjax: true\n---\n# mAP\n目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。\n-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。\n<!-- more -->\n## 相关概念\n0. Positive 表示检测结果\n1. True Positive (TP): IoU 大于等于阈值的检测 box\n2. False Positive (FP): IoU 小于阈值的检测 box\n3. Precision = TP/(TP+FP) = TP/(所有检测)\n4. Recall = TP/(TP+FN) = TP/(所有gt)\n\n由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在[项目](https://github.io/shajian/shajian.github.io)提 issue）：\n1. TP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP\n2. FP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n   如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN\n4. TN\n   \n   目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。\n\nVOC 使用阈值 `0.5`。\n## 指标\n### PR 曲线\n每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R' >= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自[stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)）。\n\n给定目标分类 \"Aeroplane\"，假设检测结果如下,\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\n（BB 表示检测结果所匹配 \"match\" 的 GT box）\n\n以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，\n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n稍作解释：\n\n1. rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14\n2. rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29\n3. rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29\n4. ...\n\n### AP\nVOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,...,1}，然后对 R' >= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R' >= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 __PR 曲线__ 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：\n\n#### 11-点插值\n取11个 R 值的 [0,1] 区间等分点计算平均正确率：\n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n其中，$\\rho(\\tilde r)$ 为计算得到的正确率。\n举个例子如图（完整例子请参考[这里](https://github.com/rafaelpadilla/Object-Detection-Metrics)），\n![](/images/mAP_fig1.png)\n\n蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### 所有点插值\nAP 计算式为，\n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。\n如下图，\n![](/images/mAP_fig2.png)\n\n蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，\n![](/images/mAP_fig3.png)\n\n于是计算 AP 为，\n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC 曲线\n## 相关概念\n1. TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)\n2. TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC 曲线\n\nROC 是常见的评价分类器的指标。\n\nROC 全称 [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)（以下很多内容均来自于这个维基百科词条）。\n\n根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。\n如下图所示，\n![](/images/mAP_fig4.png)\n\n图中 (0,0) 和 (1,1) 两点分别对应：\n1. 当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0\n2. 当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1\n\n实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。\n\n一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。\n\n## ROC 空间\n二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X>T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n两者均为阈值 T 的函数。\n\n1. TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率\n2. FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。\n\n下图表示某分类器的分类情况，\n![图 5](/images/mAP_fig5.png)\n\n横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。\n## AUC\n通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。\n\nAUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有\n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n于是，\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。\n\n最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n概率密度分别为 $f_1,f_0$。\n\n由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 < X_1$ 的概率为：\n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n与上面的计算式形式完全一样，证毕。","slug":"mAP","published":1,"updated":"2020-04-24T10:36:27.200Z","_id":"ck9dzciqx001egga6220gd7ev","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p>目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。<br>-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。</p>\n<a id=\"more\"></a>\n<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol start=\"0\">\n<li>Positive 表示检测结果</li>\n<li>True Positive (TP): IoU 大于等于阈值的检测 box</li>\n<li>False Positive (FP): IoU 小于阈值的检测 box</li>\n<li>Precision = TP/(TP+FP) = TP/(所有检测)</li>\n<li>Recall = TP/(TP+FN) = TP/(所有gt)</li>\n</ol>\n<p>由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\">项目</a>提 issue）：</p>\n<ol>\n<li><p>TP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP</p>\n</li>\n<li><p>FP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p>如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN</p>\n</li>\n<li><p>TN</p>\n<p>目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。</p>\n</li>\n</ol>\n<p>VOC 使用阈值 <code>0.5</code>。</p>\n<h2 id=\"指标\"><a href=\"#指标\" class=\"headerlink\" title=\"指标\"></a>指标</h2><h3 id=\"PR-曲线\"><a href=\"#PR-曲线\" class=\"headerlink\" title=\"PR 曲线\"></a>PR 曲线</h3><p>每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R’ &gt;= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自<a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a>）。</p>\n<p>给定目标分类 “Aeroplane”，假设检测结果如下,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n<p>（BB 表示检测结果所匹配 “match” 的 GT box）</p>\n<p>以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank&#x3D;1  precision&#x3D;1.00 and recall&#x3D;0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;2  precision&#x3D;1.00 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;3  precision&#x3D;0.66 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;4  precision&#x3D;0.50 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;5  precision&#x3D;0.40 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;6  precision&#x3D;0.50 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;7  precision&#x3D;0.43 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;8  precision&#x3D;0.38 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;9  precision&#x3D;0.44 and recall&#x3D;0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;10 precision&#x3D;0.50 and recall&#x3D;0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n<p>稍作解释：</p>\n<ol>\n<li>rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14</li>\n<li>rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29</li>\n<li>rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29</li>\n<li>…</li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,…,1}，然后对 R’ &gt;= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R’ &gt;= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 <strong>PR 曲线</strong> 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：</p>\n<h4 id=\"11-点插值\"><a href=\"#11-点插值\" class=\"headerlink\" title=\"11-点插值\"></a>11-点插值</h4><p>取11个 R 值的 [0,1] 区间等分点计算平均正确率：<br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>其中，$\\rho(\\tilde r)$ 为计算得到的正确率。<br>举个例子如图（完整例子请参考<a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\">这里</a>），<br><img src=\"/images/mAP_fig1.png\" alt=\"\"></p>\n<p>蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"所有点插值\"><a href=\"#所有点插值\" class=\"headerlink\" title=\"所有点插值\"></a>所有点插值</h4><p>AP 计算式为，<br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。<br>如下图，<br><img src=\"/images/mAP_fig2.png\" alt=\"\"></p>\n<p>蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，<br><img src=\"/images/mAP_fig3.png\" alt=\"\"></p>\n<p>于是计算 AP 为，</p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-曲线\"><a href=\"#ROC-曲线\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h1><h2 id=\"相关概念-1\"><a href=\"#相关概念-1\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol>\n<li><p>TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC-曲线-1\"><a href=\"#ROC-曲线-1\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h2><p>ROC 是常见的评价分类器的指标。</p>\n<p>ROC 全称 <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a>（以下很多内容均来自于这个维基百科词条）。</p>\n<p>根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。<br>如下图所示，<br><img src=\"/images/mAP_fig4.png\" alt=\"\"></p>\n<p>图中 (0,0) 和 (1,1) 两点分别对应：</p>\n<ol>\n<li>当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0</li>\n<li>当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1</li>\n</ol>\n<p>实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。</p>\n<p>一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。</p>\n<h2 id=\"ROC-空间\"><a href=\"#ROC-空间\" class=\"headerlink\" title=\"ROC 空间\"></a>ROC 空间</h2><p>二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X&gt;T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br>两者均为阈值 T 的函数。</p>\n<ol>\n<li>TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率</li>\n<li>FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。</li>\n</ol>\n<p>下图表示某分类器的分类情况，<br><img src=\"/images/mAP_fig5.png\" alt=\"图 5\"></p>\n<p>横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。</p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p>通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。</p>\n<p>AUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有<br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br>于是，<br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ ‘(T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T’) \\ dT’ \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T’)f_0(T) \\ dT’ dT \\\\ = P(X_1&gt;X_0)<br>$$<br>其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。</p>\n<p>最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br>概率密度分别为 $f_1,f_0$。</p>\n<p>由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 &lt; X_1$ 的概率为：<br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br>与上面的计算式形式完全一样，证毕。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p>目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。<br>-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。</p>","more":"<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol start=\"0\">\n<li>Positive 表示检测结果</li>\n<li>True Positive (TP): IoU 大于等于阈值的检测 box</li>\n<li>False Positive (FP): IoU 小于阈值的检测 box</li>\n<li>Precision = TP/(TP+FP) = TP/(所有检测)</li>\n<li>Recall = TP/(TP+FN) = TP/(所有gt)</li>\n</ol>\n<p>由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\">项目</a>提 issue）：</p>\n<ol>\n<li><p>TP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP</p>\n</li>\n<li><p>FP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p>如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN</p>\n</li>\n<li><p>TN</p>\n<p>目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。</p>\n</li>\n</ol>\n<p>VOC 使用阈值 <code>0.5</code>。</p>\n<h2 id=\"指标\"><a href=\"#指标\" class=\"headerlink\" title=\"指标\"></a>指标</h2><h3 id=\"PR-曲线\"><a href=\"#PR-曲线\" class=\"headerlink\" title=\"PR 曲线\"></a>PR 曲线</h3><p>每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R’ &gt;= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自<a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a>）。</p>\n<p>给定目标分类 “Aeroplane”，假设检测结果如下,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n<p>（BB 表示检测结果所匹配 “match” 的 GT box）</p>\n<p>以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank&#x3D;1  precision&#x3D;1.00 and recall&#x3D;0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;2  precision&#x3D;1.00 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;3  precision&#x3D;0.66 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;4  precision&#x3D;0.50 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;5  precision&#x3D;0.40 and recall&#x3D;0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;6  precision&#x3D;0.50 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;7  precision&#x3D;0.43 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;8  precision&#x3D;0.38 and recall&#x3D;0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;9  precision&#x3D;0.44 and recall&#x3D;0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank&#x3D;10 precision&#x3D;0.50 and recall&#x3D;0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n<p>稍作解释：</p>\n<ol>\n<li>rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14</li>\n<li>rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29</li>\n<li>rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29</li>\n<li>…</li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,…,1}，然后对 R’ &gt;= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R’ &gt;= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 <strong>PR 曲线</strong> 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：</p>\n<h4 id=\"11-点插值\"><a href=\"#11-点插值\" class=\"headerlink\" title=\"11-点插值\"></a>11-点插值</h4><p>取11个 R 值的 [0,1] 区间等分点计算平均正确率：<br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>其中，$\\rho(\\tilde r)$ 为计算得到的正确率。<br>举个例子如图（完整例子请参考<a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\">这里</a>），<br><img src=\"/images/mAP_fig1.png\" alt=\"\"></p>\n<p>蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"所有点插值\"><a href=\"#所有点插值\" class=\"headerlink\" title=\"所有点插值\"></a>所有点插值</h4><p>AP 计算式为，<br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。<br>如下图，<br><img src=\"/images/mAP_fig2.png\" alt=\"\"></p>\n<p>蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，<br><img src=\"/images/mAP_fig3.png\" alt=\"\"></p>\n<p>于是计算 AP 为，</p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-曲线\"><a href=\"#ROC-曲线\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h1><h2 id=\"相关概念-1\"><a href=\"#相关概念-1\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol>\n<li><p>TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC-曲线-1\"><a href=\"#ROC-曲线-1\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h2><p>ROC 是常见的评价分类器的指标。</p>\n<p>ROC 全称 <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a>（以下很多内容均来自于这个维基百科词条）。</p>\n<p>根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。<br>如下图所示，<br><img src=\"/images/mAP_fig4.png\" alt=\"\"></p>\n<p>图中 (0,0) 和 (1,1) 两点分别对应：</p>\n<ol>\n<li>当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0</li>\n<li>当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1</li>\n</ol>\n<p>实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。</p>\n<p>一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。</p>\n<h2 id=\"ROC-空间\"><a href=\"#ROC-空间\" class=\"headerlink\" title=\"ROC 空间\"></a>ROC 空间</h2><p>二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X&gt;T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br>两者均为阈值 T 的函数。</p>\n<ol>\n<li>TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率</li>\n<li>FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。</li>\n</ol>\n<p>下图表示某分类器的分类情况，<br><img src=\"/images/mAP_fig5.png\" alt=\"图 5\"></p>\n<p>横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。</p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p>通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。</p>\n<p>AUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有<br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br>于是，<br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ ‘(T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T’) \\ dT’ \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T’)f_0(T) \\ dT’ dT \\\\ = P(X_1&gt;X_0)<br>$$<br>其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。</p>\n<p>最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br>概率密度分别为 $f_1,f_0$。</p>\n<p>由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 &lt; X_1$ 的概率为：<br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br>与上面的计算式形式完全一样，证毕。</p>"},{"title":"GIoU","date":"2019-06-13T07:00:48.000Z","mathjax":true,"_content":"论文 [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n<!-- more -->\n# 摘要\nIoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。\n# 简介\n在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。\n\nIoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。\n\n然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\n预测box（黑矩形）和gt box（绿矩形）均由左下角和右上角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner 左下角的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。\n\n直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。\n\n为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。\n\n本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。\n\n我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。\n\n主要贡献如下：\n\n- 介绍了GIoU，作为比较两个任意形状差距的指标\n- 以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解\n- 将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升\n\n# 相关工作\n__目标检测准确性测量：__ IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。\n\n__bbox表示和损失：__ 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：\n1. YOLO v1\n   \n   YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。\n2. R-CNN\n   \n   R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。\n3. Fast R-CNN\n   \n   Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。\n4. Faster R-CNN\n   \n   Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。\n\n大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 \n\n__使用近似或替代函数优化IoU：__ 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。\n\n# 泛化IoU\n用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：\n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：\n- IoU作为距离同时也作为评估指标。\n  \n  IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可区分的同一性，对称性和三角不等式\n- IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关\n   \n但是，IoU的主要问题是：\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远\n\n为了解决这个问题，我们提出了泛化版IoU，即 GIoU。\n\n两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）\n\n整个计算过程总结如下算法1：\n___\n算法1：GIoU\n___\n输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$\n\n输出： GIoU\n- 在 S 空间中寻找包含 A B 的最小凸形 C\n- 计算 IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n- 计算 GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\n作为新的指标，GIoU 具有性质：\n\n- 与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可区分的同一性，对称性和三角不等式\n  \n  IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。\n- 与 IoU 类似，GIoU 具有尺度不变性。\n- GIoU 上限为 IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。\n- IoU 和 GIoU 的值域\n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。\n\n  我们看下如何获得边界值：\n   * 与 IoU 相同，只有当 A B 完全重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$\n   * 当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$\n\n综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。\n\n## GIoU用作BBox回归损失\n我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。\n\n好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} < x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}<x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}<x_A^{br}$。\n\n反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。\n_________\n算法2：IoU和GIoU用作BBox回归损失\n_________\n输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1. 因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，\n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$，故进行如下转换：\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2. 计算 GT box 面积：\n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3. 计算预测 box 面积：\n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. 计算交：\n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5. 计算最小包含凸形 c：\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6. 计算 c 的面积：\n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7. 计算 IoU：\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$\n8. 计算 GIoU：\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9. 计算 GIoU 损失：\n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\n根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，\n\n![](/images/GIoU_fig2.png)\n\n图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。\n\n### 损失稳定性\n我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。\n\n假设 GT box 是矩形，且面积大于0即，$A^g > 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U > 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$\n\n检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U > 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} <1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 < GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。\n\n### IoU=0时$\\mathcal L_{GIoU}$的行为\nGIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。\n\n# 实验结果\n引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）\n\n__数据集__ 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。\n\n__评估方案__ 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU=\\{.5,.55,...,.95\\}$，计算这些 IoU 阈值下 mAP 值的平均，记为 __AP__，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU=\\{.5,.55,...,.95\\}$，计算这些阈值下 mAP 的平均，__AP__，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 __AP75__。\n\n## YOLO v3\n__训练方案__ 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。\n\n## Faster R-CNN 和 Mask R-CNN\n__训练方案__ 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。\n\n# 结论\n介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。\n\n我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。\n\n# 后记\n这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。","source":"_posts/GIoU.md","raw":"---\ntitle: GIoU\ndate: 2019-06-13 15:00:48\ntags: object detection\nmathjax: true\n---\n论文 [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n<!-- more -->\n# 摘要\nIoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。\n# 简介\n在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。\n\nIoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。\n\n然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\n预测box（黑矩形）和gt box（绿矩形）均由左下角和右上角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner 左下角的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。\n\n直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。\n\n为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。\n\n本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。\n\n我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。\n\n主要贡献如下：\n\n- 介绍了GIoU，作为比较两个任意形状差距的指标\n- 以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解\n- 将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升\n\n# 相关工作\n__目标检测准确性测量：__ IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。\n\n__bbox表示和损失：__ 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：\n1. YOLO v1\n   \n   YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。\n2. R-CNN\n   \n   R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。\n3. Fast R-CNN\n   \n   Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。\n4. Faster R-CNN\n   \n   Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。\n\n大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 \n\n__使用近似或替代函数优化IoU：__ 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。\n\n# 泛化IoU\n用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：\n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：\n- IoU作为距离同时也作为评估指标。\n  \n  IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可区分的同一性，对称性和三角不等式\n- IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关\n   \n但是，IoU的主要问题是：\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远\n\n为了解决这个问题，我们提出了泛化版IoU，即 GIoU。\n\n两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）\n\n整个计算过程总结如下算法1：\n___\n算法1：GIoU\n___\n输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$\n\n输出： GIoU\n- 在 S 空间中寻找包含 A B 的最小凸形 C\n- 计算 IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n- 计算 GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\n作为新的指标，GIoU 具有性质：\n\n- 与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可区分的同一性，对称性和三角不等式\n  \n  IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。\n- 与 IoU 类似，GIoU 具有尺度不变性。\n- GIoU 上限为 IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。\n- IoU 和 GIoU 的值域\n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。\n\n  我们看下如何获得边界值：\n   * 与 IoU 相同，只有当 A B 完全重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$\n   * 当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$\n\n综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。\n\n## GIoU用作BBox回归损失\n我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。\n\n好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} < x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}<x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}<x_A^{br}$。\n\n反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。\n_________\n算法2：IoU和GIoU用作BBox回归损失\n_________\n输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1. 因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，\n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$，故进行如下转换：\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2. 计算 GT box 面积：\n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3. 计算预测 box 面积：\n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. 计算交：\n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5. 计算最小包含凸形 c：\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6. 计算 c 的面积：\n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7. 计算 IoU：\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$\n8. 计算 GIoU：\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9. 计算 GIoU 损失：\n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\n根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，\n\n![](/images/GIoU_fig2.png)\n\n图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。\n\n### 损失稳定性\n我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。\n\n假设 GT box 是矩形，且面积大于0即，$A^g > 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U > 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$\n\n检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U > 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} <1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 < GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。\n\n### IoU=0时$\\mathcal L_{GIoU}$的行为\nGIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。\n\n# 实验结果\n引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）\n\n__数据集__ 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。\n\n__评估方案__ 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU=\\{.5,.55,...,.95\\}$，计算这些 IoU 阈值下 mAP 值的平均，记为 __AP__，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU=\\{.5,.55,...,.95\\}$，计算这些阈值下 mAP 的平均，__AP__，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 __AP75__。\n\n## YOLO v3\n__训练方案__ 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。\n\n## Faster R-CNN 和 Mask R-CNN\n__训练方案__ 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。\n\n# 结论\n介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。\n\n我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。\n\n# 后记\n这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。","slug":"GIoU","published":1,"updated":"2020-05-15T01:24:49.573Z","_id":"ck9dzciuq001kgga6eg7ravz4","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>\n<a id=\"more\"></a>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>IoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。</p>\n<p>IoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。</p>\n<p>然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，<br><img src=\"/images/GIoU_fig1.png\" alt=\"\"></p>\n<!--  -->\n\n<p>预测box（黑矩形）和gt box（绿矩形）均由左下角和右上角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner 左下角的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。</p>\n<p>直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。</p>\n<p>为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。</p>\n<p>本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。</p>\n<p>我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。</p>\n<p>主要贡献如下：</p>\n<ul>\n<li>介绍了GIoU，作为比较两个任意形状差距的指标</li>\n<li>以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解</li>\n<li>将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p><strong>目标检测准确性测量：</strong> IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。</p>\n<p><strong>bbox表示和损失：</strong> 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。</p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。</p>\n</li>\n</ol>\n<p>大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 </p>\n<p><strong>使用近似或替代函数优化IoU：</strong> 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。</p>\n<h1 id=\"泛化IoU\"><a href=\"#泛化IoU\" class=\"headerlink\" title=\"泛化IoU\"></a>泛化IoU</h1><p>用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：<br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：</p>\n<ul>\n<li><p>IoU作为距离同时也作为评估指标。</p>\n<p>IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可区分的同一性，对称性和三角不等式</p>\n</li>\n<li><p>IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关</p>\n</li>\n</ul>\n<p>但是，IoU的主要问题是：</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远</li>\n</ul>\n<p>为了解决这个问题，我们提出了泛化版IoU，即 GIoU。</p>\n<p>两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）</p>\n<p>整个计算过程总结如下算法1：</p>\n<hr>\n<p>算法1：GIoU</p>\n<hr>\n<p>输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p>输出： GIoU</p>\n<ul>\n<li><p>在 S 空间中寻找包含 A B 的最小凸形 C</p>\n</li>\n<li><p>计算 IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p>计算 GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>作为新的指标，GIoU 具有性质：</p>\n<ul>\n<li><p>与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可区分的同一性，对称性和三角不等式</p>\n<p>IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。</p>\n</li>\n<li><p>与 IoU 类似，GIoU 具有尺度不变性。</p>\n</li>\n<li><p>GIoU 上限为 IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。</p>\n</li>\n<li><p>IoU 和 GIoU 的值域</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。</p>\n<p>我们看下如何获得边界值：</p>\n<ul>\n<li>与 IoU 相同，只有当 A B 完全重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$</li>\n<li>当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。</p>\n<h2 id=\"GIoU用作BBox回归损失\"><a href=\"#GIoU用作BBox回归损失\" class=\"headerlink\" title=\"GIoU用作BBox回归损失\"></a>GIoU用作BBox回归损失</h2><p>我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。</p>\n<p>好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} &lt; x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$。</p>\n<p>反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。</p>\n<hr>\n<p>算法2：IoU和GIoU用作BBox回归损失</p>\n<hr>\n<p>输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，</p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$，故进行如下转换：</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p>计算 GT box 面积：</p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p>计算预测 box 面积：</p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p>计算交：</p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p>计算最小包含凸形 c：</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p>计算 c 的面积：</p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p>计算 IoU：</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p>计算 GIoU：</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p>计算 GIoU 损失：</p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，</p>\n<p><img src=\"/images/GIoU_fig2.png\" alt=\"\"></p>\n<p>图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。</p>\n<h3 id=\"损失稳定性\"><a href=\"#损失稳定性\" class=\"headerlink\" title=\"损失稳定性\"></a>损失稳定性</h3><p>我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。</p>\n<p>假设 GT box 是矩形，且面积大于0即，$A^g &gt; 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U &gt; 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p>检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U &gt; 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} &lt;1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 &lt; GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。</p>\n<h3 id=\"IoU-0时-mathcal-L-GIoU-的行为\"><a href=\"#IoU-0时-mathcal-L-GIoU-的行为\" class=\"headerlink\" title=\"IoU=0时$\\mathcal L_{GIoU}$的行为\"></a>IoU=0时$\\mathcal L_{GIoU}$的行为</h3><p>GIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）</p>\n<p><strong>数据集</strong> 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。</p>\n<p><strong>评估方案</strong> 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU={.5,.55,…,.95}$，计算这些 IoU 阈值下 mAP 值的平均，记为 <strong>AP</strong>，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU={.5,.55,…,.95}$，计算这些阈值下 mAP 的平均，<strong>AP</strong>，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 <strong>AP75</strong>。</p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong>训练方案</strong> 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。</p>\n<h2 id=\"Faster-R-CNN-和-Mask-R-CNN\"><a href=\"#Faster-R-CNN-和-Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN 和 Mask R-CNN\"></a>Faster R-CNN 和 Mask R-CNN</h2><p><strong>训练方案</strong> 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。</p>\n<p>我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。</p>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>","more":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>IoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。</p>\n<p>IoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。</p>\n<p>然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，<br><img src=\"/images/GIoU_fig1.png\" alt=\"\"></p>\n<!--  -->\n\n<p>预测box（黑矩形）和gt box（绿矩形）均由左下角和右上角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner 左下角的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。</p>\n<p>直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。</p>\n<p>为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。</p>\n<p>本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。</p>\n<p>我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。</p>\n<p>主要贡献如下：</p>\n<ul>\n<li>介绍了GIoU，作为比较两个任意形状差距的指标</li>\n<li>以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解</li>\n<li>将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p><strong>目标检测准确性测量：</strong> IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。</p>\n<p><strong>bbox表示和损失：</strong> 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。</p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。</p>\n</li>\n</ol>\n<p>大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 </p>\n<p><strong>使用近似或替代函数优化IoU：</strong> 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。</p>\n<h1 id=\"泛化IoU\"><a href=\"#泛化IoU\" class=\"headerlink\" title=\"泛化IoU\"></a>泛化IoU</h1><p>用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：<br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：</p>\n<ul>\n<li><p>IoU作为距离同时也作为评估指标。</p>\n<p>IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可区分的同一性，对称性和三角不等式</p>\n</li>\n<li><p>IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关</p>\n</li>\n</ul>\n<p>但是，IoU的主要问题是：</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远</li>\n</ul>\n<p>为了解决这个问题，我们提出了泛化版IoU，即 GIoU。</p>\n<p>两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）</p>\n<p>整个计算过程总结如下算法1：</p>\n<hr>\n<p>算法1：GIoU</p>\n<hr>\n<p>输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p>输出： GIoU</p>\n<ul>\n<li><p>在 S 空间中寻找包含 A B 的最小凸形 C</p>\n</li>\n<li><p>计算 IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p>计算 GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>作为新的指标，GIoU 具有性质：</p>\n<ul>\n<li><p>与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可区分的同一性，对称性和三角不等式</p>\n<p>IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。</p>\n</li>\n<li><p>与 IoU 类似，GIoU 具有尺度不变性。</p>\n</li>\n<li><p>GIoU 上限为 IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。</p>\n</li>\n<li><p>IoU 和 GIoU 的值域</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。</p>\n<p>我们看下如何获得边界值：</p>\n<ul>\n<li>与 IoU 相同，只有当 A B 完全重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$</li>\n<li>当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。</p>\n<h2 id=\"GIoU用作BBox回归损失\"><a href=\"#GIoU用作BBox回归损失\" class=\"headerlink\" title=\"GIoU用作BBox回归损失\"></a>GIoU用作BBox回归损失</h2><p>我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。</p>\n<p>好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} &lt; x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$。</p>\n<p>反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。</p>\n<hr>\n<p>算法2：IoU和GIoU用作BBox回归损失</p>\n<hr>\n<p>输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，</p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$，故进行如下转换：</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p>计算 GT box 面积：</p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p>计算预测 box 面积：</p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p>计算交：</p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p>计算最小包含凸形 c：</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p>计算 c 的面积：</p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p>计算 IoU：</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p>计算 GIoU：</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p>计算 GIoU 损失：</p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，</p>\n<p><img src=\"/images/GIoU_fig2.png\" alt=\"\"></p>\n<p>图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。</p>\n<h3 id=\"损失稳定性\"><a href=\"#损失稳定性\" class=\"headerlink\" title=\"损失稳定性\"></a>损失稳定性</h3><p>我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。</p>\n<p>假设 GT box 是矩形，且面积大于0即，$A^g &gt; 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U &gt; 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p>检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U &gt; 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} &lt;1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 &lt; GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。</p>\n<h3 id=\"IoU-0时-mathcal-L-GIoU-的行为\"><a href=\"#IoU-0时-mathcal-L-GIoU-的行为\" class=\"headerlink\" title=\"IoU=0时$\\mathcal L_{GIoU}$的行为\"></a>IoU=0时$\\mathcal L_{GIoU}$的行为</h3><p>GIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）</p>\n<p><strong>数据集</strong> 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。</p>\n<p><strong>评估方案</strong> 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU={.5,.55,…,.95}$，计算这些 IoU 阈值下 mAP 值的平均，记为 <strong>AP</strong>，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU={.5,.55,…,.95}$，计算这些阈值下 mAP 的平均，<strong>AP</strong>，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 <strong>AP75</strong>。</p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong>训练方案</strong> 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。</p>\n<h2 id=\"Faster-R-CNN-和-Mask-R-CNN\"><a href=\"#Faster-R-CNN-和-Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN 和 Mask R-CNN\"></a>Faster R-CNN 和 Mask R-CNN</h2><p><strong>训练方案</strong> 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。</p>\n<p>我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。</p>"},{"title":"GAN","date":"2019-07-23T02:15:08.000Z","mathjax":true,"_content":"论文 [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n<!-- more -->\n# GAN\n## 原理\n生成对抗网络 GAN：一个生成模型 G 和一个判别模型 D，G 尽可能模拟真实的数据分布，D 尽可能的区分样本是模型生成的还是真实的。下文以图像数据为例说明。\n\n定义一个输入噪声随机变量 z，其分布为 $p_z(z)$，G 根据 z 生成图像 $G(z;\\theta_g)$，我们假设 G 是一个多层感知机 MLP 网络，网络参数为 $\\theta_g$。D 也是一个 MLP $D(x;\\theta_d)$ 输出是一个标量，表示 x 是真实图像的概率。训练 D 使其对输入 x 预测正确的概率最大化，即当 x 来自真实的训练数据时，$G(x)$ 尽可能大，当 x 来自 G 生成样本时，预测概率 $D(G(z))$ 尽可能小；而训练 G 目的是为了让 $D(G(x))$ 尽可能大，或者说让 $\\log(1-D(G(z)))$ 尽可能小，于是目标函数为，\n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$\n\n（对 D 而言，这是一个 log 似然函数，D 希望它越大越大，所以求最大值；而 G 却希望 D 的 log 似然函数越小越好，所以求最小值）\n\n这是一个二人零和博弈。图 1 是训练过程示意图，训练使用迭代的，数值计算的方法。\n![](/images/GAN_fig1.png)\n\n图 1 中 D 模型分布为蓝色虚线，数据 x 的分布 $p_x$ 为黑色点线，G 模型分布 $p_g$ 为绿色实线（黑绿曲线上某一点分别表示此 x 值处真实数据的概率密度和生成数据的概率密度）。下面的水平线为随机噪声变量 z 的定义域，在其上对 z 均匀采样，上面水平线是 x 的定义域，向上箭头表示映射过程 x=G(z) （G 生成过程）。  \n(a) 是收敛附近的对抗情况：此时 $p_g,\\ p_{data}$ 两者相似，D 分类不完全准确。  \n(b) 在内层循环中，训练 D 判别样本，训练过程收敛于 $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$。  \n(c) D 的梯度可指引 G(z) 移动到更容易被分类为真实数据的区域，即，G 更新后，更加逼近真实数据分布。  \n(d) 经过几次训练，G 和 D 到达一个平衡点，此时 $p_g=p_{data}$，D 无法再区分这两个分布，即，$D(x)=1/2$。\n\n训练算法如下，\n![](/images/GAN_alg1.png)\n\nk 次 D 的优化与一次 G 的优化交替进行，这可以使得 G 变化缓慢，而 D 维持在最优解附近。\n\n实际应用中，(1) 式可能无法提供足够的梯度来更新 G。训练初期，G 性能较差，生成样本与真实训练样本区别较大，所以 D 可以较高的置信度判别，此时，$\\log (1-D(G(z)))$ 达到饱和（log 曲线右端较为平坦），于是我们改为训练 G 以最大化 $\\log D(G(z))$，最终训练能到达相同的 G 和 D 的平衡点，但是训练初期的梯度较大（log 曲线的左端较为陡峭）。\n\n## 理论分析\n已知噪声随机变量 z 的分布 $p_z$ 时，可以获得 G 的模型分布，根据算法 1，如果 G 模型的假设空间和训练时间足够，G 可以拟合真实数据分布 $p_{data}$。现在我们来证明 $p_g=p_{data}$ 是 (1) 式的全局最优解。\n### 全局最优解\n__Proposition 1.__ 对于任意的 G，D 的最优解为\n$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$\n__证明：__  给定任意 G，D 的训练准则是最大化 V(G,D)  \n$$\\begin{aligned} V(G,D)&=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz\n\\\\\\\\ &=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$\n$\\forall (a,b) \\in \\Bbb R^2 \\setminus \\{0,0\\}$，函数 $y \\rightarrow a \\log y+b \\log(1-y)$ 在 (0,1) 区间上当 $y=\\frac a {a+b}$ 时有最大值（梯度为 0 求解得到），所以要使得 V(G,D) 最大，那么对于每个 x 值，都要使 D(x) 达到最大，即 (2) 式。证毕。\n\nD 的训练目标函数可以看作是条件概率 $P(Y=y|x)$ 的最大 log 似然函数（或者是最小化 binary cross-entropy），其中当 x 来自 $p_{data}$ 时 y=1，当 x 来自 $p_g$ 时 y=0。得到 D 的最优解 $D_G^{\\ast}$ 后 (1) 式变为，  \n\n$$\\begin{aligned} C(G)&=\\max_D V(G,D)\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$\n\n__Theorem 1.__ 当且仅当 $p_g=p_{data}$ 时， C(G) 有全局最优解 -log4。  \n\n__证明：__ \n\n1. 充分性  \n令 $p_g=p_{data}$，根据 (2) 式有 $D_G^{\\ast}(x)=1/2$，然后根据 (4) 式有，\n$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$\n2. 必要性  \n   $$\\begin{aligned}C(G)&=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\\\\\ &=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\\\\\ &=-\\log4+KL \\left(p_{data} \\| \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g \\| \\frac {p_{data}+p_g} 2 \\right) \\\\\\\\ &=-\\log4 + 2\\cdot JSD(p_{data} \\| p_g) \\end{aligned}$$\n   其中 KL 表示 Kullback-Leibler 散度，JSD 表示 Jensen-Shannon 散度。由于 JSD 非负，且仅在 $p_g=p_{data}$ 时取得最小值 0，所以 C(G)=-log4 时，$p_g=p_{data}$。  \n\n证毕。\n\n### 算法 1 的收敛\n上一小节我们分析了全局最优解是存在的，并且取得全局最优解的条件是 $p_g=p_{data}$。__Proposition 2__ 表明基于算法 1 的更新是有效的，训练可以收敛到全局最优解。\n\n__Proposition 2.__ 如果 G 和 D 有足够的模型空间，且在算法 1 每次迭代中给定 G 的情况下判别器可以达到最优解，且以调优（使更小） G 的训练标准 C(G) 更新 $p_g$ \n$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$\n那么，$p_g$ 趋于 $p_{data}$。\n\n__证明：__\n\n考虑 $V(G,D)=U(p_g,D)$ 是 $p_g$ 的函数，$p_g$ 可根据 (5) 式标准进行优化。注意到 $U(p_g,D)$ 是 $p_g$ （定义域）上的凸函数，不同 D 形成的凸函数集合的上确界（它也是一个凸函数）的 __次导数__ 包含了此凸函数集合在某个 D 值取得最大值所对应函数的导数，也就是说，给定任意 $p_g$（它是函数自变量），D 是可变参数，（在任意自变量 $p_g$ 处）上述结论均成立。用数学语言描述就是：\n\n- 如果 $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$，且 $f_{\\alpha}(x)$ 对任意 $\\alpha$ 在 x 上均为凸，那么当 $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ 时有 $\\partial f_{\\beta}(x) \\in \\partial f(x)$。\n\n$V(G,D)=U(p_g,D)$ 相当于上述的上确界函数，不能保证在 $p_g$ 定义域上处处严格可导，但是这个上确界函数也是一个凸函数，保证了其具有全局唯一最优解。而上面这个结论 “在任意 $p_g$ 处，其次导数包含了在某个 D 值取得最大值所对应函数的导数”，即，“包含了在 D 取最优解 D* 时 V(G,D) 的导数”，而这个导数正是对 (5) 式求导，于是可以使用这个导数进行梯度上升/下降法更新 $p_g$，并且这个更新将会使得 $p_g$ 趋于 $p_{data}$（参考 Theorem 1）。证毕\n\n对 (5) 式求导与算法 1 中的梯度本质相同，只是似然函数的期望改为批 SGD 中各样本损失的均值（没办法，数值计算使然），注意第一个期望在更新 $p_g$ 时不起作用，为什么这么讲？因为更新 $p_g$ 时，D 已经被固定，此时第一个期望与 $p_g$ 无关。\n\n实际应用中，对抗网络使用 $G(z;\\theta_g)$ 表示 $p_g$ 的分布，其中 $\\theta_g$ 是 G 模型参数，在选定 G 的网络模型如 MLP 时，$\\theta_g$ 就决定了 $p_g$ 的分布，故以上有所对 $p_g$ 的更新其实都转为对  $\\theta_g$ 的更新，例如，使用 MLP 作为 G 的模型，目标函数 (1) 式中的 $p_g$ 分布替换为某个 batch 中的生成样本分布，$p_{data}$ 则替换为 batch 中的真实样本分布，简单点说，目标函数 (1) 变为 batch 中所有样本的 log-likelihood function 的均值，包含真实数据和生成数据两部分的log 似然函数，具体可参见下文的代码分析。\n\n## 实验\n实验介绍和结果分析略。在这里，我们重点看一下源码 [adversarial](http://www.github.com/goodfeli/adversarial)\n\n> 声明：本源码使用库 Theano 和 Pylearn2，而我从来没接触过这两个库，代码分析全凭函数名、变量名和类名等。github 上也有 GAN 的其他实现如 [generative-models](https://github.com/wiseodd/generative-models)，代码通俗易懂，读者可自行查阅。\n\n从 github 上 clone 这个仓库，进入 adversarial 本项目的根目录。以 mnist 数据集为例说明。\n\n首先看下 mnist.yaml 这个文件，\n```yaml\n!obj:pylearn2.train.Train {         # 训练配置\n    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {    # 训练使用 mnist 数据集\n        which_set: 'train',                                 # 使用 train 数据的前 50000 条\n        start: 0,\n        stop: 50000\n    },\n    model: !obj:adversarial.AdversaryPair {                 # GAN：G & D\n        generator: !obj:adversarial.Generator {             # G\n            noise: 'uniform',                               # noise 分布使用均匀分布\n            monitor_ll: 1,\n            mlp: !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     !obj:pylearn2.models.mlp.RectifiedLinear { # 带 ReLu 的 FC 层\n                         layer_name: 'h0',\n                         dim: 1200,                             # 本层 output units 数量\n                         irange: .05,\n                     },\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {     # FC 层后接 sigmoid\n                         init_bias: !obj:pylearn2.models.dbm.init_sigmoid_bias_from_marginals { dataset: *train},\n                         layer_name: 'y',\n                         irange: .05,\n                         dim: 784                               # 784=28x28，为 mnist 单个样本大小\n                     }\n                    ],\n            nvis: 100,                                          # G 的噪声随机变量的向量维度\n        }},\n        discriminator:                                          # D\n            !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {\n                         layer_name: 'y',\n                         dim: 1,                                # 输出为标量\n                         irange: .005\n                     }\n                    ],\n            nvis: 784,                                          # 输入向量维度\n        },\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {      # 优化算法\n        ...\n        cost: !obj:adversarial.AdversaryCost2 {                 # 损失实现类\n            scale_grads: 0,\n            #target_scale: 1.,\n            discriminator_default_input_include_prob: .5,\n            discriminator_input_include_probs: {\n                'h0': .8\n            },\n            discriminator_default_input_scale: 2.,\n            discriminator_input_scales: {\n                'h0': 1.25   \n            }\n            },\n        ...\n    },\n    ...\n}\n```\n可以明显知道，训练使用 mnist 的 `train` 数据集中前 50000 个数据，模型类实现为 adversarial.AdversaryPair，生成器类为 adversarial.Generator，其内部封装了一个 MLP，判别器类直接使用 MLP。损失实现类为 adversarial.AdversaryCost2。这些类的实现均位于 `__init__.py` 中。这里主要分析一下 AdversaryCost2（其他类的实现均比较简单明了）。\n\n首先看一下生成样本和目标函数 `get_samples_and_objectives`，\n```python\ng=model.generator       # model is an instance of AdversaryPair\nd=model.discriminator\nX=data                  # 真实数据（来自训练样本）的 batch\nm=data.shape[space.get_batch_axis()]    # 获取 batch 的大小，即批样本数量\ny1=T.alloc(1,m,1)       # 长度为 m 的全 1 向量，代表真实数据的 label\ny0=T.alloc(0,m,1)       # 长度为 m 的全 0 向量，代表生成数据的 label\n# 1. 生成 m 个噪声作为 G 模型的输入 z\n# 2. G 前向传播生成 m 个样本 S\nS,z,other_layers=g.sample_and_noise(m,\n    default_input_include_prob=self.generator_default_input_include_prob,   # 1\n    default_input_scale=self.generator_default_input_scale,                 # 1\n    all_g_layers=(self.infer_layer is not None)                         # False\n)\nif self.noise_both !=0:     # 真实数据和生成数据均添加一个噪声干扰\n    ...\n# D 前向传播，分别得到真实数据的预测 label 和生成数据的预测 label\ny_hat1 = d.dropout_fprop(...)       # 参数略\ny_hat0 = d.dropout_fprop(...)\n# D 的目标损失。d.layers[-1] 为 Sigmoid 层，其目标损失为 KL 散度\nd_obj = 0.5*(d.layers[-1].cost(y1,y_hat1)+d.layers[-1].cost(y0,y_hat0))\n# G 的目标损失。G 希望 D 的判别结果 y_hat0 与真实 label y1 越小越好  \ng_obj = d.layers[-1].cost(y1,y_hat0)\nif model.inferer is not None:       # 模型推断器\n    ...\nelse:\n    i_obj = 0\nreturn S, d_obj, g_obj, i_obj       # 返回生成样本，D 损失和 G 损失\n```\n再来看计算梯度函数 `get_gradients` 的实现部分，\n```python\ng=model.generator\nd=model.generator\nS,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   # 调用上面分析的函数\ng_params = g.get_params()\nd_params = d.get_params()\n# 计算损失对各参数的梯度\nd_grads = T.grad(d_obj,d_params)\ng_grads = T.grad(g_obj,g_params)\nif self.scale_grads:    # 缩小 g_grads\n    S_grad = T.grad(g_obj, S)   # G 损失对生成样本（也就是 G 的输出）的梯度\n    # S_grad 的平方和的平方根的倒数作为缩小比例\n    scale = T.maximum(1.,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))\n    # 缩小 g_grads\n    g_grads = [g_grad * scale for g_grad in g_grads]\n\n# 保存各模型参数与其对应的梯度\nrval = OrderDict()\nrval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg for dg in d_grads])))\nrval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg for gg in g_grads])))\n\nupdates = OrderDict()\nif self.alternate_g:\n    updates[self.now_train_generator]=1. - self.now_train_generator\nreturn rval, updates\n```\n最终的更新操作由 Pylearn2/Theano 库完成。\n\n以上代码片段中，目标函数为损失，与 log 似然函数相差一个负号，所以上文分析中某些求最大值的地方变为求最小值，然后使用随机梯度下降更新模型参数，这与算法 1 中的情况完成相同。另外，对 `g_grads` 进行 scale 缩小，一种可能的原因是，\n\n生成样本 $S=\\theta_g \\cdot z$，损失对 $\\theta_g$ 的梯度满足\n$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$\n\n记生成样本 S 经过 D 的输出为 y_0，即，$y_0=\\theta_d \\cdot S$，于是\n$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$\n可以看出在计算损失对 G 模型参数的梯度之前，$\\nabla_S L$ 这个梯度已经经过 D 中各层的传播：\n1. 如果其 L2 范数大于 1，那么再经过 G 中各层反向传播时，极有可能出现梯度爆炸，即 $\\nabla_{\\theta_g}L$ 很大， 导致训练不稳定，所以需要将其进行 scale 缩小，缩小的比例正好能使 $\\nabla_S L$ 的 L2 范数为指定值 `self.target_scale`（默认为1）。\n   关于 G 模型参数的梯度过大导致训练不稳定，如下图，\n   ![](/images/GAN_fig2.png)<center>fig 2. 图来自于网络。左图表示 $G_0$ 时的 V(G,D) 曲线；右图表示 $G_1$ 时的 V(G,D) 曲线。（这个图我觉得有点奇怪，按道理不应该是凸函数吗，以及右图右边的红点不是说明存在合适的 $D_1^{\\ast}$ 吗，用这个图能说明什么问题，我没有搞懂。相反，我倒觉得是用来说明不要更新 G 太多以便可以达到这个图中的效果。如我理解有误，恳请大佬指正~）</center>\n\n   上图表示在 $D_0^{\\ast}$ 取得最大值 $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$，然后更新 $G_0$ 为 $G_1$后，由于 G 的更新会降低 V(G,D)，故 $V(G_1,D_0^{\\ast}) < V(G_0,D_0^{\\ast})$，但是此时更新 D 以最大化 V(G,D)，可能会出现 $V(G_1,D_1^{\\ast}) < V(G_0,D_0^{\\ast})$，这意味着判别器 $D_1^{\\ast}$ 的判别能力比之前的 $D_0^{\\ast}$ 的判别能力差，而 G 伪装能力的增强是建立在 D 判别能力的增强这个基础上，否则更新 G 就达不到应该有的效果，所以降低损失对 G 模型参数的梯度，以便不要更新 G 太多，或者多次更新 D 与一次更新 G 交替进行。\n2. 如果其 L2 范数小于等于1，则对梯度不做 scale 缩小操作。\n\n\n\n当然，还有其他损失实现类，具体请查阅源码，不再讨论。\n\n## 总结\n给定一个预先已知分布的噪声随机变量 z，G 根据 z 生成图像 G(z)，D 将 G(z) 与训练样本区分开来。训练过程根据 (1) 式交替优化 D 和 G，使得 G 尽可能拟合真实数据分布，而 D 提高判别能力，最终 G 分布与真实分布相同，D 无法判别模型分布和真实数据分布。","source":"_posts/GAN.md","raw":"---\ntitle: GAN\ndate: 2019-07-23 10:15:08\ntags: GAN\nmathjax: true\n---\n论文 [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n<!-- more -->\n# GAN\n## 原理\n生成对抗网络 GAN：一个生成模型 G 和一个判别模型 D，G 尽可能模拟真实的数据分布，D 尽可能的区分样本是模型生成的还是真实的。下文以图像数据为例说明。\n\n定义一个输入噪声随机变量 z，其分布为 $p_z(z)$，G 根据 z 生成图像 $G(z;\\theta_g)$，我们假设 G 是一个多层感知机 MLP 网络，网络参数为 $\\theta_g$。D 也是一个 MLP $D(x;\\theta_d)$ 输出是一个标量，表示 x 是真实图像的概率。训练 D 使其对输入 x 预测正确的概率最大化，即当 x 来自真实的训练数据时，$G(x)$ 尽可能大，当 x 来自 G 生成样本时，预测概率 $D(G(z))$ 尽可能小；而训练 G 目的是为了让 $D(G(x))$ 尽可能大，或者说让 $\\log(1-D(G(z)))$ 尽可能小，于是目标函数为，\n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$\n\n（对 D 而言，这是一个 log 似然函数，D 希望它越大越大，所以求最大值；而 G 却希望 D 的 log 似然函数越小越好，所以求最小值）\n\n这是一个二人零和博弈。图 1 是训练过程示意图，训练使用迭代的，数值计算的方法。\n![](/images/GAN_fig1.png)\n\n图 1 中 D 模型分布为蓝色虚线，数据 x 的分布 $p_x$ 为黑色点线，G 模型分布 $p_g$ 为绿色实线（黑绿曲线上某一点分别表示此 x 值处真实数据的概率密度和生成数据的概率密度）。下面的水平线为随机噪声变量 z 的定义域，在其上对 z 均匀采样，上面水平线是 x 的定义域，向上箭头表示映射过程 x=G(z) （G 生成过程）。  \n(a) 是收敛附近的对抗情况：此时 $p_g,\\ p_{data}$ 两者相似，D 分类不完全准确。  \n(b) 在内层循环中，训练 D 判别样本，训练过程收敛于 $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$。  \n(c) D 的梯度可指引 G(z) 移动到更容易被分类为真实数据的区域，即，G 更新后，更加逼近真实数据分布。  \n(d) 经过几次训练，G 和 D 到达一个平衡点，此时 $p_g=p_{data}$，D 无法再区分这两个分布，即，$D(x)=1/2$。\n\n训练算法如下，\n![](/images/GAN_alg1.png)\n\nk 次 D 的优化与一次 G 的优化交替进行，这可以使得 G 变化缓慢，而 D 维持在最优解附近。\n\n实际应用中，(1) 式可能无法提供足够的梯度来更新 G。训练初期，G 性能较差，生成样本与真实训练样本区别较大，所以 D 可以较高的置信度判别，此时，$\\log (1-D(G(z)))$ 达到饱和（log 曲线右端较为平坦），于是我们改为训练 G 以最大化 $\\log D(G(z))$，最终训练能到达相同的 G 和 D 的平衡点，但是训练初期的梯度较大（log 曲线的左端较为陡峭）。\n\n## 理论分析\n已知噪声随机变量 z 的分布 $p_z$ 时，可以获得 G 的模型分布，根据算法 1，如果 G 模型的假设空间和训练时间足够，G 可以拟合真实数据分布 $p_{data}$。现在我们来证明 $p_g=p_{data}$ 是 (1) 式的全局最优解。\n### 全局最优解\n__Proposition 1.__ 对于任意的 G，D 的最优解为\n$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$\n__证明：__  给定任意 G，D 的训练准则是最大化 V(G,D)  \n$$\\begin{aligned} V(G,D)&=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz\n\\\\\\\\ &=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$\n$\\forall (a,b) \\in \\Bbb R^2 \\setminus \\{0,0\\}$，函数 $y \\rightarrow a \\log y+b \\log(1-y)$ 在 (0,1) 区间上当 $y=\\frac a {a+b}$ 时有最大值（梯度为 0 求解得到），所以要使得 V(G,D) 最大，那么对于每个 x 值，都要使 D(x) 达到最大，即 (2) 式。证毕。\n\nD 的训练目标函数可以看作是条件概率 $P(Y=y|x)$ 的最大 log 似然函数（或者是最小化 binary cross-entropy），其中当 x 来自 $p_{data}$ 时 y=1，当 x 来自 $p_g$ 时 y=0。得到 D 的最优解 $D_G^{\\ast}$ 后 (1) 式变为，  \n\n$$\\begin{aligned} C(G)&=\\max_D V(G,D)\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$\n\n__Theorem 1.__ 当且仅当 $p_g=p_{data}$ 时， C(G) 有全局最优解 -log4。  \n\n__证明：__ \n\n1. 充分性  \n令 $p_g=p_{data}$，根据 (2) 式有 $D_G^{\\ast}(x)=1/2$，然后根据 (4) 式有，\n$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$\n2. 必要性  \n   $$\\begin{aligned}C(G)&=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\\\\\ &=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\\\\\ &=-\\log4+KL \\left(p_{data} \\| \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g \\| \\frac {p_{data}+p_g} 2 \\right) \\\\\\\\ &=-\\log4 + 2\\cdot JSD(p_{data} \\| p_g) \\end{aligned}$$\n   其中 KL 表示 Kullback-Leibler 散度，JSD 表示 Jensen-Shannon 散度。由于 JSD 非负，且仅在 $p_g=p_{data}$ 时取得最小值 0，所以 C(G)=-log4 时，$p_g=p_{data}$。  \n\n证毕。\n\n### 算法 1 的收敛\n上一小节我们分析了全局最优解是存在的，并且取得全局最优解的条件是 $p_g=p_{data}$。__Proposition 2__ 表明基于算法 1 的更新是有效的，训练可以收敛到全局最优解。\n\n__Proposition 2.__ 如果 G 和 D 有足够的模型空间，且在算法 1 每次迭代中给定 G 的情况下判别器可以达到最优解，且以调优（使更小） G 的训练标准 C(G) 更新 $p_g$ \n$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$\n那么，$p_g$ 趋于 $p_{data}$。\n\n__证明：__\n\n考虑 $V(G,D)=U(p_g,D)$ 是 $p_g$ 的函数，$p_g$ 可根据 (5) 式标准进行优化。注意到 $U(p_g,D)$ 是 $p_g$ （定义域）上的凸函数，不同 D 形成的凸函数集合的上确界（它也是一个凸函数）的 __次导数__ 包含了此凸函数集合在某个 D 值取得最大值所对应函数的导数，也就是说，给定任意 $p_g$（它是函数自变量），D 是可变参数，（在任意自变量 $p_g$ 处）上述结论均成立。用数学语言描述就是：\n\n- 如果 $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$，且 $f_{\\alpha}(x)$ 对任意 $\\alpha$ 在 x 上均为凸，那么当 $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ 时有 $\\partial f_{\\beta}(x) \\in \\partial f(x)$。\n\n$V(G,D)=U(p_g,D)$ 相当于上述的上确界函数，不能保证在 $p_g$ 定义域上处处严格可导，但是这个上确界函数也是一个凸函数，保证了其具有全局唯一最优解。而上面这个结论 “在任意 $p_g$ 处，其次导数包含了在某个 D 值取得最大值所对应函数的导数”，即，“包含了在 D 取最优解 D* 时 V(G,D) 的导数”，而这个导数正是对 (5) 式求导，于是可以使用这个导数进行梯度上升/下降法更新 $p_g$，并且这个更新将会使得 $p_g$ 趋于 $p_{data}$（参考 Theorem 1）。证毕\n\n对 (5) 式求导与算法 1 中的梯度本质相同，只是似然函数的期望改为批 SGD 中各样本损失的均值（没办法，数值计算使然），注意第一个期望在更新 $p_g$ 时不起作用，为什么这么讲？因为更新 $p_g$ 时，D 已经被固定，此时第一个期望与 $p_g$ 无关。\n\n实际应用中，对抗网络使用 $G(z;\\theta_g)$ 表示 $p_g$ 的分布，其中 $\\theta_g$ 是 G 模型参数，在选定 G 的网络模型如 MLP 时，$\\theta_g$ 就决定了 $p_g$ 的分布，故以上有所对 $p_g$ 的更新其实都转为对  $\\theta_g$ 的更新，例如，使用 MLP 作为 G 的模型，目标函数 (1) 式中的 $p_g$ 分布替换为某个 batch 中的生成样本分布，$p_{data}$ 则替换为 batch 中的真实样本分布，简单点说，目标函数 (1) 变为 batch 中所有样本的 log-likelihood function 的均值，包含真实数据和生成数据两部分的log 似然函数，具体可参见下文的代码分析。\n\n## 实验\n实验介绍和结果分析略。在这里，我们重点看一下源码 [adversarial](http://www.github.com/goodfeli/adversarial)\n\n> 声明：本源码使用库 Theano 和 Pylearn2，而我从来没接触过这两个库，代码分析全凭函数名、变量名和类名等。github 上也有 GAN 的其他实现如 [generative-models](https://github.com/wiseodd/generative-models)，代码通俗易懂，读者可自行查阅。\n\n从 github 上 clone 这个仓库，进入 adversarial 本项目的根目录。以 mnist 数据集为例说明。\n\n首先看下 mnist.yaml 这个文件，\n```yaml\n!obj:pylearn2.train.Train {         # 训练配置\n    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {    # 训练使用 mnist 数据集\n        which_set: 'train',                                 # 使用 train 数据的前 50000 条\n        start: 0,\n        stop: 50000\n    },\n    model: !obj:adversarial.AdversaryPair {                 # GAN：G & D\n        generator: !obj:adversarial.Generator {             # G\n            noise: 'uniform',                               # noise 分布使用均匀分布\n            monitor_ll: 1,\n            mlp: !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     !obj:pylearn2.models.mlp.RectifiedLinear { # 带 ReLu 的 FC 层\n                         layer_name: 'h0',\n                         dim: 1200,                             # 本层 output units 数量\n                         irange: .05,\n                     },\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {     # FC 层后接 sigmoid\n                         init_bias: !obj:pylearn2.models.dbm.init_sigmoid_bias_from_marginals { dataset: *train},\n                         layer_name: 'y',\n                         irange: .05,\n                         dim: 784                               # 784=28x28，为 mnist 单个样本大小\n                     }\n                    ],\n            nvis: 100,                                          # G 的噪声随机变量的向量维度\n        }},\n        discriminator:                                          # D\n            !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {\n                         layer_name: 'y',\n                         dim: 1,                                # 输出为标量\n                         irange: .005\n                     }\n                    ],\n            nvis: 784,                                          # 输入向量维度\n        },\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {      # 优化算法\n        ...\n        cost: !obj:adversarial.AdversaryCost2 {                 # 损失实现类\n            scale_grads: 0,\n            #target_scale: 1.,\n            discriminator_default_input_include_prob: .5,\n            discriminator_input_include_probs: {\n                'h0': .8\n            },\n            discriminator_default_input_scale: 2.,\n            discriminator_input_scales: {\n                'h0': 1.25   \n            }\n            },\n        ...\n    },\n    ...\n}\n```\n可以明显知道，训练使用 mnist 的 `train` 数据集中前 50000 个数据，模型类实现为 adversarial.AdversaryPair，生成器类为 adversarial.Generator，其内部封装了一个 MLP，判别器类直接使用 MLP。损失实现类为 adversarial.AdversaryCost2。这些类的实现均位于 `__init__.py` 中。这里主要分析一下 AdversaryCost2（其他类的实现均比较简单明了）。\n\n首先看一下生成样本和目标函数 `get_samples_and_objectives`，\n```python\ng=model.generator       # model is an instance of AdversaryPair\nd=model.discriminator\nX=data                  # 真实数据（来自训练样本）的 batch\nm=data.shape[space.get_batch_axis()]    # 获取 batch 的大小，即批样本数量\ny1=T.alloc(1,m,1)       # 长度为 m 的全 1 向量，代表真实数据的 label\ny0=T.alloc(0,m,1)       # 长度为 m 的全 0 向量，代表生成数据的 label\n# 1. 生成 m 个噪声作为 G 模型的输入 z\n# 2. G 前向传播生成 m 个样本 S\nS,z,other_layers=g.sample_and_noise(m,\n    default_input_include_prob=self.generator_default_input_include_prob,   # 1\n    default_input_scale=self.generator_default_input_scale,                 # 1\n    all_g_layers=(self.infer_layer is not None)                         # False\n)\nif self.noise_both !=0:     # 真实数据和生成数据均添加一个噪声干扰\n    ...\n# D 前向传播，分别得到真实数据的预测 label 和生成数据的预测 label\ny_hat1 = d.dropout_fprop(...)       # 参数略\ny_hat0 = d.dropout_fprop(...)\n# D 的目标损失。d.layers[-1] 为 Sigmoid 层，其目标损失为 KL 散度\nd_obj = 0.5*(d.layers[-1].cost(y1,y_hat1)+d.layers[-1].cost(y0,y_hat0))\n# G 的目标损失。G 希望 D 的判别结果 y_hat0 与真实 label y1 越小越好  \ng_obj = d.layers[-1].cost(y1,y_hat0)\nif model.inferer is not None:       # 模型推断器\n    ...\nelse:\n    i_obj = 0\nreturn S, d_obj, g_obj, i_obj       # 返回生成样本，D 损失和 G 损失\n```\n再来看计算梯度函数 `get_gradients` 的实现部分，\n```python\ng=model.generator\nd=model.generator\nS,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   # 调用上面分析的函数\ng_params = g.get_params()\nd_params = d.get_params()\n# 计算损失对各参数的梯度\nd_grads = T.grad(d_obj,d_params)\ng_grads = T.grad(g_obj,g_params)\nif self.scale_grads:    # 缩小 g_grads\n    S_grad = T.grad(g_obj, S)   # G 损失对生成样本（也就是 G 的输出）的梯度\n    # S_grad 的平方和的平方根的倒数作为缩小比例\n    scale = T.maximum(1.,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))\n    # 缩小 g_grads\n    g_grads = [g_grad * scale for g_grad in g_grads]\n\n# 保存各模型参数与其对应的梯度\nrval = OrderDict()\nrval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg for dg in d_grads])))\nrval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg for gg in g_grads])))\n\nupdates = OrderDict()\nif self.alternate_g:\n    updates[self.now_train_generator]=1. - self.now_train_generator\nreturn rval, updates\n```\n最终的更新操作由 Pylearn2/Theano 库完成。\n\n以上代码片段中，目标函数为损失，与 log 似然函数相差一个负号，所以上文分析中某些求最大值的地方变为求最小值，然后使用随机梯度下降更新模型参数，这与算法 1 中的情况完成相同。另外，对 `g_grads` 进行 scale 缩小，一种可能的原因是，\n\n生成样本 $S=\\theta_g \\cdot z$，损失对 $\\theta_g$ 的梯度满足\n$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$\n\n记生成样本 S 经过 D 的输出为 y_0，即，$y_0=\\theta_d \\cdot S$，于是\n$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$\n可以看出在计算损失对 G 模型参数的梯度之前，$\\nabla_S L$ 这个梯度已经经过 D 中各层的传播：\n1. 如果其 L2 范数大于 1，那么再经过 G 中各层反向传播时，极有可能出现梯度爆炸，即 $\\nabla_{\\theta_g}L$ 很大， 导致训练不稳定，所以需要将其进行 scale 缩小，缩小的比例正好能使 $\\nabla_S L$ 的 L2 范数为指定值 `self.target_scale`（默认为1）。\n   关于 G 模型参数的梯度过大导致训练不稳定，如下图，\n   ![](/images/GAN_fig2.png)<center>fig 2. 图来自于网络。左图表示 $G_0$ 时的 V(G,D) 曲线；右图表示 $G_1$ 时的 V(G,D) 曲线。（这个图我觉得有点奇怪，按道理不应该是凸函数吗，以及右图右边的红点不是说明存在合适的 $D_1^{\\ast}$ 吗，用这个图能说明什么问题，我没有搞懂。相反，我倒觉得是用来说明不要更新 G 太多以便可以达到这个图中的效果。如我理解有误，恳请大佬指正~）</center>\n\n   上图表示在 $D_0^{\\ast}$ 取得最大值 $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$，然后更新 $G_0$ 为 $G_1$后，由于 G 的更新会降低 V(G,D)，故 $V(G_1,D_0^{\\ast}) < V(G_0,D_0^{\\ast})$，但是此时更新 D 以最大化 V(G,D)，可能会出现 $V(G_1,D_1^{\\ast}) < V(G_0,D_0^{\\ast})$，这意味着判别器 $D_1^{\\ast}$ 的判别能力比之前的 $D_0^{\\ast}$ 的判别能力差，而 G 伪装能力的增强是建立在 D 判别能力的增强这个基础上，否则更新 G 就达不到应该有的效果，所以降低损失对 G 模型参数的梯度，以便不要更新 G 太多，或者多次更新 D 与一次更新 G 交替进行。\n2. 如果其 L2 范数小于等于1，则对梯度不做 scale 缩小操作。\n\n\n\n当然，还有其他损失实现类，具体请查阅源码，不再讨论。\n\n## 总结\n给定一个预先已知分布的噪声随机变量 z，G 根据 z 生成图像 G(z)，D 将 G(z) 与训练样本区分开来。训练过程根据 (1) 式交替优化 D 和 G，使得 G 尽可能拟合真实数据分布，而 D 提高判别能力，最终 G 分布与真实分布相同，D 无法判别模型分布和真实数据分布。","slug":"GAN","published":1,"updated":"2020-04-24T10:37:46.726Z","_id":"ck9dzciuw001lgga6cqdf40gu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\" rel=\"noopener\">Generative Adversarial Nets</a></p>\n<a id=\"more\"></a>\n<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>生成对抗网络 GAN：一个生成模型 G 和一个判别模型 D，G 尽可能模拟真实的数据分布，D 尽可能的区分样本是模型生成的还是真实的。下文以图像数据为例说明。</p>\n<p>定义一个输入噪声随机变量 z，其分布为 $p_z(z)$，G 根据 z 生成图像 $G(z;\\theta_g)$，我们假设 G 是一个多层感知机 MLP 网络，网络参数为 $\\theta_g$。D 也是一个 MLP $D(x;\\theta_d)$ 输出是一个标量，表示 x 是真实图像的概率。训练 D 使其对输入 x 预测正确的概率最大化，即当 x 来自真实的训练数据时，$G(x)$ 尽可能大，当 x 来自 G 生成样本时，预测概率 $D(G(z))$ 尽可能小；而训练 G 目的是为了让 $D(G(x))$ 尽可能大，或者说让 $\\log(1-D(G(z)))$ 尽可能小，于是目标函数为，</p>\n<p>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$</p>\n<p>（对 D 而言，这是一个 log 似然函数，D 希望它越大越大，所以求最大值；而 G 却希望 D 的 log 似然函数越小越好，所以求最小值）</p>\n<p>这是一个二人零和博弈。图 1 是训练过程示意图，训练使用迭代的，数值计算的方法。<br><img src=\"/images/GAN_fig1.png\" alt=\"\"></p>\n<p>图 1 中 D 模型分布为蓝色虚线，数据 x 的分布 $p_x$ 为黑色点线，G 模型分布 $p_g$ 为绿色实线（黑绿曲线上某一点分别表示此 x 值处真实数据的概率密度和生成数据的概率密度）。下面的水平线为随机噪声变量 z 的定义域，在其上对 z 均匀采样，上面水平线是 x 的定义域，向上箭头表示映射过程 x=G(z) （G 生成过程）。<br>(a) 是收敛附近的对抗情况：此时 $p_g,\\ p_{data}$ 两者相似，D 分类不完全准确。<br>(b) 在内层循环中，训练 D 判别样本，训练过程收敛于 $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$。<br>(c) D 的梯度可指引 G(z) 移动到更容易被分类为真实数据的区域，即，G 更新后，更加逼近真实数据分布。<br>(d) 经过几次训练，G 和 D 到达一个平衡点，此时 $p_g=p_{data}$，D 无法再区分这两个分布，即，$D(x)=1/2$。</p>\n<p>训练算法如下，<br><img src=\"/images/GAN_alg1.png\" alt=\"\"></p>\n<p>k 次 D 的优化与一次 G 的优化交替进行，这可以使得 G 变化缓慢，而 D 维持在最优解附近。</p>\n<p>实际应用中，(1) 式可能无法提供足够的梯度来更新 G。训练初期，G 性能较差，生成样本与真实训练样本区别较大，所以 D 可以较高的置信度判别，此时，$\\log (1-D(G(z)))$ 达到饱和（log 曲线右端较为平坦），于是我们改为训练 G 以最大化 $\\log D(G(z))$，最终训练能到达相同的 G 和 D 的平衡点，但是训练初期的梯度较大（log 曲线的左端较为陡峭）。</p>\n<h2 id=\"理论分析\"><a href=\"#理论分析\" class=\"headerlink\" title=\"理论分析\"></a>理论分析</h2><p>已知噪声随机变量 z 的分布 $p_z$ 时，可以获得 G 的模型分布，根据算法 1，如果 G 模型的假设空间和训练时间足够，G 可以拟合真实数据分布 $p_{data}$。现在我们来证明 $p_g=p_{data}$ 是 (1) 式的全局最优解。</p>\n<h3 id=\"全局最优解\"><a href=\"#全局最优解\" class=\"headerlink\" title=\"全局最优解\"></a>全局最优解</h3><p><strong>Proposition 1.</strong> 对于任意的 G，D 的最优解为<br>$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$<br><strong>证明：</strong>  给定任意 G，D 的训练准则是最大化 V(G,D)<br>$$\\begin{aligned} V(G,D)&amp;=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz<br>\\\\ &amp;=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$<br>$\\forall (a,b) \\in \\Bbb R^2 \\setminus {0,0}$，函数 $y \\rightarrow a \\log y+b \\log(1-y)$ 在 (0,1) 区间上当 $y=\\frac a {a+b}$ 时有最大值（梯度为 0 求解得到），所以要使得 V(G,D) 最大，那么对于每个 x 值，都要使 D(x) 达到最大，即 (2) 式。证毕。</p>\n<p>D 的训练目标函数可以看作是条件概率 $P(Y=y|x)$ 的最大 log 似然函数（或者是最小化 binary cross-entropy），其中当 x 来自 $p_{data}$ 时 y=1，当 x 来自 $p_g$ 时 y=0。得到 D 的最优解 $D_G^{\\ast}$ 后 (1) 式变为，  </p>\n<p>$$\\begin{aligned} C(G)&amp;=\\max_D V(G,D)<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$</p>\n<p><strong>Theorem 1.</strong> 当且仅当 $p_g=p_{data}$ 时， C(G) 有全局最优解 -log4。  </p>\n<p><strong>证明：</strong> </p>\n<ol>\n<li>充分性<br>令 $p_g=p_{data}$，根据 (2) 式有 $D_G^{\\ast}(x)=1/2$，然后根据 (4) 式有，<br>$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$</li>\n<li>必要性<br>$$\\begin{aligned}C(G)&amp;=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\ &amp;=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\ &amp;=-\\log4+KL \\left(p_{data} | \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g | \\frac {p_{data}+p_g} 2 \\right) \\\\ &amp;=-\\log4 + 2\\cdot JSD(p_{data} | p_g) \\end{aligned}$$<br>其中 KL 表示 Kullback-Leibler 散度，JSD 表示 Jensen-Shannon 散度。由于 JSD 非负，且仅在 $p_g=p_{data}$ 时取得最小值 0，所以 C(G)=-log4 时，$p_g=p_{data}$。  </li>\n</ol>\n<p>证毕。</p>\n<h3 id=\"算法-1-的收敛\"><a href=\"#算法-1-的收敛\" class=\"headerlink\" title=\"算法 1 的收敛\"></a>算法 1 的收敛</h3><p>上一小节我们分析了全局最优解是存在的，并且取得全局最优解的条件是 $p_g=p_{data}$。<strong>Proposition 2</strong> 表明基于算法 1 的更新是有效的，训练可以收敛到全局最优解。</p>\n<p><strong>Proposition 2.</strong> 如果 G 和 D 有足够的模型空间，且在算法 1 每次迭代中给定 G 的情况下判别器可以达到最优解，且以调优（使更小） G 的训练标准 C(G) 更新 $p_g$<br>$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$<br>那么，$p_g$ 趋于 $p_{data}$。</p>\n<p><strong>证明：</strong></p>\n<p>考虑 $V(G,D)=U(p_g,D)$ 是 $p_g$ 的函数，$p_g$ 可根据 (5) 式标准进行优化。注意到 $U(p_g,D)$ 是 $p_g$ （定义域）上的凸函数，不同 D 形成的凸函数集合的上确界（它也是一个凸函数）的 <strong>次导数</strong> 包含了此凸函数集合在某个 D 值取得最大值所对应函数的导数，也就是说，给定任意 $p_g$（它是函数自变量），D 是可变参数，（在任意自变量 $p_g$ 处）上述结论均成立。用数学语言描述就是：</p>\n<ul>\n<li>如果 $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$，且 $f_{\\alpha}(x)$ 对任意 $\\alpha$ 在 x 上均为凸，那么当 $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ 时有 $\\partial f_{\\beta}(x) \\in \\partial f(x)$。</li>\n</ul>\n<p>$V(G,D)=U(p_g,D)$ 相当于上述的上确界函数，不能保证在 $p_g$ 定义域上处处严格可导，但是这个上确界函数也是一个凸函数，保证了其具有全局唯一最优解。而上面这个结论 “在任意 $p_g$ 处，其次导数包含了在某个 D 值取得最大值所对应函数的导数”，即，“包含了在 D 取最优解 D* 时 V(G,D) 的导数”，而这个导数正是对 (5) 式求导，于是可以使用这个导数进行梯度上升/下降法更新 $p_g$，并且这个更新将会使得 $p_g$ 趋于 $p_{data}$（参考 Theorem 1）。证毕</p>\n<p>对 (5) 式求导与算法 1 中的梯度本质相同，只是似然函数的期望改为批 SGD 中各样本损失的均值（没办法，数值计算使然），注意第一个期望在更新 $p_g$ 时不起作用，为什么这么讲？因为更新 $p_g$ 时，D 已经被固定，此时第一个期望与 $p_g$ 无关。</p>\n<p>实际应用中，对抗网络使用 $G(z;\\theta_g)$ 表示 $p_g$ 的分布，其中 $\\theta_g$ 是 G 模型参数，在选定 G 的网络模型如 MLP 时，$\\theta_g$ 就决定了 $p_g$ 的分布，故以上有所对 $p_g$ 的更新其实都转为对  $\\theta_g$ 的更新，例如，使用 MLP 作为 G 的模型，目标函数 (1) 式中的 $p_g$ 分布替换为某个 batch 中的生成样本分布，$p_{data}$ 则替换为 batch 中的真实样本分布，简单点说，目标函数 (1) 变为 batch 中所有样本的 log-likelihood function 的均值，包含真实数据和生成数据两部分的log 似然函数，具体可参见下文的代码分析。</p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验介绍和结果分析略。在这里，我们重点看一下源码 <a href=\"http://www.github.com/goodfeli/adversarial\" target=\"_blank\" rel=\"noopener\">adversarial</a></p>\n<blockquote>\n<p>声明：本源码使用库 Theano 和 Pylearn2，而我从来没接触过这两个库，代码分析全凭函数名、变量名和类名等。github 上也有 GAN 的其他实现如 <a href=\"https://github.com/wiseodd/generative-models\" target=\"_blank\" rel=\"noopener\">generative-models</a>，代码通俗易懂，读者可自行查阅。</p>\n</blockquote>\n<p>从 github 上 clone 这个仓库，进入 adversarial 本项目的根目录。以 mnist 数据集为例说明。</p>\n<p>首先看下 mnist.yaml 这个文件，</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">!obj</span><span class=\"string\">:pylearn2.train.Train</span> <span class=\"string\">&#123;</span>         <span class=\"comment\"># 训练配置</span></span><br><span class=\"line\">    <span class=\"attr\">dataset:</span> <span class=\"string\">&amp;train</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.datasets.mnist.MNIST</span> <span class=\"string\">&#123;</span>    <span class=\"comment\"># 训练使用 mnist 数据集</span></span><br><span class=\"line\">        <span class=\"attr\">which_set:</span> <span class=\"string\">'train'</span><span class=\"string\">,</span>                                 <span class=\"comment\"># 使用 train 数据的前 50000 条</span></span><br><span class=\"line\">        <span class=\"attr\">start:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">        <span class=\"attr\">stop:</span> <span class=\"number\">50000</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"attr\">model:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryPair</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># GAN：G &amp; D</span></span><br><span class=\"line\">        <span class=\"attr\">generator:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.Generator</span> <span class=\"string\">&#123;</span>             <span class=\"comment\"># G</span></span><br><span class=\"line\">            <span class=\"attr\">noise:</span> <span class=\"string\">'uniform'</span><span class=\"string\">,</span>                               <span class=\"comment\"># noise 分布使用均匀分布</span></span><br><span class=\"line\">            <span class=\"attr\">monitor_ll:</span> <span class=\"number\">1</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"attr\">mlp:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">            <span class=\"attr\">layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.RectifiedLinear</span> <span class=\"string\">&#123;</span> <span class=\"comment\"># 带 ReLu 的 FC 层</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'h0'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">1200</span><span class=\"string\">,</span>                             <span class=\"comment\"># 本层 output units 数量</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;,</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span>     <span class=\"comment\"># FC 层后接 sigmoid</span></span><br><span class=\"line\">                         <span class=\"attr\">init_bias:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.dbm.init_sigmoid_bias_from_marginals</span> <span class=\"string\">&#123;</span> <span class=\"attr\">dataset:</span> <span class=\"string\">*train&#125;,</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">784</span>                               <span class=\"comment\"># 784=28x28，为 mnist 单个样本大小</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\">            <span class=\"attr\">nvis:</span> <span class=\"number\">100</span><span class=\"string\">,</span>                                          <span class=\"comment\"># G 的噪声随机变量的向量维度</span></span><br><span class=\"line\">        <span class=\"string\">&#125;&#125;,</span></span><br><span class=\"line\">        <span class=\"attr\">discriminator:</span>                                          <span class=\"comment\"># D</span></span><br><span class=\"line\">            <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">            <span class=\"attr\">layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">1</span><span class=\"string\">,</span>                                <span class=\"comment\"># 输出为标量</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.005</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\">            <span class=\"attr\">nvis:</span> <span class=\"number\">784</span><span class=\"string\">,</span>                                          <span class=\"comment\"># 输入向量维度</span></span><br><span class=\"line\">        <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"attr\">algorithm:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.training_algorithms.sgd.SGD</span> <span class=\"string\">&#123;</span>      <span class=\"comment\"># 优化算法</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">        <span class=\"attr\">cost:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryCost2</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># 损失实现类</span></span><br><span class=\"line\">            <span class=\"attr\">scale_grads:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"comment\">#target_scale: 1.,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_default_input_include_prob:</span> <span class=\"number\">.5</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_input_include_probs:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                <span class=\"attr\">'h0':</span> <span class=\"number\">.8</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_default_input_scale:</span> <span class=\"number\">2</span><span class=\"string\">.,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_input_scales:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                <span class=\"attr\">'h0':</span> <span class=\"number\">1.25</span>   </span><br><span class=\"line\">            <span class=\"string\">&#125;</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br></pre></td></tr></table></figure>\n<p>可以明显知道，训练使用 mnist 的 <code>train</code> 数据集中前 50000 个数据，模型类实现为 adversarial.AdversaryPair，生成器类为 adversarial.Generator，其内部封装了一个 MLP，判别器类直接使用 MLP。损失实现类为 adversarial.AdversaryCost2。这些类的实现均位于 <code>__init__.py</code> 中。这里主要分析一下 AdversaryCost2（其他类的实现均比较简单明了）。</p>\n<p>首先看一下生成样本和目标函数 <code>get_samples_and_objectives</code>，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator       <span class=\"comment\"># model is an instance of AdversaryPair</span></span><br><span class=\"line\">d=model.discriminator</span><br><span class=\"line\">X=data                  <span class=\"comment\"># 真实数据（来自训练样本）的 batch</span></span><br><span class=\"line\">m=data.shape[space.get_batch_axis()]    <span class=\"comment\"># 获取 batch 的大小，即批样本数量</span></span><br><span class=\"line\">y1=T.alloc(<span class=\"number\">1</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\"># 长度为 m 的全 1 向量，代表真实数据的 label</span></span><br><span class=\"line\">y0=T.alloc(<span class=\"number\">0</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\"># 长度为 m 的全 0 向量，代表生成数据的 label</span></span><br><span class=\"line\"><span class=\"comment\"># 1. 生成 m 个噪声作为 G 模型的输入 z</span></span><br><span class=\"line\"><span class=\"comment\"># 2. G 前向传播生成 m 个样本 S</span></span><br><span class=\"line\">S,z,other_layers=g.sample_and_noise(m,</span><br><span class=\"line\">    default_input_include_prob=self.generator_default_input_include_prob,   <span class=\"comment\"># 1</span></span><br><span class=\"line\">    default_input_scale=self.generator_default_input_scale,                 <span class=\"comment\"># 1</span></span><br><span class=\"line\">    all_g_layers=(self.infer_layer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>)                         <span class=\"comment\"># False</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.noise_both !=<span class=\"number\">0</span>:     <span class=\"comment\"># 真实数据和生成数据均添加一个噪声干扰</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"comment\"># D 前向传播，分别得到真实数据的预测 label 和生成数据的预测 label</span></span><br><span class=\"line\">y_hat1 = d.dropout_fprop(...)       <span class=\"comment\"># 参数略</span></span><br><span class=\"line\">y_hat0 = d.dropout_fprop(...)</span><br><span class=\"line\"><span class=\"comment\"># D 的目标损失。d.layers[-1] 为 Sigmoid 层，其目标损失为 KL 散度</span></span><br><span class=\"line\">d_obj = <span class=\"number\">0.5</span>*(d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat1)+d.layers[<span class=\"number\">-1</span>].cost(y0,y_hat0))</span><br><span class=\"line\"><span class=\"comment\"># G 的目标损失。G 希望 D 的判别结果 y_hat0 与真实 label y1 越小越好  </span></span><br><span class=\"line\">g_obj = d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat0)</span><br><span class=\"line\"><span class=\"keyword\">if</span> model.inferer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:       <span class=\"comment\"># 模型推断器</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    i_obj = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> S, d_obj, g_obj, i_obj       <span class=\"comment\"># 返回生成样本，D 损失和 G 损失</span></span><br></pre></td></tr></table></figure>\n<p>再来看计算梯度函数 <code>get_gradients</code> 的实现部分，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator</span><br><span class=\"line\">d=model.generator</span><br><span class=\"line\">S,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   <span class=\"comment\"># 调用上面分析的函数</span></span><br><span class=\"line\">g_params = g.get_params()</span><br><span class=\"line\">d_params = d.get_params()</span><br><span class=\"line\"><span class=\"comment\"># 计算损失对各参数的梯度</span></span><br><span class=\"line\">d_grads = T.grad(d_obj,d_params)</span><br><span class=\"line\">g_grads = T.grad(g_obj,g_params)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.scale_grads:    <span class=\"comment\"># 缩小 g_grads</span></span><br><span class=\"line\">    S_grad = T.grad(g_obj, S)   <span class=\"comment\"># G 损失对生成样本（也就是 G 的输出）的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># S_grad 的平方和的平方根的倒数作为缩小比例</span></span><br><span class=\"line\">    scale = T.maximum(<span class=\"number\">1.</span>,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))</span><br><span class=\"line\">    <span class=\"comment\"># 缩小 g_grads</span></span><br><span class=\"line\">    g_grads = [g_grad * scale <span class=\"keyword\">for</span> g_grad <span class=\"keyword\">in</span> g_grads]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存各模型参数与其对应的梯度</span></span><br><span class=\"line\">rval = OrderDict()</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg <span class=\"keyword\">for</span> dg <span class=\"keyword\">in</span> d_grads])))</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg <span class=\"keyword\">for</span> gg <span class=\"keyword\">in</span> g_grads])))</span><br><span class=\"line\"></span><br><span class=\"line\">updates = OrderDict()</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.alternate_g:</span><br><span class=\"line\">    updates[self.now_train_generator]=<span class=\"number\">1.</span> - self.now_train_generator</span><br><span class=\"line\"><span class=\"keyword\">return</span> rval, updates</span><br></pre></td></tr></table></figure>\n<p>最终的更新操作由 Pylearn2/Theano 库完成。</p>\n<p>以上代码片段中，目标函数为损失，与 log 似然函数相差一个负号，所以上文分析中某些求最大值的地方变为求最小值，然后使用随机梯度下降更新模型参数，这与算法 1 中的情况完成相同。另外，对 <code>g_grads</code> 进行 scale 缩小，一种可能的原因是，</p>\n<p>生成样本 $S=\\theta_g \\cdot z$，损失对 $\\theta_g$ 的梯度满足<br>$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$</p>\n<p>记生成样本 S 经过 D 的输出为 y_0，即，$y_0=\\theta_d \\cdot S$，于是<br>$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$<br>可以看出在计算损失对 G 模型参数的梯度之前，$\\nabla_S L$ 这个梯度已经经过 D 中各层的传播：</p>\n<ol>\n<li><p>如果其 L2 范数大于 1，那么再经过 G 中各层反向传播时，极有可能出现梯度爆炸，即 $\\nabla_{\\theta_g}L$ 很大， 导致训练不稳定，所以需要将其进行 scale 缩小，缩小的比例正好能使 $\\nabla_S L$ 的 L2 范数为指定值 <code>self.target_scale</code>（默认为1）。<br>关于 G 模型参数的梯度过大导致训练不稳定，如下图，<br><img src=\"/images/GAN_fig2.png\" alt=\"\"><center>fig 2. 图来自于网络。左图表示 $G_0$ 时的 V(G,D) 曲线；右图表示 $G_1$ 时的 V(G,D) 曲线。（这个图我觉得有点奇怪，按道理不应该是凸函数吗，以及右图右边的红点不是说明存在合适的 $D_1^{\\ast}$ 吗，用这个图能说明什么问题，我没有搞懂。相反，我倒觉得是用来说明不要更新 G 太多以便可以达到这个图中的效果。如我理解有误，恳请大佬指正~）</center></p>\n<p>上图表示在 $D_0^{\\ast}$ 取得最大值 $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$，然后更新 $G_0$ 为 $G_1$后，由于 G 的更新会降低 V(G,D)，故 $V(G_1,D_0^{\\ast}) &lt; V(G_0,D_0^{\\ast})$，但是此时更新 D 以最大化 V(G,D)，可能会出现 $V(G_1,D_1^{\\ast}) &lt; V(G_0,D_0^{\\ast})$，这意味着判别器 $D_1^{\\ast}$ 的判别能力比之前的 $D_0^{\\ast}$ 的判别能力差，而 G 伪装能力的增强是建立在 D 判别能力的增强这个基础上，否则更新 G 就达不到应该有的效果，所以降低损失对 G 模型参数的梯度，以便不要更新 G 太多，或者多次更新 D 与一次更新 G 交替进行。</p>\n</li>\n<li><p>如果其 L2 范数小于等于1，则对梯度不做 scale 缩小操作。</p>\n</li>\n</ol>\n<p>当然，还有其他损失实现类，具体请查阅源码，不再讨论。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>给定一个预先已知分布的噪声随机变量 z，G 根据 z 生成图像 G(z)，D 将 G(z) 与训练样本区分开来。训练过程根据 (1) 式交替优化 D 和 G，使得 G 尽可能拟合真实数据分布，而 D 提高判别能力，最终 G 分布与真实分布相同，D 无法判别模型分布和真实数据分布。</p>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\" rel=\"noopener\">Generative Adversarial Nets</a></p>","more":"<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>生成对抗网络 GAN：一个生成模型 G 和一个判别模型 D，G 尽可能模拟真实的数据分布，D 尽可能的区分样本是模型生成的还是真实的。下文以图像数据为例说明。</p>\n<p>定义一个输入噪声随机变量 z，其分布为 $p_z(z)$，G 根据 z 生成图像 $G(z;\\theta_g)$，我们假设 G 是一个多层感知机 MLP 网络，网络参数为 $\\theta_g$。D 也是一个 MLP $D(x;\\theta_d)$ 输出是一个标量，表示 x 是真实图像的概率。训练 D 使其对输入 x 预测正确的概率最大化，即当 x 来自真实的训练数据时，$G(x)$ 尽可能大，当 x 来自 G 生成样本时，预测概率 $D(G(z))$ 尽可能小；而训练 G 目的是为了让 $D(G(x))$ 尽可能大，或者说让 $\\log(1-D(G(z)))$ 尽可能小，于是目标函数为，</p>\n<p>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$</p>\n<p>（对 D 而言，这是一个 log 似然函数，D 希望它越大越大，所以求最大值；而 G 却希望 D 的 log 似然函数越小越好，所以求最小值）</p>\n<p>这是一个二人零和博弈。图 1 是训练过程示意图，训练使用迭代的，数值计算的方法。<br><img src=\"/images/GAN_fig1.png\" alt=\"\"></p>\n<p>图 1 中 D 模型分布为蓝色虚线，数据 x 的分布 $p_x$ 为黑色点线，G 模型分布 $p_g$ 为绿色实线（黑绿曲线上某一点分别表示此 x 值处真实数据的概率密度和生成数据的概率密度）。下面的水平线为随机噪声变量 z 的定义域，在其上对 z 均匀采样，上面水平线是 x 的定义域，向上箭头表示映射过程 x=G(z) （G 生成过程）。<br>(a) 是收敛附近的对抗情况：此时 $p_g,\\ p_{data}$ 两者相似，D 分类不完全准确。<br>(b) 在内层循环中，训练 D 判别样本，训练过程收敛于 $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$。<br>(c) D 的梯度可指引 G(z) 移动到更容易被分类为真实数据的区域，即，G 更新后，更加逼近真实数据分布。<br>(d) 经过几次训练，G 和 D 到达一个平衡点，此时 $p_g=p_{data}$，D 无法再区分这两个分布，即，$D(x)=1/2$。</p>\n<p>训练算法如下，<br><img src=\"/images/GAN_alg1.png\" alt=\"\"></p>\n<p>k 次 D 的优化与一次 G 的优化交替进行，这可以使得 G 变化缓慢，而 D 维持在最优解附近。</p>\n<p>实际应用中，(1) 式可能无法提供足够的梯度来更新 G。训练初期，G 性能较差，生成样本与真实训练样本区别较大，所以 D 可以较高的置信度判别，此时，$\\log (1-D(G(z)))$ 达到饱和（log 曲线右端较为平坦），于是我们改为训练 G 以最大化 $\\log D(G(z))$，最终训练能到达相同的 G 和 D 的平衡点，但是训练初期的梯度较大（log 曲线的左端较为陡峭）。</p>\n<h2 id=\"理论分析\"><a href=\"#理论分析\" class=\"headerlink\" title=\"理论分析\"></a>理论分析</h2><p>已知噪声随机变量 z 的分布 $p_z$ 时，可以获得 G 的模型分布，根据算法 1，如果 G 模型的假设空间和训练时间足够，G 可以拟合真实数据分布 $p_{data}$。现在我们来证明 $p_g=p_{data}$ 是 (1) 式的全局最优解。</p>\n<h3 id=\"全局最优解\"><a href=\"#全局最优解\" class=\"headerlink\" title=\"全局最优解\"></a>全局最优解</h3><p><strong>Proposition 1.</strong> 对于任意的 G，D 的最优解为<br>$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$<br><strong>证明：</strong>  给定任意 G，D 的训练准则是最大化 V(G,D)<br>$$\\begin{aligned} V(G,D)&amp;=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz<br>\\\\ &amp;=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$<br>$\\forall (a,b) \\in \\Bbb R^2 \\setminus {0,0}$，函数 $y \\rightarrow a \\log y+b \\log(1-y)$ 在 (0,1) 区间上当 $y=\\frac a {a+b}$ 时有最大值（梯度为 0 求解得到），所以要使得 V(G,D) 最大，那么对于每个 x 值，都要使 D(x) 达到最大，即 (2) 式。证毕。</p>\n<p>D 的训练目标函数可以看作是条件概率 $P(Y=y|x)$ 的最大 log 似然函数（或者是最小化 binary cross-entropy），其中当 x 来自 $p_{data}$ 时 y=1，当 x 来自 $p_g$ 时 y=0。得到 D 的最优解 $D_G^{\\ast}$ 后 (1) 式变为，  </p>\n<p>$$\\begin{aligned} C(G)&amp;=\\max_D V(G,D)<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$</p>\n<p><strong>Theorem 1.</strong> 当且仅当 $p_g=p_{data}$ 时， C(G) 有全局最优解 -log4。  </p>\n<p><strong>证明：</strong> </p>\n<ol>\n<li>充分性<br>令 $p_g=p_{data}$，根据 (2) 式有 $D_G^{\\ast}(x)=1/2$，然后根据 (4) 式有，<br>$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$</li>\n<li>必要性<br>$$\\begin{aligned}C(G)&amp;=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\ &amp;=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\ &amp;=-\\log4+KL \\left(p_{data} | \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g | \\frac {p_{data}+p_g} 2 \\right) \\\\ &amp;=-\\log4 + 2\\cdot JSD(p_{data} | p_g) \\end{aligned}$$<br>其中 KL 表示 Kullback-Leibler 散度，JSD 表示 Jensen-Shannon 散度。由于 JSD 非负，且仅在 $p_g=p_{data}$ 时取得最小值 0，所以 C(G)=-log4 时，$p_g=p_{data}$。  </li>\n</ol>\n<p>证毕。</p>\n<h3 id=\"算法-1-的收敛\"><a href=\"#算法-1-的收敛\" class=\"headerlink\" title=\"算法 1 的收敛\"></a>算法 1 的收敛</h3><p>上一小节我们分析了全局最优解是存在的，并且取得全局最优解的条件是 $p_g=p_{data}$。<strong>Proposition 2</strong> 表明基于算法 1 的更新是有效的，训练可以收敛到全局最优解。</p>\n<p><strong>Proposition 2.</strong> 如果 G 和 D 有足够的模型空间，且在算法 1 每次迭代中给定 G 的情况下判别器可以达到最优解，且以调优（使更小） G 的训练标准 C(G) 更新 $p_g$<br>$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$<br>那么，$p_g$ 趋于 $p_{data}$。</p>\n<p><strong>证明：</strong></p>\n<p>考虑 $V(G,D)=U(p_g,D)$ 是 $p_g$ 的函数，$p_g$ 可根据 (5) 式标准进行优化。注意到 $U(p_g,D)$ 是 $p_g$ （定义域）上的凸函数，不同 D 形成的凸函数集合的上确界（它也是一个凸函数）的 <strong>次导数</strong> 包含了此凸函数集合在某个 D 值取得最大值所对应函数的导数，也就是说，给定任意 $p_g$（它是函数自变量），D 是可变参数，（在任意自变量 $p_g$ 处）上述结论均成立。用数学语言描述就是：</p>\n<ul>\n<li>如果 $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$，且 $f_{\\alpha}(x)$ 对任意 $\\alpha$ 在 x 上均为凸，那么当 $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ 时有 $\\partial f_{\\beta}(x) \\in \\partial f(x)$。</li>\n</ul>\n<p>$V(G,D)=U(p_g,D)$ 相当于上述的上确界函数，不能保证在 $p_g$ 定义域上处处严格可导，但是这个上确界函数也是一个凸函数，保证了其具有全局唯一最优解。而上面这个结论 “在任意 $p_g$ 处，其次导数包含了在某个 D 值取得最大值所对应函数的导数”，即，“包含了在 D 取最优解 D* 时 V(G,D) 的导数”，而这个导数正是对 (5) 式求导，于是可以使用这个导数进行梯度上升/下降法更新 $p_g$，并且这个更新将会使得 $p_g$ 趋于 $p_{data}$（参考 Theorem 1）。证毕</p>\n<p>对 (5) 式求导与算法 1 中的梯度本质相同，只是似然函数的期望改为批 SGD 中各样本损失的均值（没办法，数值计算使然），注意第一个期望在更新 $p_g$ 时不起作用，为什么这么讲？因为更新 $p_g$ 时，D 已经被固定，此时第一个期望与 $p_g$ 无关。</p>\n<p>实际应用中，对抗网络使用 $G(z;\\theta_g)$ 表示 $p_g$ 的分布，其中 $\\theta_g$ 是 G 模型参数，在选定 G 的网络模型如 MLP 时，$\\theta_g$ 就决定了 $p_g$ 的分布，故以上有所对 $p_g$ 的更新其实都转为对  $\\theta_g$ 的更新，例如，使用 MLP 作为 G 的模型，目标函数 (1) 式中的 $p_g$ 分布替换为某个 batch 中的生成样本分布，$p_{data}$ 则替换为 batch 中的真实样本分布，简单点说，目标函数 (1) 变为 batch 中所有样本的 log-likelihood function 的均值，包含真实数据和生成数据两部分的log 似然函数，具体可参见下文的代码分析。</p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验介绍和结果分析略。在这里，我们重点看一下源码 <a href=\"http://www.github.com/goodfeli/adversarial\" target=\"_blank\" rel=\"noopener\">adversarial</a></p>\n<blockquote>\n<p>声明：本源码使用库 Theano 和 Pylearn2，而我从来没接触过这两个库，代码分析全凭函数名、变量名和类名等。github 上也有 GAN 的其他实现如 <a href=\"https://github.com/wiseodd/generative-models\" target=\"_blank\" rel=\"noopener\">generative-models</a>，代码通俗易懂，读者可自行查阅。</p>\n</blockquote>\n<p>从 github 上 clone 这个仓库，进入 adversarial 本项目的根目录。以 mnist 数据集为例说明。</p>\n<p>首先看下 mnist.yaml 这个文件，</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">!obj</span><span class=\"string\">:pylearn2.train.Train</span> <span class=\"string\">&#123;</span>         <span class=\"comment\"># 训练配置</span></span><br><span class=\"line\">    <span class=\"attr\">dataset:</span> <span class=\"string\">&amp;train</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.datasets.mnist.MNIST</span> <span class=\"string\">&#123;</span>    <span class=\"comment\"># 训练使用 mnist 数据集</span></span><br><span class=\"line\">        <span class=\"attr\">which_set:</span> <span class=\"string\">'train'</span><span class=\"string\">,</span>                                 <span class=\"comment\"># 使用 train 数据的前 50000 条</span></span><br><span class=\"line\">        <span class=\"attr\">start:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">        <span class=\"attr\">stop:</span> <span class=\"number\">50000</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"attr\">model:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryPair</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># GAN：G &amp; D</span></span><br><span class=\"line\">        <span class=\"attr\">generator:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.Generator</span> <span class=\"string\">&#123;</span>             <span class=\"comment\"># G</span></span><br><span class=\"line\">            <span class=\"attr\">noise:</span> <span class=\"string\">'uniform'</span><span class=\"string\">,</span>                               <span class=\"comment\"># noise 分布使用均匀分布</span></span><br><span class=\"line\">            <span class=\"attr\">monitor_ll:</span> <span class=\"number\">1</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"attr\">mlp:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">            <span class=\"attr\">layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.RectifiedLinear</span> <span class=\"string\">&#123;</span> <span class=\"comment\"># 带 ReLu 的 FC 层</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'h0'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">1200</span><span class=\"string\">,</span>                             <span class=\"comment\"># 本层 output units 数量</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;,</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span>     <span class=\"comment\"># FC 层后接 sigmoid</span></span><br><span class=\"line\">                         <span class=\"attr\">init_bias:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.dbm.init_sigmoid_bias_from_marginals</span> <span class=\"string\">&#123;</span> <span class=\"attr\">dataset:</span> <span class=\"string\">*train&#125;,</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">784</span>                               <span class=\"comment\"># 784=28x28，为 mnist 单个样本大小</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\">            <span class=\"attr\">nvis:</span> <span class=\"number\">100</span><span class=\"string\">,</span>                                          <span class=\"comment\"># G 的噪声随机变量的向量维度</span></span><br><span class=\"line\">        <span class=\"string\">&#125;&#125;,</span></span><br><span class=\"line\">        <span class=\"attr\">discriminator:</span>                                          <span class=\"comment\"># D</span></span><br><span class=\"line\">            <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">            <span class=\"attr\">layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                         <span class=\"attr\">layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\">                         <span class=\"attr\">dim:</span> <span class=\"number\">1</span><span class=\"string\">,</span>                                <span class=\"comment\"># 输出为标量</span></span><br><span class=\"line\">                         <span class=\"attr\">irange:</span> <span class=\"number\">.005</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\">            <span class=\"attr\">nvis:</span> <span class=\"number\">784</span><span class=\"string\">,</span>                                          <span class=\"comment\"># 输入向量维度</span></span><br><span class=\"line\">        <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"attr\">algorithm:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.training_algorithms.sgd.SGD</span> <span class=\"string\">&#123;</span>      <span class=\"comment\"># 优化算法</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">        <span class=\"attr\">cost:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryCost2</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># 损失实现类</span></span><br><span class=\"line\">            <span class=\"attr\">scale_grads:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"comment\">#target_scale: 1.,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_default_input_include_prob:</span> <span class=\"number\">.5</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_input_include_probs:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                <span class=\"attr\">'h0':</span> <span class=\"number\">.8</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_default_input_scale:</span> <span class=\"number\">2</span><span class=\"string\">.,</span></span><br><span class=\"line\">            <span class=\"attr\">discriminator_input_scales:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\">                <span class=\"attr\">'h0':</span> <span class=\"number\">1.25</span>   </span><br><span class=\"line\">            <span class=\"string\">&#125;</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br></pre></td></tr></table></figure>\n<p>可以明显知道，训练使用 mnist 的 <code>train</code> 数据集中前 50000 个数据，模型类实现为 adversarial.AdversaryPair，生成器类为 adversarial.Generator，其内部封装了一个 MLP，判别器类直接使用 MLP。损失实现类为 adversarial.AdversaryCost2。这些类的实现均位于 <code>__init__.py</code> 中。这里主要分析一下 AdversaryCost2（其他类的实现均比较简单明了）。</p>\n<p>首先看一下生成样本和目标函数 <code>get_samples_and_objectives</code>，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator       <span class=\"comment\"># model is an instance of AdversaryPair</span></span><br><span class=\"line\">d=model.discriminator</span><br><span class=\"line\">X=data                  <span class=\"comment\"># 真实数据（来自训练样本）的 batch</span></span><br><span class=\"line\">m=data.shape[space.get_batch_axis()]    <span class=\"comment\"># 获取 batch 的大小，即批样本数量</span></span><br><span class=\"line\">y1=T.alloc(<span class=\"number\">1</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\"># 长度为 m 的全 1 向量，代表真实数据的 label</span></span><br><span class=\"line\">y0=T.alloc(<span class=\"number\">0</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\"># 长度为 m 的全 0 向量，代表生成数据的 label</span></span><br><span class=\"line\"><span class=\"comment\"># 1. 生成 m 个噪声作为 G 模型的输入 z</span></span><br><span class=\"line\"><span class=\"comment\"># 2. G 前向传播生成 m 个样本 S</span></span><br><span class=\"line\">S,z,other_layers=g.sample_and_noise(m,</span><br><span class=\"line\">    default_input_include_prob=self.generator_default_input_include_prob,   <span class=\"comment\"># 1</span></span><br><span class=\"line\">    default_input_scale=self.generator_default_input_scale,                 <span class=\"comment\"># 1</span></span><br><span class=\"line\">    all_g_layers=(self.infer_layer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>)                         <span class=\"comment\"># False</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.noise_both !=<span class=\"number\">0</span>:     <span class=\"comment\"># 真实数据和生成数据均添加一个噪声干扰</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"comment\"># D 前向传播，分别得到真实数据的预测 label 和生成数据的预测 label</span></span><br><span class=\"line\">y_hat1 = d.dropout_fprop(...)       <span class=\"comment\"># 参数略</span></span><br><span class=\"line\">y_hat0 = d.dropout_fprop(...)</span><br><span class=\"line\"><span class=\"comment\"># D 的目标损失。d.layers[-1] 为 Sigmoid 层，其目标损失为 KL 散度</span></span><br><span class=\"line\">d_obj = <span class=\"number\">0.5</span>*(d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat1)+d.layers[<span class=\"number\">-1</span>].cost(y0,y_hat0))</span><br><span class=\"line\"><span class=\"comment\"># G 的目标损失。G 希望 D 的判别结果 y_hat0 与真实 label y1 越小越好  </span></span><br><span class=\"line\">g_obj = d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat0)</span><br><span class=\"line\"><span class=\"keyword\">if</span> model.inferer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:       <span class=\"comment\"># 模型推断器</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    i_obj = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> S, d_obj, g_obj, i_obj       <span class=\"comment\"># 返回生成样本，D 损失和 G 损失</span></span><br></pre></td></tr></table></figure>\n<p>再来看计算梯度函数 <code>get_gradients</code> 的实现部分，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator</span><br><span class=\"line\">d=model.generator</span><br><span class=\"line\">S,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   <span class=\"comment\"># 调用上面分析的函数</span></span><br><span class=\"line\">g_params = g.get_params()</span><br><span class=\"line\">d_params = d.get_params()</span><br><span class=\"line\"><span class=\"comment\"># 计算损失对各参数的梯度</span></span><br><span class=\"line\">d_grads = T.grad(d_obj,d_params)</span><br><span class=\"line\">g_grads = T.grad(g_obj,g_params)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.scale_grads:    <span class=\"comment\"># 缩小 g_grads</span></span><br><span class=\"line\">    S_grad = T.grad(g_obj, S)   <span class=\"comment\"># G 损失对生成样本（也就是 G 的输出）的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># S_grad 的平方和的平方根的倒数作为缩小比例</span></span><br><span class=\"line\">    scale = T.maximum(<span class=\"number\">1.</span>,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))</span><br><span class=\"line\">    <span class=\"comment\"># 缩小 g_grads</span></span><br><span class=\"line\">    g_grads = [g_grad * scale <span class=\"keyword\">for</span> g_grad <span class=\"keyword\">in</span> g_grads]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存各模型参数与其对应的梯度</span></span><br><span class=\"line\">rval = OrderDict()</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg <span class=\"keyword\">for</span> dg <span class=\"keyword\">in</span> d_grads])))</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg <span class=\"keyword\">for</span> gg <span class=\"keyword\">in</span> g_grads])))</span><br><span class=\"line\"></span><br><span class=\"line\">updates = OrderDict()</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.alternate_g:</span><br><span class=\"line\">    updates[self.now_train_generator]=<span class=\"number\">1.</span> - self.now_train_generator</span><br><span class=\"line\"><span class=\"keyword\">return</span> rval, updates</span><br></pre></td></tr></table></figure>\n<p>最终的更新操作由 Pylearn2/Theano 库完成。</p>\n<p>以上代码片段中，目标函数为损失，与 log 似然函数相差一个负号，所以上文分析中某些求最大值的地方变为求最小值，然后使用随机梯度下降更新模型参数，这与算法 1 中的情况完成相同。另外，对 <code>g_grads</code> 进行 scale 缩小，一种可能的原因是，</p>\n<p>生成样本 $S=\\theta_g \\cdot z$，损失对 $\\theta_g$ 的梯度满足<br>$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$</p>\n<p>记生成样本 S 经过 D 的输出为 y_0，即，$y_0=\\theta_d \\cdot S$，于是<br>$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$<br>可以看出在计算损失对 G 模型参数的梯度之前，$\\nabla_S L$ 这个梯度已经经过 D 中各层的传播：</p>\n<ol>\n<li><p>如果其 L2 范数大于 1，那么再经过 G 中各层反向传播时，极有可能出现梯度爆炸，即 $\\nabla_{\\theta_g}L$ 很大， 导致训练不稳定，所以需要将其进行 scale 缩小，缩小的比例正好能使 $\\nabla_S L$ 的 L2 范数为指定值 <code>self.target_scale</code>（默认为1）。<br>关于 G 模型参数的梯度过大导致训练不稳定，如下图，<br><img src=\"/images/GAN_fig2.png\" alt=\"\"><center>fig 2. 图来自于网络。左图表示 $G_0$ 时的 V(G,D) 曲线；右图表示 $G_1$ 时的 V(G,D) 曲线。（这个图我觉得有点奇怪，按道理不应该是凸函数吗，以及右图右边的红点不是说明存在合适的 $D_1^{\\ast}$ 吗，用这个图能说明什么问题，我没有搞懂。相反，我倒觉得是用来说明不要更新 G 太多以便可以达到这个图中的效果。如我理解有误，恳请大佬指正~）</center></p>\n<p>上图表示在 $D_0^{\\ast}$ 取得最大值 $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$，然后更新 $G_0$ 为 $G_1$后，由于 G 的更新会降低 V(G,D)，故 $V(G_1,D_0^{\\ast}) &lt; V(G_0,D_0^{\\ast})$，但是此时更新 D 以最大化 V(G,D)，可能会出现 $V(G_1,D_1^{\\ast}) &lt; V(G_0,D_0^{\\ast})$，这意味着判别器 $D_1^{\\ast}$ 的判别能力比之前的 $D_0^{\\ast}$ 的判别能力差，而 G 伪装能力的增强是建立在 D 判别能力的增强这个基础上，否则更新 G 就达不到应该有的效果，所以降低损失对 G 模型参数的梯度，以便不要更新 G 太多，或者多次更新 D 与一次更新 G 交替进行。</p>\n</li>\n<li><p>如果其 L2 范数小于等于1，则对梯度不做 scale 缩小操作。</p>\n</li>\n</ol>\n<p>当然，还有其他损失实现类，具体请查阅源码，不再讨论。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>给定一个预先已知分布的噪声随机变量 z，G 根据 z 生成图像 G(z)，D 将 G(z) 与训练样本区分开来。训练过程根据 (1) 式交替优化 D 和 G，使得 G 尽可能拟合真实数据分布，而 D 提高判别能力，最终 G 分布与真实分布相同，D 无法判别模型分布和真实数据分布。</p>"},{"title":"Dynamic Programming (3)","date":"2019-12-20T12:13:18.000Z","p":"dp/DP4","mathjax":true,"_content":"\n## 1. 库存问题\n一产品可通过生产或者购买获得，获得产品需要一定的成本，然后产品随着时间也会逐步被消耗掉。产品的库存也伴随着存储成本，而当需求未得到满足（供应不足，某个阶段库存为负表示消耗需求未得到满足）时也有一个惩罚损失。\n\n<!-- more -->\n\n使用 N-阶 序列决策过程来表述以上库存问题，在阶段 `k` 作出决策获得 `x` 单位的产品，损耗为 $C(k,x)$，状态为 $(k,s)$，表示在阶段 `k` 的库存数量为 `s`。$D(k)$ 表示在阶段 `k` 对产品的消耗需求，于是下一阶段状态为 $(k+1,s+x-D(k))$，这一状态转移过程的损耗包括获得产品的成本 $C(k,x)$，以及库存存储成本 $I(k,s,x), \\ s>0$，如果库存数量为负表示此阶段的消耗需求未得到满足，此时 $|s|$ 表示缺货的数量，所以惩罚损耗为 $I(k,s,x), \\ s < 0$，","source":"_posts/dp/DP4.md","raw":"---\ntitle: Dynamic Programming (3)\ndate: 2019-12-20 20:13:18\np: dp/DP4\ntags:\n    - math\n    - DP\nmathjax: true\n---\n\n## 1. 库存问题\n一产品可通过生产或者购买获得，获得产品需要一定的成本，然后产品随着时间也会逐步被消耗掉。产品的库存也伴随着存储成本，而当需求未得到满足（供应不足，某个阶段库存为负表示消耗需求未得到满足）时也有一个惩罚损失。\n\n<!-- more -->\n\n使用 N-阶 序列决策过程来表述以上库存问题，在阶段 `k` 作出决策获得 `x` 单位的产品，损耗为 $C(k,x)$，状态为 $(k,s)$，表示在阶段 `k` 的库存数量为 `s`。$D(k)$ 表示在阶段 `k` 对产品的消耗需求，于是下一阶段状态为 $(k+1,s+x-D(k))$，这一状态转移过程的损耗包括获得产品的成本 $C(k,x)$，以及库存存储成本 $I(k,s,x), \\ s>0$，如果库存数量为负表示此阶段的消耗需求未得到满足，此时 $|s|$ 表示缺货的数量，所以惩罚损耗为 $I(k,s,x), \\ s < 0$，","slug":"dp/DP4","published":1,"updated":"2020-04-24T10:33:06.640Z","_id":"ck9dzciuz001ngga6db7ecnq5","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-库存问题\"><a href=\"#1-库存问题\" class=\"headerlink\" title=\"1. 库存问题\"></a>1. 库存问题</h2><p>一产品可通过生产或者购买获得，获得产品需要一定的成本，然后产品随着时间也会逐步被消耗掉。产品的库存也伴随着存储成本，而当需求未得到满足（供应不足，某个阶段库存为负表示消耗需求未得到满足）时也有一个惩罚损失。</p>\n<a id=\"more\"></a>\n\n<p>使用 N-阶 序列决策过程来表述以上库存问题，在阶段 <code>k</code> 作出决策获得 <code>x</code> 单位的产品，损耗为 $C(k,x)$，状态为 $(k,s)$，表示在阶段 <code>k</code> 的库存数量为 <code>s</code>。$D(k)$ 表示在阶段 <code>k</code> 对产品的消耗需求，于是下一阶段状态为 $(k+1,s+x-D(k))$，这一状态转移过程的损耗包括获得产品的成本 $C(k,x)$，以及库存存储成本 $I(k,s,x), \\ s&gt;0$，如果库存数量为负表示此阶段的消耗需求未得到满足，此时 $|s|$ 表示缺货的数量，所以惩罚损耗为 $I(k,s,x), \\ s &lt; 0$，</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-库存问题\"><a href=\"#1-库存问题\" class=\"headerlink\" title=\"1. 库存问题\"></a>1. 库存问题</h2><p>一产品可通过生产或者购买获得，获得产品需要一定的成本，然后产品随着时间也会逐步被消耗掉。产品的库存也伴随着存储成本，而当需求未得到满足（供应不足，某个阶段库存为负表示消耗需求未得到满足）时也有一个惩罚损失。</p>","more":"<p>使用 N-阶 序列决策过程来表述以上库存问题，在阶段 <code>k</code> 作出决策获得 <code>x</code> 单位的产品，损耗为 $C(k,x)$，状态为 $(k,s)$，表示在阶段 <code>k</code> 的库存数量为 <code>s</code>。$D(k)$ 表示在阶段 <code>k</code> 对产品的消耗需求，于是下一阶段状态为 $(k+1,s+x-D(k))$，这一状态转移过程的损耗包括获得产品的成本 $C(k,x)$，以及库存存储成本 $I(k,s,x), \\ s&gt;0$，如果库存数量为负表示此阶段的消耗需求未得到满足，此时 $|s|$ 表示缺货的数量，所以惩罚损耗为 $I(k,s,x), \\ s &lt; 0$，</p>"},{"title":"PyTorch-5","p":"pytorch/PyTorch-5","date":"2019-08-27T06:07:27.000Z","_content":"本篇主要分析 PyTorch 中自动求导是如何进行的。要使得能够自动求导，需要设置 Tensor 的 `requires_grad=True`，例如\n<!-- more -->\n```python\nx=torch.ones(1, requires_grad=True)\n```\n根据前面 torch.empty 的底层 C++ 实现的分析，易知 torch.ones 的 C++ 底层实现由位于 torch/csrc/autograd/generated/python_torch_functions.cpp 中的 THPVariable_ones 函数实现，此函数定义的部分代码为\n```c++\nauto size = r.intlist(0);\nauto dtype = r.scalartype(2);\nauto device = r.device(4);\nconst auto options = TensorOptions()\n    .dtype(dtype)\n    .device(device)\n    .layout(r.layout(3).layout)\n    .requires_grad(r.toBool(5));\nreturn wrap(dispatch_ones(size, options));\n```\n`wrap` 则是将 C++ 的 Tensor 包装为 python 的 Tensor 类型 `torch.Tensor`。dispatch_ones 函数其内部调用 torch::ones 函数，此函数定义的关键部分为\n```c++\nat::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(false));\nauto result = autograd::make_variable(tensor, options.requires_grad());\n```\n上面代码第一句是构造全 1 的 Tensor，第二句代码则是将 Tensor 转为 Variable。Variable 继承 Tensor，Tensor 内部有 c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl> 类型字段 `impl_`，Variable::Impl 类型继承 TensorImpl，并且 Variable 的字段 `impl_` 实际上相当于指向 Variable::Impl 的指针类型，而 Variable::Impl 内部包含了字段 `requires_grad_` 记录了 Variable 是否支持自动求导。\n\n```python\ny=torch.ones(1) # y.requires_grad False\nz=x+y           # z.requires_grad True\n```\n\ntorch.Tensor 的基类 torch._C._TensorBase 的方法位于 torch/csrc/autograd/generated/python_variable_methods.cpp 中的 variable_methods，其中我们发现重载运算符 `__add__` 的实现函数为 THPVariable_add，调用栈为\n```\nTHPVariable_add -> dispatch_add -> Tensor::add -> TypeDefault::add -> native::add -> native::add_out\n```\n\n\n","source":"_posts/pytorch/PyTorch-5.md","raw":"---\ntitle: PyTorch-5\np: pytorch/PyTorch-5\ndate: 2019-08-27 14:07:27\ntags: PyTorch\ncategories: DL Framework\n---\n本篇主要分析 PyTorch 中自动求导是如何进行的。要使得能够自动求导，需要设置 Tensor 的 `requires_grad=True`，例如\n<!-- more -->\n```python\nx=torch.ones(1, requires_grad=True)\n```\n根据前面 torch.empty 的底层 C++ 实现的分析，易知 torch.ones 的 C++ 底层实现由位于 torch/csrc/autograd/generated/python_torch_functions.cpp 中的 THPVariable_ones 函数实现，此函数定义的部分代码为\n```c++\nauto size = r.intlist(0);\nauto dtype = r.scalartype(2);\nauto device = r.device(4);\nconst auto options = TensorOptions()\n    .dtype(dtype)\n    .device(device)\n    .layout(r.layout(3).layout)\n    .requires_grad(r.toBool(5));\nreturn wrap(dispatch_ones(size, options));\n```\n`wrap` 则是将 C++ 的 Tensor 包装为 python 的 Tensor 类型 `torch.Tensor`。dispatch_ones 函数其内部调用 torch::ones 函数，此函数定义的关键部分为\n```c++\nat::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(false));\nauto result = autograd::make_variable(tensor, options.requires_grad());\n```\n上面代码第一句是构造全 1 的 Tensor，第二句代码则是将 Tensor 转为 Variable。Variable 继承 Tensor，Tensor 内部有 c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl> 类型字段 `impl_`，Variable::Impl 类型继承 TensorImpl，并且 Variable 的字段 `impl_` 实际上相当于指向 Variable::Impl 的指针类型，而 Variable::Impl 内部包含了字段 `requires_grad_` 记录了 Variable 是否支持自动求导。\n\n```python\ny=torch.ones(1) # y.requires_grad False\nz=x+y           # z.requires_grad True\n```\n\ntorch.Tensor 的基类 torch._C._TensorBase 的方法位于 torch/csrc/autograd/generated/python_variable_methods.cpp 中的 variable_methods，其中我们发现重载运算符 `__add__` 的实现函数为 THPVariable_add，调用栈为\n```\nTHPVariable_add -> dispatch_add -> Tensor::add -> TypeDefault::add -> native::add -> native::add_out\n```\n\n\n","slug":"pytorch/PyTorch-5","published":1,"updated":"2020-04-24T10:34:51.985Z","_id":"ck9dzciv7001pgga66dc248n0","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本篇主要分析 PyTorch 中自动求导是如何进行的。要使得能够自动求导，需要设置 Tensor 的 <code>requires_grad=True</code>，例如</p>\n<a id=\"more\"></a>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.ones(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>根据前面 torch.empty 的底层 C++ 实现的分析，易知 torch.ones 的 C++ 底层实现由位于 torch/csrc/autograd/generated/python_torch_functions.cpp 中的 THPVariable_ones 函数实现，此函数定义的部分代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> <span class=\"built_in\">size</span> = r.intlist(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = r.scalartype(<span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> device = r.device(<span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">auto</span> options = TensorOptions()</span><br><span class=\"line\">    .dtype(dtype)</span><br><span class=\"line\">    .device(device)</span><br><span class=\"line\">    .layout(r.layout(<span class=\"number\">3</span>).layout)</span><br><span class=\"line\">    .requires_grad(r.toBool(<span class=\"number\">5</span>));</span><br><span class=\"line\"><span class=\"keyword\">return</span> wrap(dispatch_ones(<span class=\"built_in\">size</span>, options));</span><br></pre></td></tr></table></figure>\n<p><code>wrap</code> 则是将 C++ 的 Tensor 包装为 python 的 Tensor 类型 <code>torch.Tensor</code>。dispatch_ones 函数其内部调用 torch::ones 函数，此函数定义的关键部分为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::Tensor tensor = at::ones(<span class=\"built_in\">size</span>, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\"><span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad());</span><br></pre></td></tr></table></figure>\n<p>上面代码第一句是构造全 1 的 Tensor，第二句代码则是将 Tensor 转为 Variable。Variable 继承 Tensor，Tensor 内部有 c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; 类型字段 <code>impl_</code>，Variable::Impl 类型继承 TensorImpl，并且 Variable 的字段 <code>impl_</code> 实际上相当于指向 Variable::Impl 的指针类型，而 Variable::Impl 内部包含了字段 <code>requires_grad_</code> 记录了 Variable 是否支持自动求导。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y=torch.ones(<span class=\"number\">1</span>) <span class=\"comment\"># y.requires_grad False</span></span><br><span class=\"line\">z=x+y           <span class=\"comment\"># z.requires_grad True</span></span><br></pre></td></tr></table></figure>\n\n<p>torch.Tensor 的基类 torch._C._TensorBase 的方法位于 torch/csrc/autograd/generated/python_variable_methods.cpp 中的 variable_methods，其中我们发现重载运算符 <code>__add__</code> 的实现函数为 THPVariable_add，调用栈为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPVariable_add -&gt; dispatch_add -&gt; Tensor::add -&gt; TypeDefault::add -&gt; native::add -&gt; native::add_out</span><br></pre></td></tr></table></figure>\n\n\n","site":{"data":{}},"excerpt":"<p>本篇主要分析 PyTorch 中自动求导是如何进行的。要使得能够自动求导，需要设置 Tensor 的 <code>requires_grad=True</code>，例如</p>","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.ones(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>根据前面 torch.empty 的底层 C++ 实现的分析，易知 torch.ones 的 C++ 底层实现由位于 torch/csrc/autograd/generated/python_torch_functions.cpp 中的 THPVariable_ones 函数实现，此函数定义的部分代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> <span class=\"built_in\">size</span> = r.intlist(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = r.scalartype(<span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> device = r.device(<span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">auto</span> options = TensorOptions()</span><br><span class=\"line\">    .dtype(dtype)</span><br><span class=\"line\">    .device(device)</span><br><span class=\"line\">    .layout(r.layout(<span class=\"number\">3</span>).layout)</span><br><span class=\"line\">    .requires_grad(r.toBool(<span class=\"number\">5</span>));</span><br><span class=\"line\"><span class=\"keyword\">return</span> wrap(dispatch_ones(<span class=\"built_in\">size</span>, options));</span><br></pre></td></tr></table></figure>\n<p><code>wrap</code> 则是将 C++ 的 Tensor 包装为 python 的 Tensor 类型 <code>torch.Tensor</code>。dispatch_ones 函数其内部调用 torch::ones 函数，此函数定义的关键部分为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::Tensor tensor = at::ones(<span class=\"built_in\">size</span>, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\"><span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad());</span><br></pre></td></tr></table></figure>\n<p>上面代码第一句是构造全 1 的 Tensor，第二句代码则是将 Tensor 转为 Variable。Variable 继承 Tensor，Tensor 内部有 c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; 类型字段 <code>impl_</code>，Variable::Impl 类型继承 TensorImpl，并且 Variable 的字段 <code>impl_</code> 实际上相当于指向 Variable::Impl 的指针类型，而 Variable::Impl 内部包含了字段 <code>requires_grad_</code> 记录了 Variable 是否支持自动求导。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y=torch.ones(<span class=\"number\">1</span>) <span class=\"comment\"># y.requires_grad False</span></span><br><span class=\"line\">z=x+y           <span class=\"comment\"># z.requires_grad True</span></span><br></pre></td></tr></table></figure>\n\n<p>torch.Tensor 的基类 torch._C._TensorBase 的方法位于 torch/csrc/autograd/generated/python_variable_methods.cpp 中的 variable_methods，其中我们发现重载运算符 <code>__add__</code> 的实现函数为 THPVariable_add，调用栈为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPVariable_add -&gt; dispatch_add -&gt; Tensor::add -&gt; TypeDefault::add -&gt; native::add -&gt; native::add_out</span><br></pre></td></tr></table></figure>"},{"title":"PyTorch.optim","p":"pytorch/optim-2","date":"2020-01-08T10:19:54.000Z","mathjax":true,"_content":"\n# 1. Adam\nAdam 表示 Adaptive Moment Estimation。\n<!-- more -->\n## 1.1 原理\n梯度和梯度平方的衰减如下，\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n\\\\\\\\ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\qquad(1)$$\n其中 $\\beta_1 < 1, \\ \\beta_2 < 1$，$m_t$ 和 $v_t$ 分别是梯度 $g$ 的一阶矩和二阶矩的样本估计（$g$ 看作随机变量）。由于 $m$ 和 $v$ 均初始化为 0，即 $m_0=0, \\ v_0 = 0$，所以这两个样本估计均是有偏估计，且偏向 0，尤其在刚开始的时间步（t 较小）和衰减率较小时（$1-\\beta$ 较小，$\\beta$ 接近 1）。\n\n令 $E(g)=\\mu$，$g_1, g_2, ...$ 来自于 $g$ 且独立同分布，那么\n$$E(m_t)=E\\left(\\sum_{\\tau=1}^t \\beta_1^{t-\\tau} (1-\\beta_1) g_{\\tau}\\right)=(1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}E(g_{\\tau})=\\mu (1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}=\\mu(1-\\beta_1^t)$$ \n可见，当 t 较小且 $\\beta_1 \\rightarrow 1$，$E(m_t) \\rightarrow 0$\n\n为了抵消这些偏向，取以下计算进行校正，\n$$\\hat m_t=\\frac {m_t} {1-\\beta_1^t}\n\\\\\\\\ \\hat v_t = \\frac {v_t} {1-\\beta_2^t}$$\n\n其中 上标 `t` 表示指数，即 `t` 个 $\\beta$ 相乘。 通过上面的分析，可知，除以 $1-\\beta^t$ 后，$E(\\hat m_t)=\\mu$，为无偏估计。\n\n然后类似 Adadelta 和 RMSprop 中那样，更新公式为，\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t \\qquad(2)$$\n其中 $\\eta$ 为初始学习率，是一个初始时给定的超参数。\n\n## 1.2 AMSGrad 变体\n修改 $v$ 的计算式如下，\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n\\\\\\\\ v_{t,m} = \\max\\left(v_{t-1,m}, \\ \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\right)$$\n其中 $v_{0,m}=0$。\n\n然后 $v$ 的无偏估计改为，\n$$\\hat v_{t,m}=\\frac {v_{t,m}} {1-\\beta_2^t}$$\n参数更新公式调整为，\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_{t,m}}+\\epsilon} \\hat m_t \\qquad(3)$$\n其中 $\\hat m_t$ 的计算部分与前面的保持一致。\n\nAMSGrad 比 Adam 降低了学习率。\n\n# 2. Adamax\n在 Adam 的基础上将 (1) 式泛化，不局限于 $l_2$ 范数，如下\n$$v_t = \\beta_2^p v_{t-1} + (1-\\beta_2^p)|g_t|^p$$\n其中注意 $p$ 为指数。\n\n将上式中的 $v_{t-1}$ 展开，\n$$\\begin{aligned} v_t  &= (1-\\beta_2^p)|g_t|^p + \\beta_2^p[(1-\\beta_2^p)|g_{t-1}|^p+\\beta_2^p v_{t-2}]\n\\\\\\\\ & = (1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\n\\end{aligned}$$\n\n令 $p \\rightarrow \\infin$，并定义 $u_t = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p}$，结合上式有，\n$$\\begin{aligned} u_t  = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p} &= \\lim_{p \\rightarrow \\infin}\\left((1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &= \\lim_{p \\rightarrow \\infin} (1-\\beta_2^p)^{1/p} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &= \\lim_{p \\rightarrow \\infin} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &=\\max (\\beta_2^{t-1}|g_1|,\\beta_2^{t-2}|g_2|,...,\\beta_2^{0}|g_t|)\\end{aligned}$$\n\n于是可得以下迭代公式，\n$$u_t = \\max(\\beta_2 u_{t-1}, \\ |g_t|)$$\n其中初始值 $u_0=0$。\n\n用 $u_t$ 替换 Adam 中的 $\\sqrt{\\hat v_t}+\\epsilon$，于是 更新公式为，\n$$\\theta_{t+1} = \\theta_t - \\frac \\eta {u_t} \\hat m_t \\qquad(4)$$\n其中 $\\hat m_t$ 的计算方式与 Adam 中一致。\n\n# 3. AdamW\nAdam 中，梯度中事先包含了正则惩罚项，即\n$$g := g+\\lambda \\theta$$\n然后再计算梯度的一阶矩和二阶矩的无偏估计。现在考虑将权重衰减项从梯度 $g$ 中解耦出来，直接附加到参数衰减 $\\theta$ 上，调整 (2) 式得到 AdamW 的参数更新公式，\n$$\\theta_{t+1}=\\theta_t - \\lambda \\eta \\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t$$\n\n# 4. Nadam\n回顾一下 momentum 版本的 SGD 更新方式，\n$$v_{t+1} = \\gamma v_t + \\eta g_t\n\\\\\\\\ \\theta_{t+1}=\\theta_t - v_{t+1}$$\n然后 NAG 的更新方式，先从当前参数处更新 momentum 的量到达一个新的位置 （(5) 式），然后从新位置处进行梯度下降，作为本次更新后的参数（(6, 7) 式），数学描述如下，\n$$y_t = \\theta_t + \\mu v_t  \\qquad(5)\n\\\\\\\\ g_t = \\nabla f(y_t)    \\qquad(6)\n\\\\\\\\ \\theta_{t+1}=y_t - \\gamma g_t \\qquad(7)$$\n\n联合上面三式可知，\n$$v_{t+1}=\\theta_{t+1}-\\theta_t=\\mu v_t - \\gamma g_t$$\n初始时，$t=0, \\ v_0=0 \\Rightarrow y_0=\\theta_0$。\n\n根据 [PyTorch.optim.SGD](2020/01/02/pytorch/optim_SGD) 中的公式 (8)、(9)、(10)，易知 NAG 等价于以下更新过程，\n$$\\begin{cases}g_t = \\nabla f(\\theta_t)\n\\\\\\\\ v_{t+1} = \\gamma v_t + \\eta g_t\n\\\\\\\\ v_{t+1}' = \\gamma v_{t+1} + \\eta g_t\n\\\\\\\\ \\theta_{t+1} = \\theta_t - v_{t+1}'\\end{cases} \\qquad(8)$$\n可见，做了两次的 momentum 更新，相比普通的 momentum 的 SGD，增加了一次 look ahead 的 momentum。注意，$v_{t+1}'$ 与 $v_{t+2}$ 是不一样的。\n\n接着再回顾 Adam 中的参数更新，根据 (2) 式，得\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\frac {m_t} {1-\\beta_1^t}\\qquad(9)$$\n其中 $m_t$ 包含了一次 momentum 更新，\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n增加一次 momentum 更新，\n$$m_t'=\\beta_1 m_t + (1-\\beta_1) g_t$$\n代入 (9) 式，于是参数更新变为，\n$$\\begin{aligned}\\theta_{t+1}&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {m_t'} {1-\\beta_1^t}\\\\\\\\&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {\\beta_1 m_t + (1-\\beta_1) g_t} {1-\\beta_1^t}\n\\\\\\\\&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\left(\\beta_1 \\hat m_t+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t \\right)\\end{aligned} \\qquad(10)$$\n\n(10) 式就是 Nadam 的参数更新公式。\n\n也可以按如下过程理解，\n$$\\hat m_t = \\frac {m_t} {1-\\beta_1^t}=\\frac {\\beta_1 m_{t-1} + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\frac {\\beta_1 \\hat m_{t-1}(1-\\beta_1^{t-1}) + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\beta_1 \\hat m_{t-1}+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$\n其中最后一步用了近似处理。事实上 (10) 式第一步中，将 $m_t$ 替换为 $m_t'$ 时，分母也应该替换为 $1-\\beta_1^{t+1}$，因为 $m_t'$ 真正的无偏估计就应该要除以 $1-\\beta_1^{t+1}$，但是我们都忽略这个微小的差别。\n\n根据上式，可得，\n$$\\hat m_t'=\\beta_1 \\hat m_t + \\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$\n代入 (2) 得 Nesterov momentum 加成的 Adam 变体的 更新公式，与 (10) 式相同。 ","source":"_posts/pytorch/optim-2.md","raw":"---\ntitle: PyTorch.optim\np: pytorch/optim-2\ndate: 2020-01-08 18:19:54\ntags: \n    - PyTorch\n    - DL\nmathjax: true\n---\n\n# 1. Adam\nAdam 表示 Adaptive Moment Estimation。\n<!-- more -->\n## 1.1 原理\n梯度和梯度平方的衰减如下，\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n\\\\\\\\ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\qquad(1)$$\n其中 $\\beta_1 < 1, \\ \\beta_2 < 1$，$m_t$ 和 $v_t$ 分别是梯度 $g$ 的一阶矩和二阶矩的样本估计（$g$ 看作随机变量）。由于 $m$ 和 $v$ 均初始化为 0，即 $m_0=0, \\ v_0 = 0$，所以这两个样本估计均是有偏估计，且偏向 0，尤其在刚开始的时间步（t 较小）和衰减率较小时（$1-\\beta$ 较小，$\\beta$ 接近 1）。\n\n令 $E(g)=\\mu$，$g_1, g_2, ...$ 来自于 $g$ 且独立同分布，那么\n$$E(m_t)=E\\left(\\sum_{\\tau=1}^t \\beta_1^{t-\\tau} (1-\\beta_1) g_{\\tau}\\right)=(1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}E(g_{\\tau})=\\mu (1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}=\\mu(1-\\beta_1^t)$$ \n可见，当 t 较小且 $\\beta_1 \\rightarrow 1$，$E(m_t) \\rightarrow 0$\n\n为了抵消这些偏向，取以下计算进行校正，\n$$\\hat m_t=\\frac {m_t} {1-\\beta_1^t}\n\\\\\\\\ \\hat v_t = \\frac {v_t} {1-\\beta_2^t}$$\n\n其中 上标 `t` 表示指数，即 `t` 个 $\\beta$ 相乘。 通过上面的分析，可知，除以 $1-\\beta^t$ 后，$E(\\hat m_t)=\\mu$，为无偏估计。\n\n然后类似 Adadelta 和 RMSprop 中那样，更新公式为，\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t \\qquad(2)$$\n其中 $\\eta$ 为初始学习率，是一个初始时给定的超参数。\n\n## 1.2 AMSGrad 变体\n修改 $v$ 的计算式如下，\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n\\\\\\\\ v_{t,m} = \\max\\left(v_{t-1,m}, \\ \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\right)$$\n其中 $v_{0,m}=0$。\n\n然后 $v$ 的无偏估计改为，\n$$\\hat v_{t,m}=\\frac {v_{t,m}} {1-\\beta_2^t}$$\n参数更新公式调整为，\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_{t,m}}+\\epsilon} \\hat m_t \\qquad(3)$$\n其中 $\\hat m_t$ 的计算部分与前面的保持一致。\n\nAMSGrad 比 Adam 降低了学习率。\n\n# 2. Adamax\n在 Adam 的基础上将 (1) 式泛化，不局限于 $l_2$ 范数，如下\n$$v_t = \\beta_2^p v_{t-1} + (1-\\beta_2^p)|g_t|^p$$\n其中注意 $p$ 为指数。\n\n将上式中的 $v_{t-1}$ 展开，\n$$\\begin{aligned} v_t  &= (1-\\beta_2^p)|g_t|^p + \\beta_2^p[(1-\\beta_2^p)|g_{t-1}|^p+\\beta_2^p v_{t-2}]\n\\\\\\\\ & = (1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\n\\end{aligned}$$\n\n令 $p \\rightarrow \\infin$，并定义 $u_t = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p}$，结合上式有，\n$$\\begin{aligned} u_t  = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p} &= \\lim_{p \\rightarrow \\infin}\\left((1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &= \\lim_{p \\rightarrow \\infin} (1-\\beta_2^p)^{1/p} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &= \\lim_{p \\rightarrow \\infin} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}\n\\\\\\\\ &=\\max (\\beta_2^{t-1}|g_1|,\\beta_2^{t-2}|g_2|,...,\\beta_2^{0}|g_t|)\\end{aligned}$$\n\n于是可得以下迭代公式，\n$$u_t = \\max(\\beta_2 u_{t-1}, \\ |g_t|)$$\n其中初始值 $u_0=0$。\n\n用 $u_t$ 替换 Adam 中的 $\\sqrt{\\hat v_t}+\\epsilon$，于是 更新公式为，\n$$\\theta_{t+1} = \\theta_t - \\frac \\eta {u_t} \\hat m_t \\qquad(4)$$\n其中 $\\hat m_t$ 的计算方式与 Adam 中一致。\n\n# 3. AdamW\nAdam 中，梯度中事先包含了正则惩罚项，即\n$$g := g+\\lambda \\theta$$\n然后再计算梯度的一阶矩和二阶矩的无偏估计。现在考虑将权重衰减项从梯度 $g$ 中解耦出来，直接附加到参数衰减 $\\theta$ 上，调整 (2) 式得到 AdamW 的参数更新公式，\n$$\\theta_{t+1}=\\theta_t - \\lambda \\eta \\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t$$\n\n# 4. Nadam\n回顾一下 momentum 版本的 SGD 更新方式，\n$$v_{t+1} = \\gamma v_t + \\eta g_t\n\\\\\\\\ \\theta_{t+1}=\\theta_t - v_{t+1}$$\n然后 NAG 的更新方式，先从当前参数处更新 momentum 的量到达一个新的位置 （(5) 式），然后从新位置处进行梯度下降，作为本次更新后的参数（(6, 7) 式），数学描述如下，\n$$y_t = \\theta_t + \\mu v_t  \\qquad(5)\n\\\\\\\\ g_t = \\nabla f(y_t)    \\qquad(6)\n\\\\\\\\ \\theta_{t+1}=y_t - \\gamma g_t \\qquad(7)$$\n\n联合上面三式可知，\n$$v_{t+1}=\\theta_{t+1}-\\theta_t=\\mu v_t - \\gamma g_t$$\n初始时，$t=0, \\ v_0=0 \\Rightarrow y_0=\\theta_0$。\n\n根据 [PyTorch.optim.SGD](2020/01/02/pytorch/optim_SGD) 中的公式 (8)、(9)、(10)，易知 NAG 等价于以下更新过程，\n$$\\begin{cases}g_t = \\nabla f(\\theta_t)\n\\\\\\\\ v_{t+1} = \\gamma v_t + \\eta g_t\n\\\\\\\\ v_{t+1}' = \\gamma v_{t+1} + \\eta g_t\n\\\\\\\\ \\theta_{t+1} = \\theta_t - v_{t+1}'\\end{cases} \\qquad(8)$$\n可见，做了两次的 momentum 更新，相比普通的 momentum 的 SGD，增加了一次 look ahead 的 momentum。注意，$v_{t+1}'$ 与 $v_{t+2}$ 是不一样的。\n\n接着再回顾 Adam 中的参数更新，根据 (2) 式，得\n$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\frac {m_t} {1-\\beta_1^t}\\qquad(9)$$\n其中 $m_t$ 包含了一次 momentum 更新，\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n增加一次 momentum 更新，\n$$m_t'=\\beta_1 m_t + (1-\\beta_1) g_t$$\n代入 (9) 式，于是参数更新变为，\n$$\\begin{aligned}\\theta_{t+1}&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {m_t'} {1-\\beta_1^t}\\\\\\\\&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {\\beta_1 m_t + (1-\\beta_1) g_t} {1-\\beta_1^t}\n\\\\\\\\&=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\left(\\beta_1 \\hat m_t+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t \\right)\\end{aligned} \\qquad(10)$$\n\n(10) 式就是 Nadam 的参数更新公式。\n\n也可以按如下过程理解，\n$$\\hat m_t = \\frac {m_t} {1-\\beta_1^t}=\\frac {\\beta_1 m_{t-1} + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\frac {\\beta_1 \\hat m_{t-1}(1-\\beta_1^{t-1}) + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\beta_1 \\hat m_{t-1}+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$\n其中最后一步用了近似处理。事实上 (10) 式第一步中，将 $m_t$ 替换为 $m_t'$ 时，分母也应该替换为 $1-\\beta_1^{t+1}$，因为 $m_t'$ 真正的无偏估计就应该要除以 $1-\\beta_1^{t+1}$，但是我们都忽略这个微小的差别。\n\n根据上式，可得，\n$$\\hat m_t'=\\beta_1 \\hat m_t + \\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$\n代入 (2) 得 Nesterov momentum 加成的 Adam 变体的 更新公式，与 (10) 式相同。 ","slug":"pytorch/optim-2","published":1,"updated":"2020-04-24T10:34:13.810Z","_id":"ck9dzcivb001rgga6hkzd7lgz","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"1-Adam\"><a href=\"#1-Adam\" class=\"headerlink\" title=\"1. Adam\"></a>1. Adam</h1><p>Adam 表示 Adaptive Moment Estimation。</p>\n<a id=\"more\"></a>\n<h2 id=\"1-1-原理\"><a href=\"#1-1-原理\" class=\"headerlink\" title=\"1.1 原理\"></a>1.1 原理</h2><p>梯度和梯度平方的衰减如下，<br>$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t<br>\\\\ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\qquad(1)$$<br>其中 $\\beta_1 &lt; 1, \\ \\beta_2 &lt; 1$，$m_t$ 和 $v_t$ 分别是梯度 $g$ 的一阶矩和二阶矩的样本估计（$g$ 看作随机变量）。由于 $m$ 和 $v$ 均初始化为 0，即 $m_0=0, \\ v_0 = 0$，所以这两个样本估计均是有偏估计，且偏向 0，尤其在刚开始的时间步（t 较小）和衰减率较小时（$1-\\beta$ 较小，$\\beta$ 接近 1）。</p>\n<p>令 $E(g)=\\mu$，$g_1, g_2, …$ 来自于 $g$ 且独立同分布，那么<br>$$E(m_t)=E\\left(\\sum_{\\tau=1}^t \\beta_1^{t-\\tau} (1-\\beta_1) g_{\\tau}\\right)=(1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}E(g_{\\tau})=\\mu (1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}=\\mu(1-\\beta_1^t)$$<br>可见，当 t 较小且 $\\beta_1 \\rightarrow 1$，$E(m_t) \\rightarrow 0$</p>\n<p>为了抵消这些偏向，取以下计算进行校正，<br>$$\\hat m_t=\\frac {m_t} {1-\\beta_1^t}<br>\\\\ \\hat v_t = \\frac {v_t} {1-\\beta_2^t}$$</p>\n<p>其中 上标 <code>t</code> 表示指数，即 <code>t</code> 个 $\\beta$ 相乘。 通过上面的分析，可知，除以 $1-\\beta^t$ 后，$E(\\hat m_t)=\\mu$，为无偏估计。</p>\n<p>然后类似 Adadelta 和 RMSprop 中那样，更新公式为，<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t \\qquad(2)$$<br>其中 $\\eta$ 为初始学习率，是一个初始时给定的超参数。</p>\n<h2 id=\"1-2-AMSGrad-变体\"><a href=\"#1-2-AMSGrad-变体\" class=\"headerlink\" title=\"1.2 AMSGrad 变体\"></a>1.2 AMSGrad 变体</h2><p>修改 $v$ 的计算式如下，<br>$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2<br>\\\\ v_{t,m} = \\max\\left(v_{t-1,m}, \\ \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\right)$$<br>其中 $v_{0,m}=0$。</p>\n<p>然后 $v$ 的无偏估计改为，<br>$$\\hat v_{t,m}=\\frac {v_{t,m}} {1-\\beta_2^t}$$<br>参数更新公式调整为，<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_{t,m}}+\\epsilon} \\hat m_t \\qquad(3)$$<br>其中 $\\hat m_t$ 的计算部分与前面的保持一致。</p>\n<p>AMSGrad 比 Adam 降低了学习率。</p>\n<h1 id=\"2-Adamax\"><a href=\"#2-Adamax\" class=\"headerlink\" title=\"2. Adamax\"></a>2. Adamax</h1><p>在 Adam 的基础上将 (1) 式泛化，不局限于 $l_2$ 范数，如下<br>$$v_t = \\beta_2^p v_{t-1} + (1-\\beta_2^p)|g_t|^p$$<br>其中注意 $p$ 为指数。</p>\n<p>将上式中的 $v_{t-1}$ 展开，<br>$$\\begin{aligned} v_t  &amp;= (1-\\beta_2^p)|g_t|^p + \\beta_2^p[(1-\\beta_2^p)|g_{t-1}|^p+\\beta_2^p v_{t-2}]<br>\\\\ &amp; = (1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p<br>\\end{aligned}$$</p>\n<p>令 $p \\rightarrow \\infin$，并定义 $u_t = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p}$，结合上式有，<br>$$\\begin{aligned} u_t  = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p} &amp;= \\lim_{p \\rightarrow \\infin}\\left((1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;= \\lim_{p \\rightarrow \\infin} (1-\\beta_2^p)^{1/p} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;= \\lim_{p \\rightarrow \\infin} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;=\\max (\\beta_2^{t-1}|g_1|,\\beta_2^{t-2}|g_2|,…,\\beta_2^{0}|g_t|)\\end{aligned}$$</p>\n<p>于是可得以下迭代公式，<br>$$u_t = \\max(\\beta_2 u_{t-1}, \\ |g_t|)$$<br>其中初始值 $u_0=0$。</p>\n<p>用 $u_t$ 替换 Adam 中的 $\\sqrt{\\hat v_t}+\\epsilon$，于是 更新公式为，<br>$$\\theta_{t+1} = \\theta_t - \\frac \\eta {u_t} \\hat m_t \\qquad(4)$$<br>其中 $\\hat m_t$ 的计算方式与 Adam 中一致。</p>\n<h1 id=\"3-AdamW\"><a href=\"#3-AdamW\" class=\"headerlink\" title=\"3. AdamW\"></a>3. AdamW</h1><p>Adam 中，梯度中事先包含了正则惩罚项，即<br>$$g := g+\\lambda \\theta$$<br>然后再计算梯度的一阶矩和二阶矩的无偏估计。现在考虑将权重衰减项从梯度 $g$ 中解耦出来，直接附加到参数衰减 $\\theta$ 上，调整 (2) 式得到 AdamW 的参数更新公式，<br>$$\\theta_{t+1}=\\theta_t - \\lambda \\eta \\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t$$</p>\n<h1 id=\"4-Nadam\"><a href=\"#4-Nadam\" class=\"headerlink\" title=\"4. Nadam\"></a>4. Nadam</h1><p>回顾一下 momentum 版本的 SGD 更新方式，<br>$$v_{t+1} = \\gamma v_t + \\eta g_t<br>\\\\ \\theta_{t+1}=\\theta_t - v_{t+1}$$<br>然后 NAG 的更新方式，先从当前参数处更新 momentum 的量到达一个新的位置 （(5) 式），然后从新位置处进行梯度下降，作为本次更新后的参数（(6, 7) 式），数学描述如下，<br>$$y_t = \\theta_t + \\mu v_t  \\qquad(5)<br>\\\\ g_t = \\nabla f(y_t)    \\qquad(6)<br>\\\\ \\theta_{t+1}=y_t - \\gamma g_t \\qquad(7)$$</p>\n<p>联合上面三式可知，<br>$$v_{t+1}=\\theta_{t+1}-\\theta_t=\\mu v_t - \\gamma g_t$$<br>初始时，$t=0, \\ v_0=0 \\Rightarrow y_0=\\theta_0$。</p>\n<p>根据 <a href=\"2020/01/02/pytorch/optim_SGD\">PyTorch.optim.SGD</a> 中的公式 (8)、(9)、(10)，易知 NAG 等价于以下更新过程，<br>$$\\begin{cases}g_t = \\nabla f(\\theta_t)<br>\\\\ v_{t+1} = \\gamma v_t + \\eta g_t<br>\\\\ v_{t+1}’ = \\gamma v_{t+1} + \\eta g_t<br>\\\\ \\theta_{t+1} = \\theta_t - v_{t+1}’\\end{cases} \\qquad(8)$$<br>可见，做了两次的 momentum 更新，相比普通的 momentum 的 SGD，增加了一次 look ahead 的 momentum。注意，$v_{t+1}’$ 与 $v_{t+2}$ 是不一样的。</p>\n<p>接着再回顾 Adam 中的参数更新，根据 (2) 式，得<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\frac {m_t} {1-\\beta_1^t}\\qquad(9)$$<br>其中 $m_t$ 包含了一次 momentum 更新，<br>$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$<br>增加一次 momentum 更新，<br>$$m_t’=\\beta_1 m_t + (1-\\beta_1) g_t$$<br>代入 (9) 式，于是参数更新变为，<br>$$\\begin{aligned}\\theta_{t+1}&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {m_t’} {1-\\beta_1^t}\\\\&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {\\beta_1 m_t + (1-\\beta_1) g_t} {1-\\beta_1^t}<br>\\\\&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\left(\\beta_1 \\hat m_t+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t \\right)\\end{aligned} \\qquad(10)$$</p>\n<p>(10) 式就是 Nadam 的参数更新公式。</p>\n<p>也可以按如下过程理解，<br>$$\\hat m_t = \\frac {m_t} {1-\\beta_1^t}=\\frac {\\beta_1 m_{t-1} + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\frac {\\beta_1 \\hat m_{t-1}(1-\\beta_1^{t-1}) + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\beta_1 \\hat m_{t-1}+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$<br>其中最后一步用了近似处理。事实上 (10) 式第一步中，将 $m_t$ 替换为 $m_t’$ 时，分母也应该替换为 $1-\\beta_1^{t+1}$，因为 $m_t’$ 真正的无偏估计就应该要除以 $1-\\beta_1^{t+1}$，但是我们都忽略这个微小的差别。</p>\n<p>根据上式，可得，<br>$$\\hat m_t’=\\beta_1 \\hat m_t + \\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$<br>代入 (2) 得 Nesterov momentum 加成的 Adam 变体的 更新公式，与 (10) 式相同。 </p>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-Adam\"><a href=\"#1-Adam\" class=\"headerlink\" title=\"1. Adam\"></a>1. Adam</h1><p>Adam 表示 Adaptive Moment Estimation。</p>","more":"<h2 id=\"1-1-原理\"><a href=\"#1-1-原理\" class=\"headerlink\" title=\"1.1 原理\"></a>1.1 原理</h2><p>梯度和梯度平方的衰减如下，<br>$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t<br>\\\\ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\qquad(1)$$<br>其中 $\\beta_1 &lt; 1, \\ \\beta_2 &lt; 1$，$m_t$ 和 $v_t$ 分别是梯度 $g$ 的一阶矩和二阶矩的样本估计（$g$ 看作随机变量）。由于 $m$ 和 $v$ 均初始化为 0，即 $m_0=0, \\ v_0 = 0$，所以这两个样本估计均是有偏估计，且偏向 0，尤其在刚开始的时间步（t 较小）和衰减率较小时（$1-\\beta$ 较小，$\\beta$ 接近 1）。</p>\n<p>令 $E(g)=\\mu$，$g_1, g_2, …$ 来自于 $g$ 且独立同分布，那么<br>$$E(m_t)=E\\left(\\sum_{\\tau=1}^t \\beta_1^{t-\\tau} (1-\\beta_1) g_{\\tau}\\right)=(1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}E(g_{\\tau})=\\mu (1-\\beta_1)\\sum_{\\tau=1}^t \\beta_1^{t-\\tau}=\\mu(1-\\beta_1^t)$$<br>可见，当 t 较小且 $\\beta_1 \\rightarrow 1$，$E(m_t) \\rightarrow 0$</p>\n<p>为了抵消这些偏向，取以下计算进行校正，<br>$$\\hat m_t=\\frac {m_t} {1-\\beta_1^t}<br>\\\\ \\hat v_t = \\frac {v_t} {1-\\beta_2^t}$$</p>\n<p>其中 上标 <code>t</code> 表示指数，即 <code>t</code> 个 $\\beta$ 相乘。 通过上面的分析，可知，除以 $1-\\beta^t$ 后，$E(\\hat m_t)=\\mu$，为无偏估计。</p>\n<p>然后类似 Adadelta 和 RMSprop 中那样，更新公式为，<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t \\qquad(2)$$<br>其中 $\\eta$ 为初始学习率，是一个初始时给定的超参数。</p>\n<h2 id=\"1-2-AMSGrad-变体\"><a href=\"#1-2-AMSGrad-变体\" class=\"headerlink\" title=\"1.2 AMSGrad 变体\"></a>1.2 AMSGrad 变体</h2><p>修改 $v$ 的计算式如下，<br>$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2<br>\\\\ v_{t,m} = \\max\\left(v_{t-1,m}, \\ \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\right)$$<br>其中 $v_{0,m}=0$。</p>\n<p>然后 $v$ 的无偏估计改为，<br>$$\\hat v_{t,m}=\\frac {v_{t,m}} {1-\\beta_2^t}$$<br>参数更新公式调整为，<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_{t,m}}+\\epsilon} \\hat m_t \\qquad(3)$$<br>其中 $\\hat m_t$ 的计算部分与前面的保持一致。</p>\n<p>AMSGrad 比 Adam 降低了学习率。</p>\n<h1 id=\"2-Adamax\"><a href=\"#2-Adamax\" class=\"headerlink\" title=\"2. Adamax\"></a>2. Adamax</h1><p>在 Adam 的基础上将 (1) 式泛化，不局限于 $l_2$ 范数，如下<br>$$v_t = \\beta_2^p v_{t-1} + (1-\\beta_2^p)|g_t|^p$$<br>其中注意 $p$ 为指数。</p>\n<p>将上式中的 $v_{t-1}$ 展开，<br>$$\\begin{aligned} v_t  &amp;= (1-\\beta_2^p)|g_t|^p + \\beta_2^p[(1-\\beta_2^p)|g_{t-1}|^p+\\beta_2^p v_{t-2}]<br>\\\\ &amp; = (1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p<br>\\end{aligned}$$</p>\n<p>令 $p \\rightarrow \\infin$，并定义 $u_t = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p}$，结合上式有，<br>$$\\begin{aligned} u_t  = \\lim_{p \\rightarrow \\infin}(v_t)^{1/p} &amp;= \\lim_{p \\rightarrow \\infin}\\left((1-\\beta_2^p)\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;= \\lim_{p \\rightarrow \\infin} (1-\\beta_2^p)^{1/p} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;= \\lim_{p \\rightarrow \\infin} \\left(\\sum_{i=1}^t \\beta_2^{p(t-i)} |g_i|^p\\right)^{1/p}<br>\\\\ &amp;=\\max (\\beta_2^{t-1}|g_1|,\\beta_2^{t-2}|g_2|,…,\\beta_2^{0}|g_t|)\\end{aligned}$$</p>\n<p>于是可得以下迭代公式，<br>$$u_t = \\max(\\beta_2 u_{t-1}, \\ |g_t|)$$<br>其中初始值 $u_0=0$。</p>\n<p>用 $u_t$ 替换 Adam 中的 $\\sqrt{\\hat v_t}+\\epsilon$，于是 更新公式为，<br>$$\\theta_{t+1} = \\theta_t - \\frac \\eta {u_t} \\hat m_t \\qquad(4)$$<br>其中 $\\hat m_t$ 的计算方式与 Adam 中一致。</p>\n<h1 id=\"3-AdamW\"><a href=\"#3-AdamW\" class=\"headerlink\" title=\"3. AdamW\"></a>3. AdamW</h1><p>Adam 中，梯度中事先包含了正则惩罚项，即<br>$$g := g+\\lambda \\theta$$<br>然后再计算梯度的一阶矩和二阶矩的无偏估计。现在考虑将权重衰减项从梯度 $g$ 中解耦出来，直接附加到参数衰减 $\\theta$ 上，调整 (2) 式得到 AdamW 的参数更新公式，<br>$$\\theta_{t+1}=\\theta_t - \\lambda \\eta \\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\hat m_t$$</p>\n<h1 id=\"4-Nadam\"><a href=\"#4-Nadam\" class=\"headerlink\" title=\"4. Nadam\"></a>4. Nadam</h1><p>回顾一下 momentum 版本的 SGD 更新方式，<br>$$v_{t+1} = \\gamma v_t + \\eta g_t<br>\\\\ \\theta_{t+1}=\\theta_t - v_{t+1}$$<br>然后 NAG 的更新方式，先从当前参数处更新 momentum 的量到达一个新的位置 （(5) 式），然后从新位置处进行梯度下降，作为本次更新后的参数（(6, 7) 式），数学描述如下，<br>$$y_t = \\theta_t + \\mu v_t  \\qquad(5)<br>\\\\ g_t = \\nabla f(y_t)    \\qquad(6)<br>\\\\ \\theta_{t+1}=y_t - \\gamma g_t \\qquad(7)$$</p>\n<p>联合上面三式可知，<br>$$v_{t+1}=\\theta_{t+1}-\\theta_t=\\mu v_t - \\gamma g_t$$<br>初始时，$t=0, \\ v_0=0 \\Rightarrow y_0=\\theta_0$。</p>\n<p>根据 <a href=\"2020/01/02/pytorch/optim_SGD\">PyTorch.optim.SGD</a> 中的公式 (8)、(9)、(10)，易知 NAG 等价于以下更新过程，<br>$$\\begin{cases}g_t = \\nabla f(\\theta_t)<br>\\\\ v_{t+1} = \\gamma v_t + \\eta g_t<br>\\\\ v_{t+1}’ = \\gamma v_{t+1} + \\eta g_t<br>\\\\ \\theta_{t+1} = \\theta_t - v_{t+1}’\\end{cases} \\qquad(8)$$<br>可见，做了两次的 momentum 更新，相比普通的 momentum 的 SGD，增加了一次 look ahead 的 momentum。注意，$v_{t+1}’$ 与 $v_{t+2}$ 是不一样的。</p>\n<p>接着再回顾 Adam 中的参数更新，根据 (2) 式，得<br>$$\\theta_{t+1}=\\theta_t - \\frac {\\eta} {\\sqrt{\\hat v_t}+\\epsilon} \\frac {m_t} {1-\\beta_1^t}\\qquad(9)$$<br>其中 $m_t$ 包含了一次 momentum 更新，<br>$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$<br>增加一次 momentum 更新，<br>$$m_t’=\\beta_1 m_t + (1-\\beta_1) g_t$$<br>代入 (9) 式，于是参数更新变为，<br>$$\\begin{aligned}\\theta_{t+1}&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {m_t’} {1-\\beta_1^t}\\\\&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\frac {\\beta_1 m_t + (1-\\beta_1) g_t} {1-\\beta_1^t}<br>\\\\&amp;=\\theta_t - \\frac \\eta {\\sqrt {\\hat v_t} + \\epsilon}\\left(\\beta_1 \\hat m_t+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t \\right)\\end{aligned} \\qquad(10)$$</p>\n<p>(10) 式就是 Nadam 的参数更新公式。</p>\n<p>也可以按如下过程理解，<br>$$\\hat m_t = \\frac {m_t} {1-\\beta_1^t}=\\frac {\\beta_1 m_{t-1} + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\frac {\\beta_1 \\hat m_{t-1}(1-\\beta_1^{t-1}) + (1-\\beta_1) g_t} {1-\\beta_1^t}=\\beta_1 \\hat m_{t-1}+\\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$<br>其中最后一步用了近似处理。事实上 (10) 式第一步中，将 $m_t$ 替换为 $m_t’$ 时，分母也应该替换为 $1-\\beta_1^{t+1}$，因为 $m_t’$ 真正的无偏估计就应该要除以 $1-\\beta_1^{t+1}$，但是我们都忽略这个微小的差别。</p>\n<p>根据上式，可得，<br>$$\\hat m_t’=\\beta_1 \\hat m_t + \\frac {1-\\beta_1}{1-\\beta_1^t} g_t$$<br>代入 (2) 得 Nesterov momentum 加成的 Adam 变体的 更新公式，与 (10) 式相同。 </p>"},{"title":"gcc-src","date":"2019-07-02T05:49:38.000Z","_content":"多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。\n<!-- more -->\n以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去\n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone 这个位于 github 的远程仓库\n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html) 选择一个镜像网址，直接下载 gcc 源码\n\nC++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。\n\n我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。\n\n# Allocator\n我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，\n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    // 定义一系列 _Tp 内部类型的别名\nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：\n1. 提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 \n2. 提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：\n    - 前两个模板参数满足 _Tp::template rebind<_Up>::other 为有效定义，那么匹配第二个 __rebind 定义\n    - 否则，匹配第一个 __rebind 定义\n\n接着此文件定义了\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n根据前面的分析，只有 _Alloc::template rebind<_Up>::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 __rebind 模板定义，当 _Alloc 是模板类时，__alloc_rebind 为 _Alloc<_Up, ...>::type。\n\n然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，\n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type*。当然通常情况下，_Alloc::pointer 其实也就是 value_type* 类型。__pointer 来自基类成员类型，是一个模板类。  \n在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，\n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n定义了模板类 _Ptr，具有内部类型 type 为 _Func<_Alloc>，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 `pointer_traits<pointer>::template rebind<_Tp>`，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，\n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，\n```c++\n// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits<pointer>::difference_type，此时一般为（有符号长整型）\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型\nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。\n\n我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，\n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     // 假设 __cplusplus >= 201103L，其他情况这里不考虑\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits 就是上面刚讨论的那个特性类\n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域\n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n于是回到 std::allocator_traits 中查看例如 allocate 的定义，\n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏\nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。\n\n由于 std::allocator_traits::construct 的方法参数为，\n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 __gnu_cxx::__alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，\n```c++\n// 是否是自定义指针的判断\ntemplate<typename _Ptr>\nusing __is_custom_pointer\n// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 => __is_custom_pointer 为真\n// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假\n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n// 重载非标准指针类型的 构造函数\ntemplate<typename _Ptr, typename... _Args>\n// 条件判断，当 __is_custom_pointer<_Ptr> 为真时，enable_if<xx>::type 才存在\nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\n我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。\n\n接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。\n\nstd::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。\n\n# Iterator\n开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，\n```c++\n// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器\n// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法\ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。\n\n在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，\n\n1. input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。  \n   支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）\n2. output 迭代器。与 input 迭代器相反，依次向容器写入元素。  \n   支持的操作：自增（向前），解引用（左值，赋值）。\n3. forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。\n4. bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。\n5. random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。\n\n也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器\n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到\npublic:\n    // 构造函数略\n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值\n                        // 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变\n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // 反向迭代器表示从右往左，故自增表示正常迭代器的自减\n        return *this;\n    }\n}\n```\n记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如\n1. 解引用: *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\n还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）\n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // 指定迭代器标签，其他相关类型则由容器决定\n{\nprotected:\n    _Container* container;  // 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素\npublic:\n    back_insert_iterator&   // 给此迭代器赋值就等于向容器末端插入元素\n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       // 解引用不是取所指元素的值，因为是 output 迭代器\n    back_insert_iterator&\n    operator++() {return *this; }       // 自增也不是指向下一个元素，因为只能向容器末端插入值\n    back_insert_iterator&\n}\n```\n与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。\n\n## __normal_iterator\n这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，\n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator 的迭代操作实际上由 _M_current 完成\n    ...\npublic:\n    // 构造函数略\n    ...\n    // 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成\n    // 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成\n    // 确实是再 normal 不过了\n}\n```\n\n## move_iterator\n顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator 的特性类\n    typedef typename __traits_type::reference   __base_ref;     // 萃取 _Iterator 相关的元素引用类型\npublic:\n    ...\n    // 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型\n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // 将内部迭代器解引用得到的值转为右值引用\n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // 随机访问取值，也转为右值引用\n}\n```\n\n\n# Container\n## Vector\n以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor<_Tp>），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp*，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp*，所以 _Vector_base::pointer 就是 _Tp*。\n\n接着 _Vector_base 中又定义了几个内部结构\n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   // 指向 vector 中第一个元素的内存位置\n    pointer _M_finish;  // 指向 vector 中 past-the-last-element 的内存位置\n    pointer _M_end_of_storage;  // vector 分配 past-the-max-element 内存位置\n\n    // 构造函数，拷贝函数，交换数据函数。比较简单，略\n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // 构造函数，略\n    // vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer\n}\n```\n回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // 分配器变量\n    ...\n    // 构造/析构 函数\n\n    pointer _M_allocator(size_t __n) {      // 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中\n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        // 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存\n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // 分配内存，并保存内存起始位置和截止位置\n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，\n```c++\n// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，\n//  默认 _Alloc 为 std::allocator<_Tp>，显然是于 _Tp 匹配的，\n//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 __gnu::cxx::__normal_iterator 的模板参数 _Iterator。  \n然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 _M_impl._M_finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，\n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。\n\n其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size > old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size<=old_size，则重置 _M_impl._M_finish 所指位置（[_M_start, _M_finish) 范围内的元素有效），_M_finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。\n\n来看 vector 的 push_back 函数实现，\n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        // 当前分配的内存空间还足以存储新的元素 __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  // 重新分配内存，并在 _M_finish 位置插入元素 __x，然后\n                                        //  将 _M_finish 所指位置向前移动一个元素单位\n}\n```\n在 bits/vector.tcc 中找到 _M_realloc_insert 的实现，\n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    // 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; // 原来的起始元素指针和 past-the-last 元素指针\n    const size_type __elems_before = __position - begin();// 插入位置之前的元素数量\n    pointer __new_start(this->_M_allocate(__len));   // 重新分配内存，使得能容纳 __len 个元素\n    pointer __new_finish(__new_start);  // 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等\n    __try\n    {\n        // 在指定位置处插入目标对象\n        _Alloc_traits::construct(this->_M_impl,                     // 使用此分配器\n                                 __new_start + __elems_before,      // 在指定位置处\n                                 std::forward<_Args>(__args)...);   // 根据此参数构造对象\n        // 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // 如果元素类型支持移动插入\n            // 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置\n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            // 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   // 此时 __new_finish 就是新的 past-of-last 元素位置了\n        }\n        ...\n        // 失败处理，略\n        // 析构原先内存上的对象，并释放内存，略\n        // 更新元素起始和截止位置等，略\n    }\n}\n```\nvector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。\n\n本文结束","source":"_posts/gcc-src.md","raw":"---\ntitle: gcc-src\ndate: 2019-07-02 13:49:38\ntags: c++\n---\n多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。\n<!-- more -->\n以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去\n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone 这个位于 github 的远程仓库\n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html) 选择一个镜像网址，直接下载 gcc 源码\n\nC++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。\n\n我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。\n\n# Allocator\n我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，\n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    // 定义一系列 _Tp 内部类型的别名\nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：\n1. 提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 \n2. 提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：\n    - 前两个模板参数满足 _Tp::template rebind<_Up>::other 为有效定义，那么匹配第二个 __rebind 定义\n    - 否则，匹配第一个 __rebind 定义\n\n接着此文件定义了\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n根据前面的分析，只有 _Alloc::template rebind<_Up>::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 __rebind 模板定义，当 _Alloc 是模板类时，__alloc_rebind 为 _Alloc<_Up, ...>::type。\n\n然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，\n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type*。当然通常情况下，_Alloc::pointer 其实也就是 value_type* 类型。__pointer 来自基类成员类型，是一个模板类。  \n在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，\n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n定义了模板类 _Ptr，具有内部类型 type 为 _Func<_Alloc>，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 `pointer_traits<pointer>::template rebind<_Tp>`，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，\n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，\n```c++\n// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits<pointer>::difference_type，此时一般为（有符号长整型）\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型\nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。\n\n我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，\n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     // 假设 __cplusplus >= 201103L，其他情况这里不考虑\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits 就是上面刚讨论的那个特性类\n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域\n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n于是回到 std::allocator_traits 中查看例如 allocate 的定义，\n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏\nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。\n\n由于 std::allocator_traits::construct 的方法参数为，\n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 __gnu_cxx::__alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，\n```c++\n// 是否是自定义指针的判断\ntemplate<typename _Ptr>\nusing __is_custom_pointer\n// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 => __is_custom_pointer 为真\n// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假\n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n// 重载非标准指针类型的 构造函数\ntemplate<typename _Ptr, typename... _Args>\n// 条件判断，当 __is_custom_pointer<_Ptr> 为真时，enable_if<xx>::type 才存在\nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\n我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。\n\n接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。\n\nstd::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。\n\n# Iterator\n开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，\n```c++\n// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器\n// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法\ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。\n\n在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，\n\n1. input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。  \n   支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）\n2. output 迭代器。与 input 迭代器相反，依次向容器写入元素。  \n   支持的操作：自增（向前），解引用（左值，赋值）。\n3. forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。\n4. bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。\n5. random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。\n\n也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器\n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到\npublic:\n    // 构造函数略\n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值\n                        // 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变\n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // 反向迭代器表示从右往左，故自增表示正常迭代器的自减\n        return *this;\n    }\n}\n```\n记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如\n1. 解引用: *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\n还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）\n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // 指定迭代器标签，其他相关类型则由容器决定\n{\nprotected:\n    _Container* container;  // 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素\npublic:\n    back_insert_iterator&   // 给此迭代器赋值就等于向容器末端插入元素\n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       // 解引用不是取所指元素的值，因为是 output 迭代器\n    back_insert_iterator&\n    operator++() {return *this; }       // 自增也不是指向下一个元素，因为只能向容器末端插入值\n    back_insert_iterator&\n}\n```\n与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。\n\n## __normal_iterator\n这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，\n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator 的迭代操作实际上由 _M_current 完成\n    ...\npublic:\n    // 构造函数略\n    ...\n    // 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成\n    // 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成\n    // 确实是再 normal 不过了\n}\n```\n\n## move_iterator\n顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator 的特性类\n    typedef typename __traits_type::reference   __base_ref;     // 萃取 _Iterator 相关的元素引用类型\npublic:\n    ...\n    // 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型\n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // 将内部迭代器解引用得到的值转为右值引用\n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // 随机访问取值，也转为右值引用\n}\n```\n\n\n# Container\n## Vector\n以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor<_Tp>），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp*，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp*，所以 _Vector_base::pointer 就是 _Tp*。\n\n接着 _Vector_base 中又定义了几个内部结构\n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   // 指向 vector 中第一个元素的内存位置\n    pointer _M_finish;  // 指向 vector 中 past-the-last-element 的内存位置\n    pointer _M_end_of_storage;  // vector 分配 past-the-max-element 内存位置\n\n    // 构造函数，拷贝函数，交换数据函数。比较简单，略\n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // 构造函数，略\n    // vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer\n}\n```\n回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // 分配器变量\n    ...\n    // 构造/析构 函数\n\n    pointer _M_allocator(size_t __n) {      // 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中\n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        // 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存\n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // 分配内存，并保存内存起始位置和截止位置\n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，\n```c++\n// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，\n//  默认 _Alloc 为 std::allocator<_Tp>，显然是于 _Tp 匹配的，\n//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 __gnu::cxx::__normal_iterator 的模板参数 _Iterator。  \n然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 _M_impl._M_finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，\n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。\n\n其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size > old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size<=old_size，则重置 _M_impl._M_finish 所指位置（[_M_start, _M_finish) 范围内的元素有效），_M_finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。\n\n来看 vector 的 push_back 函数实现，\n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        // 当前分配的内存空间还足以存储新的元素 __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  // 重新分配内存，并在 _M_finish 位置插入元素 __x，然后\n                                        //  将 _M_finish 所指位置向前移动一个元素单位\n}\n```\n在 bits/vector.tcc 中找到 _M_realloc_insert 的实现，\n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    // 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; // 原来的起始元素指针和 past-the-last 元素指针\n    const size_type __elems_before = __position - begin();// 插入位置之前的元素数量\n    pointer __new_start(this->_M_allocate(__len));   // 重新分配内存，使得能容纳 __len 个元素\n    pointer __new_finish(__new_start);  // 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等\n    __try\n    {\n        // 在指定位置处插入目标对象\n        _Alloc_traits::construct(this->_M_impl,                     // 使用此分配器\n                                 __new_start + __elems_before,      // 在指定位置处\n                                 std::forward<_Args>(__args)...);   // 根据此参数构造对象\n        // 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // 如果元素类型支持移动插入\n            // 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置\n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            // 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   // 此时 __new_finish 就是新的 past-of-last 元素位置了\n        }\n        ...\n        // 失败处理，略\n        // 析构原先内存上的对象，并释放内存，略\n        // 更新元素起始和截止位置等，略\n    }\n}\n```\nvector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。\n\n本文结束","slug":"gcc-src","published":1,"updated":"2020-04-24T10:37:53.028Z","_id":"ck9dzcj190023gga61eu2hhsv","comments":1,"layout":"post","photos":[],"link":"","content":"<p>多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。</p>\n<a id=\"more\"></a>\n<p>以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去</p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone 这个位于 github 的远程仓库</li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a> 选择一个镜像网址，直接下载 gcc 源码</li>\n</ol>\n<p>C++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。</p>\n<p>我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。</p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p>我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义一系列 _Tp 内部类型的别名</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：</p>\n<ol>\n<li>提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 </li>\n<li>提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：<ul>\n<li>前两个模板参数满足 _Tp::template rebind&lt;_Up&gt;::other 为有效定义，那么匹配第二个 __rebind 定义</li>\n<li>否则，匹配第一个 __rebind 定义</li>\n</ul>\n</li>\n</ol>\n<p>接着此文件定义了</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n<p>根据前面的分析，只有 _Alloc::template rebind&lt;_Up&gt;::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 <strong>rebind 模板定义，当 _Alloc 是模板类时，</strong>alloc_rebind 为 _Alloc&lt;_Up, …&gt;::type。</p>\n<p>然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type<em>。当然通常情况下，_Alloc::pointer 其实也就是 value_type</em> 类型。__pointer 来自基类成员类型，是一个模板类。<br>在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>定义了模板类 _Ptr，具有内部类型 type 为 _Func&lt;_Alloc&gt;，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code>，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind &#x3D; _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits&lt;pointer&gt;::difference_type，此时一般为（有符号长整型）</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n<p>篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。</p>\n<p>我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">// 假设 __cplusplus &gt;= 201103L，其他情况这里不考虑</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits 就是上面刚讨论的那个特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>于是回到 std::allocator_traits 中查看例如 allocate 的定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏</span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。</p>\n<p>由于 std::allocator_traits::construct 的方法参数为，</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 <strong>gnu_cxx::</strong>alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 是否是自定义指针的判断</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 =&gt; __is_custom_pointer 为真</span></span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假</span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 重载非标准指针类型的 构造函数</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">// 条件判断，当 __is_custom_pointer&lt;_Ptr&gt; 为真时，enable_if&lt;xx&gt;::type 才存在</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。</p>\n<p>接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。</p>\n<p>std::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。</p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p>开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器</span></span><br><span class=\"line\"><span class=\"comment\">// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法</span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n<p>对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。</p>\n<p>在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，</p>\n<ol>\n<li>input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。<br>支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）</li>\n<li>output 迭代器。与 input 迭代器相反，依次向容器写入元素。<br>支持的操作：自增（向前），解引用（左值，赋值）。</li>\n<li>forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。</li>\n<li>bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。</li>\n<li>random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。</li>\n</ol>\n<p>也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。</p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器</span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值</span></span><br><span class=\"line\">                        <span class=\"comment\">// 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// 反向迭代器表示从右往左，故自增表示正常迭代器的自减</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如</p>\n<ol>\n<li>解引用: *r = *(i-1)</li>\n<li>++r = –i, –r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）</p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// 指定迭代器标签，其他相关类型则由容器决定</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// 给此迭代器赋值就等于向容器末端插入元素</span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 解引用不是取所指元素的值，因为是 output 迭代器</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 自增也不是指向下一个元素，因为只能向容器末端插入值</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。</p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p>这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator 的迭代操作实际上由 _M_current 完成</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 确实是再 normal 不过了</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p>顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator 的特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">// 萃取 _Iterator 相关的元素引用类型</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// 将内部迭代器解引用得到的值转为右值引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(_M_current[__n]); &#125;  <span class=\"comment\">// 随机访问取值，也转为右值引用</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p>以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor&lt;_Tp&gt;），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp<em>，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp</em>，所以 _Vector_base::pointer 就是 _Tp*。</p>\n<p>接着 _Vector_base 中又定义了几个内部结构</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">// 指向 vector 中第一个元素的内存位置</span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">// 指向 vector 中 past-the-last-element 的内存位置</span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector 分配 past-the-max-element 内存位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，拷贝函数，交换数据函数。比较简单，略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，略</span></span><br><span class=\"line\">    <span class=\"comment\">// vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// 分配器变量</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 构造/析构 函数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">// 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中</span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">// 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// 分配内存，并保存内存起始位置和截止位置</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，</span></span><br><span class=\"line\"><span class=\"comment\">//  默认 _Alloc 为 std::allocator&lt;_Tp&gt;，显然是于 _Tp 匹配的，</span></span><br><span class=\"line\"><span class=\"comment\">//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 <strong>gnu::cxx::</strong>normal_iterator 的模板参数 _Iterator。<br>然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 <em>M</em>impl.<em>M</em>finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\"><span class=\"built_in\">begin</span>() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\"><span class=\"built_in\">end</span>() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(<span class=\"built_in\">end</span>()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(<span class=\"built_in\">begin</span>()); &#125;</span><br></pre></td></tr></table></figure>\n<p>可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。</p>\n<p>其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size &gt; old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size&lt;=old_size，则重置 <em>M</em>impl.<em>M</em>finish 所指位置（[<em>M</em>start, <em>M</em>finish) 范围内的元素有效），<em>M</em>finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。</p>\n<p>来看 vector 的 push_back 函数实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 当前分配的内存空间还足以存储新的元素 __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(<span class=\"built_in\">end</span>(), __x);  <span class=\"comment\">// 重新分配内存，并在 _M_finish 位置插入元素 __x，然后</span></span><br><span class=\"line\">                                        <span class=\"comment\">//  将 _M_finish 所指位置向前移动一个元素单位</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 bits/vector.tcc 中找到 <em>M</em>realloc_insert 的实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">// 原来的起始元素指针和 past-the-last 元素指针</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - <span class=\"built_in\">begin</span>();<span class=\"comment\">// 插入位置之前的元素数量</span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">// 重新分配内存，使得能容纳 __len 个元素</span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">// 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等</span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 在指定位置处插入目标对象</span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// 使用此分配器</span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// 在指定位置处</span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// 根据此参数构造对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// 如果元素类型支持移动插入</span></span><br><span class=\"line\">            <span class=\"comment\">// 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置</span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">// 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">// 此时 __new_finish 就是新的 past-of-last 元素位置了</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// 失败处理，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 析构原先内存上的对象，并释放内存，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 更新元素起始和截止位置等，略</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>vector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。</p>\n<p>本文结束</p>\n","site":{"data":{}},"excerpt":"<p>多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。</p>","more":"<p>以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去</p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone 这个位于 github 的远程仓库</li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a> 选择一个镜像网址，直接下载 gcc 源码</li>\n</ol>\n<p>C++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。</p>\n<p>我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。</p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p>我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义一系列 _Tp 内部类型的别名</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：</p>\n<ol>\n<li>提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 </li>\n<li>提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：<ul>\n<li>前两个模板参数满足 _Tp::template rebind&lt;_Up&gt;::other 为有效定义，那么匹配第二个 __rebind 定义</li>\n<li>否则，匹配第一个 __rebind 定义</li>\n</ul>\n</li>\n</ol>\n<p>接着此文件定义了</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n<p>根据前面的分析，只有 _Alloc::template rebind&lt;_Up&gt;::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 <strong>rebind 模板定义，当 _Alloc 是模板类时，</strong>alloc_rebind 为 _Alloc&lt;_Up, …&gt;::type。</p>\n<p>然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type<em>。当然通常情况下，_Alloc::pointer 其实也就是 value_type</em> 类型。__pointer 来自基类成员类型，是一个模板类。<br>在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>定义了模板类 _Ptr，具有内部类型 type 为 _Func&lt;_Alloc&gt;，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code>，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind &#x3D; _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits&lt;pointer&gt;::difference_type，此时一般为（有符号长整型）</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n<p>篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。</p>\n<p>我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">// 假设 __cplusplus &gt;= 201103L，其他情况这里不考虑</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits 就是上面刚讨论的那个特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>于是回到 std::allocator_traits 中查看例如 allocate 的定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏</span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。</p>\n<p>由于 std::allocator_traits::construct 的方法参数为，</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 <strong>gnu_cxx::</strong>alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 是否是自定义指针的判断</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 =&gt; __is_custom_pointer 为真</span></span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假</span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 重载非标准指针类型的 构造函数</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">// 条件判断，当 __is_custom_pointer&lt;_Ptr&gt; 为真时，enable_if&lt;xx&gt;::type 才存在</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。</p>\n<p>接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。</p>\n<p>std::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。</p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p>开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器</span></span><br><span class=\"line\"><span class=\"comment\">// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法</span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n<p>对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。</p>\n<p>在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，</p>\n<ol>\n<li>input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。<br>支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）</li>\n<li>output 迭代器。与 input 迭代器相反，依次向容器写入元素。<br>支持的操作：自增（向前），解引用（左值，赋值）。</li>\n<li>forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。</li>\n<li>bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。</li>\n<li>random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。</li>\n</ol>\n<p>也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。</p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器</span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值</span></span><br><span class=\"line\">                        <span class=\"comment\">// 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// 反向迭代器表示从右往左，故自增表示正常迭代器的自减</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如</p>\n<ol>\n<li>解引用: *r = *(i-1)</li>\n<li>++r = –i, –r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）</p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// 指定迭代器标签，其他相关类型则由容器决定</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// 给此迭代器赋值就等于向容器末端插入元素</span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 解引用不是取所指元素的值，因为是 output 迭代器</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 自增也不是指向下一个元素，因为只能向容器末端插入值</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。</p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p>这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator 的迭代操作实际上由 _M_current 完成</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 确实是再 normal 不过了</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p>顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator 的特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">// 萃取 _Iterator 相关的元素引用类型</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// 将内部迭代器解引用得到的值转为右值引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(_M_current[__n]); &#125;  <span class=\"comment\">// 随机访问取值，也转为右值引用</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p>以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor&lt;_Tp&gt;），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp<em>，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp</em>，所以 _Vector_base::pointer 就是 _Tp*。</p>\n<p>接着 _Vector_base 中又定义了几个内部结构</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">// 指向 vector 中第一个元素的内存位置</span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">// 指向 vector 中 past-the-last-element 的内存位置</span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector 分配 past-the-max-element 内存位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，拷贝函数，交换数据函数。比较简单，略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，略</span></span><br><span class=\"line\">    <span class=\"comment\">// vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// 分配器变量</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 构造/析构 函数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">// 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中</span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">// 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// 分配内存，并保存内存起始位置和截止位置</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，</span></span><br><span class=\"line\"><span class=\"comment\">//  默认 _Alloc 为 std::allocator&lt;_Tp&gt;，显然是于 _Tp 匹配的，</span></span><br><span class=\"line\"><span class=\"comment\">//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 <strong>gnu::cxx::</strong>normal_iterator 的模板参数 _Iterator。<br>然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 <em>M</em>impl.<em>M</em>finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\"><span class=\"built_in\">begin</span>() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\"><span class=\"built_in\">end</span>() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(<span class=\"built_in\">end</span>()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(<span class=\"built_in\">begin</span>()); &#125;</span><br></pre></td></tr></table></figure>\n<p>可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。</p>\n<p>其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size &gt; old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size&lt;=old_size，则重置 <em>M</em>impl.<em>M</em>finish 所指位置（[<em>M</em>start, <em>M</em>finish) 范围内的元素有效），<em>M</em>finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。</p>\n<p>来看 vector 的 push_back 函数实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 当前分配的内存空间还足以存储新的元素 __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(<span class=\"built_in\">end</span>(), __x);  <span class=\"comment\">// 重新分配内存，并在 _M_finish 位置插入元素 __x，然后</span></span><br><span class=\"line\">                                        <span class=\"comment\">//  将 _M_finish 所指位置向前移动一个元素单位</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 bits/vector.tcc 中找到 <em>M</em>realloc_insert 的实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">// 原来的起始元素指针和 past-the-last 元素指针</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - <span class=\"built_in\">begin</span>();<span class=\"comment\">// 插入位置之前的元素数量</span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">// 重新分配内存，使得能容纳 __len 个元素</span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">// 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等</span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 在指定位置处插入目标对象</span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// 使用此分配器</span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// 在指定位置处</span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// 根据此参数构造对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// 如果元素类型支持移动插入</span></span><br><span class=\"line\">            <span class=\"comment\">// 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置</span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">// 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">// 此时 __new_finish 就是新的 past-of-last 元素位置了</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// 失败处理，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 析构原先内存上的对象，并释放内存，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 更新元素起始和截止位置等，略</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>vector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。</p>\n<p>本文结束</p>"},{"title":"YOLO","date":"2020-04-20T05:58:33.000Z","p":"obj_det/YOLO","mathjax":true,"_content":"\n# YOLOv1\n## 简介\none-stage 检测方法，从图像像素到bbox 坐标和分类概率，一步到位。优点为：\n1. 快。YOLO 每秒可处理 45 帧，可实时处理视频流。\n2. 从图像全局出发，作出预测。这种策略有效利用了上下文信息。\n3. 能学习目标的泛化特征。基于天然图像数据集上的训练，用于人工图像数据集上的测试，也能取得较好的结果。\n\n<!-- more -->\n\n## 方法\nYOLOv1 网络结构如图1，采用的是 VOC 数据集。\n![](/images/obj_det/YOLOv1_fig1.jpg) <center>图 1 YOLOv1 网络结构</center>\n\n输入 shape : `448,448,3`，\n输出 shape ：`7,7,35`\n\n### 检测思想\n\n1. 将输入图像划分为 `SxS` 大小的网格，如果目标中心落于某个 grid cell，那么这个 grid cell 负责检测这个目标。对于 VOC 数据集，使用 `S=7`，这是综合考虑，平衡了计算量和准确率。\n3. 每个 grid cell 预测 `B` 个 box。这里取 `B=3`。\n4. 对每个 box 预测，需要 5 个数据 `(x,y,w,h,IOU)`（全部都是归一化的）。\n5. VOC 数据集的分类数量 `C=20`。每个 grid cell 处预测 `C` 个分类概率，即，每个 grid cell 处的 `B` 个 box 共享这 `C` 个分类概率（因为实际上，`B` 个预测 box 中只有 一个 box 负责预测）。\n2. 从 `448` 到 `7`，网络的下采样率为 `448/7=64`。从图 1 也能看出，具有 `s-2` 字样的 layer 共有 6 个。输出 map 的 spatial size 变成 `7x7`，channel 变成 `35`，这是因为每个空间位置处需要 `B*5+C=3*5+20` 个预测数据。\n6. 训练阶段，计算损失，并求梯度，然后更新，具体参见下文详细分析。\n7. 测试阶段，共检测出 `S*S*B` 个 box，每个 box 有 4 个坐标值，1 个 IOU 以及 C 个分类概率。对分类概率进行一个阈值截断，阈值默认为 0.2。分别针对每个分类，根据分类概率倒序排列，对 box 进行非极大抑制（设置被抑制 box 的当前分类的概率为 0），非极大抑制阈值默认为 0.4。最后，筛选出所有检测 box 中具有大于阈值（0.2）的分类概率，为最终检测结果。\n\n__思考：__ 为什么每个 grid cell 处预测不是 1 个 box 而是多个 box？\n\n__答：__ 我们假定不会有多个目标的中心落入同一个 grid cell，如果确实存在（这种概率很低），那么只有第一个目标的数据`x,y,w,h,IOU,class`会写入 gt `Y` 中。每个 grid cell 仍然预测多个 box，这是因为这些不同的预测 box 将具有不同的 size 或 aspect ratio。如果目标中心落入某个 grid cell，那么其上的 `B` 个预测 box 中，只有与目标 IOU 最大的预测 box 才负责预测。例如，某个预测 box 适合预测高的目标（人），而另一个预测 box 可能适合预测宽的目标（车）。\n\n### 损失\n\n$$\\begin{aligned} L&=\\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(x_i-\\hat x_i)^2+(y_i-\\hat y_i)^2 \\\\\\\\ &+ \\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(\\sqrt {w_i}- \\sqrt {\\hat w_i})^2+(\\sqrt {h_i}- \\sqrt {\\hat h_i})^2 \\\\\\\\ &+ \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (C_i-\\hat C_i)^2 \\\\\\\\ &+ \\lambda_{noobj} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}(C_i-\\hat C_i)^2 \\\\\\\\ &+ \\sum_{i=1}^{S^2} \\mathbf 1_i^{obj} \\sum_{c \\in classes}\\left(p_i(c)-\\hat p_i(c)\\right)^2 \\end{aligned}\n$$\n\n__分析：__\n\n带 `^` 的为网络输出，不带 `^` 则为 ground truth 值。`x, y, w, h` 为中心点坐标，`C` 为 IOU，`pi(c)` 为分类 `c` 的概率。\n\n$\\mathbf 1_{ij}^{obj}$ 表示第 `i` 个 grid cell 有目标（中心），且此 grid cell 上第 `j` 个预测 box 与 gt box 有最大 IOU，即，第 `j` box 负责预测。\n\n对于较大 box 和 较小 box，在相同偏差$\\Delta w, \\ \\Delta h$ 下，较大 box 的损失应该比较小 box 的损失更小才合理，然而两者平方差损失相同，所以我们对宽高 `w,h`，先求平方根，再求平方差，这在一定程度上降低了这种不合理性。\n\n$\\mathbf 1_{ij}^{noobj}$ 表示 i) 第 `i` 个 grid cell 无目标（中心），或者 ii) 有目标（中心），但是第 `j` 个预测 box 不负责预测（即，与 gt box 的 IOU 不是 `B` 个预测 box 中最大的）。\n\n$\\mathbf 1_i^{obj}$ 表示第 `i` 个 grid cell 有目标（中心）。\n\n坐标损失与分类损失分属两类损失，需要进行平衡。此外，由于大部分预测 box 其实并不负责预测，来自这部分预测 box 的 IOU 损失（损失公式中第四行）将会压制负责预测的 box 的坐标损失和 IOU 损失（损失公式中前三行），所以需要提升被压制的那部分的损失贡献。综合考虑，设置 $\\lambda_{coord}=5, \\ \\lambda_{noobj}=0.5$。\n\n## 细节\n1. GT label 数据的 size 为 `S*S*(5+20)`，其中 `5` 包含了 4 个坐标值，1 个 IOU，20 为表示分类 id 的 one-hot vector 的长度。维度从高到低为 `(S,S,5+20)`，最低维数据顺序为 IOU, class id, x,y,w,h。\n2. 网络输出 size 为 `S*S*(B*5+20)`，维度从高到低为 `(5+20,S,S)`，通道顺序为 class id, IOU, x,y,w,h。\n3. GT label 数据中， x,y,w,h 先进行归一化（除以图像宽/高），然后 `x=x*S-(int)x*S, y=y*S-(int)y*S`。\n4. 网络输出中的 x,y 与 GT label 中含义一致，表示相对于 grid cell 的（归一化）偏差，而 w,h 则是经过了平方根处理。\n\n\n# YOLOv2\n## 简介\nYOLOv2 是对 YOLOv1 的改进，包括：\n1. 利用现有的分类数据集来扩展检测数据集，使得检测目标的分类种数更多。\n2. 增加 Batch Normalization\n3. 检测小目标更准确\n\n在分类集上预训练时，就使用较大分辨率的图像。YOLOv1 中使用 `224x224` 在分类集上预训练，然后直接将 `448x448` 大小的检测数据训练集喂给网络，这让网络同时适应高分辨率的图像以及学习目标检测，难免压力山大。YOLOv2 中，每隔十次 batch 训练，变更一次网络输入 size。\n\n## 方法\n以 VOC 数据集为例，YOLOv2 的网络结构可以从配置文件 `cfg/yolov2-voc.cfg` 中获取。\n### 实现细节\n1. 输入 size 每隔 10 个 batch 变更一次，从 `320, 352, ..., 608` 这十个值中随机选择。记输入大小为 `(3,d,d)`。\n2. 网络整体下采样率为 32，输出大小为 `(125, d/32, d/32)`。其中，`(d/32,d/32)` 与 YOLOv1 中类似，可以看作原图像上的 grid cell 数量 `S=d/32`。如果目标的中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 上有 `5` 个预测 box，每各 box 有 `1` 个 IOU 以及 `4` 个坐标值，每个 box 独立拥有 `20` 个分类得分，故输出 channel 为 `125=5*(1+4+20)`。注意，YOLOv1 中每个 cell 上的 `B` 个预测 box 共享 `20` 个分类得分。\n\n3. 人为假设每个图像中目标数量最多为 30，所以 GT label 大小为 `30x5`，其中 `5` 包含了 4 个坐标值以及 1 个分类 id。最低维数据顺序为 x,h,w,h,class id。GT label 靠前存储。\n4. `(route:-1，-4)` 层将浅层特征（高分辨率）与高层特征（低分辨率）融合，类似于 ResNet 中的 identity mapping，这种更细粒度的特征将有助于小目标的检测。\n\n### 损失\n损失包括：分类损失，置信度损失，坐标损失三部分。\n\n$$L=L_p+L_{box}+L_C$$\n\n__分类损失__\n\n$$L_p=\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\sum_{c=1}^{20} \\mathbf 1_{ij}^{obj} [\\hat p_{ij}(c)-p_{ij}(c)]^2$$\n\n__坐标损失__\n\n$$L_{box}=\\lambda_{obj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (\\hat x_{ij} - x_{ij})^2 + (\\hat y_{ij} - y_{ij})^2+ (\\hat w_{ij} - w_{ij})^2+ (\\hat h_{ij} - h_{ij})^2 \n\\\\\\\\ + \\lambda_{noobj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{noobj} (\\hat x_{ij} - x_{ij}^a)^2 + (\\hat y_{ij} - y_{ij}^a)^2+ (\\hat w_{ij} - w_{ij}^a)^2+ (\\hat h_{ij} - h_{ij}^a)^2$$\n\n__置信度损失__\n\n$$L_C=\\lambda_{obj}^{conf}\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj}[\\hat C_{ij}-iou(\\hat \\text{box}_{ij}, \\text{box}_{ij})]^2  \n\\\\\\\\+ \\lambda_{noobj}^{conf}\\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}[\\hat C_{ij}-0]^2$$\n\n以上，带 ^ 表示 network 输出，带 a 表示 anchor，不带这两个修饰的表示 GT label。\n\n__分析：__\n\n网络输出 shape 从高维到低维为，`batch, B, 4+1+C, S, S`（其实无论几维，在内存中都是一维）。这里假设了输出 feature map 的 height 和 width 相等，均为 `S` （grid size），且 `4` 表示 4 个坐标，`1` 表示 IOU，`C` 表示分类数量。\n\n与 YOLOv1 中类似，目标中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 有 `B=5` 个预测 box，具有不同的 size。使用 5 组 anchor box 帮助预测，参考 yolov2-voc.cfg 文件中最后一个 layer 配置中 `anchors` 的值，给了 5 组 width height 的值，这些值基于输出 feature map 的 size `SxS`，即，并没有归一化。anchor box 的中心为所在 grid cell 坐标加 0.5，即 `(i,j)` 处 grid cell 的 anchor box 中心为 `(i+0.5, j+0.5)`。\n\n网络输出坐标 `x,y,h,w` 的具体含义，如图 2，\n![](/images/obj_det/YOLOv2_fig2.png) <center>图2 预测 box 与 anchor box 的关系</center>\n\n网络输出坐标实际含义就是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。\n\n一幅图像的 GT label 的 size 为 `30*5`，低维数据排列顺序为 `x,y,w,h, class id`，其中 `x,y,w,h` 是基于 original image 的 size 进行了归一化（`x,y` 与 YOLOv1 中稍有不同）。\n\n\n坐标损失中 $x_{ij}, y_{ij}, w_{ij}, h_{ij}$ 使用的是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$，对于网络输出，不用做任何修改，而对于 GT box 以及 anchor box，则需要做变换，也就是说，将预测 box 分别替换为 GT box 和 anchor box 来计算 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。\n\n位于某 location `(i,j)` 处，将 `B` 个预测 box 与 GT label 中所有目标 box 两两求 IOU，最后得到一个最大 IOU，如果这个最大 IOU 大于阈值 0.5，那么 $\\mathbf 1_{ij}^{noobj}=0$，此时置信度损失中第二项为 0。\n\n对于每个 GT box，找出与这个 GT box 有最大 IOU 预测 box，注意这个 IOU 没有阈值限制，然后设置 $\\mathbf 1_{ij}^{obj}=1$（每个 GT box 有且只有一个负责预测的 box），此时置信度损失中第一项非零，且分类损失非零，此时计算分类损失时，$\\sum_{c=1}^C$ 求和中，当且仅当 `c` 等于 GT label 中的 class id 时，$p_{ij}(c)=1$，其余 `C-1` 种情况 $p_{ij}(c)=0$。\n\n# YOLOv3\n在 YOLOv2 基础上做了修改：\n1. 三个 scale 的输出 feature maps。每组 feature maps 的大小为 `NxNx[3*(4+1+C)]`，三个不同的 `N`，依次增大 2 倍。\n2. 使用 `9` 个不同 scale 的 anchor box 帮助预测。由于有 `3` 个 scale 的 feature maps，所以实际上，每个 scale 大小的 feature maps 上每个 grid cell 仅使用 `9/3=3` 个 anchor box。\n\n以 VOC 数据集为例，网络结构参见 `cfg/yolov3-voc.cfg`。\n\n1. 特征抽取网络的下采样率为 `32`。如果输入图像的大小为 `(h,w)`，那么输出feature map 大小为 `(h/32,w/32)`，另外两个 scale 的 feature maps 的大小则为 `(h/16,w/16)` 和 `(h/8, w/8)`。\n2. 单个图像的 GT label 大小 为 `90*5`。这表示单个图像中目标数量最大不超过 `90`。\n3. 大量使用 Residual layer。\n","source":"_posts/obj_det/YOLO.md","raw":"---\ntitle: YOLO\ndate: 2020-04-20 13:58:33\np: obj_det/YOLO\ntags: object detection\nmathjax: true\n---\n\n# YOLOv1\n## 简介\none-stage 检测方法，从图像像素到bbox 坐标和分类概率，一步到位。优点为：\n1. 快。YOLO 每秒可处理 45 帧，可实时处理视频流。\n2. 从图像全局出发，作出预测。这种策略有效利用了上下文信息。\n3. 能学习目标的泛化特征。基于天然图像数据集上的训练，用于人工图像数据集上的测试，也能取得较好的结果。\n\n<!-- more -->\n\n## 方法\nYOLOv1 网络结构如图1，采用的是 VOC 数据集。\n![](/images/obj_det/YOLOv1_fig1.jpg) <center>图 1 YOLOv1 网络结构</center>\n\n输入 shape : `448,448,3`，\n输出 shape ：`7,7,35`\n\n### 检测思想\n\n1. 将输入图像划分为 `SxS` 大小的网格，如果目标中心落于某个 grid cell，那么这个 grid cell 负责检测这个目标。对于 VOC 数据集，使用 `S=7`，这是综合考虑，平衡了计算量和准确率。\n3. 每个 grid cell 预测 `B` 个 box。这里取 `B=3`。\n4. 对每个 box 预测，需要 5 个数据 `(x,y,w,h,IOU)`（全部都是归一化的）。\n5. VOC 数据集的分类数量 `C=20`。每个 grid cell 处预测 `C` 个分类概率，即，每个 grid cell 处的 `B` 个 box 共享这 `C` 个分类概率（因为实际上，`B` 个预测 box 中只有 一个 box 负责预测）。\n2. 从 `448` 到 `7`，网络的下采样率为 `448/7=64`。从图 1 也能看出，具有 `s-2` 字样的 layer 共有 6 个。输出 map 的 spatial size 变成 `7x7`，channel 变成 `35`，这是因为每个空间位置处需要 `B*5+C=3*5+20` 个预测数据。\n6. 训练阶段，计算损失，并求梯度，然后更新，具体参见下文详细分析。\n7. 测试阶段，共检测出 `S*S*B` 个 box，每个 box 有 4 个坐标值，1 个 IOU 以及 C 个分类概率。对分类概率进行一个阈值截断，阈值默认为 0.2。分别针对每个分类，根据分类概率倒序排列，对 box 进行非极大抑制（设置被抑制 box 的当前分类的概率为 0），非极大抑制阈值默认为 0.4。最后，筛选出所有检测 box 中具有大于阈值（0.2）的分类概率，为最终检测结果。\n\n__思考：__ 为什么每个 grid cell 处预测不是 1 个 box 而是多个 box？\n\n__答：__ 我们假定不会有多个目标的中心落入同一个 grid cell，如果确实存在（这种概率很低），那么只有第一个目标的数据`x,y,w,h,IOU,class`会写入 gt `Y` 中。每个 grid cell 仍然预测多个 box，这是因为这些不同的预测 box 将具有不同的 size 或 aspect ratio。如果目标中心落入某个 grid cell，那么其上的 `B` 个预测 box 中，只有与目标 IOU 最大的预测 box 才负责预测。例如，某个预测 box 适合预测高的目标（人），而另一个预测 box 可能适合预测宽的目标（车）。\n\n### 损失\n\n$$\\begin{aligned} L&=\\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(x_i-\\hat x_i)^2+(y_i-\\hat y_i)^2 \\\\\\\\ &+ \\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(\\sqrt {w_i}- \\sqrt {\\hat w_i})^2+(\\sqrt {h_i}- \\sqrt {\\hat h_i})^2 \\\\\\\\ &+ \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (C_i-\\hat C_i)^2 \\\\\\\\ &+ \\lambda_{noobj} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}(C_i-\\hat C_i)^2 \\\\\\\\ &+ \\sum_{i=1}^{S^2} \\mathbf 1_i^{obj} \\sum_{c \\in classes}\\left(p_i(c)-\\hat p_i(c)\\right)^2 \\end{aligned}\n$$\n\n__分析：__\n\n带 `^` 的为网络输出，不带 `^` 则为 ground truth 值。`x, y, w, h` 为中心点坐标，`C` 为 IOU，`pi(c)` 为分类 `c` 的概率。\n\n$\\mathbf 1_{ij}^{obj}$ 表示第 `i` 个 grid cell 有目标（中心），且此 grid cell 上第 `j` 个预测 box 与 gt box 有最大 IOU，即，第 `j` box 负责预测。\n\n对于较大 box 和 较小 box，在相同偏差$\\Delta w, \\ \\Delta h$ 下，较大 box 的损失应该比较小 box 的损失更小才合理，然而两者平方差损失相同，所以我们对宽高 `w,h`，先求平方根，再求平方差，这在一定程度上降低了这种不合理性。\n\n$\\mathbf 1_{ij}^{noobj}$ 表示 i) 第 `i` 个 grid cell 无目标（中心），或者 ii) 有目标（中心），但是第 `j` 个预测 box 不负责预测（即，与 gt box 的 IOU 不是 `B` 个预测 box 中最大的）。\n\n$\\mathbf 1_i^{obj}$ 表示第 `i` 个 grid cell 有目标（中心）。\n\n坐标损失与分类损失分属两类损失，需要进行平衡。此外，由于大部分预测 box 其实并不负责预测，来自这部分预测 box 的 IOU 损失（损失公式中第四行）将会压制负责预测的 box 的坐标损失和 IOU 损失（损失公式中前三行），所以需要提升被压制的那部分的损失贡献。综合考虑，设置 $\\lambda_{coord}=5, \\ \\lambda_{noobj}=0.5$。\n\n## 细节\n1. GT label 数据的 size 为 `S*S*(5+20)`，其中 `5` 包含了 4 个坐标值，1 个 IOU，20 为表示分类 id 的 one-hot vector 的长度。维度从高到低为 `(S,S,5+20)`，最低维数据顺序为 IOU, class id, x,y,w,h。\n2. 网络输出 size 为 `S*S*(B*5+20)`，维度从高到低为 `(5+20,S,S)`，通道顺序为 class id, IOU, x,y,w,h。\n3. GT label 数据中， x,y,w,h 先进行归一化（除以图像宽/高），然后 `x=x*S-(int)x*S, y=y*S-(int)y*S`。\n4. 网络输出中的 x,y 与 GT label 中含义一致，表示相对于 grid cell 的（归一化）偏差，而 w,h 则是经过了平方根处理。\n\n\n# YOLOv2\n## 简介\nYOLOv2 是对 YOLOv1 的改进，包括：\n1. 利用现有的分类数据集来扩展检测数据集，使得检测目标的分类种数更多。\n2. 增加 Batch Normalization\n3. 检测小目标更准确\n\n在分类集上预训练时，就使用较大分辨率的图像。YOLOv1 中使用 `224x224` 在分类集上预训练，然后直接将 `448x448` 大小的检测数据训练集喂给网络，这让网络同时适应高分辨率的图像以及学习目标检测，难免压力山大。YOLOv2 中，每隔十次 batch 训练，变更一次网络输入 size。\n\n## 方法\n以 VOC 数据集为例，YOLOv2 的网络结构可以从配置文件 `cfg/yolov2-voc.cfg` 中获取。\n### 实现细节\n1. 输入 size 每隔 10 个 batch 变更一次，从 `320, 352, ..., 608` 这十个值中随机选择。记输入大小为 `(3,d,d)`。\n2. 网络整体下采样率为 32，输出大小为 `(125, d/32, d/32)`。其中，`(d/32,d/32)` 与 YOLOv1 中类似，可以看作原图像上的 grid cell 数量 `S=d/32`。如果目标的中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 上有 `5` 个预测 box，每各 box 有 `1` 个 IOU 以及 `4` 个坐标值，每个 box 独立拥有 `20` 个分类得分，故输出 channel 为 `125=5*(1+4+20)`。注意，YOLOv1 中每个 cell 上的 `B` 个预测 box 共享 `20` 个分类得分。\n\n3. 人为假设每个图像中目标数量最多为 30，所以 GT label 大小为 `30x5`，其中 `5` 包含了 4 个坐标值以及 1 个分类 id。最低维数据顺序为 x,h,w,h,class id。GT label 靠前存储。\n4. `(route:-1，-4)` 层将浅层特征（高分辨率）与高层特征（低分辨率）融合，类似于 ResNet 中的 identity mapping，这种更细粒度的特征将有助于小目标的检测。\n\n### 损失\n损失包括：分类损失，置信度损失，坐标损失三部分。\n\n$$L=L_p+L_{box}+L_C$$\n\n__分类损失__\n\n$$L_p=\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\sum_{c=1}^{20} \\mathbf 1_{ij}^{obj} [\\hat p_{ij}(c)-p_{ij}(c)]^2$$\n\n__坐标损失__\n\n$$L_{box}=\\lambda_{obj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (\\hat x_{ij} - x_{ij})^2 + (\\hat y_{ij} - y_{ij})^2+ (\\hat w_{ij} - w_{ij})^2+ (\\hat h_{ij} - h_{ij})^2 \n\\\\\\\\ + \\lambda_{noobj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{noobj} (\\hat x_{ij} - x_{ij}^a)^2 + (\\hat y_{ij} - y_{ij}^a)^2+ (\\hat w_{ij} - w_{ij}^a)^2+ (\\hat h_{ij} - h_{ij}^a)^2$$\n\n__置信度损失__\n\n$$L_C=\\lambda_{obj}^{conf}\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj}[\\hat C_{ij}-iou(\\hat \\text{box}_{ij}, \\text{box}_{ij})]^2  \n\\\\\\\\+ \\lambda_{noobj}^{conf}\\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}[\\hat C_{ij}-0]^2$$\n\n以上，带 ^ 表示 network 输出，带 a 表示 anchor，不带这两个修饰的表示 GT label。\n\n__分析：__\n\n网络输出 shape 从高维到低维为，`batch, B, 4+1+C, S, S`（其实无论几维，在内存中都是一维）。这里假设了输出 feature map 的 height 和 width 相等，均为 `S` （grid size），且 `4` 表示 4 个坐标，`1` 表示 IOU，`C` 表示分类数量。\n\n与 YOLOv1 中类似，目标中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 有 `B=5` 个预测 box，具有不同的 size。使用 5 组 anchor box 帮助预测，参考 yolov2-voc.cfg 文件中最后一个 layer 配置中 `anchors` 的值，给了 5 组 width height 的值，这些值基于输出 feature map 的 size `SxS`，即，并没有归一化。anchor box 的中心为所在 grid cell 坐标加 0.5，即 `(i,j)` 处 grid cell 的 anchor box 中心为 `(i+0.5, j+0.5)`。\n\n网络输出坐标 `x,y,h,w` 的具体含义，如图 2，\n![](/images/obj_det/YOLOv2_fig2.png) <center>图2 预测 box 与 anchor box 的关系</center>\n\n网络输出坐标实际含义就是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。\n\n一幅图像的 GT label 的 size 为 `30*5`，低维数据排列顺序为 `x,y,w,h, class id`，其中 `x,y,w,h` 是基于 original image 的 size 进行了归一化（`x,y` 与 YOLOv1 中稍有不同）。\n\n\n坐标损失中 $x_{ij}, y_{ij}, w_{ij}, h_{ij}$ 使用的是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$，对于网络输出，不用做任何修改，而对于 GT box 以及 anchor box，则需要做变换，也就是说，将预测 box 分别替换为 GT box 和 anchor box 来计算 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。\n\n位于某 location `(i,j)` 处，将 `B` 个预测 box 与 GT label 中所有目标 box 两两求 IOU，最后得到一个最大 IOU，如果这个最大 IOU 大于阈值 0.5，那么 $\\mathbf 1_{ij}^{noobj}=0$，此时置信度损失中第二项为 0。\n\n对于每个 GT box，找出与这个 GT box 有最大 IOU 预测 box，注意这个 IOU 没有阈值限制，然后设置 $\\mathbf 1_{ij}^{obj}=1$（每个 GT box 有且只有一个负责预测的 box），此时置信度损失中第一项非零，且分类损失非零，此时计算分类损失时，$\\sum_{c=1}^C$ 求和中，当且仅当 `c` 等于 GT label 中的 class id 时，$p_{ij}(c)=1$，其余 `C-1` 种情况 $p_{ij}(c)=0$。\n\n# YOLOv3\n在 YOLOv2 基础上做了修改：\n1. 三个 scale 的输出 feature maps。每组 feature maps 的大小为 `NxNx[3*(4+1+C)]`，三个不同的 `N`，依次增大 2 倍。\n2. 使用 `9` 个不同 scale 的 anchor box 帮助预测。由于有 `3` 个 scale 的 feature maps，所以实际上，每个 scale 大小的 feature maps 上每个 grid cell 仅使用 `9/3=3` 个 anchor box。\n\n以 VOC 数据集为例，网络结构参见 `cfg/yolov3-voc.cfg`。\n\n1. 特征抽取网络的下采样率为 `32`。如果输入图像的大小为 `(h,w)`，那么输出feature map 大小为 `(h/32,w/32)`，另外两个 scale 的 feature maps 的大小则为 `(h/16,w/16)` 和 `(h/8, w/8)`。\n2. 单个图像的 GT label 大小 为 `90*5`。这表示单个图像中目标数量最大不超过 `90`。\n3. 大量使用 Residual layer。\n","slug":"obj_det/YOLO","published":1,"updated":"2020-04-24T10:30:00.129Z","_id":"ck9dzcj1b0024gga69d3n8ess","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"YOLOv1\"><a href=\"#YOLOv1\" class=\"headerlink\" title=\"YOLOv1\"></a>YOLOv1</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>one-stage 检测方法，从图像像素到bbox 坐标和分类概率，一步到位。优点为：</p>\n<ol>\n<li>快。YOLO 每秒可处理 45 帧，可实时处理视频流。</li>\n<li>从图像全局出发，作出预测。这种策略有效利用了上下文信息。</li>\n<li>能学习目标的泛化特征。基于天然图像数据集上的训练，用于人工图像数据集上的测试，也能取得较好的结果。</li>\n</ol>\n<a id=\"more\"></a>\n\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>YOLOv1 网络结构如图1，采用的是 VOC 数据集。<br><img src=\"/images/obj_det/YOLOv1_fig1.jpg\" alt=\"\"> <center>图 1 YOLOv1 网络结构</center></p>\n<p>输入 shape : <code>448,448,3</code>，<br>输出 shape ：<code>7,7,35</code></p>\n<h3 id=\"检测思想\"><a href=\"#检测思想\" class=\"headerlink\" title=\"检测思想\"></a>检测思想</h3><ol>\n<li>将输入图像划分为 <code>SxS</code> 大小的网格，如果目标中心落于某个 grid cell，那么这个 grid cell 负责检测这个目标。对于 VOC 数据集，使用 <code>S=7</code>，这是综合考虑，平衡了计算量和准确率。</li>\n<li>每个 grid cell 预测 <code>B</code> 个 box。这里取 <code>B=3</code>。</li>\n<li>对每个 box 预测，需要 5 个数据 <code>(x,y,w,h,IOU)</code>（全部都是归一化的）。</li>\n<li>VOC 数据集的分类数量 <code>C=20</code>。每个 grid cell 处预测 <code>C</code> 个分类概率，即，每个 grid cell 处的 <code>B</code> 个 box 共享这 <code>C</code> 个分类概率（因为实际上，<code>B</code> 个预测 box 中只有 一个 box 负责预测）。</li>\n<li>从 <code>448</code> 到 <code>7</code>，网络的下采样率为 <code>448/7=64</code>。从图 1 也能看出，具有 <code>s-2</code> 字样的 layer 共有 6 个。输出 map 的 spatial size 变成 <code>7x7</code>，channel 变成 <code>35</code>，这是因为每个空间位置处需要 <code>B*5+C=3*5+20</code> 个预测数据。</li>\n<li>训练阶段，计算损失，并求梯度，然后更新，具体参见下文详细分析。</li>\n<li>测试阶段，共检测出 <code>S*S*B</code> 个 box，每个 box 有 4 个坐标值，1 个 IOU 以及 C 个分类概率。对分类概率进行一个阈值截断，阈值默认为 0.2。分别针对每个分类，根据分类概率倒序排列，对 box 进行非极大抑制（设置被抑制 box 的当前分类的概率为 0），非极大抑制阈值默认为 0.4。最后，筛选出所有检测 box 中具有大于阈值（0.2）的分类概率，为最终检测结果。</li>\n</ol>\n<p><strong>思考：</strong> 为什么每个 grid cell 处预测不是 1 个 box 而是多个 box？</p>\n<p><strong>答：</strong> 我们假定不会有多个目标的中心落入同一个 grid cell，如果确实存在（这种概率很低），那么只有第一个目标的数据<code>x,y,w,h,IOU,class</code>会写入 gt <code>Y</code> 中。每个 grid cell 仍然预测多个 box，这是因为这些不同的预测 box 将具有不同的 size 或 aspect ratio。如果目标中心落入某个 grid cell，那么其上的 <code>B</code> 个预测 box 中，只有与目标 IOU 最大的预测 box 才负责预测。例如，某个预测 box 适合预测高的目标（人），而另一个预测 box 可能适合预测宽的目标（车）。</p>\n<h3 id=\"损失\"><a href=\"#损失\" class=\"headerlink\" title=\"损失\"></a>损失</h3><p>$$\\begin{aligned} L&amp;=\\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(x_i-\\hat x_i)^2+(y_i-\\hat y_i)^2 \\\\ &amp;+ \\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(\\sqrt {w_i}- \\sqrt {\\hat w_i})^2+(\\sqrt {h_i}- \\sqrt {\\hat h_i})^2 \\\\ &amp;+ \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (C_i-\\hat C_i)^2 \\\\ &amp;+ \\lambda_{noobj} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}(C_i-\\hat C_i)^2 \\\\ &amp;+ \\sum_{i=1}^{S^2} \\mathbf 1_i^{obj} \\sum_{c \\in classes}\\left(p_i(c)-\\hat p_i(c)\\right)^2 \\end{aligned}<br>$$</p>\n<p><strong>分析：</strong></p>\n<p>带 <code>^</code> 的为网络输出，不带 <code>^</code> 则为 ground truth 值。<code>x, y, w, h</code> 为中心点坐标，<code>C</code> 为 IOU，<code>pi(c)</code> 为分类 <code>c</code> 的概率。</p>\n<p>$\\mathbf 1_{ij}^{obj}$ 表示第 <code>i</code> 个 grid cell 有目标（中心），且此 grid cell 上第 <code>j</code> 个预测 box 与 gt box 有最大 IOU，即，第 <code>j</code> box 负责预测。</p>\n<p>对于较大 box 和 较小 box，在相同偏差$\\Delta w, \\ \\Delta h$ 下，较大 box 的损失应该比较小 box 的损失更小才合理，然而两者平方差损失相同，所以我们对宽高 <code>w,h</code>，先求平方根，再求平方差，这在一定程度上降低了这种不合理性。</p>\n<p>$\\mathbf 1_{ij}^{noobj}$ 表示 i) 第 <code>i</code> 个 grid cell 无目标（中心），或者 ii) 有目标（中心），但是第 <code>j</code> 个预测 box 不负责预测（即，与 gt box 的 IOU 不是 <code>B</code> 个预测 box 中最大的）。</p>\n<p>$\\mathbf 1_i^{obj}$ 表示第 <code>i</code> 个 grid cell 有目标（中心）。</p>\n<p>坐标损失与分类损失分属两类损失，需要进行平衡。此外，由于大部分预测 box 其实并不负责预测，来自这部分预测 box 的 IOU 损失（损失公式中第四行）将会压制负责预测的 box 的坐标损失和 IOU 损失（损失公式中前三行），所以需要提升被压制的那部分的损失贡献。综合考虑，设置 $\\lambda_{coord}=5, \\ \\lambda_{noobj}=0.5$。</p>\n<h2 id=\"细节\"><a href=\"#细节\" class=\"headerlink\" title=\"细节\"></a>细节</h2><ol>\n<li>GT label 数据的 size 为 <code>S*S*(5+20)</code>，其中 <code>5</code> 包含了 4 个坐标值，1 个 IOU，20 为表示分类 id 的 one-hot vector 的长度。维度从高到低为 <code>(S,S,5+20)</code>，最低维数据顺序为 IOU, class id, x,y,w,h。</li>\n<li>网络输出 size 为 <code>S*S*(B*5+20)</code>，维度从高到低为 <code>(5+20,S,S)</code>，通道顺序为 class id, IOU, x,y,w,h。</li>\n<li>GT label 数据中， x,y,w,h 先进行归一化（除以图像宽/高），然后 <code>x=x*S-(int)x*S, y=y*S-(int)y*S</code>。</li>\n<li>网络输出中的 x,y 与 GT label 中含义一致，表示相对于 grid cell 的（归一化）偏差，而 w,h 则是经过了平方根处理。</li>\n</ol>\n<h1 id=\"YOLOv2\"><a href=\"#YOLOv2\" class=\"headerlink\" title=\"YOLOv2\"></a>YOLOv2</h1><h2 id=\"简介-1\"><a href=\"#简介-1\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>YOLOv2 是对 YOLOv1 的改进，包括：</p>\n<ol>\n<li>利用现有的分类数据集来扩展检测数据集，使得检测目标的分类种数更多。</li>\n<li>增加 Batch Normalization</li>\n<li>检测小目标更准确</li>\n</ol>\n<p>在分类集上预训练时，就使用较大分辨率的图像。YOLOv1 中使用 <code>224x224</code> 在分类集上预训练，然后直接将 <code>448x448</code> 大小的检测数据训练集喂给网络，这让网络同时适应高分辨率的图像以及学习目标检测，难免压力山大。YOLOv2 中，每隔十次 batch 训练，变更一次网络输入 size。</p>\n<h2 id=\"方法-1\"><a href=\"#方法-1\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>以 VOC 数据集为例，YOLOv2 的网络结构可以从配置文件 <code>cfg/yolov2-voc.cfg</code> 中获取。</p>\n<h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><ol>\n<li><p>输入 size 每隔 10 个 batch 变更一次，从 <code>320, 352, ..., 608</code> 这十个值中随机选择。记输入大小为 <code>(3,d,d)</code>。</p>\n</li>\n<li><p>网络整体下采样率为 32，输出大小为 <code>(125, d/32, d/32)</code>。其中，<code>(d/32,d/32)</code> 与 YOLOv1 中类似，可以看作原图像上的 grid cell 数量 <code>S=d/32</code>。如果目标的中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 上有 <code>5</code> 个预测 box，每各 box 有 <code>1</code> 个 IOU 以及 <code>4</code> 个坐标值，每个 box 独立拥有 <code>20</code> 个分类得分，故输出 channel 为 <code>125=5*(1+4+20)</code>。注意，YOLOv1 中每个 cell 上的 <code>B</code> 个预测 box 共享 <code>20</code> 个分类得分。</p>\n</li>\n<li><p>人为假设每个图像中目标数量最多为 30，所以 GT label 大小为 <code>30x5</code>，其中 <code>5</code> 包含了 4 个坐标值以及 1 个分类 id。最低维数据顺序为 x,h,w,h,class id。GT label 靠前存储。</p>\n</li>\n<li><p><code>(route:-1，-4)</code> 层将浅层特征（高分辨率）与高层特征（低分辨率）融合，类似于 ResNet 中的 identity mapping，这种更细粒度的特征将有助于小目标的检测。</p>\n</li>\n</ol>\n<h3 id=\"损失-1\"><a href=\"#损失-1\" class=\"headerlink\" title=\"损失\"></a>损失</h3><p>损失包括：分类损失，置信度损失，坐标损失三部分。</p>\n<p>$$L=L_p+L_{box}+L_C$$</p>\n<p><strong>分类损失</strong></p>\n<p>$$L_p=\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\sum_{c=1}^{20} \\mathbf 1_{ij}^{obj} [\\hat p_{ij}(c)-p_{ij}(c)]^2$$</p>\n<p><strong>坐标损失</strong></p>\n<p>$$L_{box}=\\lambda_{obj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (\\hat x_{ij} - x_{ij})^2 + (\\hat y_{ij} - y_{ij})^2+ (\\hat w_{ij} - w_{ij})^2+ (\\hat h_{ij} - h_{ij})^2<br>\\\\ + \\lambda_{noobj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{noobj} (\\hat x_{ij} - x_{ij}^a)^2 + (\\hat y_{ij} - y_{ij}^a)^2+ (\\hat w_{ij} - w_{ij}^a)^2+ (\\hat h_{ij} - h_{ij}^a)^2$$</p>\n<p><strong>置信度损失</strong></p>\n<p>$$L_C=\\lambda_{obj}^{conf}\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj}[\\hat C_{ij}-iou(\\hat \\text{box}<em>{ij}, \\text{box}</em>{ij})]^2<br>\\\\+ \\lambda_{noobj}^{conf}\\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}[\\hat C_{ij}-0]^2$$</p>\n<p>以上，带 ^ 表示 network 输出，带 a 表示 anchor，不带这两个修饰的表示 GT label。</p>\n<p><strong>分析：</strong></p>\n<p>网络输出 shape 从高维到低维为，<code>batch, B, 4+1+C, S, S</code>（其实无论几维，在内存中都是一维）。这里假设了输出 feature map 的 height 和 width 相等，均为 <code>S</code> （grid size），且 <code>4</code> 表示 4 个坐标，<code>1</code> 表示 IOU，<code>C</code> 表示分类数量。</p>\n<p>与 YOLOv1 中类似，目标中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 有 <code>B=5</code> 个预测 box，具有不同的 size。使用 5 组 anchor box 帮助预测，参考 yolov2-voc.cfg 文件中最后一个 layer 配置中 <code>anchors</code> 的值，给了 5 组 width height 的值，这些值基于输出 feature map 的 size <code>SxS</code>，即，并没有归一化。anchor box 的中心为所在 grid cell 坐标加 0.5，即 <code>(i,j)</code> 处 grid cell 的 anchor box 中心为 <code>(i+0.5, j+0.5)</code>。</p>\n<p>网络输出坐标 <code>x,y,h,w</code> 的具体含义，如图 2，<br><img src=\"/images/obj_det/YOLOv2_fig2.png\" alt=\"\"> <center>图2 预测 box 与 anchor box 的关系</center></p>\n<p>网络输出坐标实际含义就是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。</p>\n<p>一幅图像的 GT label 的 size 为 <code>30*5</code>，低维数据排列顺序为 <code>x,y,w,h, class id</code>，其中 <code>x,y,w,h</code> 是基于 original image 的 size 进行了归一化（<code>x,y</code> 与 YOLOv1 中稍有不同）。</p>\n<p>坐标损失中 $x_{ij}, y_{ij}, w_{ij}, h_{ij}$ 使用的是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$，对于网络输出，不用做任何修改，而对于 GT box 以及 anchor box，则需要做变换，也就是说，将预测 box 分别替换为 GT box 和 anchor box 来计算 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。</p>\n<p>位于某 location <code>(i,j)</code> 处，将 <code>B</code> 个预测 box 与 GT label 中所有目标 box 两两求 IOU，最后得到一个最大 IOU，如果这个最大 IOU 大于阈值 0.5，那么 $\\mathbf 1_{ij}^{noobj}=0$，此时置信度损失中第二项为 0。</p>\n<p>对于每个 GT box，找出与这个 GT box 有最大 IOU 预测 box，注意这个 IOU 没有阈值限制，然后设置 $\\mathbf 1_{ij}^{obj}=1$（每个 GT box 有且只有一个负责预测的 box），此时置信度损失中第一项非零，且分类损失非零，此时计算分类损失时，$\\sum_{c=1}^C$ 求和中，当且仅当 <code>c</code> 等于 GT label 中的 class id 时，$p_{ij}(c)=1$，其余 <code>C-1</code> 种情况 $p_{ij}(c)=0$。</p>\n<h1 id=\"YOLOv3\"><a href=\"#YOLOv3\" class=\"headerlink\" title=\"YOLOv3\"></a>YOLOv3</h1><p>在 YOLOv2 基础上做了修改：</p>\n<ol>\n<li>三个 scale 的输出 feature maps。每组 feature maps 的大小为 <code>NxNx[3*(4+1+C)]</code>，三个不同的 <code>N</code>，依次增大 2 倍。</li>\n<li>使用 <code>9</code> 个不同 scale 的 anchor box 帮助预测。由于有 <code>3</code> 个 scale 的 feature maps，所以实际上，每个 scale 大小的 feature maps 上每个 grid cell 仅使用 <code>9/3=3</code> 个 anchor box。</li>\n</ol>\n<p>以 VOC 数据集为例，网络结构参见 <code>cfg/yolov3-voc.cfg</code>。</p>\n<ol>\n<li>特征抽取网络的下采样率为 <code>32</code>。如果输入图像的大小为 <code>(h,w)</code>，那么输出feature map 大小为 <code>(h/32,w/32)</code>，另外两个 scale 的 feature maps 的大小则为 <code>(h/16,w/16)</code> 和 <code>(h/8, w/8)</code>。</li>\n<li>单个图像的 GT label 大小 为 <code>90*5</code>。这表示单个图像中目标数量最大不超过 <code>90</code>。</li>\n<li>大量使用 Residual layer。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"YOLOv1\"><a href=\"#YOLOv1\" class=\"headerlink\" title=\"YOLOv1\"></a>YOLOv1</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>one-stage 检测方法，从图像像素到bbox 坐标和分类概率，一步到位。优点为：</p>\n<ol>\n<li>快。YOLO 每秒可处理 45 帧，可实时处理视频流。</li>\n<li>从图像全局出发，作出预测。这种策略有效利用了上下文信息。</li>\n<li>能学习目标的泛化特征。基于天然图像数据集上的训练，用于人工图像数据集上的测试，也能取得较好的结果。</li>\n</ol>","more":"<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>YOLOv1 网络结构如图1，采用的是 VOC 数据集。<br><img src=\"/images/obj_det/YOLOv1_fig1.jpg\" alt=\"\"> <center>图 1 YOLOv1 网络结构</center></p>\n<p>输入 shape : <code>448,448,3</code>，<br>输出 shape ：<code>7,7,35</code></p>\n<h3 id=\"检测思想\"><a href=\"#检测思想\" class=\"headerlink\" title=\"检测思想\"></a>检测思想</h3><ol>\n<li>将输入图像划分为 <code>SxS</code> 大小的网格，如果目标中心落于某个 grid cell，那么这个 grid cell 负责检测这个目标。对于 VOC 数据集，使用 <code>S=7</code>，这是综合考虑，平衡了计算量和准确率。</li>\n<li>每个 grid cell 预测 <code>B</code> 个 box。这里取 <code>B=3</code>。</li>\n<li>对每个 box 预测，需要 5 个数据 <code>(x,y,w,h,IOU)</code>（全部都是归一化的）。</li>\n<li>VOC 数据集的分类数量 <code>C=20</code>。每个 grid cell 处预测 <code>C</code> 个分类概率，即，每个 grid cell 处的 <code>B</code> 个 box 共享这 <code>C</code> 个分类概率（因为实际上，<code>B</code> 个预测 box 中只有 一个 box 负责预测）。</li>\n<li>从 <code>448</code> 到 <code>7</code>，网络的下采样率为 <code>448/7=64</code>。从图 1 也能看出，具有 <code>s-2</code> 字样的 layer 共有 6 个。输出 map 的 spatial size 变成 <code>7x7</code>，channel 变成 <code>35</code>，这是因为每个空间位置处需要 <code>B*5+C=3*5+20</code> 个预测数据。</li>\n<li>训练阶段，计算损失，并求梯度，然后更新，具体参见下文详细分析。</li>\n<li>测试阶段，共检测出 <code>S*S*B</code> 个 box，每个 box 有 4 个坐标值，1 个 IOU 以及 C 个分类概率。对分类概率进行一个阈值截断，阈值默认为 0.2。分别针对每个分类，根据分类概率倒序排列，对 box 进行非极大抑制（设置被抑制 box 的当前分类的概率为 0），非极大抑制阈值默认为 0.4。最后，筛选出所有检测 box 中具有大于阈值（0.2）的分类概率，为最终检测结果。</li>\n</ol>\n<p><strong>思考：</strong> 为什么每个 grid cell 处预测不是 1 个 box 而是多个 box？</p>\n<p><strong>答：</strong> 我们假定不会有多个目标的中心落入同一个 grid cell，如果确实存在（这种概率很低），那么只有第一个目标的数据<code>x,y,w,h,IOU,class</code>会写入 gt <code>Y</code> 中。每个 grid cell 仍然预测多个 box，这是因为这些不同的预测 box 将具有不同的 size 或 aspect ratio。如果目标中心落入某个 grid cell，那么其上的 <code>B</code> 个预测 box 中，只有与目标 IOU 最大的预测 box 才负责预测。例如，某个预测 box 适合预测高的目标（人），而另一个预测 box 可能适合预测宽的目标（车）。</p>\n<h3 id=\"损失\"><a href=\"#损失\" class=\"headerlink\" title=\"损失\"></a>损失</h3><p>$$\\begin{aligned} L&amp;=\\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(x_i-\\hat x_i)^2+(y_i-\\hat y_i)^2 \\\\ &amp;+ \\lambda_{coord} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj}(\\sqrt {w_i}- \\sqrt {\\hat w_i})^2+(\\sqrt {h_i}- \\sqrt {\\hat h_i})^2 \\\\ &amp;+ \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (C_i-\\hat C_i)^2 \\\\ &amp;+ \\lambda_{noobj} \\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}(C_i-\\hat C_i)^2 \\\\ &amp;+ \\sum_{i=1}^{S^2} \\mathbf 1_i^{obj} \\sum_{c \\in classes}\\left(p_i(c)-\\hat p_i(c)\\right)^2 \\end{aligned}<br>$$</p>\n<p><strong>分析：</strong></p>\n<p>带 <code>^</code> 的为网络输出，不带 <code>^</code> 则为 ground truth 值。<code>x, y, w, h</code> 为中心点坐标，<code>C</code> 为 IOU，<code>pi(c)</code> 为分类 <code>c</code> 的概率。</p>\n<p>$\\mathbf 1_{ij}^{obj}$ 表示第 <code>i</code> 个 grid cell 有目标（中心），且此 grid cell 上第 <code>j</code> 个预测 box 与 gt box 有最大 IOU，即，第 <code>j</code> box 负责预测。</p>\n<p>对于较大 box 和 较小 box，在相同偏差$\\Delta w, \\ \\Delta h$ 下，较大 box 的损失应该比较小 box 的损失更小才合理，然而两者平方差损失相同，所以我们对宽高 <code>w,h</code>，先求平方根，再求平方差，这在一定程度上降低了这种不合理性。</p>\n<p>$\\mathbf 1_{ij}^{noobj}$ 表示 i) 第 <code>i</code> 个 grid cell 无目标（中心），或者 ii) 有目标（中心），但是第 <code>j</code> 个预测 box 不负责预测（即，与 gt box 的 IOU 不是 <code>B</code> 个预测 box 中最大的）。</p>\n<p>$\\mathbf 1_i^{obj}$ 表示第 <code>i</code> 个 grid cell 有目标（中心）。</p>\n<p>坐标损失与分类损失分属两类损失，需要进行平衡。此外，由于大部分预测 box 其实并不负责预测，来自这部分预测 box 的 IOU 损失（损失公式中第四行）将会压制负责预测的 box 的坐标损失和 IOU 损失（损失公式中前三行），所以需要提升被压制的那部分的损失贡献。综合考虑，设置 $\\lambda_{coord}=5, \\ \\lambda_{noobj}=0.5$。</p>\n<h2 id=\"细节\"><a href=\"#细节\" class=\"headerlink\" title=\"细节\"></a>细节</h2><ol>\n<li>GT label 数据的 size 为 <code>S*S*(5+20)</code>，其中 <code>5</code> 包含了 4 个坐标值，1 个 IOU，20 为表示分类 id 的 one-hot vector 的长度。维度从高到低为 <code>(S,S,5+20)</code>，最低维数据顺序为 IOU, class id, x,y,w,h。</li>\n<li>网络输出 size 为 <code>S*S*(B*5+20)</code>，维度从高到低为 <code>(5+20,S,S)</code>，通道顺序为 class id, IOU, x,y,w,h。</li>\n<li>GT label 数据中， x,y,w,h 先进行归一化（除以图像宽/高），然后 <code>x=x*S-(int)x*S, y=y*S-(int)y*S</code>。</li>\n<li>网络输出中的 x,y 与 GT label 中含义一致，表示相对于 grid cell 的（归一化）偏差，而 w,h 则是经过了平方根处理。</li>\n</ol>\n<h1 id=\"YOLOv2\"><a href=\"#YOLOv2\" class=\"headerlink\" title=\"YOLOv2\"></a>YOLOv2</h1><h2 id=\"简介-1\"><a href=\"#简介-1\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>YOLOv2 是对 YOLOv1 的改进，包括：</p>\n<ol>\n<li>利用现有的分类数据集来扩展检测数据集，使得检测目标的分类种数更多。</li>\n<li>增加 Batch Normalization</li>\n<li>检测小目标更准确</li>\n</ol>\n<p>在分类集上预训练时，就使用较大分辨率的图像。YOLOv1 中使用 <code>224x224</code> 在分类集上预训练，然后直接将 <code>448x448</code> 大小的检测数据训练集喂给网络，这让网络同时适应高分辨率的图像以及学习目标检测，难免压力山大。YOLOv2 中，每隔十次 batch 训练，变更一次网络输入 size。</p>\n<h2 id=\"方法-1\"><a href=\"#方法-1\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>以 VOC 数据集为例，YOLOv2 的网络结构可以从配置文件 <code>cfg/yolov2-voc.cfg</code> 中获取。</p>\n<h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><ol>\n<li><p>输入 size 每隔 10 个 batch 变更一次，从 <code>320, 352, ..., 608</code> 这十个值中随机选择。记输入大小为 <code>(3,d,d)</code>。</p>\n</li>\n<li><p>网络整体下采样率为 32，输出大小为 <code>(125, d/32, d/32)</code>。其中，<code>(d/32,d/32)</code> 与 YOLOv1 中类似，可以看作原图像上的 grid cell 数量 <code>S=d/32</code>。如果目标的中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 上有 <code>5</code> 个预测 box，每各 box 有 <code>1</code> 个 IOU 以及 <code>4</code> 个坐标值，每个 box 独立拥有 <code>20</code> 个分类得分，故输出 channel 为 <code>125=5*(1+4+20)</code>。注意，YOLOv1 中每个 cell 上的 <code>B</code> 个预测 box 共享 <code>20</code> 个分类得分。</p>\n</li>\n<li><p>人为假设每个图像中目标数量最多为 30，所以 GT label 大小为 <code>30x5</code>，其中 <code>5</code> 包含了 4 个坐标值以及 1 个分类 id。最低维数据顺序为 x,h,w,h,class id。GT label 靠前存储。</p>\n</li>\n<li><p><code>(route:-1，-4)</code> 层将浅层特征（高分辨率）与高层特征（低分辨率）融合，类似于 ResNet 中的 identity mapping，这种更细粒度的特征将有助于小目标的检测。</p>\n</li>\n</ol>\n<h3 id=\"损失-1\"><a href=\"#损失-1\" class=\"headerlink\" title=\"损失\"></a>损失</h3><p>损失包括：分类损失，置信度损失，坐标损失三部分。</p>\n<p>$$L=L_p+L_{box}+L_C$$</p>\n<p><strong>分类损失</strong></p>\n<p>$$L_p=\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\sum_{c=1}^{20} \\mathbf 1_{ij}^{obj} [\\hat p_{ij}(c)-p_{ij}(c)]^2$$</p>\n<p><strong>坐标损失</strong></p>\n<p>$$L_{box}=\\lambda_{obj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj} (\\hat x_{ij} - x_{ij})^2 + (\\hat y_{ij} - y_{ij})^2+ (\\hat w_{ij} - w_{ij})^2+ (\\hat h_{ij} - h_{ij})^2<br>\\\\ + \\lambda_{noobj}^{coord} \\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{noobj} (\\hat x_{ij} - x_{ij}^a)^2 + (\\hat y_{ij} - y_{ij}^a)^2+ (\\hat w_{ij} - w_{ij}^a)^2+ (\\hat h_{ij} - h_{ij}^a)^2$$</p>\n<p><strong>置信度损失</strong></p>\n<p>$$L_C=\\lambda_{obj}^{conf}\\sum_{i=1}^{S^2} \\sum_{j=1}^B \\mathbf 1_{ij}^{obj}[\\hat C_{ij}-iou(\\hat \\text{box}<em>{ij}, \\text{box}</em>{ij})]^2<br>\\\\+ \\lambda_{noobj}^{conf}\\sum_{i=1}^{S^2}\\sum_{j=1}^B \\mathbf 1_{ij}^{noobj}[\\hat C_{ij}-0]^2$$</p>\n<p>以上，带 ^ 表示 network 输出，带 a 表示 anchor，不带这两个修饰的表示 GT label。</p>\n<p><strong>分析：</strong></p>\n<p>网络输出 shape 从高维到低维为，<code>batch, B, 4+1+C, S, S</code>（其实无论几维，在内存中都是一维）。这里假设了输出 feature map 的 height 和 width 相等，均为 <code>S</code> （grid size），且 <code>4</code> 表示 4 个坐标，<code>1</code> 表示 IOU，<code>C</code> 表示分类数量。</p>\n<p>与 YOLOv1 中类似，目标中心落入某个 grid cell，那么这个 grid cell 负责预测目标。每个 grid cell 有 <code>B=5</code> 个预测 box，具有不同的 size。使用 5 组 anchor box 帮助预测，参考 yolov2-voc.cfg 文件中最后一个 layer 配置中 <code>anchors</code> 的值，给了 5 组 width height 的值，这些值基于输出 feature map 的 size <code>SxS</code>，即，并没有归一化。anchor box 的中心为所在 grid cell 坐标加 0.5，即 <code>(i,j)</code> 处 grid cell 的 anchor box 中心为 <code>(i+0.5, j+0.5)</code>。</p>\n<p>网络输出坐标 <code>x,y,h,w</code> 的具体含义，如图 2，<br><img src=\"/images/obj_det/YOLOv2_fig2.png\" alt=\"\"> <center>图2 预测 box 与 anchor box 的关系</center></p>\n<p>网络输出坐标实际含义就是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。</p>\n<p>一幅图像的 GT label 的 size 为 <code>30*5</code>，低维数据排列顺序为 <code>x,y,w,h, class id</code>，其中 <code>x,y,w,h</code> 是基于 original image 的 size 进行了归一化（<code>x,y</code> 与 YOLOv1 中稍有不同）。</p>\n<p>坐标损失中 $x_{ij}, y_{ij}, w_{ij}, h_{ij}$ 使用的是 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$，对于网络输出，不用做任何修改，而对于 GT box 以及 anchor box，则需要做变换，也就是说，将预测 box 分别替换为 GT box 和 anchor box 来计算 $\\sigma(t_x), \\sigma(t_y), t_w, t_h$。</p>\n<p>位于某 location <code>(i,j)</code> 处，将 <code>B</code> 个预测 box 与 GT label 中所有目标 box 两两求 IOU，最后得到一个最大 IOU，如果这个最大 IOU 大于阈值 0.5，那么 $\\mathbf 1_{ij}^{noobj}=0$，此时置信度损失中第二项为 0。</p>\n<p>对于每个 GT box，找出与这个 GT box 有最大 IOU 预测 box，注意这个 IOU 没有阈值限制，然后设置 $\\mathbf 1_{ij}^{obj}=1$（每个 GT box 有且只有一个负责预测的 box），此时置信度损失中第一项非零，且分类损失非零，此时计算分类损失时，$\\sum_{c=1}^C$ 求和中，当且仅当 <code>c</code> 等于 GT label 中的 class id 时，$p_{ij}(c)=1$，其余 <code>C-1</code> 种情况 $p_{ij}(c)=0$。</p>\n<h1 id=\"YOLOv3\"><a href=\"#YOLOv3\" class=\"headerlink\" title=\"YOLOv3\"></a>YOLOv3</h1><p>在 YOLOv2 基础上做了修改：</p>\n<ol>\n<li>三个 scale 的输出 feature maps。每组 feature maps 的大小为 <code>NxNx[3*(4+1+C)]</code>，三个不同的 <code>N</code>，依次增大 2 倍。</li>\n<li>使用 <code>9</code> 个不同 scale 的 anchor box 帮助预测。由于有 <code>3</code> 个 scale 的 feature maps，所以实际上，每个 scale 大小的 feature maps 上每个 grid cell 仅使用 <code>9/3=3</code> 个 anchor box。</li>\n</ol>\n<p>以 VOC 数据集为例，网络结构参见 <code>cfg/yolov3-voc.cfg</code>。</p>\n<ol>\n<li>特征抽取网络的下采样率为 <code>32</code>。如果输入图像的大小为 <code>(h,w)</code>，那么输出feature map 大小为 <code>(h/32,w/32)</code>，另外两个 scale 的 feature maps 的大小则为 <code>(h/16,w/16)</code> 和 <code>(h/8, w/8)</code>。</li>\n<li>单个图像的 GT label 大小 为 <code>90*5</code>。这表示单个图像中目标数量最大不超过 <code>90</code>。</li>\n<li>大量使用 Residual layer。</li>\n</ol>"},{"title":"DenseNet","p":"img_cls/densenet","date":"2019-12-31T07:27:49.000Z","mathjax":true,"_content":"\n论文 [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n\n随着现在数据集越来越大，网络容量也需要增大，否则容易出现过拟合现象，一种增大网络容量的方法是增加更多的 layer ，让网络更深（deep），但是这也会带来新的问题：随着输入（或梯度）经过越来越多的 layer，信息可能在到达最后一层（对于梯度则是反向传播在到达网络第一层）之前就消失了。ResNet 增加 shortcut，即 early layer 到 later layer 之间直接连接，以此来解决这个问题。本文提出 densenet，将这种连接风格贯彻到底，网络结构如图1，\n<!-- more -->\n\n![](/images/img_cls/densenet_1.png)\n\n\n## DenseNet 数学描述\n假设输入为 $\\mathbf x_0$，网络共有 $L$ 层，每层非线性转换操作记为 $H_l(\\cdot)$，输出记为 $\\mathbf x_l$。\n### ResNet\n传统的网络，第 $l$ 层操作为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})$ ResNet 增加一个 Identity 分支后为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})+\\mathbf x_{l-1}$，ResNet 中一个 layer 其实是一个 block，不是单层 layer，block 内部的 layer 没有 shortcut 分支，这点需要注意。\n### Dense 连接\n从某个 layer 到所有后继 layer 之间增加直接连接，对于第 $l$ 层 layer 来说，其输入是所有前序 layer 的输出以及原始网络输入，所以有\n$$\\mathbf x_l=H_l([\\mathbf x_0, ... , \\mathbf x_{l-1}])$$\n其中 $[\\mathbf x_0, ... , \\mathbf x_{l-1}]$ 表示 `concatenate` 操作，这与 ResNet 的 `sum` 操作不同。\n\n### 组合函数\nDenseNet 中每个 layer 的操作 $H_l(\\cdot)$ 是由：\n1. 批规范化 BN；\n2. ReLU；\n3. `3x3` conv\n   \n组合而得。\n### Pooling 层\n网络肯定存在下采样，这就导致 early layer 与 later layer 可能有不同的 feature maps 大小，这就导致没法直接连接。一种解决的办法如图 2，将若干个 layer 分为一组作为一个 block，称为 Dense Block，block 内部的 feature maps 大小保持不变，从而 block 内部的 layer 之间可以进行密集连接，block 之间使用过渡层进行下采样，在作者实验中过渡层包含 BN 层、`1x1` 卷积层以及 `2x2` 均值池化层。\n![](/images/img_cls/densenet_2.png)\n\n### Growth rate\n记 $k_0$ 为 Dense Block 初始输入的 channels，如果每个 $H_l$ 输出均为 $k$ 个 feature maps，由于 $l^{th}$ layer 的输入为初始网络输入 $\\mathbf x_0$ 以及前 $l-1$ 个 layer 输出的 concatenation，所以共有 $k_0+k(l-1)$ 个 feature maps，所以 $k$ 值即使较小，随着 $l^{th}$ 增大，深层 layer 的 `in_channels` 也可以很大，因为可以使用较小 $k$ 值，这就使得 DenseNet 与传统网络相比，拥有更少的网络参数。记 $k$ 为 _growth rate_ ，表示 layer 输入 feature maps 增长量。作者实验表明，即使非常小的 _growth rate_ 也可以获取很好的测试结果。作者解释为，每个 layer 可以利用同 block 内的前面所有 layer 的输出 feature maps，也就是 “collective knowledge”，将 feature maps 看作网络的 global state，每个 layer 贡献自己的 k 个 feature maps 到这个 global state，同时 _growth rate_ 控制了每个 layer 对 global state 的贡献量，并且每次某个 layer 对 global state 贡献完毕，此时的 global state 可以被之后所有 layer 利用。传统网络为了在 layer 到 layer 之间传递这种 global state，不得不使用越来越多的输出通道数，达到复制前面各阶段的 global states 的效果。\n\n### Bottleneck layers\nlater layer 的输入通道数较大，可以在这些 layer（通常是 `3x3` 卷积） 前面增加一个 `1x1` 卷积作为 bottleneck layer，降低 later layer 的输入通道数，提高计算效率。记这种带有 bottleneck layer 的 DenseNet 为 DenseNet-B，其中 layer 的操作 $H_l(\\cdot)$ 变为 BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)（参考前面 __组合函数__ 小节）。\n\n### Compression\n为了进一步压缩模型，可以在过渡层降低 feature maps 的数量，记 dense block 的输出 feature maps 数量为 $m$，注意是 dense block 初始输入和各 layer 输出的叠加，参见图 1，过渡层输出 feature maps 数量为 $\\lfloor \\theta m \\rfloor$，其中 $0 < \\theta \\le 1$，当 $\\theta=1$，过渡层不改变 feature maps 的数量，当 $\\theta <1$ 记这样的 DenseNet 为 DenseNet-C，而 DenseNet-BC 则表示既有 bottleneck layer 又有 $\\theta <1$ 的 DenseNet。\n\n### 实现细节\n作者在 CIFAR-10,CIFAR-100,SVHN 以及 ImageNet 四个数据集上评估了 DenseNet。\n\n在前三个数据集上（这三个数据集的图像 size 均为 `32x32`），DenseNet 使用了 3 个 dense block，每个 dense block 有相等数量的 layer。在进入第一个 dense block 之前，输入图像数据先经过了一个 `1x1` 16-out_channels 的卷积层（对于 DenseNet-BC 网络，输出通道数为 _growth rate_ 的两倍）。对于 `3x3` conv，进行 padding=1 的 zero 填充，以保持 feature map size 不变。在最后一个 dense block 之后，进行全局均值池化以及 softmax 操作。三个 dense block 的 feature maps 大小分别为 `32x32, 16x16, 8x8`。使用最普通的 DenseNet（不带 B\\C 后缀）进行实验时，dense block 配置分别为 $\\{L=40,k=12\\}$, $\\{L=100,k=12\\}$ 和 $\\{L=100,k=24\\}$，使用 DenseNet-BC 进行实验时，配置分别为 $\\{L=100,k=12\\}$, $\\{L=250,k=24\\}$ 和 $\\{L=190,k=40\\}$。\n\n对于 ImageNet，使用 4 个 dense block 的 DenseNet-BC，输入图像大小为 `224x224`，初始 layer 为一个 `7x7` 2k-out_channels，stride=2 的卷积，其中 k 表示 _growth-rate_ ，所有 dense block 均为 $k=32$，网络具体描述如表 1 所示，\n![](/images/img_cls/densenet_3.png) <center>ImageNet 对应的四个 DenseNet-BC 网络配置。所有网络的 k=32，表中所有 conv 均表示 __BN-ReLU-Conv__ </center>\n\n## 实验\n实验结果的对比可直接参考原论文。\n\n### 数据集\n__CIFAR：__  CIFAR-10 和 CIFAR-100 数据集包含 `32x32` 大小的彩色图像，其中 CIFAR-10 的图像分类数量为 10，CIFAR-100 图像分类数量为 100。训练集和测试集大小分别为 50000 和 10000，训练集中取 5000 作为验证集。数据扩增使用 镜像/平移 两种方法。预处理包括 RGB 三通道分别使用 `mean` 和 `std` 归一化。最后一轮训练时使用全部 50000 个图像，然后计算测试错误。\n\n__SVHN：__  SVHN 数据集包含 `32x32` 大小的彩色数字图像，训练集和测试集大小分别为 73257 和 26032，另有 531131 个图像作为额外的训练数据。作者实验中，不采用任何数据扩增手段，从训练集中取 6000 个图像作为验证集。选择具有最低验证错误的训练结果，然后计算测试错误。图像像素值除以 255 以落于 `[0,1]` 范围作为归一化处理方法。\n\n__ImageNet：__  使用 ILSVRC 2012 分类数据集，包含 120 万训练集以及 5 万的验证集，分类总数为 1000。使用与 ResNet 中相同的数据扩增方法，采用 single-crop 或者 10-crop 方法得到 `224x224` 的输入大小，最后计算验证集上的错误率。\n\n### 训练\nCIFAR 和 SVHN 训练 batch 大小为 64，epoch 分别为 300 和 40。初始学习率 lr = 0.1，在 50% 和 75% 的 epoch 时分别再降为 10%。\n\nImageNet 训练 batch 大小为 256， epoch 为 90。初始学习率 lr=0.1，在 epoch = 30 和 epoch = 60 时分别降为 10%。\n\n权重衰减因子为 $10^{-4}$，Nesterov momentum 为 0.9。\n\n由于本文着重记录 DenseNet 的网络结构，所以实验结果数据的分析以及与其他网络的比较此处省略，可参考原文进行反复阅读理解。\n\n## DenseNet 特点：\n\n1. dense block 内每两个 layer 之间均存在直接连接。\n2. 直接连接与普通前向传播的合并采用 `concatenate` 方式，而 ResNet 中则是 `sum` 方式。\n3. 比传统网络有更少的参数。layer 之间的连接更加密集，所以 layer 只需要很小的 filter 数量。\n4. 易于训练。这得益于 DenseNet 更优的信息流（梯度流）传递。early layer 可以直接利用到损失函数对原始输入的梯度，这种深度监督有益于训练。\n5. layer 之间的密集直接连接有正则化效果，在小训练集上有助于降低过拟合。","source":"_posts/img_cls/densenet.md","raw":"---\ntitle: DenseNet\np: img_cls/densenet\ndate: 2019-12-31 15:27:49\ntags: image classification\nmathjax: true\n---\n\n论文 [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n\n随着现在数据集越来越大，网络容量也需要增大，否则容易出现过拟合现象，一种增大网络容量的方法是增加更多的 layer ，让网络更深（deep），但是这也会带来新的问题：随着输入（或梯度）经过越来越多的 layer，信息可能在到达最后一层（对于梯度则是反向传播在到达网络第一层）之前就消失了。ResNet 增加 shortcut，即 early layer 到 later layer 之间直接连接，以此来解决这个问题。本文提出 densenet，将这种连接风格贯彻到底，网络结构如图1，\n<!-- more -->\n\n![](/images/img_cls/densenet_1.png)\n\n\n## DenseNet 数学描述\n假设输入为 $\\mathbf x_0$，网络共有 $L$ 层，每层非线性转换操作记为 $H_l(\\cdot)$，输出记为 $\\mathbf x_l$。\n### ResNet\n传统的网络，第 $l$ 层操作为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})$ ResNet 增加一个 Identity 分支后为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})+\\mathbf x_{l-1}$，ResNet 中一个 layer 其实是一个 block，不是单层 layer，block 内部的 layer 没有 shortcut 分支，这点需要注意。\n### Dense 连接\n从某个 layer 到所有后继 layer 之间增加直接连接，对于第 $l$ 层 layer 来说，其输入是所有前序 layer 的输出以及原始网络输入，所以有\n$$\\mathbf x_l=H_l([\\mathbf x_0, ... , \\mathbf x_{l-1}])$$\n其中 $[\\mathbf x_0, ... , \\mathbf x_{l-1}]$ 表示 `concatenate` 操作，这与 ResNet 的 `sum` 操作不同。\n\n### 组合函数\nDenseNet 中每个 layer 的操作 $H_l(\\cdot)$ 是由：\n1. 批规范化 BN；\n2. ReLU；\n3. `3x3` conv\n   \n组合而得。\n### Pooling 层\n网络肯定存在下采样，这就导致 early layer 与 later layer 可能有不同的 feature maps 大小，这就导致没法直接连接。一种解决的办法如图 2，将若干个 layer 分为一组作为一个 block，称为 Dense Block，block 内部的 feature maps 大小保持不变，从而 block 内部的 layer 之间可以进行密集连接，block 之间使用过渡层进行下采样，在作者实验中过渡层包含 BN 层、`1x1` 卷积层以及 `2x2` 均值池化层。\n![](/images/img_cls/densenet_2.png)\n\n### Growth rate\n记 $k_0$ 为 Dense Block 初始输入的 channels，如果每个 $H_l$ 输出均为 $k$ 个 feature maps，由于 $l^{th}$ layer 的输入为初始网络输入 $\\mathbf x_0$ 以及前 $l-1$ 个 layer 输出的 concatenation，所以共有 $k_0+k(l-1)$ 个 feature maps，所以 $k$ 值即使较小，随着 $l^{th}$ 增大，深层 layer 的 `in_channels` 也可以很大，因为可以使用较小 $k$ 值，这就使得 DenseNet 与传统网络相比，拥有更少的网络参数。记 $k$ 为 _growth rate_ ，表示 layer 输入 feature maps 增长量。作者实验表明，即使非常小的 _growth rate_ 也可以获取很好的测试结果。作者解释为，每个 layer 可以利用同 block 内的前面所有 layer 的输出 feature maps，也就是 “collective knowledge”，将 feature maps 看作网络的 global state，每个 layer 贡献自己的 k 个 feature maps 到这个 global state，同时 _growth rate_ 控制了每个 layer 对 global state 的贡献量，并且每次某个 layer 对 global state 贡献完毕，此时的 global state 可以被之后所有 layer 利用。传统网络为了在 layer 到 layer 之间传递这种 global state，不得不使用越来越多的输出通道数，达到复制前面各阶段的 global states 的效果。\n\n### Bottleneck layers\nlater layer 的输入通道数较大，可以在这些 layer（通常是 `3x3` 卷积） 前面增加一个 `1x1` 卷积作为 bottleneck layer，降低 later layer 的输入通道数，提高计算效率。记这种带有 bottleneck layer 的 DenseNet 为 DenseNet-B，其中 layer 的操作 $H_l(\\cdot)$ 变为 BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)（参考前面 __组合函数__ 小节）。\n\n### Compression\n为了进一步压缩模型，可以在过渡层降低 feature maps 的数量，记 dense block 的输出 feature maps 数量为 $m$，注意是 dense block 初始输入和各 layer 输出的叠加，参见图 1，过渡层输出 feature maps 数量为 $\\lfloor \\theta m \\rfloor$，其中 $0 < \\theta \\le 1$，当 $\\theta=1$，过渡层不改变 feature maps 的数量，当 $\\theta <1$ 记这样的 DenseNet 为 DenseNet-C，而 DenseNet-BC 则表示既有 bottleneck layer 又有 $\\theta <1$ 的 DenseNet。\n\n### 实现细节\n作者在 CIFAR-10,CIFAR-100,SVHN 以及 ImageNet 四个数据集上评估了 DenseNet。\n\n在前三个数据集上（这三个数据集的图像 size 均为 `32x32`），DenseNet 使用了 3 个 dense block，每个 dense block 有相等数量的 layer。在进入第一个 dense block 之前，输入图像数据先经过了一个 `1x1` 16-out_channels 的卷积层（对于 DenseNet-BC 网络，输出通道数为 _growth rate_ 的两倍）。对于 `3x3` conv，进行 padding=1 的 zero 填充，以保持 feature map size 不变。在最后一个 dense block 之后，进行全局均值池化以及 softmax 操作。三个 dense block 的 feature maps 大小分别为 `32x32, 16x16, 8x8`。使用最普通的 DenseNet（不带 B\\C 后缀）进行实验时，dense block 配置分别为 $\\{L=40,k=12\\}$, $\\{L=100,k=12\\}$ 和 $\\{L=100,k=24\\}$，使用 DenseNet-BC 进行实验时，配置分别为 $\\{L=100,k=12\\}$, $\\{L=250,k=24\\}$ 和 $\\{L=190,k=40\\}$。\n\n对于 ImageNet，使用 4 个 dense block 的 DenseNet-BC，输入图像大小为 `224x224`，初始 layer 为一个 `7x7` 2k-out_channels，stride=2 的卷积，其中 k 表示 _growth-rate_ ，所有 dense block 均为 $k=32$，网络具体描述如表 1 所示，\n![](/images/img_cls/densenet_3.png) <center>ImageNet 对应的四个 DenseNet-BC 网络配置。所有网络的 k=32，表中所有 conv 均表示 __BN-ReLU-Conv__ </center>\n\n## 实验\n实验结果的对比可直接参考原论文。\n\n### 数据集\n__CIFAR：__  CIFAR-10 和 CIFAR-100 数据集包含 `32x32` 大小的彩色图像，其中 CIFAR-10 的图像分类数量为 10，CIFAR-100 图像分类数量为 100。训练集和测试集大小分别为 50000 和 10000，训练集中取 5000 作为验证集。数据扩增使用 镜像/平移 两种方法。预处理包括 RGB 三通道分别使用 `mean` 和 `std` 归一化。最后一轮训练时使用全部 50000 个图像，然后计算测试错误。\n\n__SVHN：__  SVHN 数据集包含 `32x32` 大小的彩色数字图像，训练集和测试集大小分别为 73257 和 26032，另有 531131 个图像作为额外的训练数据。作者实验中，不采用任何数据扩增手段，从训练集中取 6000 个图像作为验证集。选择具有最低验证错误的训练结果，然后计算测试错误。图像像素值除以 255 以落于 `[0,1]` 范围作为归一化处理方法。\n\n__ImageNet：__  使用 ILSVRC 2012 分类数据集，包含 120 万训练集以及 5 万的验证集，分类总数为 1000。使用与 ResNet 中相同的数据扩增方法，采用 single-crop 或者 10-crop 方法得到 `224x224` 的输入大小，最后计算验证集上的错误率。\n\n### 训练\nCIFAR 和 SVHN 训练 batch 大小为 64，epoch 分别为 300 和 40。初始学习率 lr = 0.1，在 50% 和 75% 的 epoch 时分别再降为 10%。\n\nImageNet 训练 batch 大小为 256， epoch 为 90。初始学习率 lr=0.1，在 epoch = 30 和 epoch = 60 时分别降为 10%。\n\n权重衰减因子为 $10^{-4}$，Nesterov momentum 为 0.9。\n\n由于本文着重记录 DenseNet 的网络结构，所以实验结果数据的分析以及与其他网络的比较此处省略，可参考原文进行反复阅读理解。\n\n## DenseNet 特点：\n\n1. dense block 内每两个 layer 之间均存在直接连接。\n2. 直接连接与普通前向传播的合并采用 `concatenate` 方式，而 ResNet 中则是 `sum` 方式。\n3. 比传统网络有更少的参数。layer 之间的连接更加密集，所以 layer 只需要很小的 filter 数量。\n4. 易于训练。这得益于 DenseNet 更优的信息流（梯度流）传递。early layer 可以直接利用到损失函数对原始输入的梯度，这种深度监督有益于训练。\n5. layer 之间的密集直接连接有正则化效果，在小训练集上有助于降低过拟合。","slug":"img_cls/densenet","published":1,"updated":"2020-04-24T10:33:15.098Z","_id":"ck9dzcj1d0026gga62ewrfhlt","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文 <a href=\"https://arxiv.org/abs/1608.06993\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n<p>随着现在数据集越来越大，网络容量也需要增大，否则容易出现过拟合现象，一种增大网络容量的方法是增加更多的 layer ，让网络更深（deep），但是这也会带来新的问题：随着输入（或梯度）经过越来越多的 layer，信息可能在到达最后一层（对于梯度则是反向传播在到达网络第一层）之前就消失了。ResNet 增加 shortcut，即 early layer 到 later layer 之间直接连接，以此来解决这个问题。本文提出 densenet，将这种连接风格贯彻到底，网络结构如图1，</p>\n<a id=\"more\"></a>\n\n<p><img src=\"/images/img_cls/densenet_1.png\" alt=\"\"></p>\n<h2 id=\"DenseNet-数学描述\"><a href=\"#DenseNet-数学描述\" class=\"headerlink\" title=\"DenseNet 数学描述\"></a>DenseNet 数学描述</h2><p>假设输入为 $\\mathbf x_0$，网络共有 $L$ 层，每层非线性转换操作记为 $H_l(\\cdot)$，输出记为 $\\mathbf x_l$。</p>\n<h3 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h3><p>传统的网络，第 $l$ 层操作为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})$ ResNet 增加一个 Identity 分支后为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})+\\mathbf x_{l-1}$，ResNet 中一个 layer 其实是一个 block，不是单层 layer，block 内部的 layer 没有 shortcut 分支，这点需要注意。</p>\n<h3 id=\"Dense-连接\"><a href=\"#Dense-连接\" class=\"headerlink\" title=\"Dense 连接\"></a>Dense 连接</h3><p>从某个 layer 到所有后继 layer 之间增加直接连接，对于第 $l$ 层 layer 来说，其输入是所有前序 layer 的输出以及原始网络输入，所以有<br>$$\\mathbf x_l=H_l([\\mathbf x_0, … , \\mathbf x_{l-1}])$$<br>其中 $[\\mathbf x_0, … , \\mathbf x_{l-1}]$ 表示 <code>concatenate</code> 操作，这与 ResNet 的 <code>sum</code> 操作不同。</p>\n<h3 id=\"组合函数\"><a href=\"#组合函数\" class=\"headerlink\" title=\"组合函数\"></a>组合函数</h3><p>DenseNet 中每个 layer 的操作 $H_l(\\cdot)$ 是由：</p>\n<ol>\n<li>批规范化 BN；</li>\n<li>ReLU；</li>\n<li><code>3x3</code> conv</li>\n</ol>\n<p>组合而得。</p>\n<h3 id=\"Pooling-层\"><a href=\"#Pooling-层\" class=\"headerlink\" title=\"Pooling 层\"></a>Pooling 层</h3><p>网络肯定存在下采样，这就导致 early layer 与 later layer 可能有不同的 feature maps 大小，这就导致没法直接连接。一种解决的办法如图 2，将若干个 layer 分为一组作为一个 block，称为 Dense Block，block 内部的 feature maps 大小保持不变，从而 block 内部的 layer 之间可以进行密集连接，block 之间使用过渡层进行下采样，在作者实验中过渡层包含 BN 层、<code>1x1</code> 卷积层以及 <code>2x2</code> 均值池化层。<br><img src=\"/images/img_cls/densenet_2.png\" alt=\"\"></p>\n<h3 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h3><p>记 $k_0$ 为 Dense Block 初始输入的 channels，如果每个 $H_l$ 输出均为 $k$ 个 feature maps，由于 $l^{th}$ layer 的输入为初始网络输入 $\\mathbf x_0$ 以及前 $l-1$ 个 layer 输出的 concatenation，所以共有 $k_0+k(l-1)$ 个 feature maps，所以 $k$ 值即使较小，随着 $l^{th}$ 增大，深层 layer 的 <code>in_channels</code> 也可以很大，因为可以使用较小 $k$ 值，这就使得 DenseNet 与传统网络相比，拥有更少的网络参数。记 $k$ 为 <em>growth rate</em> ，表示 layer 输入 feature maps 增长量。作者实验表明，即使非常小的 <em>growth rate</em> 也可以获取很好的测试结果。作者解释为，每个 layer 可以利用同 block 内的前面所有 layer 的输出 feature maps，也就是 “collective knowledge”，将 feature maps 看作网络的 global state，每个 layer 贡献自己的 k 个 feature maps 到这个 global state，同时 <em>growth rate</em> 控制了每个 layer 对 global state 的贡献量，并且每次某个 layer 对 global state 贡献完毕，此时的 global state 可以被之后所有 layer 利用。传统网络为了在 layer 到 layer 之间传递这种 global state，不得不使用越来越多的输出通道数，达到复制前面各阶段的 global states 的效果。</p>\n<h3 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h3><p>later layer 的输入通道数较大，可以在这些 layer（通常是 <code>3x3</code> 卷积） 前面增加一个 <code>1x1</code> 卷积作为 bottleneck layer，降低 later layer 的输入通道数，提高计算效率。记这种带有 bottleneck layer 的 DenseNet 为 DenseNet-B，其中 layer 的操作 $H_l(\\cdot)$ 变为 BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)（参考前面 <strong>组合函数</strong> 小节）。</p>\n<h3 id=\"Compression\"><a href=\"#Compression\" class=\"headerlink\" title=\"Compression\"></a>Compression</h3><p>为了进一步压缩模型，可以在过渡层降低 feature maps 的数量，记 dense block 的输出 feature maps 数量为 $m$，注意是 dense block 初始输入和各 layer 输出的叠加，参见图 1，过渡层输出 feature maps 数量为 $\\lfloor \\theta m \\rfloor$，其中 $0 &lt; \\theta \\le 1$，当 $\\theta=1$，过渡层不改变 feature maps 的数量，当 $\\theta &lt;1$ 记这样的 DenseNet 为 DenseNet-C，而 DenseNet-BC 则表示既有 bottleneck layer 又有 $\\theta &lt;1$ 的 DenseNet。</p>\n<h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><p>作者在 CIFAR-10,CIFAR-100,SVHN 以及 ImageNet 四个数据集上评估了 DenseNet。</p>\n<p>在前三个数据集上（这三个数据集的图像 size 均为 <code>32x32</code>），DenseNet 使用了 3 个 dense block，每个 dense block 有相等数量的 layer。在进入第一个 dense block 之前，输入图像数据先经过了一个 <code>1x1</code> 16-out_channels 的卷积层（对于 DenseNet-BC 网络，输出通道数为 <em>growth rate</em> 的两倍）。对于 <code>3x3</code> conv，进行 padding=1 的 zero 填充，以保持 feature map size 不变。在最后一个 dense block 之后，进行全局均值池化以及 softmax 操作。三个 dense block 的 feature maps 大小分别为 <code>32x32, 16x16, 8x8</code>。使用最普通的 DenseNet（不带 B\\C 后缀）进行实验时，dense block 配置分别为 ${L=40,k=12}$, ${L=100,k=12}$ 和 ${L=100,k=24}$，使用 DenseNet-BC 进行实验时，配置分别为 ${L=100,k=12}$, ${L=250,k=24}$ 和 ${L=190,k=40}$。</p>\n<p>对于 ImageNet，使用 4 个 dense block 的 DenseNet-BC，输入图像大小为 <code>224x224</code>，初始 layer 为一个 <code>7x7</code> 2k-out_channels，stride=2 的卷积，其中 k 表示 <em>growth-rate</em> ，所有 dense block 均为 $k=32$，网络具体描述如表 1 所示，<br><img src=\"/images/img_cls/densenet_3.png\" alt=\"\"> <center>ImageNet 对应的四个 DenseNet-BC 网络配置。所有网络的 k=32，表中所有 conv 均表示 <strong>BN-ReLU-Conv</strong> </center></p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验结果的对比可直接参考原论文。</p>\n<h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p><strong>CIFAR：</strong>  CIFAR-10 和 CIFAR-100 数据集包含 <code>32x32</code> 大小的彩色图像，其中 CIFAR-10 的图像分类数量为 10，CIFAR-100 图像分类数量为 100。训练集和测试集大小分别为 50000 和 10000，训练集中取 5000 作为验证集。数据扩增使用 镜像/平移 两种方法。预处理包括 RGB 三通道分别使用 <code>mean</code> 和 <code>std</code> 归一化。最后一轮训练时使用全部 50000 个图像，然后计算测试错误。</p>\n<p><strong>SVHN：</strong>  SVHN 数据集包含 <code>32x32</code> 大小的彩色数字图像，训练集和测试集大小分别为 73257 和 26032，另有 531131 个图像作为额外的训练数据。作者实验中，不采用任何数据扩增手段，从训练集中取 6000 个图像作为验证集。选择具有最低验证错误的训练结果，然后计算测试错误。图像像素值除以 255 以落于 <code>[0,1]</code> 范围作为归一化处理方法。</p>\n<p><strong>ImageNet：</strong>  使用 ILSVRC 2012 分类数据集，包含 120 万训练集以及 5 万的验证集，分类总数为 1000。使用与 ResNet 中相同的数据扩增方法，采用 single-crop 或者 10-crop 方法得到 <code>224x224</code> 的输入大小，最后计算验证集上的错误率。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>CIFAR 和 SVHN 训练 batch 大小为 64，epoch 分别为 300 和 40。初始学习率 lr = 0.1，在 50% 和 75% 的 epoch 时分别再降为 10%。</p>\n<p>ImageNet 训练 batch 大小为 256， epoch 为 90。初始学习率 lr=0.1，在 epoch = 30 和 epoch = 60 时分别降为 10%。</p>\n<p>权重衰减因子为 $10^{-4}$，Nesterov momentum 为 0.9。</p>\n<p>由于本文着重记录 DenseNet 的网络结构，所以实验结果数据的分析以及与其他网络的比较此处省略，可参考原文进行反复阅读理解。</p>\n<h2 id=\"DenseNet-特点：\"><a href=\"#DenseNet-特点：\" class=\"headerlink\" title=\"DenseNet 特点：\"></a>DenseNet 特点：</h2><ol>\n<li>dense block 内每两个 layer 之间均存在直接连接。</li>\n<li>直接连接与普通前向传播的合并采用 <code>concatenate</code> 方式，而 ResNet 中则是 <code>sum</code> 方式。</li>\n<li>比传统网络有更少的参数。layer 之间的连接更加密集，所以 layer 只需要很小的 filter 数量。</li>\n<li>易于训练。这得益于 DenseNet 更优的信息流（梯度流）传递。early layer 可以直接利用到损失函数对原始输入的梯度，这种深度监督有益于训练。</li>\n<li>layer 之间的密集直接连接有正则化效果，在小训练集上有助于降低过拟合。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>论文 <a href=\"https://arxiv.org/abs/1608.06993\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n<p>随着现在数据集越来越大，网络容量也需要增大，否则容易出现过拟合现象，一种增大网络容量的方法是增加更多的 layer ，让网络更深（deep），但是这也会带来新的问题：随着输入（或梯度）经过越来越多的 layer，信息可能在到达最后一层（对于梯度则是反向传播在到达网络第一层）之前就消失了。ResNet 增加 shortcut，即 early layer 到 later layer 之间直接连接，以此来解决这个问题。本文提出 densenet，将这种连接风格贯彻到底，网络结构如图1，</p>","more":"<p><img src=\"/images/img_cls/densenet_1.png\" alt=\"\"></p>\n<h2 id=\"DenseNet-数学描述\"><a href=\"#DenseNet-数学描述\" class=\"headerlink\" title=\"DenseNet 数学描述\"></a>DenseNet 数学描述</h2><p>假设输入为 $\\mathbf x_0$，网络共有 $L$ 层，每层非线性转换操作记为 $H_l(\\cdot)$，输出记为 $\\mathbf x_l$。</p>\n<h3 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h3><p>传统的网络，第 $l$ 层操作为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})$ ResNet 增加一个 Identity 分支后为 $\\mathbf x_l = H_l(\\mathbf x_{l-1})+\\mathbf x_{l-1}$，ResNet 中一个 layer 其实是一个 block，不是单层 layer，block 内部的 layer 没有 shortcut 分支，这点需要注意。</p>\n<h3 id=\"Dense-连接\"><a href=\"#Dense-连接\" class=\"headerlink\" title=\"Dense 连接\"></a>Dense 连接</h3><p>从某个 layer 到所有后继 layer 之间增加直接连接，对于第 $l$ 层 layer 来说，其输入是所有前序 layer 的输出以及原始网络输入，所以有<br>$$\\mathbf x_l=H_l([\\mathbf x_0, … , \\mathbf x_{l-1}])$$<br>其中 $[\\mathbf x_0, … , \\mathbf x_{l-1}]$ 表示 <code>concatenate</code> 操作，这与 ResNet 的 <code>sum</code> 操作不同。</p>\n<h3 id=\"组合函数\"><a href=\"#组合函数\" class=\"headerlink\" title=\"组合函数\"></a>组合函数</h3><p>DenseNet 中每个 layer 的操作 $H_l(\\cdot)$ 是由：</p>\n<ol>\n<li>批规范化 BN；</li>\n<li>ReLU；</li>\n<li><code>3x3</code> conv</li>\n</ol>\n<p>组合而得。</p>\n<h3 id=\"Pooling-层\"><a href=\"#Pooling-层\" class=\"headerlink\" title=\"Pooling 层\"></a>Pooling 层</h3><p>网络肯定存在下采样，这就导致 early layer 与 later layer 可能有不同的 feature maps 大小，这就导致没法直接连接。一种解决的办法如图 2，将若干个 layer 分为一组作为一个 block，称为 Dense Block，block 内部的 feature maps 大小保持不变，从而 block 内部的 layer 之间可以进行密集连接，block 之间使用过渡层进行下采样，在作者实验中过渡层包含 BN 层、<code>1x1</code> 卷积层以及 <code>2x2</code> 均值池化层。<br><img src=\"/images/img_cls/densenet_2.png\" alt=\"\"></p>\n<h3 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h3><p>记 $k_0$ 为 Dense Block 初始输入的 channels，如果每个 $H_l$ 输出均为 $k$ 个 feature maps，由于 $l^{th}$ layer 的输入为初始网络输入 $\\mathbf x_0$ 以及前 $l-1$ 个 layer 输出的 concatenation，所以共有 $k_0+k(l-1)$ 个 feature maps，所以 $k$ 值即使较小，随着 $l^{th}$ 增大，深层 layer 的 <code>in_channels</code> 也可以很大，因为可以使用较小 $k$ 值，这就使得 DenseNet 与传统网络相比，拥有更少的网络参数。记 $k$ 为 <em>growth rate</em> ，表示 layer 输入 feature maps 增长量。作者实验表明，即使非常小的 <em>growth rate</em> 也可以获取很好的测试结果。作者解释为，每个 layer 可以利用同 block 内的前面所有 layer 的输出 feature maps，也就是 “collective knowledge”，将 feature maps 看作网络的 global state，每个 layer 贡献自己的 k 个 feature maps 到这个 global state，同时 <em>growth rate</em> 控制了每个 layer 对 global state 的贡献量，并且每次某个 layer 对 global state 贡献完毕，此时的 global state 可以被之后所有 layer 利用。传统网络为了在 layer 到 layer 之间传递这种 global state，不得不使用越来越多的输出通道数，达到复制前面各阶段的 global states 的效果。</p>\n<h3 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h3><p>later layer 的输入通道数较大，可以在这些 layer（通常是 <code>3x3</code> 卷积） 前面增加一个 <code>1x1</code> 卷积作为 bottleneck layer，降低 later layer 的输入通道数，提高计算效率。记这种带有 bottleneck layer 的 DenseNet 为 DenseNet-B，其中 layer 的操作 $H_l(\\cdot)$ 变为 BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)（参考前面 <strong>组合函数</strong> 小节）。</p>\n<h3 id=\"Compression\"><a href=\"#Compression\" class=\"headerlink\" title=\"Compression\"></a>Compression</h3><p>为了进一步压缩模型，可以在过渡层降低 feature maps 的数量，记 dense block 的输出 feature maps 数量为 $m$，注意是 dense block 初始输入和各 layer 输出的叠加，参见图 1，过渡层输出 feature maps 数量为 $\\lfloor \\theta m \\rfloor$，其中 $0 &lt; \\theta \\le 1$，当 $\\theta=1$，过渡层不改变 feature maps 的数量，当 $\\theta &lt;1$ 记这样的 DenseNet 为 DenseNet-C，而 DenseNet-BC 则表示既有 bottleneck layer 又有 $\\theta &lt;1$ 的 DenseNet。</p>\n<h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><p>作者在 CIFAR-10,CIFAR-100,SVHN 以及 ImageNet 四个数据集上评估了 DenseNet。</p>\n<p>在前三个数据集上（这三个数据集的图像 size 均为 <code>32x32</code>），DenseNet 使用了 3 个 dense block，每个 dense block 有相等数量的 layer。在进入第一个 dense block 之前，输入图像数据先经过了一个 <code>1x1</code> 16-out_channels 的卷积层（对于 DenseNet-BC 网络，输出通道数为 <em>growth rate</em> 的两倍）。对于 <code>3x3</code> conv，进行 padding=1 的 zero 填充，以保持 feature map size 不变。在最后一个 dense block 之后，进行全局均值池化以及 softmax 操作。三个 dense block 的 feature maps 大小分别为 <code>32x32, 16x16, 8x8</code>。使用最普通的 DenseNet（不带 B\\C 后缀）进行实验时，dense block 配置分别为 ${L=40,k=12}$, ${L=100,k=12}$ 和 ${L=100,k=24}$，使用 DenseNet-BC 进行实验时，配置分别为 ${L=100,k=12}$, ${L=250,k=24}$ 和 ${L=190,k=40}$。</p>\n<p>对于 ImageNet，使用 4 个 dense block 的 DenseNet-BC，输入图像大小为 <code>224x224</code>，初始 layer 为一个 <code>7x7</code> 2k-out_channels，stride=2 的卷积，其中 k 表示 <em>growth-rate</em> ，所有 dense block 均为 $k=32$，网络具体描述如表 1 所示，<br><img src=\"/images/img_cls/densenet_3.png\" alt=\"\"> <center>ImageNet 对应的四个 DenseNet-BC 网络配置。所有网络的 k=32，表中所有 conv 均表示 <strong>BN-ReLU-Conv</strong> </center></p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>实验结果的对比可直接参考原论文。</p>\n<h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p><strong>CIFAR：</strong>  CIFAR-10 和 CIFAR-100 数据集包含 <code>32x32</code> 大小的彩色图像，其中 CIFAR-10 的图像分类数量为 10，CIFAR-100 图像分类数量为 100。训练集和测试集大小分别为 50000 和 10000，训练集中取 5000 作为验证集。数据扩增使用 镜像/平移 两种方法。预处理包括 RGB 三通道分别使用 <code>mean</code> 和 <code>std</code> 归一化。最后一轮训练时使用全部 50000 个图像，然后计算测试错误。</p>\n<p><strong>SVHN：</strong>  SVHN 数据集包含 <code>32x32</code> 大小的彩色数字图像，训练集和测试集大小分别为 73257 和 26032，另有 531131 个图像作为额外的训练数据。作者实验中，不采用任何数据扩增手段，从训练集中取 6000 个图像作为验证集。选择具有最低验证错误的训练结果，然后计算测试错误。图像像素值除以 255 以落于 <code>[0,1]</code> 范围作为归一化处理方法。</p>\n<p><strong>ImageNet：</strong>  使用 ILSVRC 2012 分类数据集，包含 120 万训练集以及 5 万的验证集，分类总数为 1000。使用与 ResNet 中相同的数据扩增方法，采用 single-crop 或者 10-crop 方法得到 <code>224x224</code> 的输入大小，最后计算验证集上的错误率。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>CIFAR 和 SVHN 训练 batch 大小为 64，epoch 分别为 300 和 40。初始学习率 lr = 0.1，在 50% 和 75% 的 epoch 时分别再降为 10%。</p>\n<p>ImageNet 训练 batch 大小为 256， epoch 为 90。初始学习率 lr=0.1，在 epoch = 30 和 epoch = 60 时分别降为 10%。</p>\n<p>权重衰减因子为 $10^{-4}$，Nesterov momentum 为 0.9。</p>\n<p>由于本文着重记录 DenseNet 的网络结构，所以实验结果数据的分析以及与其他网络的比较此处省略，可参考原文进行反复阅读理解。</p>\n<h2 id=\"DenseNet-特点：\"><a href=\"#DenseNet-特点：\" class=\"headerlink\" title=\"DenseNet 特点：\"></a>DenseNet 特点：</h2><ol>\n<li>dense block 内每两个 layer 之间均存在直接连接。</li>\n<li>直接连接与普通前向传播的合并采用 <code>concatenate</code> 方式，而 ResNet 中则是 <code>sum</code> 方式。</li>\n<li>比传统网络有更少的参数。layer 之间的连接更加密集，所以 layer 只需要很小的 filter 数量。</li>\n<li>易于训练。这得益于 DenseNet 更优的信息流（梯度流）传递。early layer 可以直接利用到损失函数对原始输入的梯度，这种深度监督有益于训练。</li>\n<li>layer 之间的密集直接连接有正则化效果，在小训练集上有助于降低过拟合。</li>\n</ol>"},{"title":"Dynamic Programming (3)","date":"2019-08-27T11:03:44.000Z","p":"dp/DP3","mathjax":true,"_content":"本篇接上一篇 [DP2](2019/08/14/DP2) 讨论各种 DP 问题模型。\n\n## 流水作业问题 FLOWSHOP\n这是一个进程调度问题，每个进程有两个任务 A 和 B，B 必须在 A 完成之后才能执行。任务置于独立的处理器上执行，选择进程执行顺序，使得总执行时间（损失）最小。注意，A B 任务顺序与进程顺序一致。例如，有进程 $i=\\{0,1,2,3\\}$，且 A 任务的执行时间分别为 $p_i=\\{3,4,8,10\\}$，B 任务的执行时间分别为 $q_i=\\{6,2,9,15\\}$，如果选择进程执行顺序为 $0,1,2,3$，那么各任务执行的开始时间和结束时间如下表：\n\n<!-- more -->\n\n| processor 1 | $A_0: 0-3$ | $A_1:3-7$ | $A_2:7-15$ | $A_3:15-25$ | |\n|:---------:|:--------:| :-----------: | :-------:  | :--------:  |:--:|\n| __processor 2__  |  |$B_0: 3-9$ | $B_1:9-11$ | $B_2:15-24$ | $B_3:25-40$ |\n\n总执行时间为最后一个 B 任务的结束时间，这个例子中为 40。进程执行顺序是可变的，要寻找具有最小执行时间的进程顺序，可以使用 DP 解决。每次决策 d 表示选择某个进程，定义状态为 (k,S)，其中 k 表示最近调度的进程的 A B 任务结束时间之间的差，S 为剩余的未调度进程集合。初始时（尚未做任何决策）k 为 0。如果当前决策 d 满足 $k \\le p_d$，那么 $B_d$ 任务的执行将不会有延时，也就是说 $A_d$ 执行完了立马执行 $B_d$ 任务，__于是下一决策的 k' 为 $q_d$__，否则的话 $B_d$ 任务 在 $A_d$ 执行完了还需要延时 $k-p_d$ 才开始执行，这就导致下一决策 k' 为  $k-p_d+q_d$。\n\n例如上面例子中，初始时 k = 0，在第一次决策 $d_1=0$ 时，$k=0<p_0$，于是 $B_0$ 任务没有延时，紧接着 $A_0$ 完成后就开始执行，然后下一决策 $d_2=1$ 的 k 为 $q_0=6$，又因为此时 $k>p_1$，所以 $B_1$ 延时 $k-p_1=6-4=2$ 才开始执行，从上表中也可以看出，$B_1$ 从 $A_1$ 结束时间 7 时开始延时 2 时间才开始执行，于是下一决策 $d_3=2$ 对应的 k 为 $k:=k-p_1+q_1=6-4+2=4$，此时 $k<p_2$，所以 $B_2$ 任务执行没有延时，紧接着 $A_2$ 结束之后就（在时间 15 时）开始执行，于是最后决策 $d_4=3$ 对应的 k 为 $k:=q_2=9$，此时 $k<p_3$，这说明 $B_3$ 任务也没有延时，紧接着 $A_3$ 结束（在 25 时）就开始执行，下一决策对应的 k 为 $k=q_3=15$，由于此时决策空间已经为空，所以决策结束，此为基本条件，即 $f(k,S)=k  \\ 当 S=\\emptyset$。\n\nDPFE 为\n\n$$f(k,S)=\\min_{d \\in S} \\{p_d + f(\\max (k-p_d,0)+q_d, S-\\{d\\})\\}$$\n\n终止条件为 $f(k,\\emptyset)=k$。要求的目标为 $f(0,S^{\\ast})$，$S^{\\ast}$ 为初始进程集合。上面例子使用代码实现如下：\n```python\ni=[0,1,2,3]\np=[3,4,8,10]\nq=[6,2,9,15]\n\ndef flowshop(k,S)\n    if len(S)==0:\n        return k, []\n    m=1e8\n    for j in range(len(S)):\n        d=S[j]\n        m1, path1=flowshow(max(k-p[d],0)+q[d], S[:j]+S[j+1:])\n        m1+=p[d]\n        if m>m1:\n            m=m1\n            path=[s]+path1\n    return m, path\n\nm, path=flowshow(0,i)\nprint(m)\nprint(path)\n```\n\n## 汉诺塔问题 HANOI\n移动 N 个盘子（大小从上到下递增）从一个桩 x 到另一个桩 y 上，使用第三个桩 z 作为辅助，并保证每个桩上的盘子大小从上到下递增，总共需要移动的次数记为 $f(N)$，一次移动指将盘子从某桩移动到另一个桩上。显然有关系：\n\n$$f(i)=2f(i-1)+1$$\n\n这表明，从 x 移动 i 个盘子到 y 上，等价于从 x 移动 i-1 个盘子到 z 上，然后移动 x 的最后一个盘子到 y 上，最后从 z 上移动 i-1 个盘子到 y 上。基本态为 $f(1)=1$，于是递归可计算得 $f(2)=2 f(1)+1=3, \\ f(3)=2f(2)+1=7, \\ \\cdots$\n\n上式仅给出了移动次数，然而我们还需要确定移动序列。\n### 非最优问题\n记从桩 x 移动一个盘子到桩 y 为 $<x,y>$，定义 $F(S)$ 为移动序列，与之前求最优问题中使用加法操作不同，这里使用连接操作（concatenation），那么有\n\n$$F(N,x,y)=F(N-1,x,z)F(1,x,y)F(N-1,z,y)$$\n\n其中状态 $S=(N,x,y)$，原理与上面一致。基本态为 $F(1,x,y)=<x,y>$。于是可一步步推导得到：\n$$\\begin{aligned} F(2,x,y)&=F(1,x,z)F(1,x,y)F(1,z,y)=<x,z><x,y><z,y>\n\\\\\\\\F(3,x,y)&=F(2,x,z)F(1,x,y)F(2,z,y)\n\\\\\\\\ &=<x,y><x,z><y,z><x,y><z,x><z,y><x,y>\\end{aligned}$$\n\n代码实现如下\n```python\npegs = [1,2,3]\ndef hanoi(n,i=1,j=2):\n    if n==1:\n        return [(i,j)]\n    k = pegs.difference({i,j}).pop()\n    return hanoi(n-1,i,k)+hanoi(1,i,j)+hanoi(n-1,k,j)\n\nif __name__ == '__main__':\n    s=hanoi(3)\n    print(s)\n```\n\n## 整型线性规划问题 ILP\n考虑如下形式的优化问题\n$$\\max c^{\\top}x\n\\\\\\\\ s.t. Ax \\le b\n\\\\\\\\ x_1,...,x_n \\in \\mathbf N \\cup \\{0\\}$$\n其中，矩阵 A 和向量 b, c 中的元素均为非负整数。这其实是一种很典型的 DP 问题，首先选择某个 $x_1$ 的值，当然无论 $x_1$ 是何值，一旦选定，就转化为 $\\sum_{i=2}^n c_i x_i$ 这个子问题的最优解，由于 $x$ 向量的所有元素一起需要满足一组条件，所以在决策 $x_i$ 元素为何值时，需要知道 $x_1,...,x_{i-1}$ 这些已经决策过的元素的值，以保证它们满足条件，所以状态 S 需要包含已经决策过的元素值以及元素在向量中的位置下标，我们约定在阶段 j 时决策 $x_{j+1}$ 的值（这个不是唯一的，也可以约定来决策 $x_j$ 的值，DPFE 形式稍作调整即可），于是 DPFE 为\n$$f(j,S)=\\begin{cases} \\max_{x_{j+1} \\in D} \\{c_{j+1}x_{j+1}+f(j+1,S \\cup \\{(j+1,x_{j+1})\\})\\} & j < n\n\\\\\\\\ 0 & j=n \\end{cases}$$\n决策空间 $D$ 由给定的条件以及状态 $S$ 决定。此问题的求解目标是 $f(0,\\emptyset)$。\n\n以上是一种求解思路，还有一种思路。从给定的条件出发，已知\n\n$$Ax \\le b$$\n\n记 $A$ 维度为 $m \\times n$，于是上式表示一共有 m 个限制条件，每个限制条件形式为 \n\n$$A_{i,:}x \\le b_i \\Rightarrow \\sum_{j=1}^n A_{i,j}x_j \\le b_i$$\n\n每做一次决策决定一个 $x_j$ 的值，将决策后的 $x_j$ 的值移到式子右边，在阶段 j，与上面一样，将决策 $x_{j+1}$ 的值，决策后上式不等式改写为\n\n$$\\sum_{k=j+2}^n A_{i,k}x_k \\le b_i - A_{i,1}x_1 - \\cdots A_{i,j+1}x_{j+1}$$\n\n也就是说，每次决策不等式右边部分均会变化，于是可定义状态 S 表示限制条件的不等式右侧部分，DPFE 如下\n\n$$f(j,(y_1,...,y_m))=\\begin{cases} \\max_{x_{j+1} \\in D} \\{c_{j+1}x_{j+1}+f(j+1,(y_1-A_{1,j+1}x_{j+1},...,y_m-A_{m,j+1}x_{j+1}))\\} & j < n\n\\\\\\\\ 0 & j=n \\end{cases}$$\n\n求解目标是 $f(0,(b_1,...,b_m))$。我们来看一下决策空间 $D$，在阶段 j，状态为 $S=(j,(y_1,...,y_m))$，由于限制条件为\n\n$$A_{1,j+1}x_{j+1} + A_{1,j+2}x_{j+2} + \\cdots + A_{1,n}x_n \\le y_1\n\\\\\\\\ \\vdots\n\\\\\\\\ A_{m,j+1}x_{j+1} + A_{m,j+2}x_{j+2} + \\cdots + A_{m,n}x_n \\le y_m$$\n易知此时 $x_{j+1}$ 的决策空间为 \n$$\\{0,...,\\min \\{\\lfloor \\frac{y_1}{A_{1,j+1}} \\rfloor, ..., \\lfloor \\frac{y_m}{A_{m,j+1}} \\rfloor\\}\\}$$\n\n注意，如果出现 $\\frac {y_i} 0$，则解释为正无穷 $\\infty$，表示第 i 个限制条件对 $x_{j+1}$ 没有上限。\n\n第一种解决方法中的决策空间 $D$ 也是类似求解，令 \n$$y_i=b_i-\\sum_{p \\in S} A_{i,p_1}p_2$$\n然后就与第二章解决方法中的决策空间的求解一样了。\n\n例：$c=(3,5), \\ b=(4,12,18)$，$A=\\begin{pmatrix} 1 & 0 \\\\\\\\ 0 & 2 \\\\\\\\ 3 & 2 \\end{pmatrix}$，求解 $x=(x_1,x_2)$。\n代码如下\n```python\nc=[3,5]\nb=[4,12,18]\na=[[1,0],\n   [0,2],\n   [3,2]]\n\nm,n=len(b),len(c)\n\ndef d=(j,y):\n    return min([y[i]//a[i][j] if a[i][j] > 0 else 1e8 for i in range(m)])\n\ndef ilp(j,y):\n    if j==n:\n        return 0, []\n    dm=d(j,y)\n    m_=-1\n    x_=None\n    for d_ in range(dm+1):\n        y_=[y[i]-a[i][j]*d_ for i in range(m)]\n        m1,x1=ilp(j+1,y_)\n        m1+=c[j]*d_\n        if m_ < m1:\n            m_ = m1\n            x_ = [d_]+x1\n    return m_, x_\n\nif __name__ == '__main__':\n    m_, x_ = ilp(0,b)\n    print(m_)   # 36\n    print(x_)   # [2,6]\n```\n\n## 背包型 ILP 问题 ILPKNAP\n假设有三类物体 n=3，每种物体的价值为 $(v_0,v_1,v_2)=(15,25,24)$，重量为 $(w_0,w_1,w_2)=(10,18,15)$，假设背包总共可装物体重量上限为 22，现在每种物体各选择多少个装包，使得价值最大？这个问题可以使用 ILP 模型解决，每种物体选择的数量为 $(x_0,x_1,x_2)$，系数向量为 $c=(v_0,v_1,v_2)$，限制条件的不等式左侧矩阵 $A=(w_0,w_1,w_2)$，右侧向量为 $b=(22)$，且 $x_0,x_1,x_2 \\in \\mathbf N \\cup \\{0\\}$。\n\n## 区间调度问题 INTVL\n假设有 N 个进程，标号为 $P=\\{0,...,N-1\\}$，选择其中的一个子集，选中的进程放置在单处理器上执行，已知每个进程有区间 $(s_i,t_i)$ 表示起始时间和截止时间，在这个时间段内，进程 $i$ 得到运行，那么就获得收益 $w_i$，由于是单处理器，所以各进程执行时间不得重叠，求选择的子集，使得收益最大，DPFE 为\n\n$$f(p,q)=\\max_{d \\in P} \\{f(p,s_d)+c(d|p,q)+f(t_d,q)\\}$$\n其中 f(p,q) 表示时间段 $[p,q]$ 内的最大收益，上式是很显然，如果做出当前决策 d，那么理论上 $[s_d,t_d]$ 这个时间段用来执行进程 d，然后还剩两个区间 $[p,s_d]$ 和 $[t_d,q]$ 再继续做决策。当前决策 d 有收益当且仅当 $p \\le s_d, t_d \\le q$。基本态是 $f(p,q)=0, \\ p \\ge q$，求解目标是 $f(0,T)$，其中 $T \\ge \\max_i \\{t_i\\}$。\n\n根据上式，在当前决策之后的两个区间 $[p,s_d]$ 和 $[t_d,q]$ 求解最大收益 $f(p,s_d), \\ f(t_d, q)$时，决策空间依然还是 $P$，虽然基本态 $f(p,q)=0, \\ p \\ge q$ 保证了递归过程可以退出，但显然决策空间应该缩小，这样可以减少递归次数，DPFE 为\n\n$$f(S,p,q)=\\max_{d \\in S} \\{f(S_L,p,s_d)+c(d|p,q)+f(S_R,t_d,q)\\}$$\n其中 $S_L, \\ S_R \\subset P$ 分别对应 $[p,s_d]$ 和 $[t_d,q]$ 两个区间内合适的进程集合，所谓合适，就是进程的 $(s_i,t_i)$ 包含在对应区间内。基本态是 $f(S,p,q)=0, \\ p \\ge q \\text{ or } S=\\emptyset$，求解目标是 $f(P,0,T)$，其中 $T \\ge \\max_i \\{t_i\\}$。代码如下，\n```python\nP=[0,1,2,3,4,5]\ns=[9,8,3,5,2,1]\nt=[12,11,10,7,6,4]\nw=[1,2,7,4,4,2]\nT=max(t)\nn=len(s)\n\ndef get_S(prev_S, p, q):\n    return [i for i in prev_S if s[i]>=p and t[i]<=q]\n\ndef intvl(S, p, q):\n    if len(S)==0 or p>=q:\n        return 0, []\n    m_=0\n    d_=None\n    for d in S:\n        m1, d1=intvl(get_S(S, p, s[d]), p, s[d])\n        m2, d2=intvl(get_S(S, t[d], q), t[d], q)\n        m=m1+m2+w[d]\n        if m_<m:\n            m_=m\n            d_=[d]+d1+d2\n    return m_, d_\n\nif __name__ == '__main__':\n    m, d= intvl(P, 0, T)\n    print(m)\n    print(d)\n```\n\n还有一种思路。我们将进程按其截止时间升序排列，排列后的进程序号为 $P$，进程数量为 N，然后从 $P$ 的后端到前端依次做决策，也就是第 i 个决策决定是否选择 $P$ 中第 N-i-1 个进程，例如第一个决策决定是否选择最后一个进程，第 N 个决策决定是否选择第一个进程，这样的话，假设第 N-1-i 个决策决定选择第 i 个进程，那么接下来只有 $D_i=\\{j|t_j \\le s_i\\}$ 的进程集合可供选择，我们令 $\\pi(i)=\\max D_i$，因为决策是按进程序号从大到小进行的，所以下一次决策直接决定是否选择序号为 $\\pi(i)$ 的进程，DPFE 为\n\n$$f(k)=\\max\\{w_k+f(\\pi(k)), f(k-1)\\}$$\n\n其中，k 所代表的进程下标从 1 开始编号（注意与程序中数组下标从 0 开始的区别）。理解上式也很简单，当前决策，要么选择进程 k，此时收益为 $w_k+f(\\pi(k))$，要么不选择进程 k，此时收益为 $f(k-1)$，通过比较哪个收益大来决定是否选择进程 k。上式可改写为\n\n$$f(k)=\\max_{d \\in \\{0,1\\}} \\{d\\cdot(w_k+f(\\pi (k-1)))+(1-d)\\cdot f(k-1)\\}$$\n\n代码实现如下，\n```python\nimport numpy as np\n\ns=[9,8,3,5,2,1]\nt=[12,11,10,7,6,4]\nw=[1,2,7,4,4,2]\n\nP=np.argsort(t)\nt=np.sort(t)\ns=np.array(s)[P]\nw=np.array(w)[P]\n\ndef pi(k):\n    for i in range(k-1,-1,-1):\n        if t[i]<=s[k]:\n            return i\n    return -1\n\ndef intvl1(k):\n    if k==-1:\n        return 0, []\n    m1,d1=intvl1(pi(k))\n    m2,d2=intvl2(k-1)\n    if m1+w[k]>=m2:\n        return m1+w[k], [k]+d1\n    else:\n        return m2, d2\n\nif __name__ == '__main__':\n    m, d= intvl(P, 0, T)\n    print(m)\n    print(P[d])\n```\n","source":"_posts/dp/DP3.md","raw":"---\ntitle: Dynamic Programming (3)\ndate: 2019-08-27 19:03:44\np: dp/DP3\ntags: \n    - math\n    - DP\nmathjax: true\n---\n本篇接上一篇 [DP2](2019/08/14/DP2) 讨论各种 DP 问题模型。\n\n## 流水作业问题 FLOWSHOP\n这是一个进程调度问题，每个进程有两个任务 A 和 B，B 必须在 A 完成之后才能执行。任务置于独立的处理器上执行，选择进程执行顺序，使得总执行时间（损失）最小。注意，A B 任务顺序与进程顺序一致。例如，有进程 $i=\\{0,1,2,3\\}$，且 A 任务的执行时间分别为 $p_i=\\{3,4,8,10\\}$，B 任务的执行时间分别为 $q_i=\\{6,2,9,15\\}$，如果选择进程执行顺序为 $0,1,2,3$，那么各任务执行的开始时间和结束时间如下表：\n\n<!-- more -->\n\n| processor 1 | $A_0: 0-3$ | $A_1:3-7$ | $A_2:7-15$ | $A_3:15-25$ | |\n|:---------:|:--------:| :-----------: | :-------:  | :--------:  |:--:|\n| __processor 2__  |  |$B_0: 3-9$ | $B_1:9-11$ | $B_2:15-24$ | $B_3:25-40$ |\n\n总执行时间为最后一个 B 任务的结束时间，这个例子中为 40。进程执行顺序是可变的，要寻找具有最小执行时间的进程顺序，可以使用 DP 解决。每次决策 d 表示选择某个进程，定义状态为 (k,S)，其中 k 表示最近调度的进程的 A B 任务结束时间之间的差，S 为剩余的未调度进程集合。初始时（尚未做任何决策）k 为 0。如果当前决策 d 满足 $k \\le p_d$，那么 $B_d$ 任务的执行将不会有延时，也就是说 $A_d$ 执行完了立马执行 $B_d$ 任务，__于是下一决策的 k' 为 $q_d$__，否则的话 $B_d$ 任务 在 $A_d$ 执行完了还需要延时 $k-p_d$ 才开始执行，这就导致下一决策 k' 为  $k-p_d+q_d$。\n\n例如上面例子中，初始时 k = 0，在第一次决策 $d_1=0$ 时，$k=0<p_0$，于是 $B_0$ 任务没有延时，紧接着 $A_0$ 完成后就开始执行，然后下一决策 $d_2=1$ 的 k 为 $q_0=6$，又因为此时 $k>p_1$，所以 $B_1$ 延时 $k-p_1=6-4=2$ 才开始执行，从上表中也可以看出，$B_1$ 从 $A_1$ 结束时间 7 时开始延时 2 时间才开始执行，于是下一决策 $d_3=2$ 对应的 k 为 $k:=k-p_1+q_1=6-4+2=4$，此时 $k<p_2$，所以 $B_2$ 任务执行没有延时，紧接着 $A_2$ 结束之后就（在时间 15 时）开始执行，于是最后决策 $d_4=3$ 对应的 k 为 $k:=q_2=9$，此时 $k<p_3$，这说明 $B_3$ 任务也没有延时，紧接着 $A_3$ 结束（在 25 时）就开始执行，下一决策对应的 k 为 $k=q_3=15$，由于此时决策空间已经为空，所以决策结束，此为基本条件，即 $f(k,S)=k  \\ 当 S=\\emptyset$。\n\nDPFE 为\n\n$$f(k,S)=\\min_{d \\in S} \\{p_d + f(\\max (k-p_d,0)+q_d, S-\\{d\\})\\}$$\n\n终止条件为 $f(k,\\emptyset)=k$。要求的目标为 $f(0,S^{\\ast})$，$S^{\\ast}$ 为初始进程集合。上面例子使用代码实现如下：\n```python\ni=[0,1,2,3]\np=[3,4,8,10]\nq=[6,2,9,15]\n\ndef flowshop(k,S)\n    if len(S)==0:\n        return k, []\n    m=1e8\n    for j in range(len(S)):\n        d=S[j]\n        m1, path1=flowshow(max(k-p[d],0)+q[d], S[:j]+S[j+1:])\n        m1+=p[d]\n        if m>m1:\n            m=m1\n            path=[s]+path1\n    return m, path\n\nm, path=flowshow(0,i)\nprint(m)\nprint(path)\n```\n\n## 汉诺塔问题 HANOI\n移动 N 个盘子（大小从上到下递增）从一个桩 x 到另一个桩 y 上，使用第三个桩 z 作为辅助，并保证每个桩上的盘子大小从上到下递增，总共需要移动的次数记为 $f(N)$，一次移动指将盘子从某桩移动到另一个桩上。显然有关系：\n\n$$f(i)=2f(i-1)+1$$\n\n这表明，从 x 移动 i 个盘子到 y 上，等价于从 x 移动 i-1 个盘子到 z 上，然后移动 x 的最后一个盘子到 y 上，最后从 z 上移动 i-1 个盘子到 y 上。基本态为 $f(1)=1$，于是递归可计算得 $f(2)=2 f(1)+1=3, \\ f(3)=2f(2)+1=7, \\ \\cdots$\n\n上式仅给出了移动次数，然而我们还需要确定移动序列。\n### 非最优问题\n记从桩 x 移动一个盘子到桩 y 为 $<x,y>$，定义 $F(S)$ 为移动序列，与之前求最优问题中使用加法操作不同，这里使用连接操作（concatenation），那么有\n\n$$F(N,x,y)=F(N-1,x,z)F(1,x,y)F(N-1,z,y)$$\n\n其中状态 $S=(N,x,y)$，原理与上面一致。基本态为 $F(1,x,y)=<x,y>$。于是可一步步推导得到：\n$$\\begin{aligned} F(2,x,y)&=F(1,x,z)F(1,x,y)F(1,z,y)=<x,z><x,y><z,y>\n\\\\\\\\F(3,x,y)&=F(2,x,z)F(1,x,y)F(2,z,y)\n\\\\\\\\ &=<x,y><x,z><y,z><x,y><z,x><z,y><x,y>\\end{aligned}$$\n\n代码实现如下\n```python\npegs = [1,2,3]\ndef hanoi(n,i=1,j=2):\n    if n==1:\n        return [(i,j)]\n    k = pegs.difference({i,j}).pop()\n    return hanoi(n-1,i,k)+hanoi(1,i,j)+hanoi(n-1,k,j)\n\nif __name__ == '__main__':\n    s=hanoi(3)\n    print(s)\n```\n\n## 整型线性规划问题 ILP\n考虑如下形式的优化问题\n$$\\max c^{\\top}x\n\\\\\\\\ s.t. Ax \\le b\n\\\\\\\\ x_1,...,x_n \\in \\mathbf N \\cup \\{0\\}$$\n其中，矩阵 A 和向量 b, c 中的元素均为非负整数。这其实是一种很典型的 DP 问题，首先选择某个 $x_1$ 的值，当然无论 $x_1$ 是何值，一旦选定，就转化为 $\\sum_{i=2}^n c_i x_i$ 这个子问题的最优解，由于 $x$ 向量的所有元素一起需要满足一组条件，所以在决策 $x_i$ 元素为何值时，需要知道 $x_1,...,x_{i-1}$ 这些已经决策过的元素的值，以保证它们满足条件，所以状态 S 需要包含已经决策过的元素值以及元素在向量中的位置下标，我们约定在阶段 j 时决策 $x_{j+1}$ 的值（这个不是唯一的，也可以约定来决策 $x_j$ 的值，DPFE 形式稍作调整即可），于是 DPFE 为\n$$f(j,S)=\\begin{cases} \\max_{x_{j+1} \\in D} \\{c_{j+1}x_{j+1}+f(j+1,S \\cup \\{(j+1,x_{j+1})\\})\\} & j < n\n\\\\\\\\ 0 & j=n \\end{cases}$$\n决策空间 $D$ 由给定的条件以及状态 $S$ 决定。此问题的求解目标是 $f(0,\\emptyset)$。\n\n以上是一种求解思路，还有一种思路。从给定的条件出发，已知\n\n$$Ax \\le b$$\n\n记 $A$ 维度为 $m \\times n$，于是上式表示一共有 m 个限制条件，每个限制条件形式为 \n\n$$A_{i,:}x \\le b_i \\Rightarrow \\sum_{j=1}^n A_{i,j}x_j \\le b_i$$\n\n每做一次决策决定一个 $x_j$ 的值，将决策后的 $x_j$ 的值移到式子右边，在阶段 j，与上面一样，将决策 $x_{j+1}$ 的值，决策后上式不等式改写为\n\n$$\\sum_{k=j+2}^n A_{i,k}x_k \\le b_i - A_{i,1}x_1 - \\cdots A_{i,j+1}x_{j+1}$$\n\n也就是说，每次决策不等式右边部分均会变化，于是可定义状态 S 表示限制条件的不等式右侧部分，DPFE 如下\n\n$$f(j,(y_1,...,y_m))=\\begin{cases} \\max_{x_{j+1} \\in D} \\{c_{j+1}x_{j+1}+f(j+1,(y_1-A_{1,j+1}x_{j+1},...,y_m-A_{m,j+1}x_{j+1}))\\} & j < n\n\\\\\\\\ 0 & j=n \\end{cases}$$\n\n求解目标是 $f(0,(b_1,...,b_m))$。我们来看一下决策空间 $D$，在阶段 j，状态为 $S=(j,(y_1,...,y_m))$，由于限制条件为\n\n$$A_{1,j+1}x_{j+1} + A_{1,j+2}x_{j+2} + \\cdots + A_{1,n}x_n \\le y_1\n\\\\\\\\ \\vdots\n\\\\\\\\ A_{m,j+1}x_{j+1} + A_{m,j+2}x_{j+2} + \\cdots + A_{m,n}x_n \\le y_m$$\n易知此时 $x_{j+1}$ 的决策空间为 \n$$\\{0,...,\\min \\{\\lfloor \\frac{y_1}{A_{1,j+1}} \\rfloor, ..., \\lfloor \\frac{y_m}{A_{m,j+1}} \\rfloor\\}\\}$$\n\n注意，如果出现 $\\frac {y_i} 0$，则解释为正无穷 $\\infty$，表示第 i 个限制条件对 $x_{j+1}$ 没有上限。\n\n第一种解决方法中的决策空间 $D$ 也是类似求解，令 \n$$y_i=b_i-\\sum_{p \\in S} A_{i,p_1}p_2$$\n然后就与第二章解决方法中的决策空间的求解一样了。\n\n例：$c=(3,5), \\ b=(4,12,18)$，$A=\\begin{pmatrix} 1 & 0 \\\\\\\\ 0 & 2 \\\\\\\\ 3 & 2 \\end{pmatrix}$，求解 $x=(x_1,x_2)$。\n代码如下\n```python\nc=[3,5]\nb=[4,12,18]\na=[[1,0],\n   [0,2],\n   [3,2]]\n\nm,n=len(b),len(c)\n\ndef d=(j,y):\n    return min([y[i]//a[i][j] if a[i][j] > 0 else 1e8 for i in range(m)])\n\ndef ilp(j,y):\n    if j==n:\n        return 0, []\n    dm=d(j,y)\n    m_=-1\n    x_=None\n    for d_ in range(dm+1):\n        y_=[y[i]-a[i][j]*d_ for i in range(m)]\n        m1,x1=ilp(j+1,y_)\n        m1+=c[j]*d_\n        if m_ < m1:\n            m_ = m1\n            x_ = [d_]+x1\n    return m_, x_\n\nif __name__ == '__main__':\n    m_, x_ = ilp(0,b)\n    print(m_)   # 36\n    print(x_)   # [2,6]\n```\n\n## 背包型 ILP 问题 ILPKNAP\n假设有三类物体 n=3，每种物体的价值为 $(v_0,v_1,v_2)=(15,25,24)$，重量为 $(w_0,w_1,w_2)=(10,18,15)$，假设背包总共可装物体重量上限为 22，现在每种物体各选择多少个装包，使得价值最大？这个问题可以使用 ILP 模型解决，每种物体选择的数量为 $(x_0,x_1,x_2)$，系数向量为 $c=(v_0,v_1,v_2)$，限制条件的不等式左侧矩阵 $A=(w_0,w_1,w_2)$，右侧向量为 $b=(22)$，且 $x_0,x_1,x_2 \\in \\mathbf N \\cup \\{0\\}$。\n\n## 区间调度问题 INTVL\n假设有 N 个进程，标号为 $P=\\{0,...,N-1\\}$，选择其中的一个子集，选中的进程放置在单处理器上执行，已知每个进程有区间 $(s_i,t_i)$ 表示起始时间和截止时间，在这个时间段内，进程 $i$ 得到运行，那么就获得收益 $w_i$，由于是单处理器，所以各进程执行时间不得重叠，求选择的子集，使得收益最大，DPFE 为\n\n$$f(p,q)=\\max_{d \\in P} \\{f(p,s_d)+c(d|p,q)+f(t_d,q)\\}$$\n其中 f(p,q) 表示时间段 $[p,q]$ 内的最大收益，上式是很显然，如果做出当前决策 d，那么理论上 $[s_d,t_d]$ 这个时间段用来执行进程 d，然后还剩两个区间 $[p,s_d]$ 和 $[t_d,q]$ 再继续做决策。当前决策 d 有收益当且仅当 $p \\le s_d, t_d \\le q$。基本态是 $f(p,q)=0, \\ p \\ge q$，求解目标是 $f(0,T)$，其中 $T \\ge \\max_i \\{t_i\\}$。\n\n根据上式，在当前决策之后的两个区间 $[p,s_d]$ 和 $[t_d,q]$ 求解最大收益 $f(p,s_d), \\ f(t_d, q)$时，决策空间依然还是 $P$，虽然基本态 $f(p,q)=0, \\ p \\ge q$ 保证了递归过程可以退出，但显然决策空间应该缩小，这样可以减少递归次数，DPFE 为\n\n$$f(S,p,q)=\\max_{d \\in S} \\{f(S_L,p,s_d)+c(d|p,q)+f(S_R,t_d,q)\\}$$\n其中 $S_L, \\ S_R \\subset P$ 分别对应 $[p,s_d]$ 和 $[t_d,q]$ 两个区间内合适的进程集合，所谓合适，就是进程的 $(s_i,t_i)$ 包含在对应区间内。基本态是 $f(S,p,q)=0, \\ p \\ge q \\text{ or } S=\\emptyset$，求解目标是 $f(P,0,T)$，其中 $T \\ge \\max_i \\{t_i\\}$。代码如下，\n```python\nP=[0,1,2,3,4,5]\ns=[9,8,3,5,2,1]\nt=[12,11,10,7,6,4]\nw=[1,2,7,4,4,2]\nT=max(t)\nn=len(s)\n\ndef get_S(prev_S, p, q):\n    return [i for i in prev_S if s[i]>=p and t[i]<=q]\n\ndef intvl(S, p, q):\n    if len(S)==0 or p>=q:\n        return 0, []\n    m_=0\n    d_=None\n    for d in S:\n        m1, d1=intvl(get_S(S, p, s[d]), p, s[d])\n        m2, d2=intvl(get_S(S, t[d], q), t[d], q)\n        m=m1+m2+w[d]\n        if m_<m:\n            m_=m\n            d_=[d]+d1+d2\n    return m_, d_\n\nif __name__ == '__main__':\n    m, d= intvl(P, 0, T)\n    print(m)\n    print(d)\n```\n\n还有一种思路。我们将进程按其截止时间升序排列，排列后的进程序号为 $P$，进程数量为 N，然后从 $P$ 的后端到前端依次做决策，也就是第 i 个决策决定是否选择 $P$ 中第 N-i-1 个进程，例如第一个决策决定是否选择最后一个进程，第 N 个决策决定是否选择第一个进程，这样的话，假设第 N-1-i 个决策决定选择第 i 个进程，那么接下来只有 $D_i=\\{j|t_j \\le s_i\\}$ 的进程集合可供选择，我们令 $\\pi(i)=\\max D_i$，因为决策是按进程序号从大到小进行的，所以下一次决策直接决定是否选择序号为 $\\pi(i)$ 的进程，DPFE 为\n\n$$f(k)=\\max\\{w_k+f(\\pi(k)), f(k-1)\\}$$\n\n其中，k 所代表的进程下标从 1 开始编号（注意与程序中数组下标从 0 开始的区别）。理解上式也很简单，当前决策，要么选择进程 k，此时收益为 $w_k+f(\\pi(k))$，要么不选择进程 k，此时收益为 $f(k-1)$，通过比较哪个收益大来决定是否选择进程 k。上式可改写为\n\n$$f(k)=\\max_{d \\in \\{0,1\\}} \\{d\\cdot(w_k+f(\\pi (k-1)))+(1-d)\\cdot f(k-1)\\}$$\n\n代码实现如下，\n```python\nimport numpy as np\n\ns=[9,8,3,5,2,1]\nt=[12,11,10,7,6,4]\nw=[1,2,7,4,4,2]\n\nP=np.argsort(t)\nt=np.sort(t)\ns=np.array(s)[P]\nw=np.array(w)[P]\n\ndef pi(k):\n    for i in range(k-1,-1,-1):\n        if t[i]<=s[k]:\n            return i\n    return -1\n\ndef intvl1(k):\n    if k==-1:\n        return 0, []\n    m1,d1=intvl1(pi(k))\n    m2,d2=intvl2(k-1)\n    if m1+w[k]>=m2:\n        return m1+w[k], [k]+d1\n    else:\n        return m2, d2\n\nif __name__ == '__main__':\n    m, d= intvl(P, 0, T)\n    print(m)\n    print(P[d])\n```\n","slug":"dp/DP3","published":1,"updated":"2020-04-24T10:32:58.204Z","_id":"ck9dzcj1l0028gga6g4ri2yxi","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本篇接上一篇 <a href=\"2019/08/14/DP2\">DP2</a> 讨论各种 DP 问题模型。</p>\n<h2 id=\"流水作业问题-FLOWSHOP\"><a href=\"#流水作业问题-FLOWSHOP\" class=\"headerlink\" title=\"流水作业问题 FLOWSHOP\"></a>流水作业问题 FLOWSHOP</h2><p>这是一个进程调度问题，每个进程有两个任务 A 和 B，B 必须在 A 完成之后才能执行。任务置于独立的处理器上执行，选择进程执行顺序，使得总执行时间（损失）最小。注意，A B 任务顺序与进程顺序一致。例如，有进程 $i={0,1,2,3}$，且 A 任务的执行时间分别为 $p_i={3,4,8,10}$，B 任务的执行时间分别为 $q_i={6,2,9,15}$，如果选择进程执行顺序为 $0,1,2,3$，那么各任务执行的开始时间和结束时间如下表：</p>\n<a id=\"more\"></a>\n\n<table>\n<thead>\n<tr>\n<th align=\"center\">processor 1</th>\n<th align=\"center\">$A_0: 0-3$</th>\n<th align=\"center\">$A_1:3-7$</th>\n<th align=\"center\">$A_2:7-15$</th>\n<th align=\"center\">$A_3:15-25$</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>processor 2</strong></td>\n<td align=\"center\"></td>\n<td align=\"center\">$B_0: 3-9$</td>\n<td align=\"center\">$B_1:9-11$</td>\n<td align=\"center\">$B_2:15-24$</td>\n<td align=\"center\">$B_3:25-40$</td>\n</tr>\n</tbody></table>\n<p>总执行时间为最后一个 B 任务的结束时间，这个例子中为 40。进程执行顺序是可变的，要寻找具有最小执行时间的进程顺序，可以使用 DP 解决。每次决策 d 表示选择某个进程，定义状态为 (k,S)，其中 k 表示最近调度的进程的 A B 任务结束时间之间的差，S 为剩余的未调度进程集合。初始时（尚未做任何决策）k 为 0。如果当前决策 d 满足 $k \\le p_d$，那么 $B_d$ 任务的执行将不会有延时，也就是说 $A_d$ 执行完了立马执行 $B_d$ 任务，<strong>于是下一决策的 k’ 为 $q_d$</strong>，否则的话 $B_d$ 任务 在 $A_d$ 执行完了还需要延时 $k-p_d$ 才开始执行，这就导致下一决策 k’ 为  $k-p_d+q_d$。</p>\n<p>例如上面例子中，初始时 k = 0，在第一次决策 $d_1=0$ 时，$k=0&lt;p_0$，于是 $B_0$ 任务没有延时，紧接着 $A_0$ 完成后就开始执行，然后下一决策 $d_2=1$ 的 k 为 $q_0=6$，又因为此时 $k&gt;p_1$，所以 $B_1$ 延时 $k-p_1=6-4=2$ 才开始执行，从上表中也可以看出，$B_1$ 从 $A_1$ 结束时间 7 时开始延时 2 时间才开始执行，于是下一决策 $d_3=2$ 对应的 k 为 $k:=k-p_1+q_1=6-4+2=4$，此时 $k&lt;p_2$，所以 $B_2$ 任务执行没有延时，紧接着 $A_2$ 结束之后就（在时间 15 时）开始执行，于是最后决策 $d_4=3$ 对应的 k 为 $k:=q_2=9$，此时 $k&lt;p_3$，这说明 $B_3$ 任务也没有延时，紧接着 $A_3$ 结束（在 25 时）就开始执行，下一决策对应的 k 为 $k=q_3=15$，由于此时决策空间已经为空，所以决策结束，此为基本条件，即 $f(k,S)=k  \\ 当 S=\\emptyset$。</p>\n<p>DPFE 为</p>\n<p>$$f(k,S)=\\min_{d \\in S} {p_d + f(\\max (k-p_d,0)+q_d, S-{d})}$$</p>\n<p>终止条件为 $f(k,\\emptyset)=k$。要求的目标为 $f(0,S^{\\ast})$，$S^{\\ast}$ 为初始进程集合。上面例子使用代码实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">10</span>]</span><br><span class=\"line\">q=[<span class=\"number\">6</span>,<span class=\"number\">2</span>,<span class=\"number\">9</span>,<span class=\"number\">15</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flowshop</span><span class=\"params\">(k,S)</span></span></span><br><span class=\"line\">    if len(S)==0:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> k, []</span><br><span class=\"line\">    m=<span class=\"number\">1e8</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(len(S)):</span><br><span class=\"line\">        d=S[j]</span><br><span class=\"line\">        m1, path1=flowshow(max(k-p[d],<span class=\"number\">0</span>)+q[d], S[:j]+S[j+<span class=\"number\">1</span>:])</span><br><span class=\"line\">        m1+=p[d]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m&gt;m1:</span><br><span class=\"line\">            m=m1</span><br><span class=\"line\">            path=[s]+path1</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m, path</span><br><span class=\"line\"></span><br><span class=\"line\">m, path=flowshow(<span class=\"number\">0</span>,i)</span><br><span class=\"line\">print(m)</span><br><span class=\"line\">print(path)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"汉诺塔问题-HANOI\"><a href=\"#汉诺塔问题-HANOI\" class=\"headerlink\" title=\"汉诺塔问题 HANOI\"></a>汉诺塔问题 HANOI</h2><p>移动 N 个盘子（大小从上到下递增）从一个桩 x 到另一个桩 y 上，使用第三个桩 z 作为辅助，并保证每个桩上的盘子大小从上到下递增，总共需要移动的次数记为 $f(N)$，一次移动指将盘子从某桩移动到另一个桩上。显然有关系：</p>\n<p>$$f(i)=2f(i-1)+1$$</p>\n<p>这表明，从 x 移动 i 个盘子到 y 上，等价于从 x 移动 i-1 个盘子到 z 上，然后移动 x 的最后一个盘子到 y 上，最后从 z 上移动 i-1 个盘子到 y 上。基本态为 $f(1)=1$，于是递归可计算得 $f(2)=2 f(1)+1=3, \\ f(3)=2f(2)+1=7, \\ \\cdots$</p>\n<p>上式仅给出了移动次数，然而我们还需要确定移动序列。</p>\n<h3 id=\"非最优问题\"><a href=\"#非最优问题\" class=\"headerlink\" title=\"非最优问题\"></a>非最优问题</h3><p>记从桩 x 移动一个盘子到桩 y 为 $&lt;x,y&gt;$，定义 $F(S)$ 为移动序列，与之前求最优问题中使用加法操作不同，这里使用连接操作（concatenation），那么有</p>\n<p>$$F(N,x,y)=F(N-1,x,z)F(1,x,y)F(N-1,z,y)$$</p>\n<p>其中状态 $S=(N,x,y)$，原理与上面一致。基本态为 $F(1,x,y)=&lt;x,y&gt;$。于是可一步步推导得到：<br>$$\\begin{aligned} F(2,x,y)&amp;=F(1,x,z)F(1,x,y)F(1,z,y)=&lt;x,z&gt;&lt;x,y&gt;&lt;z,y&gt;<br>\\\\F(3,x,y)&amp;=F(2,x,z)F(1,x,y)F(2,z,y)<br>\\\\ &amp;=&lt;x,y&gt;&lt;x,z&gt;&lt;y,z&gt;&lt;x,y&gt;&lt;z,x&gt;&lt;z,y&gt;&lt;x,y&gt;\\end{aligned}$$</p>\n<p>代码实现如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pegs = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hanoi</span><span class=\"params\">(n,i=<span class=\"number\">1</span>,j=<span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [(i,j)]</span><br><span class=\"line\">    k = pegs.difference(&#123;i,j&#125;).pop()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> hanoi(n<span class=\"number\">-1</span>,i,k)+hanoi(<span class=\"number\">1</span>,i,j)+hanoi(n<span class=\"number\">-1</span>,k,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=hanoi(<span class=\"number\">3</span>)</span><br><span class=\"line\">    print(s)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"整型线性规划问题-ILP\"><a href=\"#整型线性规划问题-ILP\" class=\"headerlink\" title=\"整型线性规划问题 ILP\"></a>整型线性规划问题 ILP</h2><p>考虑如下形式的优化问题<br>$$\\max c^{\\top}x<br>\\\\ s.t. Ax \\le b<br>\\\\ x_1,…,x_n \\in \\mathbf N \\cup {0}$$<br>其中，矩阵 A 和向量 b, c 中的元素均为非负整数。这其实是一种很典型的 DP 问题，首先选择某个 $x_1$ 的值，当然无论 $x_1$ 是何值，一旦选定，就转化为 $\\sum_{i=2}^n c_i x_i$ 这个子问题的最优解，由于 $x$ 向量的所有元素一起需要满足一组条件，所以在决策 $x_i$ 元素为何值时，需要知道 $x_1,…,x_{i-1}$ 这些已经决策过的元素的值，以保证它们满足条件，所以状态 S 需要包含已经决策过的元素值以及元素在向量中的位置下标，我们约定在阶段 j 时决策 $x_{j+1}$ 的值（这个不是唯一的，也可以约定来决策 $x_j$ 的值，DPFE 形式稍作调整即可），于是 DPFE 为<br>$$f(j,S)=\\begin{cases} \\max_{x_{j+1} \\in D} {c_{j+1}x_{j+1}+f(j+1,S \\cup {(j+1,x_{j+1})})} &amp; j &lt; n<br>\\\\ 0 &amp; j=n \\end{cases}$$<br>决策空间 $D$ 由给定的条件以及状态 $S$ 决定。此问题的求解目标是 $f(0,\\emptyset)$。</p>\n<p>以上是一种求解思路，还有一种思路。从给定的条件出发，已知</p>\n<p>$$Ax \\le b$$</p>\n<p>记 $A$ 维度为 $m \\times n$，于是上式表示一共有 m 个限制条件，每个限制条件形式为 </p>\n<p>$$A_{i,:}x \\le b_i \\Rightarrow \\sum_{j=1}^n A_{i,j}x_j \\le b_i$$</p>\n<p>每做一次决策决定一个 $x_j$ 的值，将决策后的 $x_j$ 的值移到式子右边，在阶段 j，与上面一样，将决策 $x_{j+1}$ 的值，决策后上式不等式改写为</p>\n<p>$$\\sum_{k=j+2}^n A_{i,k}x_k \\le b_i - A_{i,1}x_1 - \\cdots A_{i,j+1}x_{j+1}$$</p>\n<p>也就是说，每次决策不等式右边部分均会变化，于是可定义状态 S 表示限制条件的不等式右侧部分，DPFE 如下</p>\n<p>$$f(j,(y_1,…,y_m))=\\begin{cases} \\max_{x_{j+1} \\in D} {c_{j+1}x_{j+1}+f(j+1,(y_1-A_{1,j+1}x_{j+1},…,y_m-A_{m,j+1}x_{j+1}))} &amp; j &lt; n<br>\\\\ 0 &amp; j=n \\end{cases}$$</p>\n<p>求解目标是 $f(0,(b_1,…,b_m))$。我们来看一下决策空间 $D$，在阶段 j，状态为 $S=(j,(y_1,…,y_m))$，由于限制条件为</p>\n<p>$$A_{1,j+1}x_{j+1} + A_{1,j+2}x_{j+2} + \\cdots + A_{1,n}x_n \\le y_1<br>\\\\ \\vdots<br>\\\\ A_{m,j+1}x_{j+1} + A_{m,j+2}x_{j+2} + \\cdots + A_{m,n}x_n \\le y_m$$<br>易知此时 $x_{j+1}$ 的决策空间为<br>$${0,…,\\min {\\lfloor \\frac{y_1}{A_{1,j+1}} \\rfloor, …, \\lfloor \\frac{y_m}{A_{m,j+1}} \\rfloor}}$$</p>\n<p>注意，如果出现 $\\frac {y_i} 0$，则解释为正无穷 $\\infty$，表示第 i 个限制条件对 $x_{j+1}$ 没有上限。</p>\n<p>第一种解决方法中的决策空间 $D$ 也是类似求解，令<br>$$y_i=b_i-\\sum_{p \\in S} A_{i,p_1}p_2$$<br>然后就与第二章解决方法中的决策空间的求解一样了。</p>\n<p>例：$c=(3,5), \\ b=(4,12,18)$，$A=\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\\\ 3 &amp; 2 \\end{pmatrix}$，求解 $x=(x_1,x_2)$。<br>代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c=[<span class=\"number\">3</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">b=[<span class=\"number\">4</span>,<span class=\"number\">12</span>,<span class=\"number\">18</span>]</span><br><span class=\"line\">a=[[<span class=\"number\">1</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">   [<span class=\"number\">0</span>,<span class=\"number\">2</span>],</span><br><span class=\"line\">   [<span class=\"number\">3</span>,<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">m,n=len(b),len(c)</span><br><span class=\"line\"></span><br><span class=\"line\">def d=(j,y):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> min([y[i]//a[i][j] <span class=\"keyword\">if</span> a[i][j] &gt; <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1e8</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m)])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ilp</span><span class=\"params\">(j,y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> j==n:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    dm=d(j,y)</span><br><span class=\"line\">    m_=<span class=\"number\">-1</span></span><br><span class=\"line\">    x_=<span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> d_ <span class=\"keyword\">in</span> range(dm+<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_=[y[i]-a[i][j]*d_ <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m)]</span><br><span class=\"line\">        m1,x1=ilp(j+<span class=\"number\">1</span>,y_)</span><br><span class=\"line\">        m1+=c[j]*d_</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m_ &lt; m1:</span><br><span class=\"line\">            m_ = m1</span><br><span class=\"line\">            x_ = [d_]+x1</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m_, x_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m_, x_ = ilp(<span class=\"number\">0</span>,b)</span><br><span class=\"line\">    print(m_)   <span class=\"comment\"># 36</span></span><br><span class=\"line\">    print(x_)   <span class=\"comment\"># [2,6]</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"背包型-ILP-问题-ILPKNAP\"><a href=\"#背包型-ILP-问题-ILPKNAP\" class=\"headerlink\" title=\"背包型 ILP 问题 ILPKNAP\"></a>背包型 ILP 问题 ILPKNAP</h2><p>假设有三类物体 n=3，每种物体的价值为 $(v_0,v_1,v_2)=(15,25,24)$，重量为 $(w_0,w_1,w_2)=(10,18,15)$，假设背包总共可装物体重量上限为 22，现在每种物体各选择多少个装包，使得价值最大？这个问题可以使用 ILP 模型解决，每种物体选择的数量为 $(x_0,x_1,x_2)$，系数向量为 $c=(v_0,v_1,v_2)$，限制条件的不等式左侧矩阵 $A=(w_0,w_1,w_2)$，右侧向量为 $b=(22)$，且 $x_0,x_1,x_2 \\in \\mathbf N \\cup {0}$。</p>\n<h2 id=\"区间调度问题-INTVL\"><a href=\"#区间调度问题-INTVL\" class=\"headerlink\" title=\"区间调度问题 INTVL\"></a>区间调度问题 INTVL</h2><p>假设有 N 个进程，标号为 $P={0,…,N-1}$，选择其中的一个子集，选中的进程放置在单处理器上执行，已知每个进程有区间 $(s_i,t_i)$ 表示起始时间和截止时间，在这个时间段内，进程 $i$ 得到运行，那么就获得收益 $w_i$，由于是单处理器，所以各进程执行时间不得重叠，求选择的子集，使得收益最大，DPFE 为</p>\n<p>$$f(p,q)=\\max_{d \\in P} {f(p,s_d)+c(d|p,q)+f(t_d,q)}$$<br>其中 f(p,q) 表示时间段 $[p,q]$ 内的最大收益，上式是很显然，如果做出当前决策 d，那么理论上 $[s_d,t_d]$ 这个时间段用来执行进程 d，然后还剩两个区间 $[p,s_d]$ 和 $[t_d,q]$ 再继续做决策。当前决策 d 有收益当且仅当 $p \\le s_d, t_d \\le q$。基本态是 $f(p,q)=0, \\ p \\ge q$，求解目标是 $f(0,T)$，其中 $T \\ge \\max_i {t_i}$。</p>\n<p>根据上式，在当前决策之后的两个区间 $[p,s_d]$ 和 $[t_d,q]$ 求解最大收益 $f(p,s_d), \\ f(t_d, q)$时，决策空间依然还是 $P$，虽然基本态 $f(p,q)=0, \\ p \\ge q$ 保证了递归过程可以退出，但显然决策空间应该缩小，这样可以减少递归次数，DPFE 为</p>\n<p>$$f(S,p,q)=\\max_{d \\in S} {f(S_L,p,s_d)+c(d|p,q)+f(S_R,t_d,q)}$$<br>其中 $S_L, \\ S_R \\subset P$ 分别对应 $[p,s_d]$ 和 $[t_d,q]$ 两个区间内合适的进程集合，所谓合适，就是进程的 $(s_i,t_i)$ 包含在对应区间内。基本态是 $f(S,p,q)=0, \\ p \\ge q \\text{ or } S=\\emptyset$，求解目标是 $f(P,0,T)$，其中 $T \\ge \\max_i {t_i}$。代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">P=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">s=[<span class=\"number\">9</span>,<span class=\"number\">8</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">t=[<span class=\"number\">12</span>,<span class=\"number\">11</span>,<span class=\"number\">10</span>,<span class=\"number\">7</span>,<span class=\"number\">6</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">w=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">T=max(t)</span><br><span class=\"line\">n=len(s)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_S</span><span class=\"params\">(prev_S, p, q)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> prev_S <span class=\"keyword\">if</span> s[i]&gt;=p <span class=\"keyword\">and</span> t[i]&lt;=q]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intvl</span><span class=\"params\">(S, p, q)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> p&gt;=q:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    m_=<span class=\"number\">0</span></span><br><span class=\"line\">    d_=<span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> S:</span><br><span class=\"line\">        m1, d1=intvl(get_S(S, p, s[d]), p, s[d])</span><br><span class=\"line\">        m2, d2=intvl(get_S(S, t[d], q), t[d], q)</span><br><span class=\"line\">        m=m1+m2+w[d]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m_&lt;m:</span><br><span class=\"line\">            m_=m</span><br><span class=\"line\">            d_=[d]+d1+d2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m_, d_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m, d= intvl(P, <span class=\"number\">0</span>, T)</span><br><span class=\"line\">    print(m)</span><br><span class=\"line\">    print(d)</span><br></pre></td></tr></table></figure>\n\n<p>还有一种思路。我们将进程按其截止时间升序排列，排列后的进程序号为 $P$，进程数量为 N，然后从 $P$ 的后端到前端依次做决策，也就是第 i 个决策决定是否选择 $P$ 中第 N-i-1 个进程，例如第一个决策决定是否选择最后一个进程，第 N 个决策决定是否选择第一个进程，这样的话，假设第 N-1-i 个决策决定选择第 i 个进程，那么接下来只有 $D_i={j|t_j \\le s_i}$ 的进程集合可供选择，我们令 $\\pi(i)=\\max D_i$，因为决策是按进程序号从大到小进行的，所以下一次决策直接决定是否选择序号为 $\\pi(i)$ 的进程，DPFE 为</p>\n<p>$$f(k)=\\max{w_k+f(\\pi(k)), f(k-1)}$$</p>\n<p>其中，k 所代表的进程下标从 1 开始编号（注意与程序中数组下标从 0 开始的区别）。理解上式也很简单，当前决策，要么选择进程 k，此时收益为 $w_k+f(\\pi(k))$，要么不选择进程 k，此时收益为 $f(k-1)$，通过比较哪个收益大来决定是否选择进程 k。上式可改写为</p>\n<p>$$f(k)=\\max_{d \\in {0,1}} {d\\cdot(w_k+f(\\pi (k-1)))+(1-d)\\cdot f(k-1)}$$</p>\n<p>代码实现如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">s=[<span class=\"number\">9</span>,<span class=\"number\">8</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">t=[<span class=\"number\">12</span>,<span class=\"number\">11</span>,<span class=\"number\">10</span>,<span class=\"number\">7</span>,<span class=\"number\">6</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">w=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">P=np.argsort(t)</span><br><span class=\"line\">t=np.sort(t)</span><br><span class=\"line\">s=np.array(s)[P]</span><br><span class=\"line\">w=np.array(w)[P]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pi</span><span class=\"params\">(k)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> t[i]&lt;=s[k]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intvl1</span><span class=\"params\">(k)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> k==<span class=\"number\">-1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    m1,d1=intvl1(pi(k))</span><br><span class=\"line\">    m2,d2=intvl2(k<span class=\"number\">-1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> m1+w[k]&gt;=m2:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> m1+w[k], [k]+d1</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> m2, d2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m, d= intvl(P, <span class=\"number\">0</span>, T)</span><br><span class=\"line\">    print(m)</span><br><span class=\"line\">    print(P[d])</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>本篇接上一篇 <a href=\"2019/08/14/DP2\">DP2</a> 讨论各种 DP 问题模型。</p>\n<h2 id=\"流水作业问题-FLOWSHOP\"><a href=\"#流水作业问题-FLOWSHOP\" class=\"headerlink\" title=\"流水作业问题 FLOWSHOP\"></a>流水作业问题 FLOWSHOP</h2><p>这是一个进程调度问题，每个进程有两个任务 A 和 B，B 必须在 A 完成之后才能执行。任务置于独立的处理器上执行，选择进程执行顺序，使得总执行时间（损失）最小。注意，A B 任务顺序与进程顺序一致。例如，有进程 $i={0,1,2,3}$，且 A 任务的执行时间分别为 $p_i={3,4,8,10}$，B 任务的执行时间分别为 $q_i={6,2,9,15}$，如果选择进程执行顺序为 $0,1,2,3$，那么各任务执行的开始时间和结束时间如下表：</p>","more":"<table>\n<thead>\n<tr>\n<th align=\"center\">processor 1</th>\n<th align=\"center\">$A_0: 0-3$</th>\n<th align=\"center\">$A_1:3-7$</th>\n<th align=\"center\">$A_2:7-15$</th>\n<th align=\"center\">$A_3:15-25$</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>processor 2</strong></td>\n<td align=\"center\"></td>\n<td align=\"center\">$B_0: 3-9$</td>\n<td align=\"center\">$B_1:9-11$</td>\n<td align=\"center\">$B_2:15-24$</td>\n<td align=\"center\">$B_3:25-40$</td>\n</tr>\n</tbody></table>\n<p>总执行时间为最后一个 B 任务的结束时间，这个例子中为 40。进程执行顺序是可变的，要寻找具有最小执行时间的进程顺序，可以使用 DP 解决。每次决策 d 表示选择某个进程，定义状态为 (k,S)，其中 k 表示最近调度的进程的 A B 任务结束时间之间的差，S 为剩余的未调度进程集合。初始时（尚未做任何决策）k 为 0。如果当前决策 d 满足 $k \\le p_d$，那么 $B_d$ 任务的执行将不会有延时，也就是说 $A_d$ 执行完了立马执行 $B_d$ 任务，<strong>于是下一决策的 k’ 为 $q_d$</strong>，否则的话 $B_d$ 任务 在 $A_d$ 执行完了还需要延时 $k-p_d$ 才开始执行，这就导致下一决策 k’ 为  $k-p_d+q_d$。</p>\n<p>例如上面例子中，初始时 k = 0，在第一次决策 $d_1=0$ 时，$k=0&lt;p_0$，于是 $B_0$ 任务没有延时，紧接着 $A_0$ 完成后就开始执行，然后下一决策 $d_2=1$ 的 k 为 $q_0=6$，又因为此时 $k&gt;p_1$，所以 $B_1$ 延时 $k-p_1=6-4=2$ 才开始执行，从上表中也可以看出，$B_1$ 从 $A_1$ 结束时间 7 时开始延时 2 时间才开始执行，于是下一决策 $d_3=2$ 对应的 k 为 $k:=k-p_1+q_1=6-4+2=4$，此时 $k&lt;p_2$，所以 $B_2$ 任务执行没有延时，紧接着 $A_2$ 结束之后就（在时间 15 时）开始执行，于是最后决策 $d_4=3$ 对应的 k 为 $k:=q_2=9$，此时 $k&lt;p_3$，这说明 $B_3$ 任务也没有延时，紧接着 $A_3$ 结束（在 25 时）就开始执行，下一决策对应的 k 为 $k=q_3=15$，由于此时决策空间已经为空，所以决策结束，此为基本条件，即 $f(k,S)=k  \\ 当 S=\\emptyset$。</p>\n<p>DPFE 为</p>\n<p>$$f(k,S)=\\min_{d \\in S} {p_d + f(\\max (k-p_d,0)+q_d, S-{d})}$$</p>\n<p>终止条件为 $f(k,\\emptyset)=k$。要求的目标为 $f(0,S^{\\ast})$，$S^{\\ast}$ 为初始进程集合。上面例子使用代码实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">10</span>]</span><br><span class=\"line\">q=[<span class=\"number\">6</span>,<span class=\"number\">2</span>,<span class=\"number\">9</span>,<span class=\"number\">15</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flowshop</span><span class=\"params\">(k,S)</span></span></span><br><span class=\"line\">    if len(S)==0:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> k, []</span><br><span class=\"line\">    m=<span class=\"number\">1e8</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(len(S)):</span><br><span class=\"line\">        d=S[j]</span><br><span class=\"line\">        m1, path1=flowshow(max(k-p[d],<span class=\"number\">0</span>)+q[d], S[:j]+S[j+<span class=\"number\">1</span>:])</span><br><span class=\"line\">        m1+=p[d]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m&gt;m1:</span><br><span class=\"line\">            m=m1</span><br><span class=\"line\">            path=[s]+path1</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m, path</span><br><span class=\"line\"></span><br><span class=\"line\">m, path=flowshow(<span class=\"number\">0</span>,i)</span><br><span class=\"line\">print(m)</span><br><span class=\"line\">print(path)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"汉诺塔问题-HANOI\"><a href=\"#汉诺塔问题-HANOI\" class=\"headerlink\" title=\"汉诺塔问题 HANOI\"></a>汉诺塔问题 HANOI</h2><p>移动 N 个盘子（大小从上到下递增）从一个桩 x 到另一个桩 y 上，使用第三个桩 z 作为辅助，并保证每个桩上的盘子大小从上到下递增，总共需要移动的次数记为 $f(N)$，一次移动指将盘子从某桩移动到另一个桩上。显然有关系：</p>\n<p>$$f(i)=2f(i-1)+1$$</p>\n<p>这表明，从 x 移动 i 个盘子到 y 上，等价于从 x 移动 i-1 个盘子到 z 上，然后移动 x 的最后一个盘子到 y 上，最后从 z 上移动 i-1 个盘子到 y 上。基本态为 $f(1)=1$，于是递归可计算得 $f(2)=2 f(1)+1=3, \\ f(3)=2f(2)+1=7, \\ \\cdots$</p>\n<p>上式仅给出了移动次数，然而我们还需要确定移动序列。</p>\n<h3 id=\"非最优问题\"><a href=\"#非最优问题\" class=\"headerlink\" title=\"非最优问题\"></a>非最优问题</h3><p>记从桩 x 移动一个盘子到桩 y 为 $&lt;x,y&gt;$，定义 $F(S)$ 为移动序列，与之前求最优问题中使用加法操作不同，这里使用连接操作（concatenation），那么有</p>\n<p>$$F(N,x,y)=F(N-1,x,z)F(1,x,y)F(N-1,z,y)$$</p>\n<p>其中状态 $S=(N,x,y)$，原理与上面一致。基本态为 $F(1,x,y)=&lt;x,y&gt;$。于是可一步步推导得到：<br>$$\\begin{aligned} F(2,x,y)&amp;=F(1,x,z)F(1,x,y)F(1,z,y)=&lt;x,z&gt;&lt;x,y&gt;&lt;z,y&gt;<br>\\\\F(3,x,y)&amp;=F(2,x,z)F(1,x,y)F(2,z,y)<br>\\\\ &amp;=&lt;x,y&gt;&lt;x,z&gt;&lt;y,z&gt;&lt;x,y&gt;&lt;z,x&gt;&lt;z,y&gt;&lt;x,y&gt;\\end{aligned}$$</p>\n<p>代码实现如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pegs = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hanoi</span><span class=\"params\">(n,i=<span class=\"number\">1</span>,j=<span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [(i,j)]</span><br><span class=\"line\">    k = pegs.difference(&#123;i,j&#125;).pop()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> hanoi(n<span class=\"number\">-1</span>,i,k)+hanoi(<span class=\"number\">1</span>,i,j)+hanoi(n<span class=\"number\">-1</span>,k,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=hanoi(<span class=\"number\">3</span>)</span><br><span class=\"line\">    print(s)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"整型线性规划问题-ILP\"><a href=\"#整型线性规划问题-ILP\" class=\"headerlink\" title=\"整型线性规划问题 ILP\"></a>整型线性规划问题 ILP</h2><p>考虑如下形式的优化问题<br>$$\\max c^{\\top}x<br>\\\\ s.t. Ax \\le b<br>\\\\ x_1,…,x_n \\in \\mathbf N \\cup {0}$$<br>其中，矩阵 A 和向量 b, c 中的元素均为非负整数。这其实是一种很典型的 DP 问题，首先选择某个 $x_1$ 的值，当然无论 $x_1$ 是何值，一旦选定，就转化为 $\\sum_{i=2}^n c_i x_i$ 这个子问题的最优解，由于 $x$ 向量的所有元素一起需要满足一组条件，所以在决策 $x_i$ 元素为何值时，需要知道 $x_1,…,x_{i-1}$ 这些已经决策过的元素的值，以保证它们满足条件，所以状态 S 需要包含已经决策过的元素值以及元素在向量中的位置下标，我们约定在阶段 j 时决策 $x_{j+1}$ 的值（这个不是唯一的，也可以约定来决策 $x_j$ 的值，DPFE 形式稍作调整即可），于是 DPFE 为<br>$$f(j,S)=\\begin{cases} \\max_{x_{j+1} \\in D} {c_{j+1}x_{j+1}+f(j+1,S \\cup {(j+1,x_{j+1})})} &amp; j &lt; n<br>\\\\ 0 &amp; j=n \\end{cases}$$<br>决策空间 $D$ 由给定的条件以及状态 $S$ 决定。此问题的求解目标是 $f(0,\\emptyset)$。</p>\n<p>以上是一种求解思路，还有一种思路。从给定的条件出发，已知</p>\n<p>$$Ax \\le b$$</p>\n<p>记 $A$ 维度为 $m \\times n$，于是上式表示一共有 m 个限制条件，每个限制条件形式为 </p>\n<p>$$A_{i,:}x \\le b_i \\Rightarrow \\sum_{j=1}^n A_{i,j}x_j \\le b_i$$</p>\n<p>每做一次决策决定一个 $x_j$ 的值，将决策后的 $x_j$ 的值移到式子右边，在阶段 j，与上面一样，将决策 $x_{j+1}$ 的值，决策后上式不等式改写为</p>\n<p>$$\\sum_{k=j+2}^n A_{i,k}x_k \\le b_i - A_{i,1}x_1 - \\cdots A_{i,j+1}x_{j+1}$$</p>\n<p>也就是说，每次决策不等式右边部分均会变化，于是可定义状态 S 表示限制条件的不等式右侧部分，DPFE 如下</p>\n<p>$$f(j,(y_1,…,y_m))=\\begin{cases} \\max_{x_{j+1} \\in D} {c_{j+1}x_{j+1}+f(j+1,(y_1-A_{1,j+1}x_{j+1},…,y_m-A_{m,j+1}x_{j+1}))} &amp; j &lt; n<br>\\\\ 0 &amp; j=n \\end{cases}$$</p>\n<p>求解目标是 $f(0,(b_1,…,b_m))$。我们来看一下决策空间 $D$，在阶段 j，状态为 $S=(j,(y_1,…,y_m))$，由于限制条件为</p>\n<p>$$A_{1,j+1}x_{j+1} + A_{1,j+2}x_{j+2} + \\cdots + A_{1,n}x_n \\le y_1<br>\\\\ \\vdots<br>\\\\ A_{m,j+1}x_{j+1} + A_{m,j+2}x_{j+2} + \\cdots + A_{m,n}x_n \\le y_m$$<br>易知此时 $x_{j+1}$ 的决策空间为<br>$${0,…,\\min {\\lfloor \\frac{y_1}{A_{1,j+1}} \\rfloor, …, \\lfloor \\frac{y_m}{A_{m,j+1}} \\rfloor}}$$</p>\n<p>注意，如果出现 $\\frac {y_i} 0$，则解释为正无穷 $\\infty$，表示第 i 个限制条件对 $x_{j+1}$ 没有上限。</p>\n<p>第一种解决方法中的决策空间 $D$ 也是类似求解，令<br>$$y_i=b_i-\\sum_{p \\in S} A_{i,p_1}p_2$$<br>然后就与第二章解决方法中的决策空间的求解一样了。</p>\n<p>例：$c=(3,5), \\ b=(4,12,18)$，$A=\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\\\ 3 &amp; 2 \\end{pmatrix}$，求解 $x=(x_1,x_2)$。<br>代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c=[<span class=\"number\">3</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">b=[<span class=\"number\">4</span>,<span class=\"number\">12</span>,<span class=\"number\">18</span>]</span><br><span class=\"line\">a=[[<span class=\"number\">1</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">   [<span class=\"number\">0</span>,<span class=\"number\">2</span>],</span><br><span class=\"line\">   [<span class=\"number\">3</span>,<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">m,n=len(b),len(c)</span><br><span class=\"line\"></span><br><span class=\"line\">def d=(j,y):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> min([y[i]//a[i][j] <span class=\"keyword\">if</span> a[i][j] &gt; <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1e8</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m)])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ilp</span><span class=\"params\">(j,y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> j==n:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    dm=d(j,y)</span><br><span class=\"line\">    m_=<span class=\"number\">-1</span></span><br><span class=\"line\">    x_=<span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> d_ <span class=\"keyword\">in</span> range(dm+<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_=[y[i]-a[i][j]*d_ <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m)]</span><br><span class=\"line\">        m1,x1=ilp(j+<span class=\"number\">1</span>,y_)</span><br><span class=\"line\">        m1+=c[j]*d_</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m_ &lt; m1:</span><br><span class=\"line\">            m_ = m1</span><br><span class=\"line\">            x_ = [d_]+x1</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m_, x_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m_, x_ = ilp(<span class=\"number\">0</span>,b)</span><br><span class=\"line\">    print(m_)   <span class=\"comment\"># 36</span></span><br><span class=\"line\">    print(x_)   <span class=\"comment\"># [2,6]</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"背包型-ILP-问题-ILPKNAP\"><a href=\"#背包型-ILP-问题-ILPKNAP\" class=\"headerlink\" title=\"背包型 ILP 问题 ILPKNAP\"></a>背包型 ILP 问题 ILPKNAP</h2><p>假设有三类物体 n=3，每种物体的价值为 $(v_0,v_1,v_2)=(15,25,24)$，重量为 $(w_0,w_1,w_2)=(10,18,15)$，假设背包总共可装物体重量上限为 22，现在每种物体各选择多少个装包，使得价值最大？这个问题可以使用 ILP 模型解决，每种物体选择的数量为 $(x_0,x_1,x_2)$，系数向量为 $c=(v_0,v_1,v_2)$，限制条件的不等式左侧矩阵 $A=(w_0,w_1,w_2)$，右侧向量为 $b=(22)$，且 $x_0,x_1,x_2 \\in \\mathbf N \\cup {0}$。</p>\n<h2 id=\"区间调度问题-INTVL\"><a href=\"#区间调度问题-INTVL\" class=\"headerlink\" title=\"区间调度问题 INTVL\"></a>区间调度问题 INTVL</h2><p>假设有 N 个进程，标号为 $P={0,…,N-1}$，选择其中的一个子集，选中的进程放置在单处理器上执行，已知每个进程有区间 $(s_i,t_i)$ 表示起始时间和截止时间，在这个时间段内，进程 $i$ 得到运行，那么就获得收益 $w_i$，由于是单处理器，所以各进程执行时间不得重叠，求选择的子集，使得收益最大，DPFE 为</p>\n<p>$$f(p,q)=\\max_{d \\in P} {f(p,s_d)+c(d|p,q)+f(t_d,q)}$$<br>其中 f(p,q) 表示时间段 $[p,q]$ 内的最大收益，上式是很显然，如果做出当前决策 d，那么理论上 $[s_d,t_d]$ 这个时间段用来执行进程 d，然后还剩两个区间 $[p,s_d]$ 和 $[t_d,q]$ 再继续做决策。当前决策 d 有收益当且仅当 $p \\le s_d, t_d \\le q$。基本态是 $f(p,q)=0, \\ p \\ge q$，求解目标是 $f(0,T)$，其中 $T \\ge \\max_i {t_i}$。</p>\n<p>根据上式，在当前决策之后的两个区间 $[p,s_d]$ 和 $[t_d,q]$ 求解最大收益 $f(p,s_d), \\ f(t_d, q)$时，决策空间依然还是 $P$，虽然基本态 $f(p,q)=0, \\ p \\ge q$ 保证了递归过程可以退出，但显然决策空间应该缩小，这样可以减少递归次数，DPFE 为</p>\n<p>$$f(S,p,q)=\\max_{d \\in S} {f(S_L,p,s_d)+c(d|p,q)+f(S_R,t_d,q)}$$<br>其中 $S_L, \\ S_R \\subset P$ 分别对应 $[p,s_d]$ 和 $[t_d,q]$ 两个区间内合适的进程集合，所谓合适，就是进程的 $(s_i,t_i)$ 包含在对应区间内。基本态是 $f(S,p,q)=0, \\ p \\ge q \\text{ or } S=\\emptyset$，求解目标是 $f(P,0,T)$，其中 $T \\ge \\max_i {t_i}$。代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">P=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">s=[<span class=\"number\">9</span>,<span class=\"number\">8</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">t=[<span class=\"number\">12</span>,<span class=\"number\">11</span>,<span class=\"number\">10</span>,<span class=\"number\">7</span>,<span class=\"number\">6</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">w=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">T=max(t)</span><br><span class=\"line\">n=len(s)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_S</span><span class=\"params\">(prev_S, p, q)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> prev_S <span class=\"keyword\">if</span> s[i]&gt;=p <span class=\"keyword\">and</span> t[i]&lt;=q]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intvl</span><span class=\"params\">(S, p, q)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> p&gt;=q:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    m_=<span class=\"number\">0</span></span><br><span class=\"line\">    d_=<span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> S:</span><br><span class=\"line\">        m1, d1=intvl(get_S(S, p, s[d]), p, s[d])</span><br><span class=\"line\">        m2, d2=intvl(get_S(S, t[d], q), t[d], q)</span><br><span class=\"line\">        m=m1+m2+w[d]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> m_&lt;m:</span><br><span class=\"line\">            m_=m</span><br><span class=\"line\">            d_=[d]+d1+d2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m_, d_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m, d= intvl(P, <span class=\"number\">0</span>, T)</span><br><span class=\"line\">    print(m)</span><br><span class=\"line\">    print(d)</span><br></pre></td></tr></table></figure>\n\n<p>还有一种思路。我们将进程按其截止时间升序排列，排列后的进程序号为 $P$，进程数量为 N，然后从 $P$ 的后端到前端依次做决策，也就是第 i 个决策决定是否选择 $P$ 中第 N-i-1 个进程，例如第一个决策决定是否选择最后一个进程，第 N 个决策决定是否选择第一个进程，这样的话，假设第 N-1-i 个决策决定选择第 i 个进程，那么接下来只有 $D_i={j|t_j \\le s_i}$ 的进程集合可供选择，我们令 $\\pi(i)=\\max D_i$，因为决策是按进程序号从大到小进行的，所以下一次决策直接决定是否选择序号为 $\\pi(i)$ 的进程，DPFE 为</p>\n<p>$$f(k)=\\max{w_k+f(\\pi(k)), f(k-1)}$$</p>\n<p>其中，k 所代表的进程下标从 1 开始编号（注意与程序中数组下标从 0 开始的区别）。理解上式也很简单，当前决策，要么选择进程 k，此时收益为 $w_k+f(\\pi(k))$，要么不选择进程 k，此时收益为 $f(k-1)$，通过比较哪个收益大来决定是否选择进程 k。上式可改写为</p>\n<p>$$f(k)=\\max_{d \\in {0,1}} {d\\cdot(w_k+f(\\pi (k-1)))+(1-d)\\cdot f(k-1)}$$</p>\n<p>代码实现如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">s=[<span class=\"number\">9</span>,<span class=\"number\">8</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">t=[<span class=\"number\">12</span>,<span class=\"number\">11</span>,<span class=\"number\">10</span>,<span class=\"number\">7</span>,<span class=\"number\">6</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">w=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">P=np.argsort(t)</span><br><span class=\"line\">t=np.sort(t)</span><br><span class=\"line\">s=np.array(s)[P]</span><br><span class=\"line\">w=np.array(w)[P]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pi</span><span class=\"params\">(k)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> t[i]&lt;=s[k]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intvl1</span><span class=\"params\">(k)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> k==<span class=\"number\">-1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>, []</span><br><span class=\"line\">    m1,d1=intvl1(pi(k))</span><br><span class=\"line\">    m2,d2=intvl2(k<span class=\"number\">-1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> m1+w[k]&gt;=m2:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> m1+w[k], [k]+d1</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> m2, d2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    m, d= intvl(P, <span class=\"number\">0</span>, T)</span><br><span class=\"line\">    print(m)</span><br><span class=\"line\">    print(P[d])</span><br></pre></td></tr></table></figure>"},{"title":"PyTorch-4","p":"pytorch/PyTorch-4","date":"2019-08-22T06:34:33.000Z","_content":"## Tensor\ntorch 模块中包含了各种 Tensor：FloatTensor，DoubleTensor，HalfTensor，ByteTensor 等。这些 Tensor 是怎么来的呢？首先，入口是 `torch/__init__.py` 中的 `_C._initExtension(manager_path())`，其中 manager_path 用于获取 torch_shm_manager 的文件路径，shm 实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，这里跳过。_initExtension 在 torch/csrc/Module.cpp 中初始化 _C 模块时注册到 _C 模块中，其底层 c++ 实现函数为 THPModule_initExtension，这个函数定义中初始化了很多东西，我们依次来看看。\n<!-- more -->\n### initializeLayouts\n初始化内存布局。当前有三种布局（位于文件 c10/core/Layout.h 中）：\n1. Strided，使用密集多维数组的内存布局\n2. Sparse，使用稀疏多维数组的内存布局\n3. Mkldnn，使用 Intel 的 Mkldnn 库加速 CPU 时，由于 Mkldnn 使用了内部特殊内存布局，所以增加对应的内存布局枚举\n\n以最常用的 Strided 布局为例，使用 THPLayout_New 生成 THPLayoutType/THPLayout 的类型对象，指定 layout 为 Strided，name 为 \"torch.strided\"，然后 __将这个类型添加到 torch 模块中__，其他两种内存布局方式也类似处理。最后注册这些布局类型：\n- CPU, CUDA, MSNPU, XLA, QuantizedCPU -> strided_layout\n- SparseCPU, SparseCUDA -> sparse_coo_layout\n- MkldnnCPU -> mkldnn_layout\n  \n即，将 Backend 与 Layout 关联起来，以便将来根据 Backend 获取对应的 Layout。\n\n### initializeMemoryFormats\n初始化内存格式。内存格式表明 Tensor 中的数据是如何组织的。当前有三种：Preserve, Contiguous 和 ChannelsLast。例如 ChannelsLast 表示内存中数据的格式为 NHWC，假设正常顺序 NCHW 的各维度值为 sizes，那么 ChannelsLast 下的各维度步幅 strides 应为：\n```c++\nstrides[1]=1;           // ChannelsLast 中 C 为最低一级维度，故步幅为 1\nstrides[3]=sizes[1];    // ChannelsLast 中 W 为次低一级维度，故步幅为 C 维度即 sizes[1]\nstrides[2]=strides[3]*sizes[3]; // ChannelsLast 中 H 为再次低一级维度，步幅为 W*C\nstrides[0]=strides[2]*sizes[2]; // ChannelsLast 中 N 为最高一级维度，步幅为 H*W*C\n```\n注意，上面 strides 和 sizes 的顺序均为 NCHW。\n创建三种内存格式类型对象 preserve_format, contiguous_format, channels_last 并 __添加到 torch 模块中__。\n\n### initializeQScheme\n初始化量化机制，量化是将连续型的输入限制为离散型，比如将浮点计算转为整型计算，显然使用小型整型比浮点型计算更高效，且内存占用更小，这在模型 inference 阶段尤其重要。关于量化的具体概念以及相关操作可参考 [Introducing-Quantized-Tensor](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor)。当前量化机制有 5 种，类似以上布局和内存格式，分别创建对应的类型对象，然后 __添加到 torch 模块中__。\n\n### initializeDtypes\n初始化数据类型。直接看此函数的部分定义\n```c++\n#define DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,\nat:ScalarType all_scalar_type[] = {\n    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n```\n其中 AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS 这个宏罗列了所有的标量类型，包括 complex 类型和 quantization 类型，展开后为\n```\nat::ScalarType::Byte,\nat::ScalarType::Char,\nat::ScalarType::Short,\nat::ScalarType::Int,\nat::ScalarType::Long,\nat::ScalarType::Half,\nat::ScalarType::Float,\nat::ScalarType::Double,\nat::ScalarType::ComplexHalf,\nat::ScalarType::ComplexFloat,\nat::ScalarType::ComplexDouble,\nat::ScalarType::Bool,\nat::ScalarType::QInt8,\nat::ScalarType::QUInt8,\nat::ScalarType::QInt32\nat::ScalarType::BFloat16\n```\n然后根据以上标量类型获取其主名称和传统旧名称，如没有传统旧名称，则默认为空字符串 `\"\"`，然后创建各个对应的类型对象，并注册这些类型对象（即，将类型对象与 at::ScalarType 值关联起来，存储到字典，以便将来能根据 at::ScalarType 获取对应的类型对象），相关代码如下\n```c++\nstd::tie(primary_name, legacy_name) = getDtypeName(scalarType);\nPyObject *dtype = THPDtype_New(scalarType, primary_name);\ntorch::registerDtypeObject((THPDtype*)dtype, scalarType);\n```\n然后将类型对象（THPDtypeType/THPDtype 实例）__添加到 torch 模块中__，如果存在传统旧名称，则也同样添加到 torch 模块中。\n\n### initialize_python_bindings\n初始化 python 绑定\n#### initialize_aten_types\n根据 all_declared_types 函数获取所有声明过的类型，Backend 有 `CPU, CUDA, SparseCPU, SparseCUDA` 四种，ScalarType 除去 Complex 和 Quantization 类型，则一共有\n```\nByte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16\n```\n然后所有 Backend 与 ScalarType 不同的组合构成这里所需要的声明类型，不过 (SparseCUDA|SparseCPU,Bool) 除外，这样的组合一共有 4*10-2=38 种，根据这 38 种组合构建对应的 PyTensorType 类型，看看这个 PyTensorType 类型定义，\n```c++\nstruct PyTensorType {\n    PyTypeObject py_type;   // python 类型对应的扩展类型，这个字段后文会再次讲到\n    THPDtype* dtype;        // 对应上文 initializeDtypes 中注册的某个数据标量类型\n    THPLayout* layout;      // 对应上文 initializeLayouts 中注册的某个内存布局类型\n    bool is_cuda;           // 指示是 cuda 还是 cpu\n    char name[64];          // tensor 类型名称\n    int backend;            // CPU, CUDA, SparseCPU, SparseCUDA 四种之一\n    int scalar_type;        // Byte 等十种之一\n};\n```\n上述 38 种组合，每种组合构建一个 PyTensorType 对象。Python 接口中某种 Tensor，比如 FloatTensor 其底层就对应这里的某个 PyTensorType 对象。\n- layout，根据 backend 字段获取，initializeLayouts 中注册了所有 Backend 与 Layout 的映射关系\n- is_cuda，当 backend = CUDA|SparseCUD 时为 true\n- name，名称构成为 `[模块名].[ScalarType名]Tensor`。  \n  模块名：\n  ```\n  CPU -> torch\n  CUDA -> torch.cuda\n  SparseCPU -> torch.sparse\n  SparseCUDA -> torch.cuda.sparse\n  ```\n  ScalarType名 就是 ScalarType 字面量的字符串形式，如 Byte -> \"Byte\", Float -> \"Float\" 等。例如组合 (CPU, Float) 对应的 PyTensorType 对象名为 \"torch.FloatTensor\"，(SparseCUDA, Double) 对应的 PyTensorType 对象名为 \"torch.cuda.sparse.DoubleTensor\"。\n\n注意到，\n```c++\nif (backend==Backend::CPU && scalar_type==at::kFloat) {\n    set_default_tensor_type(&tensor_type);\n}\n```\n可见默认 Tensor 类型为 torch.FloatTensor（其 Backend 为 CPU，注意对应 CUDA 的为 torch.cuda.FloatTensor）。\n\n总结：initialize_aten_types 根据 38 种组合构建 PyTensorType 类型对象，并保存到 tensor_types 这个 vector 中。\n\n#### py_initialize_metaclass(metaclass)\n初始化元类，这是一个 python 的扩展类型 PyTypeObject，对应的 python 类型名为 \"torch.tensortype\"，顾名思义表示 tensor 类型类，即，tensor 各种类型如 torch.FloatTensor 等的元类型，这个元类具有的属性为\n- dtype  \n    对应 initializeDtypes 中某个 THPDtype 对象\n- layout  \n    对应 initializeLayouts 中某个内存布局类型 THPLayout 对象\n- is_cuda  \n    是否使用 cuda\n- is_sparse  \n    是否是稀疏存储\n\n具有方法\n- `__instancecheck__` 检测某个 Tensor 是否与当前 tensor 类型类匹配，当 type_id 和 scalar_type 这两个字段均分别相同时，则匹配，否则不匹配。\n\nPyTensorType 是表示 python 的 Tensor 类型，我们指定 python 的类型本身也是一种对象，这种类型对象的类型为元类型，也就是这里的 metaclass。\n\n总结：初始化 PyTypeObject 类型对象 metaclass，它表示 tensor 类型的元类，且具有上述属性和方法。\n\n#### get_tensor_dict\n获取 torch.Tensor 以及其基类 _C._TensorBase 的初始属性（名称与值构成的字典）\n\n#### py_initialize_tensor_type\n对于前面构造的 38 个 PyTensorType 对象，设置每个对象的 py_type 字段。py_type 类型为 PyTypeObject，表示一个类型对象，也就是 python 中的某个类型，为这个类型对象设置元类 metaclass，名称，以及将上一小节中的属性字典并入这个类型，从而使得类型具有 torch.Tensor 的全部初始属性。\n```\ndir(torch.Tensor)\ndir(torch.FloatTensor)\n```\n以上两个指令输出内容一样。\n\n#### py_bind_tensor_types\n至此，以上 38 种 PyTensorType 对象均已准备好，将他们添加进相应的模块中，前文可能说 \"添加进 torch 模块中\"，因为当时没有讨论到 PyTensorType 对象名，所以笼统的那么说了一下，实际上应为 \"添加进相应的模块中\"，比如 \"torch.FloatTensor\"，则将相应的 PyTensorType 对象以 FloatTensor 作为 python 端的名称添加进 torch 模块中，\"torch.cuda.sparse.DoubleTensor\" 则将相应的 PyTensorType 对象以 DoubleTensor 作为 python 端的名称添加进 torch.cuda.sparse 模块中，即最后一个 `.` 后面的部分表示类型，而之前的部分表示模块。\n\n但是，现在还存在一个问题，那就是这些 torch.FloatTensor, torch.IntTensor 等类型与 torch.Tensor 是什么关系？\n```python\na=torch.empty(1,2,dtype=torch.int)\nisinstance(a, torch.IntTensor)  # True\nisinstance(a, torch.Tensor)     # True\nissubclass(torch.IntTensor, torch.Tensor)   # False\nissubclass(torch.Tensor, torch.IntTensor)   # False\n```\n根据 [Pytorch-3](2019/06/18/Pytorch-3) 最后的分析，我们知道 torch.empty 函数最后使用 THPVariable_Wrap 将 c++ Variable 类型包装成 python 的 torch.Tensor 类型，甚至直接调用 torch.IntTensor 构造的对象最后也是经过 THPVariable_Wrap 包装成 torch.Tensor 类型，\n```\n>>> type(torch.IntTensor([1,2]))\n<class 'torch.Tensor'>\n```\n既然返回的都是 torch.Tensor 类型，那怎么跟 torch.IntTensor 联系起来的呢？其实，torch.IntTensor 等 38 个 Tensor 类型与 torch.Tensor 没有直接关系\n```\n>>> torch.IntTensor.__bases__\n(<class 'object'>)\n>>> torch.Tensor.__bases__\n(<class 'torch._C._TensorBases'>)\n```\n上面所说的各种构造 Tensor 的方法返回的类型也确实是 torch.Tensor，但是 `isinstance(a, torch.IntTensor)` 结果为 True 也没错，因为 `isinstance` 实际上内部调用 `__instancecheck__` 进行判断，前面讨论 metaclass 时讲到这个方法的 c++ 底层实现函数为 Tensor_instancecheck，其定义如下\n```c++\nstatic PyObject *Tensor_instancecheck(PyTensorType *self, PyObject * arg) {\n    try{\n        if(THPVariable_Check(arg)) {    // 检测参数是否是 THPVariable 类型\n            auto& var = ((THPVariable*)arg)->cdata; // 获取内部的 Variable 类型对象\n            if (var.type_id() == self->get_type_id() &&\n                var.scalar_type() == static_cast<ScalarType>(self->scalar_type)) {\n                Py_RETURN_TRUE;     // 如果 type_id 和 ScalarType 均分别相同，则返回 True\n            }\n        }\n        Py_RETURN_FALSE;\n    } catch(python_error & e){\n        return nullptr;\n    }\n}\n```\n所以，不难理解 `isinstance(a, torch.IntTensor)=True`。\n\n__总结：在 [PyTorch-2]() 中，我们讨论了 PyTorch 中的函数返回或直接构造的 Tensor 均为 torch.Tensor，其继承自 `torch._C.Tensor`，所以要理解 Tensor 类的各种方法，需要从 `torch._C.Tensor` 的类型构造开始着手。__\n\n接下来是一系列的 THPxxxStorage_postInit 函数执行，这在 [PyTorch-2](2019/06/13/PyTorch-2) 中已经进行了介绍（THPxxxStorage_init），这里仅仅给出结论：\n1. 函数声明\n    ```c++\n    #define THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) //torch/csrc/Storage.h\n    bool THPStorage_(postInit)(PyObject *module);   // torch/csrc/generic/Storage.h\n    ```\n    THPStorage_(NAME) 这个宏展开后就得到 THPxxxStorage_init， 其中 Real 在宏展开时被替换为具体的 ScalarType，NAME 被替换为 init。于是，最终得到的函数声明为 THPxxxStorage_init(PyObject *module);\n\n2. torch/csrc/generic/Storage.cpp 中\n   ```c++\n   PyObject *THPStorageClass = nullptr;\n   bool THPStorage_(postInit)(PyObject *module){\n       // 从 torch 模块中获取名为 RealStorage 的属性，其中 Real 可为 Float, Bool, Double 等 ScalarType\n       THPStorageClass = PyObject_GetAttrString(module, (char*)TH_CONCAT_STRING_2(Real, Storage));\n       at::Backend backend = at::Backend::CPU;\n       #ifdef THC_GENERIC_FILE\n       backend = at::Backend::CUDA;\n       #endif\n       #ifdef THQUANTIZED\n       backend = at::Backend::QuantizedCPU;\n       #endif\n       torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));\n   }\n   ```\n   注意到在 `torch/__init__.py` 中定义了 FloatStorage 等类型，\n   ```python\n   class FloatStorage(_C.FloatStorageBase, _StorageBase):\n       pass\n   ```\n   torch._C.FloatStorageBase 等类型是在 THPxxxStorage_init 函数中被添加到模块 torch._C 中。THPxxxStorage_postInit 函数先是从 torch 模块中获取 RealStorage 类型对象，然后进行注册即，将 RealStorage 类型对象与对应的组合 (Backend, ScalarType) 进行映射，这样以后就可以根据 (Backend, ScalarType) 获取对应的 RealStorage 类型对象，反过来亦可。\n\n","source":"_posts/pytorch/PyTorch-4.md","raw":"---\ntitle: PyTorch-4\np: pytorch/PyTorch-4\ndate: 2019-08-22 14:34:33\ntags: PyTorch\ncategories: DL Framework\n---\n## Tensor\ntorch 模块中包含了各种 Tensor：FloatTensor，DoubleTensor，HalfTensor，ByteTensor 等。这些 Tensor 是怎么来的呢？首先，入口是 `torch/__init__.py` 中的 `_C._initExtension(manager_path())`，其中 manager_path 用于获取 torch_shm_manager 的文件路径，shm 实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，这里跳过。_initExtension 在 torch/csrc/Module.cpp 中初始化 _C 模块时注册到 _C 模块中，其底层 c++ 实现函数为 THPModule_initExtension，这个函数定义中初始化了很多东西，我们依次来看看。\n<!-- more -->\n### initializeLayouts\n初始化内存布局。当前有三种布局（位于文件 c10/core/Layout.h 中）：\n1. Strided，使用密集多维数组的内存布局\n2. Sparse，使用稀疏多维数组的内存布局\n3. Mkldnn，使用 Intel 的 Mkldnn 库加速 CPU 时，由于 Mkldnn 使用了内部特殊内存布局，所以增加对应的内存布局枚举\n\n以最常用的 Strided 布局为例，使用 THPLayout_New 生成 THPLayoutType/THPLayout 的类型对象，指定 layout 为 Strided，name 为 \"torch.strided\"，然后 __将这个类型添加到 torch 模块中__，其他两种内存布局方式也类似处理。最后注册这些布局类型：\n- CPU, CUDA, MSNPU, XLA, QuantizedCPU -> strided_layout\n- SparseCPU, SparseCUDA -> sparse_coo_layout\n- MkldnnCPU -> mkldnn_layout\n  \n即，将 Backend 与 Layout 关联起来，以便将来根据 Backend 获取对应的 Layout。\n\n### initializeMemoryFormats\n初始化内存格式。内存格式表明 Tensor 中的数据是如何组织的。当前有三种：Preserve, Contiguous 和 ChannelsLast。例如 ChannelsLast 表示内存中数据的格式为 NHWC，假设正常顺序 NCHW 的各维度值为 sizes，那么 ChannelsLast 下的各维度步幅 strides 应为：\n```c++\nstrides[1]=1;           // ChannelsLast 中 C 为最低一级维度，故步幅为 1\nstrides[3]=sizes[1];    // ChannelsLast 中 W 为次低一级维度，故步幅为 C 维度即 sizes[1]\nstrides[2]=strides[3]*sizes[3]; // ChannelsLast 中 H 为再次低一级维度，步幅为 W*C\nstrides[0]=strides[2]*sizes[2]; // ChannelsLast 中 N 为最高一级维度，步幅为 H*W*C\n```\n注意，上面 strides 和 sizes 的顺序均为 NCHW。\n创建三种内存格式类型对象 preserve_format, contiguous_format, channels_last 并 __添加到 torch 模块中__。\n\n### initializeQScheme\n初始化量化机制，量化是将连续型的输入限制为离散型，比如将浮点计算转为整型计算，显然使用小型整型比浮点型计算更高效，且内存占用更小，这在模型 inference 阶段尤其重要。关于量化的具体概念以及相关操作可参考 [Introducing-Quantized-Tensor](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor)。当前量化机制有 5 种，类似以上布局和内存格式，分别创建对应的类型对象，然后 __添加到 torch 模块中__。\n\n### initializeDtypes\n初始化数据类型。直接看此函数的部分定义\n```c++\n#define DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,\nat:ScalarType all_scalar_type[] = {\n    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n```\n其中 AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS 这个宏罗列了所有的标量类型，包括 complex 类型和 quantization 类型，展开后为\n```\nat::ScalarType::Byte,\nat::ScalarType::Char,\nat::ScalarType::Short,\nat::ScalarType::Int,\nat::ScalarType::Long,\nat::ScalarType::Half,\nat::ScalarType::Float,\nat::ScalarType::Double,\nat::ScalarType::ComplexHalf,\nat::ScalarType::ComplexFloat,\nat::ScalarType::ComplexDouble,\nat::ScalarType::Bool,\nat::ScalarType::QInt8,\nat::ScalarType::QUInt8,\nat::ScalarType::QInt32\nat::ScalarType::BFloat16\n```\n然后根据以上标量类型获取其主名称和传统旧名称，如没有传统旧名称，则默认为空字符串 `\"\"`，然后创建各个对应的类型对象，并注册这些类型对象（即，将类型对象与 at::ScalarType 值关联起来，存储到字典，以便将来能根据 at::ScalarType 获取对应的类型对象），相关代码如下\n```c++\nstd::tie(primary_name, legacy_name) = getDtypeName(scalarType);\nPyObject *dtype = THPDtype_New(scalarType, primary_name);\ntorch::registerDtypeObject((THPDtype*)dtype, scalarType);\n```\n然后将类型对象（THPDtypeType/THPDtype 实例）__添加到 torch 模块中__，如果存在传统旧名称，则也同样添加到 torch 模块中。\n\n### initialize_python_bindings\n初始化 python 绑定\n#### initialize_aten_types\n根据 all_declared_types 函数获取所有声明过的类型，Backend 有 `CPU, CUDA, SparseCPU, SparseCUDA` 四种，ScalarType 除去 Complex 和 Quantization 类型，则一共有\n```\nByte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16\n```\n然后所有 Backend 与 ScalarType 不同的组合构成这里所需要的声明类型，不过 (SparseCUDA|SparseCPU,Bool) 除外，这样的组合一共有 4*10-2=38 种，根据这 38 种组合构建对应的 PyTensorType 类型，看看这个 PyTensorType 类型定义，\n```c++\nstruct PyTensorType {\n    PyTypeObject py_type;   // python 类型对应的扩展类型，这个字段后文会再次讲到\n    THPDtype* dtype;        // 对应上文 initializeDtypes 中注册的某个数据标量类型\n    THPLayout* layout;      // 对应上文 initializeLayouts 中注册的某个内存布局类型\n    bool is_cuda;           // 指示是 cuda 还是 cpu\n    char name[64];          // tensor 类型名称\n    int backend;            // CPU, CUDA, SparseCPU, SparseCUDA 四种之一\n    int scalar_type;        // Byte 等十种之一\n};\n```\n上述 38 种组合，每种组合构建一个 PyTensorType 对象。Python 接口中某种 Tensor，比如 FloatTensor 其底层就对应这里的某个 PyTensorType 对象。\n- layout，根据 backend 字段获取，initializeLayouts 中注册了所有 Backend 与 Layout 的映射关系\n- is_cuda，当 backend = CUDA|SparseCUD 时为 true\n- name，名称构成为 `[模块名].[ScalarType名]Tensor`。  \n  模块名：\n  ```\n  CPU -> torch\n  CUDA -> torch.cuda\n  SparseCPU -> torch.sparse\n  SparseCUDA -> torch.cuda.sparse\n  ```\n  ScalarType名 就是 ScalarType 字面量的字符串形式，如 Byte -> \"Byte\", Float -> \"Float\" 等。例如组合 (CPU, Float) 对应的 PyTensorType 对象名为 \"torch.FloatTensor\"，(SparseCUDA, Double) 对应的 PyTensorType 对象名为 \"torch.cuda.sparse.DoubleTensor\"。\n\n注意到，\n```c++\nif (backend==Backend::CPU && scalar_type==at::kFloat) {\n    set_default_tensor_type(&tensor_type);\n}\n```\n可见默认 Tensor 类型为 torch.FloatTensor（其 Backend 为 CPU，注意对应 CUDA 的为 torch.cuda.FloatTensor）。\n\n总结：initialize_aten_types 根据 38 种组合构建 PyTensorType 类型对象，并保存到 tensor_types 这个 vector 中。\n\n#### py_initialize_metaclass(metaclass)\n初始化元类，这是一个 python 的扩展类型 PyTypeObject，对应的 python 类型名为 \"torch.tensortype\"，顾名思义表示 tensor 类型类，即，tensor 各种类型如 torch.FloatTensor 等的元类型，这个元类具有的属性为\n- dtype  \n    对应 initializeDtypes 中某个 THPDtype 对象\n- layout  \n    对应 initializeLayouts 中某个内存布局类型 THPLayout 对象\n- is_cuda  \n    是否使用 cuda\n- is_sparse  \n    是否是稀疏存储\n\n具有方法\n- `__instancecheck__` 检测某个 Tensor 是否与当前 tensor 类型类匹配，当 type_id 和 scalar_type 这两个字段均分别相同时，则匹配，否则不匹配。\n\nPyTensorType 是表示 python 的 Tensor 类型，我们指定 python 的类型本身也是一种对象，这种类型对象的类型为元类型，也就是这里的 metaclass。\n\n总结：初始化 PyTypeObject 类型对象 metaclass，它表示 tensor 类型的元类，且具有上述属性和方法。\n\n#### get_tensor_dict\n获取 torch.Tensor 以及其基类 _C._TensorBase 的初始属性（名称与值构成的字典）\n\n#### py_initialize_tensor_type\n对于前面构造的 38 个 PyTensorType 对象，设置每个对象的 py_type 字段。py_type 类型为 PyTypeObject，表示一个类型对象，也就是 python 中的某个类型，为这个类型对象设置元类 metaclass，名称，以及将上一小节中的属性字典并入这个类型，从而使得类型具有 torch.Tensor 的全部初始属性。\n```\ndir(torch.Tensor)\ndir(torch.FloatTensor)\n```\n以上两个指令输出内容一样。\n\n#### py_bind_tensor_types\n至此，以上 38 种 PyTensorType 对象均已准备好，将他们添加进相应的模块中，前文可能说 \"添加进 torch 模块中\"，因为当时没有讨论到 PyTensorType 对象名，所以笼统的那么说了一下，实际上应为 \"添加进相应的模块中\"，比如 \"torch.FloatTensor\"，则将相应的 PyTensorType 对象以 FloatTensor 作为 python 端的名称添加进 torch 模块中，\"torch.cuda.sparse.DoubleTensor\" 则将相应的 PyTensorType 对象以 DoubleTensor 作为 python 端的名称添加进 torch.cuda.sparse 模块中，即最后一个 `.` 后面的部分表示类型，而之前的部分表示模块。\n\n但是，现在还存在一个问题，那就是这些 torch.FloatTensor, torch.IntTensor 等类型与 torch.Tensor 是什么关系？\n```python\na=torch.empty(1,2,dtype=torch.int)\nisinstance(a, torch.IntTensor)  # True\nisinstance(a, torch.Tensor)     # True\nissubclass(torch.IntTensor, torch.Tensor)   # False\nissubclass(torch.Tensor, torch.IntTensor)   # False\n```\n根据 [Pytorch-3](2019/06/18/Pytorch-3) 最后的分析，我们知道 torch.empty 函数最后使用 THPVariable_Wrap 将 c++ Variable 类型包装成 python 的 torch.Tensor 类型，甚至直接调用 torch.IntTensor 构造的对象最后也是经过 THPVariable_Wrap 包装成 torch.Tensor 类型，\n```\n>>> type(torch.IntTensor([1,2]))\n<class 'torch.Tensor'>\n```\n既然返回的都是 torch.Tensor 类型，那怎么跟 torch.IntTensor 联系起来的呢？其实，torch.IntTensor 等 38 个 Tensor 类型与 torch.Tensor 没有直接关系\n```\n>>> torch.IntTensor.__bases__\n(<class 'object'>)\n>>> torch.Tensor.__bases__\n(<class 'torch._C._TensorBases'>)\n```\n上面所说的各种构造 Tensor 的方法返回的类型也确实是 torch.Tensor，但是 `isinstance(a, torch.IntTensor)` 结果为 True 也没错，因为 `isinstance` 实际上内部调用 `__instancecheck__` 进行判断，前面讨论 metaclass 时讲到这个方法的 c++ 底层实现函数为 Tensor_instancecheck，其定义如下\n```c++\nstatic PyObject *Tensor_instancecheck(PyTensorType *self, PyObject * arg) {\n    try{\n        if(THPVariable_Check(arg)) {    // 检测参数是否是 THPVariable 类型\n            auto& var = ((THPVariable*)arg)->cdata; // 获取内部的 Variable 类型对象\n            if (var.type_id() == self->get_type_id() &&\n                var.scalar_type() == static_cast<ScalarType>(self->scalar_type)) {\n                Py_RETURN_TRUE;     // 如果 type_id 和 ScalarType 均分别相同，则返回 True\n            }\n        }\n        Py_RETURN_FALSE;\n    } catch(python_error & e){\n        return nullptr;\n    }\n}\n```\n所以，不难理解 `isinstance(a, torch.IntTensor)=True`。\n\n__总结：在 [PyTorch-2]() 中，我们讨论了 PyTorch 中的函数返回或直接构造的 Tensor 均为 torch.Tensor，其继承自 `torch._C.Tensor`，所以要理解 Tensor 类的各种方法，需要从 `torch._C.Tensor` 的类型构造开始着手。__\n\n接下来是一系列的 THPxxxStorage_postInit 函数执行，这在 [PyTorch-2](2019/06/13/PyTorch-2) 中已经进行了介绍（THPxxxStorage_init），这里仅仅给出结论：\n1. 函数声明\n    ```c++\n    #define THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) //torch/csrc/Storage.h\n    bool THPStorage_(postInit)(PyObject *module);   // torch/csrc/generic/Storage.h\n    ```\n    THPStorage_(NAME) 这个宏展开后就得到 THPxxxStorage_init， 其中 Real 在宏展开时被替换为具体的 ScalarType，NAME 被替换为 init。于是，最终得到的函数声明为 THPxxxStorage_init(PyObject *module);\n\n2. torch/csrc/generic/Storage.cpp 中\n   ```c++\n   PyObject *THPStorageClass = nullptr;\n   bool THPStorage_(postInit)(PyObject *module){\n       // 从 torch 模块中获取名为 RealStorage 的属性，其中 Real 可为 Float, Bool, Double 等 ScalarType\n       THPStorageClass = PyObject_GetAttrString(module, (char*)TH_CONCAT_STRING_2(Real, Storage));\n       at::Backend backend = at::Backend::CPU;\n       #ifdef THC_GENERIC_FILE\n       backend = at::Backend::CUDA;\n       #endif\n       #ifdef THQUANTIZED\n       backend = at::Backend::QuantizedCPU;\n       #endif\n       torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));\n   }\n   ```\n   注意到在 `torch/__init__.py` 中定义了 FloatStorage 等类型，\n   ```python\n   class FloatStorage(_C.FloatStorageBase, _StorageBase):\n       pass\n   ```\n   torch._C.FloatStorageBase 等类型是在 THPxxxStorage_init 函数中被添加到模块 torch._C 中。THPxxxStorage_postInit 函数先是从 torch 模块中获取 RealStorage 类型对象，然后进行注册即，将 RealStorage 类型对象与对应的组合 (Backend, ScalarType) 进行映射，这样以后就可以根据 (Backend, ScalarType) 获取对应的 RealStorage 类型对象，反过来亦可。\n\n","slug":"pytorch/PyTorch-4","published":1,"updated":"2020-04-24T10:34:45.043Z","_id":"ck9dzcj1o002agga6ffs82n5e","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h2><p>torch 模块中包含了各种 Tensor：FloatTensor，DoubleTensor，HalfTensor，ByteTensor 等。这些 Tensor 是怎么来的呢？首先，入口是 <code>torch/__init__.py</code> 中的 <code>_C._initExtension(manager_path())</code>，其中 manager_path 用于获取 torch_shm_manager 的文件路径，shm 实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，这里跳过。_initExtension 在 torch/csrc/Module.cpp 中初始化 _C 模块时注册到 _C 模块中，其底层 c++ 实现函数为 THPModule_initExtension，这个函数定义中初始化了很多东西，我们依次来看看。</p>\n<a id=\"more\"></a>\n<h3 id=\"initializeLayouts\"><a href=\"#initializeLayouts\" class=\"headerlink\" title=\"initializeLayouts\"></a>initializeLayouts</h3><p>初始化内存布局。当前有三种布局（位于文件 c10/core/Layout.h 中）：</p>\n<ol>\n<li>Strided，使用密集多维数组的内存布局</li>\n<li>Sparse，使用稀疏多维数组的内存布局</li>\n<li>Mkldnn，使用 Intel 的 Mkldnn 库加速 CPU 时，由于 Mkldnn 使用了内部特殊内存布局，所以增加对应的内存布局枚举</li>\n</ol>\n<p>以最常用的 Strided 布局为例，使用 THPLayout_New 生成 THPLayoutType/THPLayout 的类型对象，指定 layout 为 Strided，name 为 “torch.strided”，然后 <strong>将这个类型添加到 torch 模块中</strong>，其他两种内存布局方式也类似处理。最后注册这些布局类型：</p>\n<ul>\n<li>CPU, CUDA, MSNPU, XLA, QuantizedCPU -&gt; strided_layout</li>\n<li>SparseCPU, SparseCUDA -&gt; sparse_coo_layout</li>\n<li>MkldnnCPU -&gt; mkldnn_layout</li>\n</ul>\n<p>即，将 Backend 与 Layout 关联起来，以便将来根据 Backend 获取对应的 Layout。</p>\n<h3 id=\"initializeMemoryFormats\"><a href=\"#initializeMemoryFormats\" class=\"headerlink\" title=\"initializeMemoryFormats\"></a>initializeMemoryFormats</h3><p>初始化内存格式。内存格式表明 Tensor 中的数据是如何组织的。当前有三种：Preserve, Contiguous 和 ChannelsLast。例如 ChannelsLast 表示内存中数据的格式为 NHWC，假设正常顺序 NCHW 的各维度值为 sizes，那么 ChannelsLast 下的各维度步幅 strides 应为：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">strides[<span class=\"number\">1</span>]=<span class=\"number\">1</span>;           <span class=\"comment\">// ChannelsLast 中 C 为最低一级维度，故步幅为 1</span></span><br><span class=\"line\">strides[<span class=\"number\">3</span>]=sizes[<span class=\"number\">1</span>];    <span class=\"comment\">// ChannelsLast 中 W 为次低一级维度，故步幅为 C 维度即 sizes[1]</span></span><br><span class=\"line\">strides[<span class=\"number\">2</span>]=strides[<span class=\"number\">3</span>]*sizes[<span class=\"number\">3</span>]; <span class=\"comment\">// ChannelsLast 中 H 为再次低一级维度，步幅为 W*C</span></span><br><span class=\"line\">strides[<span class=\"number\">0</span>]=strides[<span class=\"number\">2</span>]*sizes[<span class=\"number\">2</span>]; <span class=\"comment\">// ChannelsLast 中 N 为最高一级维度，步幅为 H*W*C</span></span><br></pre></td></tr></table></figure>\n<p>注意，上面 strides 和 sizes 的顺序均为 NCHW。<br>创建三种内存格式类型对象 preserve_format, contiguous_format, channels_last 并 <strong>添加到 torch 模块中</strong>。</p>\n<h3 id=\"initializeQScheme\"><a href=\"#initializeQScheme\" class=\"headerlink\" title=\"initializeQScheme\"></a>initializeQScheme</h3><p>初始化量化机制，量化是将连续型的输入限制为离散型，比如将浮点计算转为整型计算，显然使用小型整型比浮点型计算更高效，且内存占用更小，这在模型 inference 阶段尤其重要。关于量化的具体概念以及相关操作可参考 <a href=\"https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor\" target=\"_blank\" rel=\"noopener\">Introducing-Quantized-Tensor</a>。当前量化机制有 5 种，类似以上布局和内存格式，分别创建对应的类型对象，然后 <strong>添加到 torch 模块中</strong>。</p>\n<h3 id=\"initializeDtypes\"><a href=\"#initializeDtypes\" class=\"headerlink\" title=\"initializeDtypes\"></a>initializeDtypes</h3><p>初始化数据类型。直接看此函数的部分定义</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,</span></span><br><span class=\"line\">at:ScalarType all_scalar_type[] = &#123;</span><br><span class=\"line\">    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)&#125;;</span><br></pre></td></tr></table></figure>\n<p>其中 AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS 这个宏罗列了所有的标量类型，包括 complex 类型和 quantization 类型，展开后为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::ScalarType::Byte,</span><br><span class=\"line\">at::ScalarType::Char,</span><br><span class=\"line\">at::ScalarType::Short,</span><br><span class=\"line\">at::ScalarType::Int,</span><br><span class=\"line\">at::ScalarType::Long,</span><br><span class=\"line\">at::ScalarType::Half,</span><br><span class=\"line\">at::ScalarType::Float,</span><br><span class=\"line\">at::ScalarType::Double,</span><br><span class=\"line\">at::ScalarType::ComplexHalf,</span><br><span class=\"line\">at::ScalarType::ComplexFloat,</span><br><span class=\"line\">at::ScalarType::ComplexDouble,</span><br><span class=\"line\">at::ScalarType::Bool,</span><br><span class=\"line\">at::ScalarType::QInt8,</span><br><span class=\"line\">at::ScalarType::QUInt8,</span><br><span class=\"line\">at::ScalarType::QInt32</span><br><span class=\"line\">at::ScalarType::BFloat16</span><br></pre></td></tr></table></figure>\n<p>然后根据以上标量类型获取其主名称和传统旧名称，如没有传统旧名称，则默认为空字符串 <code>&quot;&quot;</code>，然后创建各个对应的类型对象，并注册这些类型对象（即，将类型对象与 at::ScalarType 值关联起来，存储到字典，以便将来能根据 at::ScalarType 获取对应的类型对象），相关代码如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">std</span>::tie(primary_name, legacy_name) = getDtypeName(scalarType);</span><br><span class=\"line\">PyObject *dtype = THPDtype_New(scalarType, primary_name);</span><br><span class=\"line\">torch::registerDtypeObject((THPDtype*)dtype, scalarType);</span><br></pre></td></tr></table></figure>\n<p>然后将类型对象（THPDtypeType/THPDtype 实例）<strong>添加到 torch 模块中</strong>，如果存在传统旧名称，则也同样添加到 torch 模块中。</p>\n<h3 id=\"initialize-python-bindings\"><a href=\"#initialize-python-bindings\" class=\"headerlink\" title=\"initialize_python_bindings\"></a>initialize_python_bindings</h3><p>初始化 python 绑定</p>\n<h4 id=\"initialize-aten-types\"><a href=\"#initialize-aten-types\" class=\"headerlink\" title=\"initialize_aten_types\"></a>initialize_aten_types</h4><p>根据 all_declared_types 函数获取所有声明过的类型，Backend 有 <code>CPU, CUDA, SparseCPU, SparseCUDA</code> 四种，ScalarType 除去 Complex 和 Quantization 类型，则一共有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Byte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16</span><br></pre></td></tr></table></figure>\n<p>然后所有 Backend 与 ScalarType 不同的组合构成这里所需要的声明类型，不过 (SparseCUDA|SparseCPU,Bool) 除外，这样的组合一共有 4*10-2=38 种，根据这 38 种组合构建对应的 PyTensorType 类型，看看这个 PyTensorType 类型定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">PyTensorType</span> &#123;</span></span><br><span class=\"line\">    PyTypeObject py_type;   <span class=\"comment\">// python 类型对应的扩展类型，这个字段后文会再次讲到</span></span><br><span class=\"line\">    THPDtype* dtype;        <span class=\"comment\">// 对应上文 initializeDtypes 中注册的某个数据标量类型</span></span><br><span class=\"line\">    THPLayout* layout;      <span class=\"comment\">// 对应上文 initializeLayouts 中注册的某个内存布局类型</span></span><br><span class=\"line\">    <span class=\"keyword\">bool</span> is_cuda;           <span class=\"comment\">// 指示是 cuda 还是 cpu</span></span><br><span class=\"line\">    <span class=\"keyword\">char</span> name[<span class=\"number\">64</span>];          <span class=\"comment\">// tensor 类型名称</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> backend;            <span class=\"comment\">// CPU, CUDA, SparseCPU, SparseCUDA 四种之一</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> scalar_type;        <span class=\"comment\">// Byte 等十种之一</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>上述 38 种组合，每种组合构建一个 PyTensorType 对象。Python 接口中某种 Tensor，比如 FloatTensor 其底层就对应这里的某个 PyTensorType 对象。</p>\n<ul>\n<li>layout，根据 backend 字段获取，initializeLayouts 中注册了所有 Backend 与 Layout 的映射关系</li>\n<li>is_cuda，当 backend = CUDA|SparseCUD 时为 true</li>\n<li>name，名称构成为 <code>[模块名].[ScalarType名]Tensor</code>。<br>模块名：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPU -&gt; torch</span><br><span class=\"line\">CUDA -&gt; torch.cuda</span><br><span class=\"line\">SparseCPU -&gt; torch.sparse</span><br><span class=\"line\">SparseCUDA -&gt; torch.cuda.sparse</span><br></pre></td></tr></table></figure>\nScalarType名 就是 ScalarType 字面量的字符串形式，如 Byte -&gt; “Byte”, Float -&gt; “Float” 等。例如组合 (CPU, Float) 对应的 PyTensorType 对象名为 “torch.FloatTensor”，(SparseCUDA, Double) 对应的 PyTensorType 对象名为 “torch.cuda.sparse.DoubleTensor”。</li>\n</ul>\n<p>注意到，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (backend==Backend::CPU &amp;&amp; scalar_type==at::kFloat) &#123;</span><br><span class=\"line\">    set_default_tensor_type(&amp;tensor_type);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见默认 Tensor 类型为 torch.FloatTensor（其 Backend 为 CPU，注意对应 CUDA 的为 torch.cuda.FloatTensor）。</p>\n<p>总结：initialize_aten_types 根据 38 种组合构建 PyTensorType 类型对象，并保存到 tensor_types 这个 vector 中。</p>\n<h4 id=\"py-initialize-metaclass-metaclass\"><a href=\"#py-initialize-metaclass-metaclass\" class=\"headerlink\" title=\"py_initialize_metaclass(metaclass)\"></a>py_initialize_metaclass(metaclass)</h4><p>初始化元类，这是一个 python 的扩展类型 PyTypeObject，对应的 python 类型名为 “torch.tensortype”，顾名思义表示 tensor 类型类，即，tensor 各种类型如 torch.FloatTensor 等的元类型，这个元类具有的属性为</p>\n<ul>\n<li>dtype<br>  对应 initializeDtypes 中某个 THPDtype 对象</li>\n<li>layout<br>  对应 initializeLayouts 中某个内存布局类型 THPLayout 对象</li>\n<li>is_cuda<br>  是否使用 cuda</li>\n<li>is_sparse<br>  是否是稀疏存储</li>\n</ul>\n<p>具有方法</p>\n<ul>\n<li><code>__instancecheck__</code> 检测某个 Tensor 是否与当前 tensor 类型类匹配，当 type_id 和 scalar_type 这两个字段均分别相同时，则匹配，否则不匹配。</li>\n</ul>\n<p>PyTensorType 是表示 python 的 Tensor 类型，我们指定 python 的类型本身也是一种对象，这种类型对象的类型为元类型，也就是这里的 metaclass。</p>\n<p>总结：初始化 PyTypeObject 类型对象 metaclass，它表示 tensor 类型的元类，且具有上述属性和方法。</p>\n<h4 id=\"get-tensor-dict\"><a href=\"#get-tensor-dict\" class=\"headerlink\" title=\"get_tensor_dict\"></a>get_tensor_dict</h4><p>获取 torch.Tensor 以及其基类 _C._TensorBase 的初始属性（名称与值构成的字典）</p>\n<h4 id=\"py-initialize-tensor-type\"><a href=\"#py-initialize-tensor-type\" class=\"headerlink\" title=\"py_initialize_tensor_type\"></a>py_initialize_tensor_type</h4><p>对于前面构造的 38 个 PyTensorType 对象，设置每个对象的 py_type 字段。py_type 类型为 PyTypeObject，表示一个类型对象，也就是 python 中的某个类型，为这个类型对象设置元类 metaclass，名称，以及将上一小节中的属性字典并入这个类型，从而使得类型具有 torch.Tensor 的全部初始属性。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dir(torch.Tensor)</span><br><span class=\"line\">dir(torch.FloatTensor)</span><br></pre></td></tr></table></figure>\n<p>以上两个指令输出内容一样。</p>\n<h4 id=\"py-bind-tensor-types\"><a href=\"#py-bind-tensor-types\" class=\"headerlink\" title=\"py_bind_tensor_types\"></a>py_bind_tensor_types</h4><p>至此，以上 38 种 PyTensorType 对象均已准备好，将他们添加进相应的模块中，前文可能说 “添加进 torch 模块中”，因为当时没有讨论到 PyTensorType 对象名，所以笼统的那么说了一下，实际上应为 “添加进相应的模块中”，比如 “torch.FloatTensor”，则将相应的 PyTensorType 对象以 FloatTensor 作为 python 端的名称添加进 torch 模块中，”torch.cuda.sparse.DoubleTensor” 则将相应的 PyTensorType 对象以 DoubleTensor 作为 python 端的名称添加进 torch.cuda.sparse 模块中，即最后一个 <code>.</code> 后面的部分表示类型，而之前的部分表示模块。</p>\n<p>但是，现在还存在一个问题，那就是这些 torch.FloatTensor, torch.IntTensor 等类型与 torch.Tensor 是什么关系？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,dtype=torch.int)</span><br><span class=\"line\">isinstance(a, torch.IntTensor)  <span class=\"comment\"># True</span></span><br><span class=\"line\">isinstance(a, torch.Tensor)     <span class=\"comment\"># True</span></span><br><span class=\"line\">issubclass(torch.IntTensor, torch.Tensor)   <span class=\"comment\"># False</span></span><br><span class=\"line\">issubclass(torch.Tensor, torch.IntTensor)   <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure>\n<p>根据 <a href=\"2019/06/18/Pytorch-3\">Pytorch-3</a> 最后的分析，我们知道 torch.empty 函数最后使用 THPVariable_Wrap 将 c++ Variable 类型包装成 python 的 torch.Tensor 类型，甚至直接调用 torch.IntTensor 构造的对象最后也是经过 THPVariable_Wrap 包装成 torch.Tensor 类型，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; type(torch.IntTensor([1,2]))</span><br><span class=\"line\">&lt;class &#39;torch.Tensor&#39;&gt;</span><br></pre></td></tr></table></figure>\n<p>既然返回的都是 torch.Tensor 类型，那怎么跟 torch.IntTensor 联系起来的呢？其实，torch.IntTensor 等 38 个 Tensor 类型与 torch.Tensor 没有直接关系</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; torch.IntTensor.__bases__</span><br><span class=\"line\">(&lt;class &#39;object&#39;&gt;)</span><br><span class=\"line\">&gt;&gt;&gt; torch.Tensor.__bases__</span><br><span class=\"line\">(&lt;class &#39;torch._C._TensorBases&#39;&gt;)</span><br></pre></td></tr></table></figure>\n<p>上面所说的各种构造 Tensor 的方法返回的类型也确实是 torch.Tensor，但是 <code>isinstance(a, torch.IntTensor)</code> 结果为 True 也没错，因为 <code>isinstance</code> 实际上内部调用 <code>__instancecheck__</code> 进行判断，前面讨论 metaclass 时讲到这个方法的 c++ 底层实现函数为 Tensor_instancecheck，其定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject *<span class=\"title\">Tensor_instancecheck</span><span class=\"params\">(PyTensorType *self, PyObject * arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(THPVariable_Check(arg)) &#123;    <span class=\"comment\">// 检测参数是否是 THPVariable 类型</span></span><br><span class=\"line\">            <span class=\"keyword\">auto</span>&amp; var = ((THPVariable*)arg)-&gt;cdata; <span class=\"comment\">// 获取内部的 Variable 类型对象</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (var.type_id() == self-&gt;get_type_id() &amp;&amp;</span><br><span class=\"line\">                var.scalar_type() == <span class=\"keyword\">static_cast</span>&lt;ScalarType&gt;(self-&gt;scalar_type)) &#123;</span><br><span class=\"line\">                Py_RETURN_TRUE;     <span class=\"comment\">// 如果 type_id 和 ScalarType 均分别相同，则返回 True</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Py_RETURN_FALSE;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span>(python_error &amp; e)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所以，不难理解 <code>isinstance(a, torch.IntTensor)=True</code>。</p>\n<p><strong>总结：在 <a href=\"\">PyTorch-2</a> 中，我们讨论了 PyTorch 中的函数返回或直接构造的 Tensor 均为 torch.Tensor，其继承自 <code>torch._C.Tensor</code>，所以要理解 Tensor 类的各种方法，需要从 <code>torch._C.Tensor</code> 的类型构造开始着手。</strong></p>\n<p>接下来是一系列的 THPxxxStorage_postInit 函数执行，这在 <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> 中已经进行了介绍（THPxxxStorage_init），这里仅仅给出结论：</p>\n<ol>\n<li><p>函数声明</p>\n <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) <span class=\"comment\">//torch/csrc/Storage.h</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>;   <span class=\"comment\">// torch/csrc/generic/Storage.h</span></span><br></pre></td></tr></table></figure>\n<p> THPStorage_(NAME) 这个宏展开后就得到 THPxxxStorage_init， 其中 Real 在宏展开时被替换为具体的 ScalarType，NAME 被替换为 init。于是，最终得到的函数声明为 THPxxxStorage_init(PyObject *module);</p>\n</li>\n<li><p>torch/csrc/generic/Storage.cpp 中</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyObject *THPStorageClass = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 从 torch 模块中获取名为 RealStorage 的属性，其中 Real 可为 Float, Bool, Double 等 ScalarType</span></span><br><span class=\"line\">    THPStorageClass = PyObject_GetAttrString(<span class=\"keyword\">module</span>, (<span class=\"keyword\">char</span>*)TH_CONCAT_STRING_2(Real, Storage));</span><br><span class=\"line\">    at::Backend backend = at::Backend::CPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THC_GENERIC_FILE</span></span><br><span class=\"line\">    backend = at::Backend::CUDA;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THQUANTIZED</span></span><br><span class=\"line\">    backend = at::Backend::QuantizedCPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意到在 <code>torch/__init__.py</code> 中定义了 FloatStorage 等类型，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FloatStorage</span><span class=\"params\">(_C.FloatStorageBase, _StorageBase)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>torch._C.FloatStorageBase 等类型是在 THPxxxStorage_init 函数中被添加到模块 torch._C 中。THPxxxStorage_postInit 函数先是从 torch 模块中获取 RealStorage 类型对象，然后进行注册即，将 RealStorage 类型对象与对应的组合 (Backend, ScalarType) 进行映射，这样以后就可以根据 (Backend, ScalarType) 获取对应的 RealStorage 类型对象，反过来亦可。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h2><p>torch 模块中包含了各种 Tensor：FloatTensor，DoubleTensor，HalfTensor，ByteTensor 等。这些 Tensor 是怎么来的呢？首先，入口是 <code>torch/__init__.py</code> 中的 <code>_C._initExtension(manager_path())</code>，其中 manager_path 用于获取 torch_shm_manager 的文件路径，shm 实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，这里跳过。_initExtension 在 torch/csrc/Module.cpp 中初始化 _C 模块时注册到 _C 模块中，其底层 c++ 实现函数为 THPModule_initExtension，这个函数定义中初始化了很多东西，我们依次来看看。</p>","more":"<h3 id=\"initializeLayouts\"><a href=\"#initializeLayouts\" class=\"headerlink\" title=\"initializeLayouts\"></a>initializeLayouts</h3><p>初始化内存布局。当前有三种布局（位于文件 c10/core/Layout.h 中）：</p>\n<ol>\n<li>Strided，使用密集多维数组的内存布局</li>\n<li>Sparse，使用稀疏多维数组的内存布局</li>\n<li>Mkldnn，使用 Intel 的 Mkldnn 库加速 CPU 时，由于 Mkldnn 使用了内部特殊内存布局，所以增加对应的内存布局枚举</li>\n</ol>\n<p>以最常用的 Strided 布局为例，使用 THPLayout_New 生成 THPLayoutType/THPLayout 的类型对象，指定 layout 为 Strided，name 为 “torch.strided”，然后 <strong>将这个类型添加到 torch 模块中</strong>，其他两种内存布局方式也类似处理。最后注册这些布局类型：</p>\n<ul>\n<li>CPU, CUDA, MSNPU, XLA, QuantizedCPU -&gt; strided_layout</li>\n<li>SparseCPU, SparseCUDA -&gt; sparse_coo_layout</li>\n<li>MkldnnCPU -&gt; mkldnn_layout</li>\n</ul>\n<p>即，将 Backend 与 Layout 关联起来，以便将来根据 Backend 获取对应的 Layout。</p>\n<h3 id=\"initializeMemoryFormats\"><a href=\"#initializeMemoryFormats\" class=\"headerlink\" title=\"initializeMemoryFormats\"></a>initializeMemoryFormats</h3><p>初始化内存格式。内存格式表明 Tensor 中的数据是如何组织的。当前有三种：Preserve, Contiguous 和 ChannelsLast。例如 ChannelsLast 表示内存中数据的格式为 NHWC，假设正常顺序 NCHW 的各维度值为 sizes，那么 ChannelsLast 下的各维度步幅 strides 应为：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">strides[<span class=\"number\">1</span>]=<span class=\"number\">1</span>;           <span class=\"comment\">// ChannelsLast 中 C 为最低一级维度，故步幅为 1</span></span><br><span class=\"line\">strides[<span class=\"number\">3</span>]=sizes[<span class=\"number\">1</span>];    <span class=\"comment\">// ChannelsLast 中 W 为次低一级维度，故步幅为 C 维度即 sizes[1]</span></span><br><span class=\"line\">strides[<span class=\"number\">2</span>]=strides[<span class=\"number\">3</span>]*sizes[<span class=\"number\">3</span>]; <span class=\"comment\">// ChannelsLast 中 H 为再次低一级维度，步幅为 W*C</span></span><br><span class=\"line\">strides[<span class=\"number\">0</span>]=strides[<span class=\"number\">2</span>]*sizes[<span class=\"number\">2</span>]; <span class=\"comment\">// ChannelsLast 中 N 为最高一级维度，步幅为 H*W*C</span></span><br></pre></td></tr></table></figure>\n<p>注意，上面 strides 和 sizes 的顺序均为 NCHW。<br>创建三种内存格式类型对象 preserve_format, contiguous_format, channels_last 并 <strong>添加到 torch 模块中</strong>。</p>\n<h3 id=\"initializeQScheme\"><a href=\"#initializeQScheme\" class=\"headerlink\" title=\"initializeQScheme\"></a>initializeQScheme</h3><p>初始化量化机制，量化是将连续型的输入限制为离散型，比如将浮点计算转为整型计算，显然使用小型整型比浮点型计算更高效，且内存占用更小，这在模型 inference 阶段尤其重要。关于量化的具体概念以及相关操作可参考 <a href=\"https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor\" target=\"_blank\" rel=\"noopener\">Introducing-Quantized-Tensor</a>。当前量化机制有 5 种，类似以上布局和内存格式，分别创建对应的类型对象，然后 <strong>添加到 torch 模块中</strong>。</p>\n<h3 id=\"initializeDtypes\"><a href=\"#initializeDtypes\" class=\"headerlink\" title=\"initializeDtypes\"></a>initializeDtypes</h3><p>初始化数据类型。直接看此函数的部分定义</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,</span></span><br><span class=\"line\">at:ScalarType all_scalar_type[] = &#123;</span><br><span class=\"line\">    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)&#125;;</span><br></pre></td></tr></table></figure>\n<p>其中 AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS 这个宏罗列了所有的标量类型，包括 complex 类型和 quantization 类型，展开后为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::ScalarType::Byte,</span><br><span class=\"line\">at::ScalarType::Char,</span><br><span class=\"line\">at::ScalarType::Short,</span><br><span class=\"line\">at::ScalarType::Int,</span><br><span class=\"line\">at::ScalarType::Long,</span><br><span class=\"line\">at::ScalarType::Half,</span><br><span class=\"line\">at::ScalarType::Float,</span><br><span class=\"line\">at::ScalarType::Double,</span><br><span class=\"line\">at::ScalarType::ComplexHalf,</span><br><span class=\"line\">at::ScalarType::ComplexFloat,</span><br><span class=\"line\">at::ScalarType::ComplexDouble,</span><br><span class=\"line\">at::ScalarType::Bool,</span><br><span class=\"line\">at::ScalarType::QInt8,</span><br><span class=\"line\">at::ScalarType::QUInt8,</span><br><span class=\"line\">at::ScalarType::QInt32</span><br><span class=\"line\">at::ScalarType::BFloat16</span><br></pre></td></tr></table></figure>\n<p>然后根据以上标量类型获取其主名称和传统旧名称，如没有传统旧名称，则默认为空字符串 <code>&quot;&quot;</code>，然后创建各个对应的类型对象，并注册这些类型对象（即，将类型对象与 at::ScalarType 值关联起来，存储到字典，以便将来能根据 at::ScalarType 获取对应的类型对象），相关代码如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">std</span>::tie(primary_name, legacy_name) = getDtypeName(scalarType);</span><br><span class=\"line\">PyObject *dtype = THPDtype_New(scalarType, primary_name);</span><br><span class=\"line\">torch::registerDtypeObject((THPDtype*)dtype, scalarType);</span><br></pre></td></tr></table></figure>\n<p>然后将类型对象（THPDtypeType/THPDtype 实例）<strong>添加到 torch 模块中</strong>，如果存在传统旧名称，则也同样添加到 torch 模块中。</p>\n<h3 id=\"initialize-python-bindings\"><a href=\"#initialize-python-bindings\" class=\"headerlink\" title=\"initialize_python_bindings\"></a>initialize_python_bindings</h3><p>初始化 python 绑定</p>\n<h4 id=\"initialize-aten-types\"><a href=\"#initialize-aten-types\" class=\"headerlink\" title=\"initialize_aten_types\"></a>initialize_aten_types</h4><p>根据 all_declared_types 函数获取所有声明过的类型，Backend 有 <code>CPU, CUDA, SparseCPU, SparseCUDA</code> 四种，ScalarType 除去 Complex 和 Quantization 类型，则一共有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Byte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16</span><br></pre></td></tr></table></figure>\n<p>然后所有 Backend 与 ScalarType 不同的组合构成这里所需要的声明类型，不过 (SparseCUDA|SparseCPU,Bool) 除外，这样的组合一共有 4*10-2=38 种，根据这 38 种组合构建对应的 PyTensorType 类型，看看这个 PyTensorType 类型定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">PyTensorType</span> &#123;</span></span><br><span class=\"line\">    PyTypeObject py_type;   <span class=\"comment\">// python 类型对应的扩展类型，这个字段后文会再次讲到</span></span><br><span class=\"line\">    THPDtype* dtype;        <span class=\"comment\">// 对应上文 initializeDtypes 中注册的某个数据标量类型</span></span><br><span class=\"line\">    THPLayout* layout;      <span class=\"comment\">// 对应上文 initializeLayouts 中注册的某个内存布局类型</span></span><br><span class=\"line\">    <span class=\"keyword\">bool</span> is_cuda;           <span class=\"comment\">// 指示是 cuda 还是 cpu</span></span><br><span class=\"line\">    <span class=\"keyword\">char</span> name[<span class=\"number\">64</span>];          <span class=\"comment\">// tensor 类型名称</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> backend;            <span class=\"comment\">// CPU, CUDA, SparseCPU, SparseCUDA 四种之一</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> scalar_type;        <span class=\"comment\">// Byte 等十种之一</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>上述 38 种组合，每种组合构建一个 PyTensorType 对象。Python 接口中某种 Tensor，比如 FloatTensor 其底层就对应这里的某个 PyTensorType 对象。</p>\n<ul>\n<li>layout，根据 backend 字段获取，initializeLayouts 中注册了所有 Backend 与 Layout 的映射关系</li>\n<li>is_cuda，当 backend = CUDA|SparseCUD 时为 true</li>\n<li>name，名称构成为 <code>[模块名].[ScalarType名]Tensor</code>。<br>模块名：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPU -&gt; torch</span><br><span class=\"line\">CUDA -&gt; torch.cuda</span><br><span class=\"line\">SparseCPU -&gt; torch.sparse</span><br><span class=\"line\">SparseCUDA -&gt; torch.cuda.sparse</span><br></pre></td></tr></table></figure>\nScalarType名 就是 ScalarType 字面量的字符串形式，如 Byte -&gt; “Byte”, Float -&gt; “Float” 等。例如组合 (CPU, Float) 对应的 PyTensorType 对象名为 “torch.FloatTensor”，(SparseCUDA, Double) 对应的 PyTensorType 对象名为 “torch.cuda.sparse.DoubleTensor”。</li>\n</ul>\n<p>注意到，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (backend==Backend::CPU &amp;&amp; scalar_type==at::kFloat) &#123;</span><br><span class=\"line\">    set_default_tensor_type(&amp;tensor_type);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见默认 Tensor 类型为 torch.FloatTensor（其 Backend 为 CPU，注意对应 CUDA 的为 torch.cuda.FloatTensor）。</p>\n<p>总结：initialize_aten_types 根据 38 种组合构建 PyTensorType 类型对象，并保存到 tensor_types 这个 vector 中。</p>\n<h4 id=\"py-initialize-metaclass-metaclass\"><a href=\"#py-initialize-metaclass-metaclass\" class=\"headerlink\" title=\"py_initialize_metaclass(metaclass)\"></a>py_initialize_metaclass(metaclass)</h4><p>初始化元类，这是一个 python 的扩展类型 PyTypeObject，对应的 python 类型名为 “torch.tensortype”，顾名思义表示 tensor 类型类，即，tensor 各种类型如 torch.FloatTensor 等的元类型，这个元类具有的属性为</p>\n<ul>\n<li>dtype<br>  对应 initializeDtypes 中某个 THPDtype 对象</li>\n<li>layout<br>  对应 initializeLayouts 中某个内存布局类型 THPLayout 对象</li>\n<li>is_cuda<br>  是否使用 cuda</li>\n<li>is_sparse<br>  是否是稀疏存储</li>\n</ul>\n<p>具有方法</p>\n<ul>\n<li><code>__instancecheck__</code> 检测某个 Tensor 是否与当前 tensor 类型类匹配，当 type_id 和 scalar_type 这两个字段均分别相同时，则匹配，否则不匹配。</li>\n</ul>\n<p>PyTensorType 是表示 python 的 Tensor 类型，我们指定 python 的类型本身也是一种对象，这种类型对象的类型为元类型，也就是这里的 metaclass。</p>\n<p>总结：初始化 PyTypeObject 类型对象 metaclass，它表示 tensor 类型的元类，且具有上述属性和方法。</p>\n<h4 id=\"get-tensor-dict\"><a href=\"#get-tensor-dict\" class=\"headerlink\" title=\"get_tensor_dict\"></a>get_tensor_dict</h4><p>获取 torch.Tensor 以及其基类 _C._TensorBase 的初始属性（名称与值构成的字典）</p>\n<h4 id=\"py-initialize-tensor-type\"><a href=\"#py-initialize-tensor-type\" class=\"headerlink\" title=\"py_initialize_tensor_type\"></a>py_initialize_tensor_type</h4><p>对于前面构造的 38 个 PyTensorType 对象，设置每个对象的 py_type 字段。py_type 类型为 PyTypeObject，表示一个类型对象，也就是 python 中的某个类型，为这个类型对象设置元类 metaclass，名称，以及将上一小节中的属性字典并入这个类型，从而使得类型具有 torch.Tensor 的全部初始属性。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dir(torch.Tensor)</span><br><span class=\"line\">dir(torch.FloatTensor)</span><br></pre></td></tr></table></figure>\n<p>以上两个指令输出内容一样。</p>\n<h4 id=\"py-bind-tensor-types\"><a href=\"#py-bind-tensor-types\" class=\"headerlink\" title=\"py_bind_tensor_types\"></a>py_bind_tensor_types</h4><p>至此，以上 38 种 PyTensorType 对象均已准备好，将他们添加进相应的模块中，前文可能说 “添加进 torch 模块中”，因为当时没有讨论到 PyTensorType 对象名，所以笼统的那么说了一下，实际上应为 “添加进相应的模块中”，比如 “torch.FloatTensor”，则将相应的 PyTensorType 对象以 FloatTensor 作为 python 端的名称添加进 torch 模块中，”torch.cuda.sparse.DoubleTensor” 则将相应的 PyTensorType 对象以 DoubleTensor 作为 python 端的名称添加进 torch.cuda.sparse 模块中，即最后一个 <code>.</code> 后面的部分表示类型，而之前的部分表示模块。</p>\n<p>但是，现在还存在一个问题，那就是这些 torch.FloatTensor, torch.IntTensor 等类型与 torch.Tensor 是什么关系？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,dtype=torch.int)</span><br><span class=\"line\">isinstance(a, torch.IntTensor)  <span class=\"comment\"># True</span></span><br><span class=\"line\">isinstance(a, torch.Tensor)     <span class=\"comment\"># True</span></span><br><span class=\"line\">issubclass(torch.IntTensor, torch.Tensor)   <span class=\"comment\"># False</span></span><br><span class=\"line\">issubclass(torch.Tensor, torch.IntTensor)   <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure>\n<p>根据 <a href=\"2019/06/18/Pytorch-3\">Pytorch-3</a> 最后的分析，我们知道 torch.empty 函数最后使用 THPVariable_Wrap 将 c++ Variable 类型包装成 python 的 torch.Tensor 类型，甚至直接调用 torch.IntTensor 构造的对象最后也是经过 THPVariable_Wrap 包装成 torch.Tensor 类型，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; type(torch.IntTensor([1,2]))</span><br><span class=\"line\">&lt;class &#39;torch.Tensor&#39;&gt;</span><br></pre></td></tr></table></figure>\n<p>既然返回的都是 torch.Tensor 类型，那怎么跟 torch.IntTensor 联系起来的呢？其实，torch.IntTensor 等 38 个 Tensor 类型与 torch.Tensor 没有直接关系</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; torch.IntTensor.__bases__</span><br><span class=\"line\">(&lt;class &#39;object&#39;&gt;)</span><br><span class=\"line\">&gt;&gt;&gt; torch.Tensor.__bases__</span><br><span class=\"line\">(&lt;class &#39;torch._C._TensorBases&#39;&gt;)</span><br></pre></td></tr></table></figure>\n<p>上面所说的各种构造 Tensor 的方法返回的类型也确实是 torch.Tensor，但是 <code>isinstance(a, torch.IntTensor)</code> 结果为 True 也没错，因为 <code>isinstance</code> 实际上内部调用 <code>__instancecheck__</code> 进行判断，前面讨论 metaclass 时讲到这个方法的 c++ 底层实现函数为 Tensor_instancecheck，其定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject *<span class=\"title\">Tensor_instancecheck</span><span class=\"params\">(PyTensorType *self, PyObject * arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(THPVariable_Check(arg)) &#123;    <span class=\"comment\">// 检测参数是否是 THPVariable 类型</span></span><br><span class=\"line\">            <span class=\"keyword\">auto</span>&amp; var = ((THPVariable*)arg)-&gt;cdata; <span class=\"comment\">// 获取内部的 Variable 类型对象</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (var.type_id() == self-&gt;get_type_id() &amp;&amp;</span><br><span class=\"line\">                var.scalar_type() == <span class=\"keyword\">static_cast</span>&lt;ScalarType&gt;(self-&gt;scalar_type)) &#123;</span><br><span class=\"line\">                Py_RETURN_TRUE;     <span class=\"comment\">// 如果 type_id 和 ScalarType 均分别相同，则返回 True</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Py_RETURN_FALSE;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span>(python_error &amp; e)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所以，不难理解 <code>isinstance(a, torch.IntTensor)=True</code>。</p>\n<p><strong>总结：在 <a href=\"\">PyTorch-2</a> 中，我们讨论了 PyTorch 中的函数返回或直接构造的 Tensor 均为 torch.Tensor，其继承自 <code>torch._C.Tensor</code>，所以要理解 Tensor 类的各种方法，需要从 <code>torch._C.Tensor</code> 的类型构造开始着手。</strong></p>\n<p>接下来是一系列的 THPxxxStorage_postInit 函数执行，这在 <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> 中已经进行了介绍（THPxxxStorage_init），这里仅仅给出结论：</p>\n<ol>\n<li><p>函数声明</p>\n <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) <span class=\"comment\">//torch/csrc/Storage.h</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>;   <span class=\"comment\">// torch/csrc/generic/Storage.h</span></span><br></pre></td></tr></table></figure>\n<p> THPStorage_(NAME) 这个宏展开后就得到 THPxxxStorage_init， 其中 Real 在宏展开时被替换为具体的 ScalarType，NAME 被替换为 init。于是，最终得到的函数声明为 THPxxxStorage_init(PyObject *module);</p>\n</li>\n<li><p>torch/csrc/generic/Storage.cpp 中</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyObject *THPStorageClass = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 从 torch 模块中获取名为 RealStorage 的属性，其中 Real 可为 Float, Bool, Double 等 ScalarType</span></span><br><span class=\"line\">    THPStorageClass = PyObject_GetAttrString(<span class=\"keyword\">module</span>, (<span class=\"keyword\">char</span>*)TH_CONCAT_STRING_2(Real, Storage));</span><br><span class=\"line\">    at::Backend backend = at::Backend::CPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THC_GENERIC_FILE</span></span><br><span class=\"line\">    backend = at::Backend::CUDA;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THQUANTIZED</span></span><br><span class=\"line\">    backend = at::Backend::QuantizedCPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意到在 <code>torch/__init__.py</code> 中定义了 FloatStorage 等类型，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FloatStorage</span><span class=\"params\">(_C.FloatStorageBase, _StorageBase)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>torch._C.FloatStorageBase 等类型是在 THPxxxStorage_init 函数中被添加到模块 torch._C 中。THPxxxStorage_postInit 函数先是从 torch 模块中获取 RealStorage 类型对象，然后进行注册即，将 RealStorage 类型对象与对应的组合 (Backend, ScalarType) 进行映射，这样以后就可以根据 (Backend, ScalarType) 获取对应的 RealStorage 类型对象，反过来亦可。</p>\n</li>\n</ol>"},{"title":"PyTorch.optim","p":"pytorch/optim-1","date":"2020-01-06T06:38:40.000Z","mathjax":true,"_content":"\n# 1. Adagrad\n<!-- more -->\n## 1.1 原理\n所有的参数形成一个参数向量，对每个参数使用不同的学习率。例如在时间步 `t`，第 `i` 个参数 $\\theta_i$ 的梯度为 $g_{t,i}$，\n$$g_{t,i} = \\nabla_{\\theta}J(\\theta_{t,i})$$\nSGD 的更新方式为，\n$$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$\n其中学习率 $\\eta$ 恒定。\n\nAdagrad 对每个参数在不同时间步调整学习率，参数更新为\n$$\\theta_{t+1,i}=\\theta_{t,i}-\\frac {\\eta} {\\sqrt{G_{t,ii}+\\epsilon}} \\cdot g_{t,i} \\qquad(1)$$\n\n其中 $G_t \\in \\mathbb R^{d \\times d}$ 是一个对角矩阵，对角线上每个元素 $G_{t,ii}$ 是参数 $\\theta_i$ 从时间步 `0` 到时间步 `t` 的梯度的平方和，\n$$G_{t,ii}=\\sum_{\\tau=0}^t g_{\\tau,i}^2 \\qquad(2)$$\n\n$\\epsilon$ 为平滑因子，用于避免分母为 0，一般取值 `1e-8`。\n\n将 (1) 式向量化，\n$$\\theta_{t+1}=\\theta_t - \\frac \\eta {\\sqrt {G_t+\\epsilon}} \\odot g_t \\qquad(3)$$\n\n其中 $\\odot$ 表示矩阵与向量相乘。通常，$\\eta=0.01$。\n\nAdagrad 的优点是不需要手动调整学习率，缺点是随着迭代次数的增加，分母逐渐增大，导致最后变得非常小，学习过程非常缓慢甚至停止。\n\n关于 Adagrad 调整学习率的理论分析可参考论文 [1]。\n\n## 1.2 PyTorch 实现\nPyTorch 的 Adagrad 实现中除了学习率 `lr` 和平滑因子 `eps`，还是增加了几个参数：\n1. 学习率衰减因子 `lr_decay`\n2. 权重衰减因子 `weight_decay`\n3. 累加初始值 `G`，这是 (2) 式中累加的一个初始值\n\n参数更新步骤如下：\n\n设置累加初值\n$$G_0=[G,...,G]$$\n其中 $G_0$ 是一个向量（对角矩阵的对角线元素），与参数数量相同。\n\n在时间步 `t`，\n\n1. 增加权重衰减项（正则项）的梯度\n   \n   $$g_t := g_t + \\lambda_{\\theta} \\cdot \\theta_t$$\n\n2. 学习率衰减为 \n   \n   $$\\eta := \\frac {\\eta} {1+ t \\cdot \\lambda_{\\eta}}  $$\n\n3. 累加梯度平方\n   \n   $$G_{t+1} = G_t+ g_t \\cdot g_t$$\n\n4. 更新参数\n   \n   $$\\theta_{t+1} = \\theta_t - \\frac \\eta {\\sqrt{G_t} + \\epsilon}\\cdot g_t$$\n\n以上，向量的计算全部按元素进行（标量则在需要的时候广播为向量）。（不同的参数具有不同的调整后的学习率）\n\n# 2. Adadelta\n## 2.1 原理\nAdadelta 是在 Adagrad 的基础上对学习率一味单调递减进行修改，不再对之前所有时间步的梯度做平方和，而是限制一个最近时间步的窗口，窗口大小为 `w`。\n\n然而，由于存储 `w` 个梯度平方值效率较低，所以改为使用梯度的衰减均值，如下\n$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1- \\gamma)g_t^2$$\n它的平方根就变成了 RMS（均方根，区别是每个元素的权重由 `1/n` 变成依次递增的值），\n$$\\text{RMS}[g]_t = \\sqrt{E[g^2]_t + \\epsilon}$$\n这样，越早期时间步的梯度平方，其权重越低，贡献也小，越近期的梯度平方，贡献越大。$\\gamma$ 可取 `0.9`。\n\n于是，参数更新为，\n$$\\Delta \\theta_t = -\\frac \\eta {\\text{RMS}[g]_t}\\cdot g_t \\qquad(4)\n\\\\\\\\\\theta_{t+1}=\\theta_t + \\Delta \\theta_t$$\n\n更进一步地，更新量 $\\Delta \\theta$ 与 $\\theta$ 在单位空间上不匹配，这在 SGD，momentum 以及 Adagrad 中也存在同样的问题，即\n$$\\Delta x 单位 \\propto g 单位 \\propto \\frac {\\partial f} {\\partial x} \\propto  \\frac 1 {x 单位}$$\n上式最后一步中假定了目标函数 `f` 是无单位的。这个单位空间不匹配如何理解呢？假设 `x` 表示距离，例如 米 $m$，损失函数 `f` 无量纲，根据上式，发现 `x` 的更新量的单位为 $m^{-1}$，显然这是不匹配的。为了实现匹配的目的，首先类似 $g^2$ 的衰减均值，定义更新量的衰减均值，\n$$E[\\Delta \\theta^2]_t = \\gamma \\cdot E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta_t^2$$\n\n均方根为，\n$$\\text{RMS}[\\Delta\\theta]_t=\\sqrt {E[\\Delta \\theta^2]_t+\\epsilon}$$\n残念，由于 $\\Delta \\theta_t$ 未知，所以上式也未知，所以近似使用 $\\text{RMS}[\\Delta\\theta]_{t-1}$ 来代替，然后这个值就作为 (4) 式中的 $\\eta$。\n\n于是最终参数更新方式为，\n$$\\Delta \\theta_t = -\\frac {\\text{RMS}[\\Delta\\theta]_{t-1}} {\\text{RMS}[g]_t}\\cdot g_t \\qquad(5)\n\\\\\\\\\\theta_{t+1}=\\theta_t + \\Delta \\theta_t$$\n注意到 `RMS` 中有平方根计算，所以，$\\text{RMS}[\\Delta\\theta]_{t-1}$ 与 $\\theta$ 量纲匹配，而 $\\text{RMS}[g]_t$ 与 $g$ 量纲匹配，所以 (5) 式中 $\\Delta \\theta$ 与 $\\theta$ 量纲匹配。\n\n## 2.2 PyTorch 实现\nPyTorch 的 Adadelta 实现使用 (5) 式，非常简单，不再啰嗦。\n\n# 3. RMSprop\nRMSprop 就是 Adadelta 中 (4) 式，通常 $\\gamma=0.9$，$\\eta=0.001$。简单，我们直接看 PyTorch 实现部分。\n\n## 3.1 PyTorch 实现\nRMSprop 的 `step` 方法部分代码如下，\n```python\n# 对每个参数\nsquare_avg = state['square_avg']    # 参数对应的梯度平方的衰减均值（也称 moving average）\nalpha = group['alpha']              # 对应上文公式中的 gamma\n\nif group['weight_decay'] != 0:\n    grad = grad.add(group['weight_decay'], p.data)  # 添加正则项的梯度\n\nsquare_avg.mul_(alpha).addcmul_(1-alpha, grad, grad)    # 计算 E[g^2]\n\nif group['centered']:               # 使用 centered 版本的 RMSprop\n    grad_avg = state['grad_avg']    # 获取 梯度衰减平均\n    grad_avg.mul_(alpha).add_(1-alpha, grad)    # 更新 梯度衰减平均\n    # 先归一化，然后计算 RMS[g]\n    avg = square_avg.addcmul_(-1, grad_avg, grad_avg).sqrt_().add_(group['eps'])\nelse:\n    # 直接计算 RMS[g]\n    avg = square_avg.sqrt_().add_(group['eps'])\n\nif group['momentum'] > 0:       # 使用动量\n    buf = state['momentum_buffer']  # 获取动量缓存\n    buf.mul_(group'momentum').addcdiv_(grad, avg)   # 更新 velocity，与 (6) 式一致\n    p.data.add_(-group['lr'], buf)\nelse:\n    p.data.addcdiv_(-group['lr'], grad, avg)\n```\n上面代码中，如果不使用 `centered` 和 `momentum`，那么代码逻辑与 (4) 式完全一致，所以我们只看 `centered` 和 `momentum` 是如何进行的。\n### centered\n对 梯度 $g$ 归一化，然后计算 平方 的衰减均值，如下\n$$E\\{[g-E(g)]^2\\}=E(g^2)-[E(g)]^2$$\n其中 $E(\\cdot)$ 计算衰减均值。于是，\n$$RMS[g]=\\sqrt{E\\{[g-E(g)]^2\\}}+\\epsilon=\\sqrt{E(g^2)-[E(g)]^2}+\\epsilon$$\n\n### momentum\n我们回顾一下普通的 SGD 参数更新方式：\n$$\\theta_{t+1}=\\theta_t - \\eta \\cdot \\nabla f(\\theta_t)$$\n然后带有 momentum 的 SGD 参数更新方式：\n$$v_{t+1}=\\mu \\cdot v_t + \\nabla f(\\theta_t)\n\\\\\\\\\\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$\n\n根据 (4) 式，现在已知 RMSprop 的参数更新方式为，\n$$\\theta_{t+1}=\\theta_t -\\frac \\eta {\\text{RMS}[g]_t}\\cdot g_t$$\n类比 SGD，可知带有 momentum 的 RMSprop 参数更新方式为，\n$$v_{t+1}=\\mu \\cdot v_t + \\frac {g_t} {\\text{RMS}[g]_t} \\qquad(6)\n\\\\\\\\ \\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$\n\n# 4. Rprop\n## 4.1 原理\nRprop 表示 resilient propagation。\n\n在 SGD 中，参数更新方向为负梯度方向，更新步长为梯度乘以一个系数（学习率），但是让更新步长直接与梯度成正比不一定是好选择，例如（来源 [2]）\n![]()<center>图 1. 三个函数在相同的地方有最小值，但是 `f'(x)` 不同</center>\n上图中，三个函数的最小值均在相同地方，所以各自更新步长可以差不多，但是如果使用 学习率乘以梯度 作为步长，显然三者的更新步长将会相差几个数量级，更糟的是，可能还会出现 梯度消失 和 梯度爆炸。\n\nRprop 仅利用梯度的（负）方向，参数更新如下，\n$$\\theta_{t+1} = \\theta_t + \\Delta \\theta_t=\\theta_t - \\Delta_t \\cdot \\text{sign}[\\nabla f(\\theta_t)] \\qquad(7)$$\n其中 $\\Delta_t$ 表示时间步 `t` 处的更新步长，并且不同参数的更新步长也不同，例如第 `i` 个参数在时间步 `t` 的更新步长为 $\\Delta_{t,i}$。\n\n在每个时间步，计算各参数的梯度以及更新步长。根据当前时间步的梯度与上一时间步的梯度的符号是否一致，来调整更新步长，思路如下：\n- 如果符号一致，那么应该增大更新步长，以更快的到达最小值处\n- 如果符号相反，这表示刚好跨过最小值处，那么应该减小更新步长，以避免再次跨过最小值处\n  \n更新步长调整方案如下，\n$$\\Delta_t=\\begin{cases}\\min(\\Delta_{t-1} \\cdot \\eta^+, \\ \\Delta_{max}) & \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) > 0 \\\\\\\\ \\max(\\Delta_{t-1} \\cdot \\eta^-, \\Delta_{min}) & \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) < 0 \\\\\\\\ \\Delta_{t-1} & \\text{otherwise} \\end{cases}$$\n\n其中 $\\eta^+ > 1 > \\eta^->0$，$\\eta^+, \\ \\eta^-$ 分别用于增大步长和减小步长，并使用 $\\Delta_{min}, \\ \\Delta_{max}$ 来限制步长范围。通常，$\\Delta_{min}$ 过小 或者 $\\Delta_{max}$ 过大 都不是问题，因为实际的更新步长可以快速调整到合适值。$\\alpha$ 通常取 `1.2`，$\\beta$ 取 `0.5`。$\\Delta_0$ 为初始更新步长，作为超参数，事先给定，在 PyTorch 实现中为 `0.01`。\n\n在论文 [3] 中，作者具体讨论了四种参数更新方式，`Rprop+`，`Rprop-`，`iRprop+`，`iRprop-`，上述的参数更新方式对应 `Rprop-`，其余三种方法可阅读 [3]，这里不再一一具体介绍。[3] 的实验结果表明，`iRprop-` 的更新方式综合最优，PyTorch 的实现正是采用了 `iRprop-`。\n\n# 参考\n[1] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. John Duchi.\n\n[2] [RProp](https://florian.github.io/rprop/) \n\n[3] Improving the Rprop Learning Algorithm. Christian Igel.","source":"_posts/pytorch/optim-1.md","raw":"---\ntitle: PyTorch.optim\np: pytorch/optim-1\ndate: 2020-01-06 14:38:40\ntags: PyTorch\nmathjax: true\n---\n\n# 1. Adagrad\n<!-- more -->\n## 1.1 原理\n所有的参数形成一个参数向量，对每个参数使用不同的学习率。例如在时间步 `t`，第 `i` 个参数 $\\theta_i$ 的梯度为 $g_{t,i}$，\n$$g_{t,i} = \\nabla_{\\theta}J(\\theta_{t,i})$$\nSGD 的更新方式为，\n$$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$\n其中学习率 $\\eta$ 恒定。\n\nAdagrad 对每个参数在不同时间步调整学习率，参数更新为\n$$\\theta_{t+1,i}=\\theta_{t,i}-\\frac {\\eta} {\\sqrt{G_{t,ii}+\\epsilon}} \\cdot g_{t,i} \\qquad(1)$$\n\n其中 $G_t \\in \\mathbb R^{d \\times d}$ 是一个对角矩阵，对角线上每个元素 $G_{t,ii}$ 是参数 $\\theta_i$ 从时间步 `0` 到时间步 `t` 的梯度的平方和，\n$$G_{t,ii}=\\sum_{\\tau=0}^t g_{\\tau,i}^2 \\qquad(2)$$\n\n$\\epsilon$ 为平滑因子，用于避免分母为 0，一般取值 `1e-8`。\n\n将 (1) 式向量化，\n$$\\theta_{t+1}=\\theta_t - \\frac \\eta {\\sqrt {G_t+\\epsilon}} \\odot g_t \\qquad(3)$$\n\n其中 $\\odot$ 表示矩阵与向量相乘。通常，$\\eta=0.01$。\n\nAdagrad 的优点是不需要手动调整学习率，缺点是随着迭代次数的增加，分母逐渐增大，导致最后变得非常小，学习过程非常缓慢甚至停止。\n\n关于 Adagrad 调整学习率的理论分析可参考论文 [1]。\n\n## 1.2 PyTorch 实现\nPyTorch 的 Adagrad 实现中除了学习率 `lr` 和平滑因子 `eps`，还是增加了几个参数：\n1. 学习率衰减因子 `lr_decay`\n2. 权重衰减因子 `weight_decay`\n3. 累加初始值 `G`，这是 (2) 式中累加的一个初始值\n\n参数更新步骤如下：\n\n设置累加初值\n$$G_0=[G,...,G]$$\n其中 $G_0$ 是一个向量（对角矩阵的对角线元素），与参数数量相同。\n\n在时间步 `t`，\n\n1. 增加权重衰减项（正则项）的梯度\n   \n   $$g_t := g_t + \\lambda_{\\theta} \\cdot \\theta_t$$\n\n2. 学习率衰减为 \n   \n   $$\\eta := \\frac {\\eta} {1+ t \\cdot \\lambda_{\\eta}}  $$\n\n3. 累加梯度平方\n   \n   $$G_{t+1} = G_t+ g_t \\cdot g_t$$\n\n4. 更新参数\n   \n   $$\\theta_{t+1} = \\theta_t - \\frac \\eta {\\sqrt{G_t} + \\epsilon}\\cdot g_t$$\n\n以上，向量的计算全部按元素进行（标量则在需要的时候广播为向量）。（不同的参数具有不同的调整后的学习率）\n\n# 2. Adadelta\n## 2.1 原理\nAdadelta 是在 Adagrad 的基础上对学习率一味单调递减进行修改，不再对之前所有时间步的梯度做平方和，而是限制一个最近时间步的窗口，窗口大小为 `w`。\n\n然而，由于存储 `w` 个梯度平方值效率较低，所以改为使用梯度的衰减均值，如下\n$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1- \\gamma)g_t^2$$\n它的平方根就变成了 RMS（均方根，区别是每个元素的权重由 `1/n` 变成依次递增的值），\n$$\\text{RMS}[g]_t = \\sqrt{E[g^2]_t + \\epsilon}$$\n这样，越早期时间步的梯度平方，其权重越低，贡献也小，越近期的梯度平方，贡献越大。$\\gamma$ 可取 `0.9`。\n\n于是，参数更新为，\n$$\\Delta \\theta_t = -\\frac \\eta {\\text{RMS}[g]_t}\\cdot g_t \\qquad(4)\n\\\\\\\\\\theta_{t+1}=\\theta_t + \\Delta \\theta_t$$\n\n更进一步地，更新量 $\\Delta \\theta$ 与 $\\theta$ 在单位空间上不匹配，这在 SGD，momentum 以及 Adagrad 中也存在同样的问题，即\n$$\\Delta x 单位 \\propto g 单位 \\propto \\frac {\\partial f} {\\partial x} \\propto  \\frac 1 {x 单位}$$\n上式最后一步中假定了目标函数 `f` 是无单位的。这个单位空间不匹配如何理解呢？假设 `x` 表示距离，例如 米 $m$，损失函数 `f` 无量纲，根据上式，发现 `x` 的更新量的单位为 $m^{-1}$，显然这是不匹配的。为了实现匹配的目的，首先类似 $g^2$ 的衰减均值，定义更新量的衰减均值，\n$$E[\\Delta \\theta^2]_t = \\gamma \\cdot E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta_t^2$$\n\n均方根为，\n$$\\text{RMS}[\\Delta\\theta]_t=\\sqrt {E[\\Delta \\theta^2]_t+\\epsilon}$$\n残念，由于 $\\Delta \\theta_t$ 未知，所以上式也未知，所以近似使用 $\\text{RMS}[\\Delta\\theta]_{t-1}$ 来代替，然后这个值就作为 (4) 式中的 $\\eta$。\n\n于是最终参数更新方式为，\n$$\\Delta \\theta_t = -\\frac {\\text{RMS}[\\Delta\\theta]_{t-1}} {\\text{RMS}[g]_t}\\cdot g_t \\qquad(5)\n\\\\\\\\\\theta_{t+1}=\\theta_t + \\Delta \\theta_t$$\n注意到 `RMS` 中有平方根计算，所以，$\\text{RMS}[\\Delta\\theta]_{t-1}$ 与 $\\theta$ 量纲匹配，而 $\\text{RMS}[g]_t$ 与 $g$ 量纲匹配，所以 (5) 式中 $\\Delta \\theta$ 与 $\\theta$ 量纲匹配。\n\n## 2.2 PyTorch 实现\nPyTorch 的 Adadelta 实现使用 (5) 式，非常简单，不再啰嗦。\n\n# 3. RMSprop\nRMSprop 就是 Adadelta 中 (4) 式，通常 $\\gamma=0.9$，$\\eta=0.001$。简单，我们直接看 PyTorch 实现部分。\n\n## 3.1 PyTorch 实现\nRMSprop 的 `step` 方法部分代码如下，\n```python\n# 对每个参数\nsquare_avg = state['square_avg']    # 参数对应的梯度平方的衰减均值（也称 moving average）\nalpha = group['alpha']              # 对应上文公式中的 gamma\n\nif group['weight_decay'] != 0:\n    grad = grad.add(group['weight_decay'], p.data)  # 添加正则项的梯度\n\nsquare_avg.mul_(alpha).addcmul_(1-alpha, grad, grad)    # 计算 E[g^2]\n\nif group['centered']:               # 使用 centered 版本的 RMSprop\n    grad_avg = state['grad_avg']    # 获取 梯度衰减平均\n    grad_avg.mul_(alpha).add_(1-alpha, grad)    # 更新 梯度衰减平均\n    # 先归一化，然后计算 RMS[g]\n    avg = square_avg.addcmul_(-1, grad_avg, grad_avg).sqrt_().add_(group['eps'])\nelse:\n    # 直接计算 RMS[g]\n    avg = square_avg.sqrt_().add_(group['eps'])\n\nif group['momentum'] > 0:       # 使用动量\n    buf = state['momentum_buffer']  # 获取动量缓存\n    buf.mul_(group'momentum').addcdiv_(grad, avg)   # 更新 velocity，与 (6) 式一致\n    p.data.add_(-group['lr'], buf)\nelse:\n    p.data.addcdiv_(-group['lr'], grad, avg)\n```\n上面代码中，如果不使用 `centered` 和 `momentum`，那么代码逻辑与 (4) 式完全一致，所以我们只看 `centered` 和 `momentum` 是如何进行的。\n### centered\n对 梯度 $g$ 归一化，然后计算 平方 的衰减均值，如下\n$$E\\{[g-E(g)]^2\\}=E(g^2)-[E(g)]^2$$\n其中 $E(\\cdot)$ 计算衰减均值。于是，\n$$RMS[g]=\\sqrt{E\\{[g-E(g)]^2\\}}+\\epsilon=\\sqrt{E(g^2)-[E(g)]^2}+\\epsilon$$\n\n### momentum\n我们回顾一下普通的 SGD 参数更新方式：\n$$\\theta_{t+1}=\\theta_t - \\eta \\cdot \\nabla f(\\theta_t)$$\n然后带有 momentum 的 SGD 参数更新方式：\n$$v_{t+1}=\\mu \\cdot v_t + \\nabla f(\\theta_t)\n\\\\\\\\\\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$\n\n根据 (4) 式，现在已知 RMSprop 的参数更新方式为，\n$$\\theta_{t+1}=\\theta_t -\\frac \\eta {\\text{RMS}[g]_t}\\cdot g_t$$\n类比 SGD，可知带有 momentum 的 RMSprop 参数更新方式为，\n$$v_{t+1}=\\mu \\cdot v_t + \\frac {g_t} {\\text{RMS}[g]_t} \\qquad(6)\n\\\\\\\\ \\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$\n\n# 4. Rprop\n## 4.1 原理\nRprop 表示 resilient propagation。\n\n在 SGD 中，参数更新方向为负梯度方向，更新步长为梯度乘以一个系数（学习率），但是让更新步长直接与梯度成正比不一定是好选择，例如（来源 [2]）\n![]()<center>图 1. 三个函数在相同的地方有最小值，但是 `f'(x)` 不同</center>\n上图中，三个函数的最小值均在相同地方，所以各自更新步长可以差不多，但是如果使用 学习率乘以梯度 作为步长，显然三者的更新步长将会相差几个数量级，更糟的是，可能还会出现 梯度消失 和 梯度爆炸。\n\nRprop 仅利用梯度的（负）方向，参数更新如下，\n$$\\theta_{t+1} = \\theta_t + \\Delta \\theta_t=\\theta_t - \\Delta_t \\cdot \\text{sign}[\\nabla f(\\theta_t)] \\qquad(7)$$\n其中 $\\Delta_t$ 表示时间步 `t` 处的更新步长，并且不同参数的更新步长也不同，例如第 `i` 个参数在时间步 `t` 的更新步长为 $\\Delta_{t,i}$。\n\n在每个时间步，计算各参数的梯度以及更新步长。根据当前时间步的梯度与上一时间步的梯度的符号是否一致，来调整更新步长，思路如下：\n- 如果符号一致，那么应该增大更新步长，以更快的到达最小值处\n- 如果符号相反，这表示刚好跨过最小值处，那么应该减小更新步长，以避免再次跨过最小值处\n  \n更新步长调整方案如下，\n$$\\Delta_t=\\begin{cases}\\min(\\Delta_{t-1} \\cdot \\eta^+, \\ \\Delta_{max}) & \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) > 0 \\\\\\\\ \\max(\\Delta_{t-1} \\cdot \\eta^-, \\Delta_{min}) & \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) < 0 \\\\\\\\ \\Delta_{t-1} & \\text{otherwise} \\end{cases}$$\n\n其中 $\\eta^+ > 1 > \\eta^->0$，$\\eta^+, \\ \\eta^-$ 分别用于增大步长和减小步长，并使用 $\\Delta_{min}, \\ \\Delta_{max}$ 来限制步长范围。通常，$\\Delta_{min}$ 过小 或者 $\\Delta_{max}$ 过大 都不是问题，因为实际的更新步长可以快速调整到合适值。$\\alpha$ 通常取 `1.2`，$\\beta$ 取 `0.5`。$\\Delta_0$ 为初始更新步长，作为超参数，事先给定，在 PyTorch 实现中为 `0.01`。\n\n在论文 [3] 中，作者具体讨论了四种参数更新方式，`Rprop+`，`Rprop-`，`iRprop+`，`iRprop-`，上述的参数更新方式对应 `Rprop-`，其余三种方法可阅读 [3]，这里不再一一具体介绍。[3] 的实验结果表明，`iRprop-` 的更新方式综合最优，PyTorch 的实现正是采用了 `iRprop-`。\n\n# 参考\n[1] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. John Duchi.\n\n[2] [RProp](https://florian.github.io/rprop/) \n\n[3] Improving the Rprop Learning Algorithm. Christian Igel.","slug":"pytorch/optim-1","published":1,"updated":"2020-04-24T10:34:05.306Z","_id":"ck9dzcj1t002egga649zs01re","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"1-Adagrad\"><a href=\"#1-Adagrad\" class=\"headerlink\" title=\"1. Adagrad\"></a>1. Adagrad</h1><a id=\"more\"></a>\n<h2 id=\"1-1-原理\"><a href=\"#1-1-原理\" class=\"headerlink\" title=\"1.1 原理\"></a>1.1 原理</h2><p>所有的参数形成一个参数向量，对每个参数使用不同的学习率。例如在时间步 <code>t</code>，第 <code>i</code> 个参数 $\\theta_i$ 的梯度为 $g_{t,i}$，<br>$$g_{t,i} = \\nabla_{\\theta}J(\\theta_{t,i})$$<br>SGD 的更新方式为，<br>$$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$<br>其中学习率 $\\eta$ 恒定。</p>\n<p>Adagrad 对每个参数在不同时间步调整学习率，参数更新为<br>$$\\theta_{t+1,i}=\\theta_{t,i}-\\frac {\\eta} {\\sqrt{G_{t,ii}+\\epsilon}} \\cdot g_{t,i} \\qquad(1)$$</p>\n<p>其中 $G_t \\in \\mathbb R^{d \\times d}$ 是一个对角矩阵，对角线上每个元素 $G_{t,ii}$ 是参数 $\\theta_i$ 从时间步 <code>0</code> 到时间步 <code>t</code> 的梯度的平方和，<br>$$G_{t,ii}=\\sum_{\\tau=0}^t g_{\\tau,i}^2 \\qquad(2)$$</p>\n<p>$\\epsilon$ 为平滑因子，用于避免分母为 0，一般取值 <code>1e-8</code>。</p>\n<p>将 (1) 式向量化，<br>$$\\theta_{t+1}=\\theta_t - \\frac \\eta {\\sqrt {G_t+\\epsilon}} \\odot g_t \\qquad(3)$$</p>\n<p>其中 $\\odot$ 表示矩阵与向量相乘。通常，$\\eta=0.01$。</p>\n<p>Adagrad 的优点是不需要手动调整学习率，缺点是随着迭代次数的增加，分母逐渐增大，导致最后变得非常小，学习过程非常缓慢甚至停止。</p>\n<p>关于 Adagrad 调整学习率的理论分析可参考论文 [1]。</p>\n<h2 id=\"1-2-PyTorch-实现\"><a href=\"#1-2-PyTorch-实现\" class=\"headerlink\" title=\"1.2 PyTorch 实现\"></a>1.2 PyTorch 实现</h2><p>PyTorch 的 Adagrad 实现中除了学习率 <code>lr</code> 和平滑因子 <code>eps</code>，还是增加了几个参数：</p>\n<ol>\n<li>学习率衰减因子 <code>lr_decay</code></li>\n<li>权重衰减因子 <code>weight_decay</code></li>\n<li>累加初始值 <code>G</code>，这是 (2) 式中累加的一个初始值</li>\n</ol>\n<p>参数更新步骤如下：</p>\n<p>设置累加初值<br>$$G_0=[G,…,G]$$<br>其中 $G_0$ 是一个向量（对角矩阵的对角线元素），与参数数量相同。</p>\n<p>在时间步 <code>t</code>，</p>\n<ol>\n<li><p>增加权重衰减项（正则项）的梯度</p>\n<p>$$g_t := g_t + \\lambda_{\\theta} \\cdot \\theta_t$$</p>\n</li>\n<li><p>学习率衰减为 </p>\n<p>$$\\eta := \\frac {\\eta} {1+ t \\cdot \\lambda_{\\eta}}  $$</p>\n</li>\n<li><p>累加梯度平方</p>\n<p>$$G_{t+1} = G_t+ g_t \\cdot g_t$$</p>\n</li>\n<li><p>更新参数</p>\n<p>$$\\theta_{t+1} = \\theta_t - \\frac \\eta {\\sqrt{G_t} + \\epsilon}\\cdot g_t$$</p>\n</li>\n</ol>\n<p>以上，向量的计算全部按元素进行（标量则在需要的时候广播为向量）。（不同的参数具有不同的调整后的学习率）</p>\n<h1 id=\"2-Adadelta\"><a href=\"#2-Adadelta\" class=\"headerlink\" title=\"2. Adadelta\"></a>2. Adadelta</h1><h2 id=\"2-1-原理\"><a href=\"#2-1-原理\" class=\"headerlink\" title=\"2.1 原理\"></a>2.1 原理</h2><p>Adadelta 是在 Adagrad 的基础上对学习率一味单调递减进行修改，不再对之前所有时间步的梯度做平方和，而是限制一个最近时间步的窗口，窗口大小为 <code>w</code>。</p>\n<p>然而，由于存储 <code>w</code> 个梯度平方值效率较低，所以改为使用梯度的衰减均值，如下<br>$$E[g^2]<em>t = \\gamma E[g^2]</em>{t-1} + (1- \\gamma)g_t^2$$<br>它的平方根就变成了 RMS（均方根，区别是每个元素的权重由 <code>1/n</code> 变成依次递增的值），<br>$$\\text{RMS}[g]_t = \\sqrt{E[g^2]_t + \\epsilon}$$<br>这样，越早期时间步的梯度平方，其权重越低，贡献也小，越近期的梯度平方，贡献越大。$\\gamma$ 可取 <code>0.9</code>。</p>\n<p>于是，参数更新为，<br>$$\\Delta \\theta_t = -\\frac \\eta {\\text{RMS}[g]<em>t}\\cdot g_t \\qquad(4)<br>\\\\\\theta</em>{t+1}=\\theta_t + \\Delta \\theta_t$$</p>\n<p>更进一步地，更新量 $\\Delta \\theta$ 与 $\\theta$ 在单位空间上不匹配，这在 SGD，momentum 以及 Adagrad 中也存在同样的问题，即<br>$$\\Delta x 单位 \\propto g 单位 \\propto \\frac {\\partial f} {\\partial x} \\propto  \\frac 1 {x 单位}$$<br>上式最后一步中假定了目标函数 <code>f</code> 是无单位的。这个单位空间不匹配如何理解呢？假设 <code>x</code> 表示距离，例如 米 $m$，损失函数 <code>f</code> 无量纲，根据上式，发现 <code>x</code> 的更新量的单位为 $m^{-1}$，显然这是不匹配的。为了实现匹配的目的，首先类似 $g^2$ 的衰减均值，定义更新量的衰减均值，<br>$$E[\\Delta \\theta^2]<em>t = \\gamma \\cdot E[\\Delta \\theta^2]</em>{t-1} + (1-\\gamma)\\Delta \\theta_t^2$$</p>\n<p>均方根为，<br>$$\\text{RMS}[\\Delta\\theta]<em>t=\\sqrt {E[\\Delta \\theta^2]_t+\\epsilon}$$<br>残念，由于 $\\Delta \\theta_t$ 未知，所以上式也未知，所以近似使用 $\\text{RMS}[\\Delta\\theta]</em>{t-1}$ 来代替，然后这个值就作为 (4) 式中的 $\\eta$。</p>\n<p>于是最终参数更新方式为，<br>$$\\Delta \\theta_t = -\\frac {\\text{RMS}[\\Delta\\theta]<em>{t-1}} {\\text{RMS}[g]_t}\\cdot g_t \\qquad(5)<br>\\\\\\theta</em>{t+1}=\\theta_t + \\Delta \\theta_t$$<br>注意到 <code>RMS</code> 中有平方根计算，所以，$\\text{RMS}[\\Delta\\theta]_{t-1}$ 与 $\\theta$ 量纲匹配，而 $\\text{RMS}[g]_t$ 与 $g$ 量纲匹配，所以 (5) 式中 $\\Delta \\theta$ 与 $\\theta$ 量纲匹配。</p>\n<h2 id=\"2-2-PyTorch-实现\"><a href=\"#2-2-PyTorch-实现\" class=\"headerlink\" title=\"2.2 PyTorch 实现\"></a>2.2 PyTorch 实现</h2><p>PyTorch 的 Adadelta 实现使用 (5) 式，非常简单，不再啰嗦。</p>\n<h1 id=\"3-RMSprop\"><a href=\"#3-RMSprop\" class=\"headerlink\" title=\"3. RMSprop\"></a>3. RMSprop</h1><p>RMSprop 就是 Adadelta 中 (4) 式，通常 $\\gamma=0.9$，$\\eta=0.001$。简单，我们直接看 PyTorch 实现部分。</p>\n<h2 id=\"3-1-PyTorch-实现\"><a href=\"#3-1-PyTorch-实现\" class=\"headerlink\" title=\"3.1 PyTorch 实现\"></a>3.1 PyTorch 实现</h2><p>RMSprop 的 <code>step</code> 方法部分代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对每个参数</span></span><br><span class=\"line\">square_avg = state[<span class=\"string\">'square_avg'</span>]    <span class=\"comment\"># 参数对应的梯度平方的衰减均值（也称 moving average）</span></span><br><span class=\"line\">alpha = group[<span class=\"string\">'alpha'</span>]              <span class=\"comment\"># 对应上文公式中的 gamma</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'weight_decay'</span>] != <span class=\"number\">0</span>:</span><br><span class=\"line\">    grad = grad.add(group[<span class=\"string\">'weight_decay'</span>], p.data)  <span class=\"comment\"># 添加正则项的梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\">square_avg.mul_(alpha).addcmul_(<span class=\"number\">1</span>-alpha, grad, grad)    <span class=\"comment\"># 计算 E[g^2]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'centered'</span>]:               <span class=\"comment\"># 使用 centered 版本的 RMSprop</span></span><br><span class=\"line\">    grad_avg = state[<span class=\"string\">'grad_avg'</span>]    <span class=\"comment\"># 获取 梯度衰减平均</span></span><br><span class=\"line\">    grad_avg.mul_(alpha).add_(<span class=\"number\">1</span>-alpha, grad)    <span class=\"comment\"># 更新 梯度衰减平均</span></span><br><span class=\"line\">    <span class=\"comment\"># 先归一化，然后计算 RMS[g]</span></span><br><span class=\"line\">    avg = square_avg.addcmul_(<span class=\"number\">-1</span>, grad_avg, grad_avg).sqrt_().add_(group[<span class=\"string\">'eps'</span>])</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 直接计算 RMS[g]</span></span><br><span class=\"line\">    avg = square_avg.sqrt_().add_(group[<span class=\"string\">'eps'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'momentum'</span>] &gt; <span class=\"number\">0</span>:       <span class=\"comment\"># 使用动量</span></span><br><span class=\"line\">    buf = state[<span class=\"string\">'momentum_buffer'</span>]  <span class=\"comment\"># 获取动量缓存</span></span><br><span class=\"line\">    buf.mul_(group<span class=\"string\">'momentum'</span>).addcdiv_(grad, avg)   <span class=\"comment\"># 更新 velocity，与 (6) 式一致</span></span><br><span class=\"line\">    p.data.add_(-group[<span class=\"string\">'lr'</span>], buf)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    p.data.addcdiv_(-group[<span class=\"string\">'lr'</span>], grad, avg)</span><br></pre></td></tr></table></figure>\n<p>上面代码中，如果不使用 <code>centered</code> 和 <code>momentum</code>，那么代码逻辑与 (4) 式完全一致，所以我们只看 <code>centered</code> 和 <code>momentum</code> 是如何进行的。</p>\n<h3 id=\"centered\"><a href=\"#centered\" class=\"headerlink\" title=\"centered\"></a>centered</h3><p>对 梯度 $g$ 归一化，然后计算 平方 的衰减均值，如下<br>$$E{[g-E(g)]^2}=E(g^2)-[E(g)]^2$$<br>其中 $E(\\cdot)$ 计算衰减均值。于是，<br>$$RMS[g]=\\sqrt{E{[g-E(g)]^2}}+\\epsilon=\\sqrt{E(g^2)-[E(g)]^2}+\\epsilon$$</p>\n<h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>我们回顾一下普通的 SGD 参数更新方式：<br>$$\\theta_{t+1}=\\theta_t - \\eta \\cdot \\nabla f(\\theta_t)$$<br>然后带有 momentum 的 SGD 参数更新方式：<br>$$v_{t+1}=\\mu \\cdot v_t + \\nabla f(\\theta_t)<br>\\\\\\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$</p>\n<p>根据 (4) 式，现在已知 RMSprop 的参数更新方式为，<br>$$\\theta_{t+1}=\\theta_t -\\frac \\eta {\\text{RMS}[g]<em>t}\\cdot g_t$$<br>类比 SGD，可知带有 momentum 的 RMSprop 参数更新方式为，<br>$$v</em>{t+1}=\\mu \\cdot v_t + \\frac {g_t} {\\text{RMS}[g]<em>t} \\qquad(6)<br>\\\\ \\theta</em>{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$</p>\n<h1 id=\"4-Rprop\"><a href=\"#4-Rprop\" class=\"headerlink\" title=\"4. Rprop\"></a>4. Rprop</h1><h2 id=\"4-1-原理\"><a href=\"#4-1-原理\" class=\"headerlink\" title=\"4.1 原理\"></a>4.1 原理</h2><p>Rprop 表示 resilient propagation。</p>\n<p>在 SGD 中，参数更新方向为负梯度方向，更新步长为梯度乘以一个系数（学习率），但是让更新步长直接与梯度成正比不一定是好选择，例如（来源 [2]）<br><img src=\"\" alt=\"\"><center>图 1. 三个函数在相同的地方有最小值，但是 <code>f&#39;(x)</code> 不同</center><br>上图中，三个函数的最小值均在相同地方，所以各自更新步长可以差不多，但是如果使用 学习率乘以梯度 作为步长，显然三者的更新步长将会相差几个数量级，更糟的是，可能还会出现 梯度消失 和 梯度爆炸。</p>\n<p>Rprop 仅利用梯度的（负）方向，参数更新如下，<br>$$\\theta_{t+1} = \\theta_t + \\Delta \\theta_t=\\theta_t - \\Delta_t \\cdot \\text{sign}[\\nabla f(\\theta_t)] \\qquad(7)$$<br>其中 $\\Delta_t$ 表示时间步 <code>t</code> 处的更新步长，并且不同参数的更新步长也不同，例如第 <code>i</code> 个参数在时间步 <code>t</code> 的更新步长为 $\\Delta_{t,i}$。</p>\n<p>在每个时间步，计算各参数的梯度以及更新步长。根据当前时间步的梯度与上一时间步的梯度的符号是否一致，来调整更新步长，思路如下：</p>\n<ul>\n<li>如果符号一致，那么应该增大更新步长，以更快的到达最小值处</li>\n<li>如果符号相反，这表示刚好跨过最小值处，那么应该减小更新步长，以避免再次跨过最小值处</li>\n</ul>\n<p>更新步长调整方案如下，<br>$$\\Delta_t=\\begin{cases}\\min(\\Delta_{t-1} \\cdot \\eta^+, \\ \\Delta_{max}) &amp; \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) &gt; 0 \\\\ \\max(\\Delta_{t-1} \\cdot \\eta^-, \\Delta_{min}) &amp; \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) &lt; 0 \\\\ \\Delta_{t-1} &amp; \\text{otherwise} \\end{cases}$$</p>\n<p>其中 $\\eta^+ &gt; 1 &gt; \\eta^-&gt;0$，$\\eta^+, \\ \\eta^-$ 分别用于增大步长和减小步长，并使用 $\\Delta_{min}, \\ \\Delta_{max}$ 来限制步长范围。通常，$\\Delta_{min}$ 过小 或者 $\\Delta_{max}$ 过大 都不是问题，因为实际的更新步长可以快速调整到合适值。$\\alpha$ 通常取 <code>1.2</code>，$\\beta$ 取 <code>0.5</code>。$\\Delta_0$ 为初始更新步长，作为超参数，事先给定，在 PyTorch 实现中为 <code>0.01</code>。</p>\n<p>在论文 [3] 中，作者具体讨论了四种参数更新方式，<code>Rprop+</code>，<code>Rprop-</code>，<code>iRprop+</code>，<code>iRprop-</code>，上述的参数更新方式对应 <code>Rprop-</code>，其余三种方法可阅读 [3]，这里不再一一具体介绍。[3] 的实验结果表明，<code>iRprop-</code> 的更新方式综合最优，PyTorch 的实现正是采用了 <code>iRprop-</code>。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>[1] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. John Duchi.</p>\n<p>[2] <a href=\"https://florian.github.io/rprop/\" target=\"_blank\" rel=\"noopener\">RProp</a> </p>\n<p>[3] Improving the Rprop Learning Algorithm. Christian Igel.</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-Adagrad\"><a href=\"#1-Adagrad\" class=\"headerlink\" title=\"1. Adagrad\"></a>1. Adagrad</h1>","more":"<h2 id=\"1-1-原理\"><a href=\"#1-1-原理\" class=\"headerlink\" title=\"1.1 原理\"></a>1.1 原理</h2><p>所有的参数形成一个参数向量，对每个参数使用不同的学习率。例如在时间步 <code>t</code>，第 <code>i</code> 个参数 $\\theta_i$ 的梯度为 $g_{t,i}$，<br>$$g_{t,i} = \\nabla_{\\theta}J(\\theta_{t,i})$$<br>SGD 的更新方式为，<br>$$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$<br>其中学习率 $\\eta$ 恒定。</p>\n<p>Adagrad 对每个参数在不同时间步调整学习率，参数更新为<br>$$\\theta_{t+1,i}=\\theta_{t,i}-\\frac {\\eta} {\\sqrt{G_{t,ii}+\\epsilon}} \\cdot g_{t,i} \\qquad(1)$$</p>\n<p>其中 $G_t \\in \\mathbb R^{d \\times d}$ 是一个对角矩阵，对角线上每个元素 $G_{t,ii}$ 是参数 $\\theta_i$ 从时间步 <code>0</code> 到时间步 <code>t</code> 的梯度的平方和，<br>$$G_{t,ii}=\\sum_{\\tau=0}^t g_{\\tau,i}^2 \\qquad(2)$$</p>\n<p>$\\epsilon$ 为平滑因子，用于避免分母为 0，一般取值 <code>1e-8</code>。</p>\n<p>将 (1) 式向量化，<br>$$\\theta_{t+1}=\\theta_t - \\frac \\eta {\\sqrt {G_t+\\epsilon}} \\odot g_t \\qquad(3)$$</p>\n<p>其中 $\\odot$ 表示矩阵与向量相乘。通常，$\\eta=0.01$。</p>\n<p>Adagrad 的优点是不需要手动调整学习率，缺点是随着迭代次数的增加，分母逐渐增大，导致最后变得非常小，学习过程非常缓慢甚至停止。</p>\n<p>关于 Adagrad 调整学习率的理论分析可参考论文 [1]。</p>\n<h2 id=\"1-2-PyTorch-实现\"><a href=\"#1-2-PyTorch-实现\" class=\"headerlink\" title=\"1.2 PyTorch 实现\"></a>1.2 PyTorch 实现</h2><p>PyTorch 的 Adagrad 实现中除了学习率 <code>lr</code> 和平滑因子 <code>eps</code>，还是增加了几个参数：</p>\n<ol>\n<li>学习率衰减因子 <code>lr_decay</code></li>\n<li>权重衰减因子 <code>weight_decay</code></li>\n<li>累加初始值 <code>G</code>，这是 (2) 式中累加的一个初始值</li>\n</ol>\n<p>参数更新步骤如下：</p>\n<p>设置累加初值<br>$$G_0=[G,…,G]$$<br>其中 $G_0$ 是一个向量（对角矩阵的对角线元素），与参数数量相同。</p>\n<p>在时间步 <code>t</code>，</p>\n<ol>\n<li><p>增加权重衰减项（正则项）的梯度</p>\n<p>$$g_t := g_t + \\lambda_{\\theta} \\cdot \\theta_t$$</p>\n</li>\n<li><p>学习率衰减为 </p>\n<p>$$\\eta := \\frac {\\eta} {1+ t \\cdot \\lambda_{\\eta}}  $$</p>\n</li>\n<li><p>累加梯度平方</p>\n<p>$$G_{t+1} = G_t+ g_t \\cdot g_t$$</p>\n</li>\n<li><p>更新参数</p>\n<p>$$\\theta_{t+1} = \\theta_t - \\frac \\eta {\\sqrt{G_t} + \\epsilon}\\cdot g_t$$</p>\n</li>\n</ol>\n<p>以上，向量的计算全部按元素进行（标量则在需要的时候广播为向量）。（不同的参数具有不同的调整后的学习率）</p>\n<h1 id=\"2-Adadelta\"><a href=\"#2-Adadelta\" class=\"headerlink\" title=\"2. Adadelta\"></a>2. Adadelta</h1><h2 id=\"2-1-原理\"><a href=\"#2-1-原理\" class=\"headerlink\" title=\"2.1 原理\"></a>2.1 原理</h2><p>Adadelta 是在 Adagrad 的基础上对学习率一味单调递减进行修改，不再对之前所有时间步的梯度做平方和，而是限制一个最近时间步的窗口，窗口大小为 <code>w</code>。</p>\n<p>然而，由于存储 <code>w</code> 个梯度平方值效率较低，所以改为使用梯度的衰减均值，如下<br>$$E[g^2]<em>t = \\gamma E[g^2]</em>{t-1} + (1- \\gamma)g_t^2$$<br>它的平方根就变成了 RMS（均方根，区别是每个元素的权重由 <code>1/n</code> 变成依次递增的值），<br>$$\\text{RMS}[g]_t = \\sqrt{E[g^2]_t + \\epsilon}$$<br>这样，越早期时间步的梯度平方，其权重越低，贡献也小，越近期的梯度平方，贡献越大。$\\gamma$ 可取 <code>0.9</code>。</p>\n<p>于是，参数更新为，<br>$$\\Delta \\theta_t = -\\frac \\eta {\\text{RMS}[g]<em>t}\\cdot g_t \\qquad(4)<br>\\\\\\theta</em>{t+1}=\\theta_t + \\Delta \\theta_t$$</p>\n<p>更进一步地，更新量 $\\Delta \\theta$ 与 $\\theta$ 在单位空间上不匹配，这在 SGD，momentum 以及 Adagrad 中也存在同样的问题，即<br>$$\\Delta x 单位 \\propto g 单位 \\propto \\frac {\\partial f} {\\partial x} \\propto  \\frac 1 {x 单位}$$<br>上式最后一步中假定了目标函数 <code>f</code> 是无单位的。这个单位空间不匹配如何理解呢？假设 <code>x</code> 表示距离，例如 米 $m$，损失函数 <code>f</code> 无量纲，根据上式，发现 <code>x</code> 的更新量的单位为 $m^{-1}$，显然这是不匹配的。为了实现匹配的目的，首先类似 $g^2$ 的衰减均值，定义更新量的衰减均值，<br>$$E[\\Delta \\theta^2]<em>t = \\gamma \\cdot E[\\Delta \\theta^2]</em>{t-1} + (1-\\gamma)\\Delta \\theta_t^2$$</p>\n<p>均方根为，<br>$$\\text{RMS}[\\Delta\\theta]<em>t=\\sqrt {E[\\Delta \\theta^2]_t+\\epsilon}$$<br>残念，由于 $\\Delta \\theta_t$ 未知，所以上式也未知，所以近似使用 $\\text{RMS}[\\Delta\\theta]</em>{t-1}$ 来代替，然后这个值就作为 (4) 式中的 $\\eta$。</p>\n<p>于是最终参数更新方式为，<br>$$\\Delta \\theta_t = -\\frac {\\text{RMS}[\\Delta\\theta]<em>{t-1}} {\\text{RMS}[g]_t}\\cdot g_t \\qquad(5)<br>\\\\\\theta</em>{t+1}=\\theta_t + \\Delta \\theta_t$$<br>注意到 <code>RMS</code> 中有平方根计算，所以，$\\text{RMS}[\\Delta\\theta]_{t-1}$ 与 $\\theta$ 量纲匹配，而 $\\text{RMS}[g]_t$ 与 $g$ 量纲匹配，所以 (5) 式中 $\\Delta \\theta$ 与 $\\theta$ 量纲匹配。</p>\n<h2 id=\"2-2-PyTorch-实现\"><a href=\"#2-2-PyTorch-实现\" class=\"headerlink\" title=\"2.2 PyTorch 实现\"></a>2.2 PyTorch 实现</h2><p>PyTorch 的 Adadelta 实现使用 (5) 式，非常简单，不再啰嗦。</p>\n<h1 id=\"3-RMSprop\"><a href=\"#3-RMSprop\" class=\"headerlink\" title=\"3. RMSprop\"></a>3. RMSprop</h1><p>RMSprop 就是 Adadelta 中 (4) 式，通常 $\\gamma=0.9$，$\\eta=0.001$。简单，我们直接看 PyTorch 实现部分。</p>\n<h2 id=\"3-1-PyTorch-实现\"><a href=\"#3-1-PyTorch-实现\" class=\"headerlink\" title=\"3.1 PyTorch 实现\"></a>3.1 PyTorch 实现</h2><p>RMSprop 的 <code>step</code> 方法部分代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对每个参数</span></span><br><span class=\"line\">square_avg = state[<span class=\"string\">'square_avg'</span>]    <span class=\"comment\"># 参数对应的梯度平方的衰减均值（也称 moving average）</span></span><br><span class=\"line\">alpha = group[<span class=\"string\">'alpha'</span>]              <span class=\"comment\"># 对应上文公式中的 gamma</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'weight_decay'</span>] != <span class=\"number\">0</span>:</span><br><span class=\"line\">    grad = grad.add(group[<span class=\"string\">'weight_decay'</span>], p.data)  <span class=\"comment\"># 添加正则项的梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\">square_avg.mul_(alpha).addcmul_(<span class=\"number\">1</span>-alpha, grad, grad)    <span class=\"comment\"># 计算 E[g^2]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'centered'</span>]:               <span class=\"comment\"># 使用 centered 版本的 RMSprop</span></span><br><span class=\"line\">    grad_avg = state[<span class=\"string\">'grad_avg'</span>]    <span class=\"comment\"># 获取 梯度衰减平均</span></span><br><span class=\"line\">    grad_avg.mul_(alpha).add_(<span class=\"number\">1</span>-alpha, grad)    <span class=\"comment\"># 更新 梯度衰减平均</span></span><br><span class=\"line\">    <span class=\"comment\"># 先归一化，然后计算 RMS[g]</span></span><br><span class=\"line\">    avg = square_avg.addcmul_(<span class=\"number\">-1</span>, grad_avg, grad_avg).sqrt_().add_(group[<span class=\"string\">'eps'</span>])</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 直接计算 RMS[g]</span></span><br><span class=\"line\">    avg = square_avg.sqrt_().add_(group[<span class=\"string\">'eps'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> group[<span class=\"string\">'momentum'</span>] &gt; <span class=\"number\">0</span>:       <span class=\"comment\"># 使用动量</span></span><br><span class=\"line\">    buf = state[<span class=\"string\">'momentum_buffer'</span>]  <span class=\"comment\"># 获取动量缓存</span></span><br><span class=\"line\">    buf.mul_(group<span class=\"string\">'momentum'</span>).addcdiv_(grad, avg)   <span class=\"comment\"># 更新 velocity，与 (6) 式一致</span></span><br><span class=\"line\">    p.data.add_(-group[<span class=\"string\">'lr'</span>], buf)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    p.data.addcdiv_(-group[<span class=\"string\">'lr'</span>], grad, avg)</span><br></pre></td></tr></table></figure>\n<p>上面代码中，如果不使用 <code>centered</code> 和 <code>momentum</code>，那么代码逻辑与 (4) 式完全一致，所以我们只看 <code>centered</code> 和 <code>momentum</code> 是如何进行的。</p>\n<h3 id=\"centered\"><a href=\"#centered\" class=\"headerlink\" title=\"centered\"></a>centered</h3><p>对 梯度 $g$ 归一化，然后计算 平方 的衰减均值，如下<br>$$E{[g-E(g)]^2}=E(g^2)-[E(g)]^2$$<br>其中 $E(\\cdot)$ 计算衰减均值。于是，<br>$$RMS[g]=\\sqrt{E{[g-E(g)]^2}}+\\epsilon=\\sqrt{E(g^2)-[E(g)]^2}+\\epsilon$$</p>\n<h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>我们回顾一下普通的 SGD 参数更新方式：<br>$$\\theta_{t+1}=\\theta_t - \\eta \\cdot \\nabla f(\\theta_t)$$<br>然后带有 momentum 的 SGD 参数更新方式：<br>$$v_{t+1}=\\mu \\cdot v_t + \\nabla f(\\theta_t)<br>\\\\\\theta_{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$</p>\n<p>根据 (4) 式，现在已知 RMSprop 的参数更新方式为，<br>$$\\theta_{t+1}=\\theta_t -\\frac \\eta {\\text{RMS}[g]<em>t}\\cdot g_t$$<br>类比 SGD，可知带有 momentum 的 RMSprop 参数更新方式为，<br>$$v</em>{t+1}=\\mu \\cdot v_t + \\frac {g_t} {\\text{RMS}[g]<em>t} \\qquad(6)<br>\\\\ \\theta</em>{t+1}=\\theta_t - \\eta \\cdot v_{t+1}$$</p>\n<h1 id=\"4-Rprop\"><a href=\"#4-Rprop\" class=\"headerlink\" title=\"4. Rprop\"></a>4. Rprop</h1><h2 id=\"4-1-原理\"><a href=\"#4-1-原理\" class=\"headerlink\" title=\"4.1 原理\"></a>4.1 原理</h2><p>Rprop 表示 resilient propagation。</p>\n<p>在 SGD 中，参数更新方向为负梯度方向，更新步长为梯度乘以一个系数（学习率），但是让更新步长直接与梯度成正比不一定是好选择，例如（来源 [2]）<br><img src=\"\" alt=\"\"><center>图 1. 三个函数在相同的地方有最小值，但是 <code>f&#39;(x)</code> 不同</center><br>上图中，三个函数的最小值均在相同地方，所以各自更新步长可以差不多，但是如果使用 学习率乘以梯度 作为步长，显然三者的更新步长将会相差几个数量级，更糟的是，可能还会出现 梯度消失 和 梯度爆炸。</p>\n<p>Rprop 仅利用梯度的（负）方向，参数更新如下，<br>$$\\theta_{t+1} = \\theta_t + \\Delta \\theta_t=\\theta_t - \\Delta_t \\cdot \\text{sign}[\\nabla f(\\theta_t)] \\qquad(7)$$<br>其中 $\\Delta_t$ 表示时间步 <code>t</code> 处的更新步长，并且不同参数的更新步长也不同，例如第 <code>i</code> 个参数在时间步 <code>t</code> 的更新步长为 $\\Delta_{t,i}$。</p>\n<p>在每个时间步，计算各参数的梯度以及更新步长。根据当前时间步的梯度与上一时间步的梯度的符号是否一致，来调整更新步长，思路如下：</p>\n<ul>\n<li>如果符号一致，那么应该增大更新步长，以更快的到达最小值处</li>\n<li>如果符号相反，这表示刚好跨过最小值处，那么应该减小更新步长，以避免再次跨过最小值处</li>\n</ul>\n<p>更新步长调整方案如下，<br>$$\\Delta_t=\\begin{cases}\\min(\\Delta_{t-1} \\cdot \\eta^+, \\ \\Delta_{max}) &amp; \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) &gt; 0 \\\\ \\max(\\Delta_{t-1} \\cdot \\eta^-, \\Delta_{min}) &amp; \\nabla f(\\theta_t) \\cdot \\nabla f(\\theta_{t-1}) &lt; 0 \\\\ \\Delta_{t-1} &amp; \\text{otherwise} \\end{cases}$$</p>\n<p>其中 $\\eta^+ &gt; 1 &gt; \\eta^-&gt;0$，$\\eta^+, \\ \\eta^-$ 分别用于增大步长和减小步长，并使用 $\\Delta_{min}, \\ \\Delta_{max}$ 来限制步长范围。通常，$\\Delta_{min}$ 过小 或者 $\\Delta_{max}$ 过大 都不是问题，因为实际的更新步长可以快速调整到合适值。$\\alpha$ 通常取 <code>1.2</code>，$\\beta$ 取 <code>0.5</code>。$\\Delta_0$ 为初始更新步长，作为超参数，事先给定，在 PyTorch 实现中为 <code>0.01</code>。</p>\n<p>在论文 [3] 中，作者具体讨论了四种参数更新方式，<code>Rprop+</code>，<code>Rprop-</code>，<code>iRprop+</code>，<code>iRprop-</code>，上述的参数更新方式对应 <code>Rprop-</code>，其余三种方法可阅读 [3]，这里不再一一具体介绍。[3] 的实验结果表明，<code>iRprop-</code> 的更新方式综合最优，PyTorch 的实现正是采用了 <code>iRprop-</code>。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>[1] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. John Duchi.</p>\n<p>[2] <a href=\"https://florian.github.io/rprop/\" target=\"_blank\" rel=\"noopener\">RProp</a> </p>\n<p>[3] Improving the Rprop Learning Algorithm. Christian Igel.</p>"},{"title":"PyTorch.optim.SGD","p":"pytorch/optim_SGD","date":"2020-01-02T08:25:32.000Z","mathjax":true,"_content":"\n# 1. SGD\n<!-- more -->\n\n## 1.1 weight decay\n为了过拟合，通常在损失函数中增加正则项，记原来损失（MSE 或者 CE 等）为 $L_0$，那么添加正则项后的损失为，\n\n$$L=L_0+\\frac 1 2 \\lambda \\cdot \\|\\mathbf \\theta\\|_2^2$$\n如图 1 所示，\n![](/images/pytorch/overfitting.png) <center>图 1 过拟合图示（来源《Deep Learning with PyTorch》）</center>\n下半部分展示了过拟合的图，这种曲线的多项式比上图曲线的多项式，其系数 $\\mathbf \\theta$ 的绝对值更大（曲线的部分段变化更快，说明斜率绝对值更大，也就是 $|\\mathbf \\theta_i|$ 更大），所以增加正则项作为惩罚，其中 $\\lambda$ 作为平衡因子，也称 `weight decay`。于是，求导时，损失对每个权重参数的梯度多了一项\n$$\\frac {\\partial L}{\\partial \\mathbf \\theta_i}=\\frac {\\partial L_0}{\\partial \\mathbf \\theta_i}+\\lambda \\cdot \\mathbf \\theta_i$$\n\n## 1.2 momentum\n使用 SGD 训练时，有时候下降比较慢，甚至会陷入导局部最小值中，如图 2 所示，引入 momentum 可以加快收敛速度，我们知道 SGD 的参数更新公式为\n$$\\theta_{t+1} = \\theta_t -\\epsilon \\cdot d\\theta_t$$\n而使用 momentum 的更新公式为\n$$\\begin{aligned} v_{t+1} & = \\mu \\cdot v_t + d\\theta_t\n\\\\\\\\ \\theta_{t+1} &= \\theta_t - \\epsilon \\cdot v_{t+1}=\\theta_t-\\epsilon \\cdot \\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t \\end{aligned} \\qquad(1)$$\n其中 $\\theta_0$ 为初始权值参数值，$v_0=0$，$\\epsilon$ 为学习率，$\\mu$ 为 momentum 系数。从上两式中可见，如果当前 velocity（v 值） 与梯度方向一致，那么将会加快权值参数的变化量。在局部最小值附近（见图 2），由于 velocity 累计了之前的梯度，所以有望冲出局部最小值区域。\n![](/images/pytorch/momentum.png) <center>图 2</center>\n有的地方写成如下形式：\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - d\\theta_t\n\\\\\\\\\\theta_{t+1}&=\\theta_t+\\epsilon \\cdot v_{t+1}=\\theta_t +\\epsilon \\cdot \\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t \\end{aligned}\\qquad(2)$$\n实际上，当 $v_0=0$ 时，这两组更新公式本质相同。\n\ncaffe 框架以及在 Sutskever. [1] 中，更新公式为：\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t + \\epsilon \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}&=\\theta_t - v_{t+1}=\\theta_t-\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3)$$\n\n或者 \n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}&=\\theta_t + v_{t+1}=\\theta_t+\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3')$$\n\n假设学习率 $\\epsilon$ 保持不变，那么在 $v_0=0$ 时，(3) 式中的 $v_{t+1}$ 是 (1) 式中的 $\\epsilon$ 倍（在其他变量均相同的情况下），\n$$\\begin{aligned}v_1^{(3)}&=\\epsilon \\cdot d\\theta_0 = \\epsilon \\cdot v_1^{(1)}\n\\\\\\\\ v_2^{(3)}& = \\mu \\cdot v_1^{(3)}+\\epsilon \\cdot d\\theta_1=\\epsilon \\cdot [\\mu \\cdot v_1^{(1)}+d\\theta_1]=\\epsilon \\cdot v_2^{(1)}\n\\\\\\\\ &\\cdots \\end{aligned}$$\n\n所以 (1) 式中更新 $\\theta_{t+1}$ 时 $v_{t+1}$ 前面添加了系数 $\\epsilon$ 后，(1) 和 (3) 也是等价的，但是前提条件是: 1. $v_0=0$；2. 学习率 $\\epsilon$ 保持不变。\n\n随着训练 epoch 增加，学习率可能会衰减，例如衰减为 10%，那么 (1) 和 (3) 会出现不同，我们继续写下迭代计算过程以探究为何会发生不同。记 在 t+1 时刻发生 $\\epsilon$ 衰减，衰减前后分别为 $\\epsilon_1, \\ \\epsilon_2$，且 $\\epsilon_2 = 0.1 \\epsilon_1$，\n$$\\begin{aligned}v_t^{(3)} &= \\epsilon_1 \\cdot v_t^{(1)}\n\\\\\\\\ \\theta_{t+1}^{(3)}&=\\theta_t- \\mu \\cdot v_t^{(3)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t- \\epsilon_1 \\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}^{(1)}&=\\theta_t-\\epsilon_2\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t-0.1\\epsilon_1\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\\end{aligned}$$\n\n显然，(1) 式的更新方式在学习率衰减时能够有更加明显的体现，参数更新量明显变小，而 (3) 式此时的 velocity 相对较大，参数更新的量没有明显变小。当然随着训练迭代的推进，(3) 式的参数更新量也会逐渐变小，这是因为 velocity 不断更新后，逐渐被较新的 $\\epsilon_2 \\cdot d\\theta$ 主导，而先前累积的 $v_t^{(3)}$ 占比会越来越小，\n$$\\begin{aligned} v_{t+n}&=\\mu \\cdot v_{t+n-1}+\\epsilon_2 d\\theta_{t+n-1}\\\\\\\\ &=\\mu^2 \\cdot v_{t+n-2}+\\mu \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-2}+\\epsilon_2 \\cdot d\\theta_{t+n-1}\n\\\\\\\\&=\\cdots\n\\\\\\\\&=\\mu^n \\cdot v_t + \\mu^{n-1} \\cdot \\epsilon_2 \\cdot d\\theta_{t}+\\mu^{n-2} \\cdot \\epsilon_2 \\cdot  d\\theta_{t+1} + \\cdots + \\mu^0 \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-1}\\end{aligned}$$\n由于 $\\mu <1$，当 $n$ 较大时，上式第一项即 $v_t$ 对 velocity 贡献可以忽略。\n\n### 1.2.1 dampening\n\n阅读 PyTorch 中这部分的[源码](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71)，发现还使用了一个参数 `dampening`，这个参数指使用 momentum 更新时，对当前梯度的抑制，即 (1) 式中 velocity 更新变为\n$$v_{t+1} = \\mu \\cdot v_t + \\text{dampening} \\cdot d\\theta_t$$\n其实很多地方写成 $v_{t+1} = \\mu \\cdot v_t + (1-\\mu) \\cdot d\\theta_t$，由于 $0 \\le \\mu < 1$，这表示 $v_{t+1}$ 在 $\\min (v_t, d\\theta_t)$ 与 $\\max (v_t, d\\theta_t)$ 之间。不过，实际计算中，`dampening` 常使用默认值 `0`。\n\n## 1.3 Nesterov\n在 momentum 一节使用式 (1) (3) 进行介绍，其实是为了与 PyTorch [源码](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71) 或者 [文档](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) 对应，但是很多论文或者博客中更常用式 (3') 的形式（将学习率放入 $v_{t+1}$ 的计算式中），\n$$\\theta_{t+1}=\\theta_t+v_{t+1}$$\n这里，$v_{t+1}$ 的定义有很多种，例如经典 momentum，NAG (Nesterov Accelerated Gradient) 等。\n\n### 1.3.1 经典 momentum（CE）\n\n我们重写一遍经典 momentum 的参数更新公式（式 2），\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\n\\\\\\\\ \\theta_{t+1}&=\\theta_t + \\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\\end{aligned}$$\n\n### 1.3.2 NAG\nNAG 一次迭代过程分为两步：\n1. 梯度下降步骤\n   \n   $$\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t) \\qquad(4)$$\n2. momentum\n   \n   $$ y_{t+1}=\\theta_{t+1} + \\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t) \\qquad(5)$$\n   \n初始时令 $y_0=\\theta_0$，即这两个变量起点相同。\n\nNAG 中我们对 $y$ 做梯度下降，得到的值为 $\\theta$ 的新值，而非 $y$ 新值，$y$ 的新值是在 $\\theta$ 的基础之上再增加 $\\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t)$ 这么多的更新量。如图 3，\n![](/images/pytorch/NAS_0.png) <center>图 3. NAG 过程示意图</center>\n如果 $\\mu \\equiv 0$，NAG 就是普通的梯度下降 SD。\n\n\n注意，这里将 $\\mu, \\ \\epsilon$ 系数带上时刻下标。下面我们推导 velocity 的迭代计算式。\n\n\n### 1.3.3 Sutskever Nesterov Momentum\nNAG 中参数 $\\theta$ 的更新在梯度下降之后，在 momentum 之前。现在我们根据 NAG 推导出 velocity 项。首先要说明的是，需要将 NAG 迭代过程的两个步骤顺序对换，即 `momentum-GD-momentum-GD-...` 的顺序。\n\n已知，\n$$y_t=\\theta_t+\\mu_t \\cdot(\\theta_t-\\theta_{t-1})$$\n\n写成以下形式，\n$$y_t=\\theta_t + \\mu_t \\cdot v_t$$\n\n可根据 (4) 式消去 $y_t$，需要注意的是，(4) 式表示 t 时刻迭代过程中的梯度下降步骤，到 Sutskever Nesterov Momentum 中则为 t-1 时刻迭代中的 momentum 步骤，即 $\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t)$，与上式联合可消去 $y_t$， 得\n$$\\theta_{t+1} = \\theta_t+\\mu_t \\cdot v_t-\\epsilon_t \\cdot \\nabla f(\\theta_t + \\mu_t \\cdot v_t) \\qquad(6)$$\n\n于是，\n\n$$v_{t+1} = \\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\theta_t+\\mu_t \\cdot v_t)  \\qquad(7) $$\n\n图 4 是经典 momentum 与 NAG 方法的图示比较。\n![](/images/pytorch/NAG.png) <center>图 4 </center>\n\n\n### 1.3.4 Bengio Nesterov Momentum\nNAG 中我们的模型参数是 $\\theta$，但是其更新不是对自身做梯度下降，而是对 $y$ 做梯度下降，进一步地，$y$ 的更新则又反过来依赖于 $\\theta$ 的 momentum。\n\n定义一个新变量，表示经过 momentum 更新后的 $\\theta$ 值，或者更准确地讲，是 momentum 更新后的模型参数的值。\n\n$$\\Theta_{t-1}=\\theta_{t-1} + \\mu_{t-1} \\cdot v_{t-1}$$\n这里可能感觉有点绕，一会 $\\theta$，一会 $\\Theta$，到底哪个是表示模型参数。我是这么理解的，初始时模型参数为 $\\theta_0$，此后更新迭代过程中，$\\Theta$ 才表示模型参数，$\\theta$ 只作为中间变量。\n\n\n根据 velocity 的定义 (7) 式，有\n$$v_t=\\mu_{t-1} \\cdot v_{t-1} - \\epsilon_{t-1} \\cdot \\nabla f(\\Theta_{t-1})$$\n\n$v_t$ 依然是 $\\theta$ “中间”变量的更新量。\n\n根据 $\\Theta$ 定义，\n$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}= \\theta_{t+1}\n\\\\\\\\ \\Theta_t-\\mu_t \\cdot v_t= \\theta_t$$\n以及 (6) 式，有\n$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}=\\Theta_t-\\mu_t \\cdot v_t+\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n化简得，\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot v_{t+1}-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n\n继续代入 (7) 式，有\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot[\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)]-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n\n展开得，\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t) \\qquad(8)$$\n\n写成 $\\Theta_{t+1}=\\Theta_t + V_{t+1}$ 的形式，于是\n$$V_{t+1}=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n 就是 $\\Theta$ 的更新量，等价于 `(3')` 式中的 $v_{t+1}$，对应到 (1) 式中的 $v_{t+1}$ 的形式，去掉 $\\epsilon$，以及 $-$ 变成 $+$，易得， \n$$\\begin{aligned} V_{t+1}&=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t+\\mu_{t+1} \\cdot \\nabla f(\\Theta_t)+ \\nabla f(\\Theta_t) \n\\\\\\\\ &=\\mu_{t+1} \\cdot [\\mu_t \\cdot v_t+ \\nabla f(\\Theta_t)] + \\nabla f(\\Theta_t)  \\end{aligned} \\qquad(9)$$\n此时 $\\Theta$ 的更新为\n$$\\Theta_{t+1}=\\Theta_t - \\epsilon_t \\cdot V_{t+1} \\qquad(10)$$\n\n__(9) 和 (10) 式就对应 PyTorch 源码中 `SGD.step` 在 `nesterov=True` 时的计算过程。__\n\n\n# 参考：\n\n[1] On the importance of initialization and momentum in deep learning. Ilya Sutskever\n\n[2] [Nesterov Accelerated Gradient and Momentum](https://jlmelville.github.io/mize/mesterov.html)\n\n# 更多阅读\n[1] [ORF523: Nesterov's Accelerated Gradient Descent](https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/)","source":"_posts/pytorch/optim_SGD.md","raw":"---\ntitle: PyTorch.optim.SGD\np: pytorch/optim_SGD\ndate: 2020-01-02 16:25:32\ntags: PyTorch\nmathjax: true\n---\n\n# 1. SGD\n<!-- more -->\n\n## 1.1 weight decay\n为了过拟合，通常在损失函数中增加正则项，记原来损失（MSE 或者 CE 等）为 $L_0$，那么添加正则项后的损失为，\n\n$$L=L_0+\\frac 1 2 \\lambda \\cdot \\|\\mathbf \\theta\\|_2^2$$\n如图 1 所示，\n![](/images/pytorch/overfitting.png) <center>图 1 过拟合图示（来源《Deep Learning with PyTorch》）</center>\n下半部分展示了过拟合的图，这种曲线的多项式比上图曲线的多项式，其系数 $\\mathbf \\theta$ 的绝对值更大（曲线的部分段变化更快，说明斜率绝对值更大，也就是 $|\\mathbf \\theta_i|$ 更大），所以增加正则项作为惩罚，其中 $\\lambda$ 作为平衡因子，也称 `weight decay`。于是，求导时，损失对每个权重参数的梯度多了一项\n$$\\frac {\\partial L}{\\partial \\mathbf \\theta_i}=\\frac {\\partial L_0}{\\partial \\mathbf \\theta_i}+\\lambda \\cdot \\mathbf \\theta_i$$\n\n## 1.2 momentum\n使用 SGD 训练时，有时候下降比较慢，甚至会陷入导局部最小值中，如图 2 所示，引入 momentum 可以加快收敛速度，我们知道 SGD 的参数更新公式为\n$$\\theta_{t+1} = \\theta_t -\\epsilon \\cdot d\\theta_t$$\n而使用 momentum 的更新公式为\n$$\\begin{aligned} v_{t+1} & = \\mu \\cdot v_t + d\\theta_t\n\\\\\\\\ \\theta_{t+1} &= \\theta_t - \\epsilon \\cdot v_{t+1}=\\theta_t-\\epsilon \\cdot \\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t \\end{aligned} \\qquad(1)$$\n其中 $\\theta_0$ 为初始权值参数值，$v_0=0$，$\\epsilon$ 为学习率，$\\mu$ 为 momentum 系数。从上两式中可见，如果当前 velocity（v 值） 与梯度方向一致，那么将会加快权值参数的变化量。在局部最小值附近（见图 2），由于 velocity 累计了之前的梯度，所以有望冲出局部最小值区域。\n![](/images/pytorch/momentum.png) <center>图 2</center>\n有的地方写成如下形式：\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - d\\theta_t\n\\\\\\\\\\theta_{t+1}&=\\theta_t+\\epsilon \\cdot v_{t+1}=\\theta_t +\\epsilon \\cdot \\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t \\end{aligned}\\qquad(2)$$\n实际上，当 $v_0=0$ 时，这两组更新公式本质相同。\n\ncaffe 框架以及在 Sutskever. [1] 中，更新公式为：\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t + \\epsilon \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}&=\\theta_t - v_{t+1}=\\theta_t-\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3)$$\n\n或者 \n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}&=\\theta_t + v_{t+1}=\\theta_t+\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3')$$\n\n假设学习率 $\\epsilon$ 保持不变，那么在 $v_0=0$ 时，(3) 式中的 $v_{t+1}$ 是 (1) 式中的 $\\epsilon$ 倍（在其他变量均相同的情况下），\n$$\\begin{aligned}v_1^{(3)}&=\\epsilon \\cdot d\\theta_0 = \\epsilon \\cdot v_1^{(1)}\n\\\\\\\\ v_2^{(3)}& = \\mu \\cdot v_1^{(3)}+\\epsilon \\cdot d\\theta_1=\\epsilon \\cdot [\\mu \\cdot v_1^{(1)}+d\\theta_1]=\\epsilon \\cdot v_2^{(1)}\n\\\\\\\\ &\\cdots \\end{aligned}$$\n\n所以 (1) 式中更新 $\\theta_{t+1}$ 时 $v_{t+1}$ 前面添加了系数 $\\epsilon$ 后，(1) 和 (3) 也是等价的，但是前提条件是: 1. $v_0=0$；2. 学习率 $\\epsilon$ 保持不变。\n\n随着训练 epoch 增加，学习率可能会衰减，例如衰减为 10%，那么 (1) 和 (3) 会出现不同，我们继续写下迭代计算过程以探究为何会发生不同。记 在 t+1 时刻发生 $\\epsilon$ 衰减，衰减前后分别为 $\\epsilon_1, \\ \\epsilon_2$，且 $\\epsilon_2 = 0.1 \\epsilon_1$，\n$$\\begin{aligned}v_t^{(3)} &= \\epsilon_1 \\cdot v_t^{(1)}\n\\\\\\\\ \\theta_{t+1}^{(3)}&=\\theta_t- \\mu \\cdot v_t^{(3)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t- \\epsilon_1 \\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\n\\\\\\\\ \\theta_{t+1}^{(1)}&=\\theta_t-\\epsilon_2\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t-0.1\\epsilon_1\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\\end{aligned}$$\n\n显然，(1) 式的更新方式在学习率衰减时能够有更加明显的体现，参数更新量明显变小，而 (3) 式此时的 velocity 相对较大，参数更新的量没有明显变小。当然随着训练迭代的推进，(3) 式的参数更新量也会逐渐变小，这是因为 velocity 不断更新后，逐渐被较新的 $\\epsilon_2 \\cdot d\\theta$ 主导，而先前累积的 $v_t^{(3)}$ 占比会越来越小，\n$$\\begin{aligned} v_{t+n}&=\\mu \\cdot v_{t+n-1}+\\epsilon_2 d\\theta_{t+n-1}\\\\\\\\ &=\\mu^2 \\cdot v_{t+n-2}+\\mu \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-2}+\\epsilon_2 \\cdot d\\theta_{t+n-1}\n\\\\\\\\&=\\cdots\n\\\\\\\\&=\\mu^n \\cdot v_t + \\mu^{n-1} \\cdot \\epsilon_2 \\cdot d\\theta_{t}+\\mu^{n-2} \\cdot \\epsilon_2 \\cdot  d\\theta_{t+1} + \\cdots + \\mu^0 \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-1}\\end{aligned}$$\n由于 $\\mu <1$，当 $n$ 较大时，上式第一项即 $v_t$ 对 velocity 贡献可以忽略。\n\n### 1.2.1 dampening\n\n阅读 PyTorch 中这部分的[源码](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71)，发现还使用了一个参数 `dampening`，这个参数指使用 momentum 更新时，对当前梯度的抑制，即 (1) 式中 velocity 更新变为\n$$v_{t+1} = \\mu \\cdot v_t + \\text{dampening} \\cdot d\\theta_t$$\n其实很多地方写成 $v_{t+1} = \\mu \\cdot v_t + (1-\\mu) \\cdot d\\theta_t$，由于 $0 \\le \\mu < 1$，这表示 $v_{t+1}$ 在 $\\min (v_t, d\\theta_t)$ 与 $\\max (v_t, d\\theta_t)$ 之间。不过，实际计算中，`dampening` 常使用默认值 `0`。\n\n## 1.3 Nesterov\n在 momentum 一节使用式 (1) (3) 进行介绍，其实是为了与 PyTorch [源码](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71) 或者 [文档](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) 对应，但是很多论文或者博客中更常用式 (3') 的形式（将学习率放入 $v_{t+1}$ 的计算式中），\n$$\\theta_{t+1}=\\theta_t+v_{t+1}$$\n这里，$v_{t+1}$ 的定义有很多种，例如经典 momentum，NAG (Nesterov Accelerated Gradient) 等。\n\n### 1.3.1 经典 momentum（CE）\n\n我们重写一遍经典 momentum 的参数更新公式（式 2），\n$$\\begin{aligned}v_{t+1}&=\\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\n\\\\\\\\ \\theta_{t+1}&=\\theta_t + \\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\\end{aligned}$$\n\n### 1.3.2 NAG\nNAG 一次迭代过程分为两步：\n1. 梯度下降步骤\n   \n   $$\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t) \\qquad(4)$$\n2. momentum\n   \n   $$ y_{t+1}=\\theta_{t+1} + \\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t) \\qquad(5)$$\n   \n初始时令 $y_0=\\theta_0$，即这两个变量起点相同。\n\nNAG 中我们对 $y$ 做梯度下降，得到的值为 $\\theta$ 的新值，而非 $y$ 新值，$y$ 的新值是在 $\\theta$ 的基础之上再增加 $\\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t)$ 这么多的更新量。如图 3，\n![](/images/pytorch/NAS_0.png) <center>图 3. NAG 过程示意图</center>\n如果 $\\mu \\equiv 0$，NAG 就是普通的梯度下降 SD。\n\n\n注意，这里将 $\\mu, \\ \\epsilon$ 系数带上时刻下标。下面我们推导 velocity 的迭代计算式。\n\n\n### 1.3.3 Sutskever Nesterov Momentum\nNAG 中参数 $\\theta$ 的更新在梯度下降之后，在 momentum 之前。现在我们根据 NAG 推导出 velocity 项。首先要说明的是，需要将 NAG 迭代过程的两个步骤顺序对换，即 `momentum-GD-momentum-GD-...` 的顺序。\n\n已知，\n$$y_t=\\theta_t+\\mu_t \\cdot(\\theta_t-\\theta_{t-1})$$\n\n写成以下形式，\n$$y_t=\\theta_t + \\mu_t \\cdot v_t$$\n\n可根据 (4) 式消去 $y_t$，需要注意的是，(4) 式表示 t 时刻迭代过程中的梯度下降步骤，到 Sutskever Nesterov Momentum 中则为 t-1 时刻迭代中的 momentum 步骤，即 $\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t)$，与上式联合可消去 $y_t$， 得\n$$\\theta_{t+1} = \\theta_t+\\mu_t \\cdot v_t-\\epsilon_t \\cdot \\nabla f(\\theta_t + \\mu_t \\cdot v_t) \\qquad(6)$$\n\n于是，\n\n$$v_{t+1} = \\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\theta_t+\\mu_t \\cdot v_t)  \\qquad(7) $$\n\n图 4 是经典 momentum 与 NAG 方法的图示比较。\n![](/images/pytorch/NAG.png) <center>图 4 </center>\n\n\n### 1.3.4 Bengio Nesterov Momentum\nNAG 中我们的模型参数是 $\\theta$，但是其更新不是对自身做梯度下降，而是对 $y$ 做梯度下降，进一步地，$y$ 的更新则又反过来依赖于 $\\theta$ 的 momentum。\n\n定义一个新变量，表示经过 momentum 更新后的 $\\theta$ 值，或者更准确地讲，是 momentum 更新后的模型参数的值。\n\n$$\\Theta_{t-1}=\\theta_{t-1} + \\mu_{t-1} \\cdot v_{t-1}$$\n这里可能感觉有点绕，一会 $\\theta$，一会 $\\Theta$，到底哪个是表示模型参数。我是这么理解的，初始时模型参数为 $\\theta_0$，此后更新迭代过程中，$\\Theta$ 才表示模型参数，$\\theta$ 只作为中间变量。\n\n\n根据 velocity 的定义 (7) 式，有\n$$v_t=\\mu_{t-1} \\cdot v_{t-1} - \\epsilon_{t-1} \\cdot \\nabla f(\\Theta_{t-1})$$\n\n$v_t$ 依然是 $\\theta$ “中间”变量的更新量。\n\n根据 $\\Theta$ 定义，\n$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}= \\theta_{t+1}\n\\\\\\\\ \\Theta_t-\\mu_t \\cdot v_t= \\theta_t$$\n以及 (6) 式，有\n$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}=\\Theta_t-\\mu_t \\cdot v_t+\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n化简得，\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot v_{t+1}-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n\n继续代入 (7) 式，有\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot[\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)]-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n\n展开得，\n$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t) \\qquad(8)$$\n\n写成 $\\Theta_{t+1}=\\Theta_t + V_{t+1}$ 的形式，于是\n$$V_{t+1}=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$\n 就是 $\\Theta$ 的更新量，等价于 `(3')` 式中的 $v_{t+1}$，对应到 (1) 式中的 $v_{t+1}$ 的形式，去掉 $\\epsilon$，以及 $-$ 变成 $+$，易得， \n$$\\begin{aligned} V_{t+1}&=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t+\\mu_{t+1} \\cdot \\nabla f(\\Theta_t)+ \\nabla f(\\Theta_t) \n\\\\\\\\ &=\\mu_{t+1} \\cdot [\\mu_t \\cdot v_t+ \\nabla f(\\Theta_t)] + \\nabla f(\\Theta_t)  \\end{aligned} \\qquad(9)$$\n此时 $\\Theta$ 的更新为\n$$\\Theta_{t+1}=\\Theta_t - \\epsilon_t \\cdot V_{t+1} \\qquad(10)$$\n\n__(9) 和 (10) 式就对应 PyTorch 源码中 `SGD.step` 在 `nesterov=True` 时的计算过程。__\n\n\n# 参考：\n\n[1] On the importance of initialization and momentum in deep learning. Ilya Sutskever\n\n[2] [Nesterov Accelerated Gradient and Momentum](https://jlmelville.github.io/mize/mesterov.html)\n\n# 更多阅读\n[1] [ORF523: Nesterov's Accelerated Gradient Descent](https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/)","slug":"pytorch/optim_SGD","published":1,"updated":"2020-04-24T10:33:35.048Z","_id":"ck9dzcj1v002ggga69jso7kei","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"1-SGD\"><a href=\"#1-SGD\" class=\"headerlink\" title=\"1. SGD\"></a>1. SGD</h1><a id=\"more\"></a>\n\n<h2 id=\"1-1-weight-decay\"><a href=\"#1-1-weight-decay\" class=\"headerlink\" title=\"1.1 weight decay\"></a>1.1 weight decay</h2><p>为了过拟合，通常在损失函数中增加正则项，记原来损失（MSE 或者 CE 等）为 $L_0$，那么添加正则项后的损失为，</p>\n<p>$$L=L_0+\\frac 1 2 \\lambda \\cdot |\\mathbf \\theta|_2^2$$<br>如图 1 所示，<br><img src=\"/images/pytorch/overfitting.png\" alt=\"\"> <center>图 1 过拟合图示（来源《Deep Learning with PyTorch》）</center><br>下半部分展示了过拟合的图，这种曲线的多项式比上图曲线的多项式，其系数 $\\mathbf \\theta$ 的绝对值更大（曲线的部分段变化更快，说明斜率绝对值更大，也就是 $|\\mathbf \\theta_i|$ 更大），所以增加正则项作为惩罚，其中 $\\lambda$ 作为平衡因子，也称 <code>weight decay</code>。于是，求导时，损失对每个权重参数的梯度多了一项<br>$$\\frac {\\partial L}{\\partial \\mathbf \\theta_i}=\\frac {\\partial L_0}{\\partial \\mathbf \\theta_i}+\\lambda \\cdot \\mathbf \\theta_i$$</p>\n<h2 id=\"1-2-momentum\"><a href=\"#1-2-momentum\" class=\"headerlink\" title=\"1.2 momentum\"></a>1.2 momentum</h2><p>使用 SGD 训练时，有时候下降比较慢，甚至会陷入导局部最小值中，如图 2 所示，引入 momentum 可以加快收敛速度，我们知道 SGD 的参数更新公式为<br>$$\\theta_{t+1} = \\theta_t -\\epsilon \\cdot d\\theta_t$$<br>而使用 momentum 的更新公式为<br>$$\\begin{aligned} v_{t+1} &amp; = \\mu \\cdot v_t + d\\theta_t<br>\\\\ \\theta_{t+1} &amp;= \\theta_t - \\epsilon \\cdot v_{t+1}=\\theta_t-\\epsilon \\cdot \\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t \\end{aligned} \\qquad(1)$$<br>其中 $\\theta_0$ 为初始权值参数值，$v_0=0$，$\\epsilon$ 为学习率，$\\mu$ 为 momentum 系数。从上两式中可见，如果当前 velocity（v 值） 与梯度方向一致，那么将会加快权值参数的变化量。在局部最小值附近（见图 2），由于 velocity 累计了之前的梯度，所以有望冲出局部最小值区域。<br><img src=\"/images/pytorch/momentum.png\" alt=\"\"> <center>图 2</center><br>有的地方写成如下形式：<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - d\\theta_t<br>\\\\\\theta_{t+1}&amp;=\\theta_t+\\epsilon \\cdot v_{t+1}=\\theta_t +\\epsilon \\cdot \\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t \\end{aligned}\\qquad(2)$$<br>实际上，当 $v_0=0$ 时，这两组更新公式本质相同。</p>\n<p>caffe 框架以及在 Sutskever. [1] 中，更新公式为：<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t + \\epsilon \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}&amp;=\\theta_t - v_{t+1}=\\theta_t-\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3)$$</p>\n<p>或者<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}&amp;=\\theta_t + v_{t+1}=\\theta_t+\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3’)$$</p>\n<p>假设学习率 $\\epsilon$ 保持不变，那么在 $v_0=0$ 时，(3) 式中的 $v_{t+1}$ 是 (1) 式中的 $\\epsilon$ 倍（在其他变量均相同的情况下），<br>$$\\begin{aligned}v_1^{(3)}&amp;=\\epsilon \\cdot d\\theta_0 = \\epsilon \\cdot v_1^{(1)}<br>\\\\ v_2^{(3)}&amp; = \\mu \\cdot v_1^{(3)}+\\epsilon \\cdot d\\theta_1=\\epsilon \\cdot [\\mu \\cdot v_1^{(1)}+d\\theta_1]=\\epsilon \\cdot v_2^{(1)}<br>\\\\ &amp;\\cdots \\end{aligned}$$</p>\n<p>所以 (1) 式中更新 $\\theta_{t+1}$ 时 $v_{t+1}$ 前面添加了系数 $\\epsilon$ 后，(1) 和 (3) 也是等价的，但是前提条件是: 1. $v_0=0$；2. 学习率 $\\epsilon$ 保持不变。</p>\n<p>随着训练 epoch 增加，学习率可能会衰减，例如衰减为 10%，那么 (1) 和 (3) 会出现不同，我们继续写下迭代计算过程以探究为何会发生不同。记 在 t+1 时刻发生 $\\epsilon$ 衰减，衰减前后分别为 $\\epsilon_1, \\ \\epsilon_2$，且 $\\epsilon_2 = 0.1 \\epsilon_1$，<br>$$\\begin{aligned}v_t^{(3)} &amp;= \\epsilon_1 \\cdot v_t^{(1)}<br>\\\\ \\theta_{t+1}^{(3)}&amp;=\\theta_t- \\mu \\cdot v_t^{(3)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t- \\epsilon_1 \\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}^{(1)}&amp;=\\theta_t-\\epsilon_2\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t-0.1\\epsilon_1\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\\end{aligned}$$</p>\n<p>显然，(1) 式的更新方式在学习率衰减时能够有更加明显的体现，参数更新量明显变小，而 (3) 式此时的 velocity 相对较大，参数更新的量没有明显变小。当然随着训练迭代的推进，(3) 式的参数更新量也会逐渐变小，这是因为 velocity 不断更新后，逐渐被较新的 $\\epsilon_2 \\cdot d\\theta$ 主导，而先前累积的 $v_t^{(3)}$ 占比会越来越小，<br>$$\\begin{aligned} v_{t+n}&amp;=\\mu \\cdot v_{t+n-1}+\\epsilon_2 d\\theta_{t+n-1}\\\\ &amp;=\\mu^2 \\cdot v_{t+n-2}+\\mu \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-2}+\\epsilon_2 \\cdot d\\theta_{t+n-1}<br>\\\\&amp;=\\cdots<br>\\\\&amp;=\\mu^n \\cdot v_t + \\mu^{n-1} \\cdot \\epsilon_2 \\cdot d\\theta_{t}+\\mu^{n-2} \\cdot \\epsilon_2 \\cdot  d\\theta_{t+1} + \\cdots + \\mu^0 \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-1}\\end{aligned}$$<br>由于 $\\mu &lt;1$，当 $n$ 较大时，上式第一项即 $v_t$ 对 velocity 贡献可以忽略。</p>\n<h3 id=\"1-2-1-dampening\"><a href=\"#1-2-1-dampening\" class=\"headerlink\" title=\"1.2.1 dampening\"></a>1.2.1 dampening</h3><p>阅读 PyTorch 中这部分的<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71\" target=\"_blank\" rel=\"noopener\">源码</a>，发现还使用了一个参数 <code>dampening</code>，这个参数指使用 momentum 更新时，对当前梯度的抑制，即 (1) 式中 velocity 更新变为<br>$$v_{t+1} = \\mu \\cdot v_t + \\text{dampening} \\cdot d\\theta_t$$<br>其实很多地方写成 $v_{t+1} = \\mu \\cdot v_t + (1-\\mu) \\cdot d\\theta_t$，由于 $0 \\le \\mu &lt; 1$，这表示 $v_{t+1}$ 在 $\\min (v_t, d\\theta_t)$ 与 $\\max (v_t, d\\theta_t)$ 之间。不过，实际计算中，<code>dampening</code> 常使用默认值 <code>0</code>。</p>\n<h2 id=\"1-3-Nesterov\"><a href=\"#1-3-Nesterov\" class=\"headerlink\" title=\"1.3 Nesterov\"></a>1.3 Nesterov</h2><p>在 momentum 一节使用式 (1) (3) 进行介绍，其实是为了与 PyTorch <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71\" target=\"_blank\" rel=\"noopener\">源码</a> 或者 <a href=\"https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\" target=\"_blank\" rel=\"noopener\">文档</a> 对应，但是很多论文或者博客中更常用式 (3’) 的形式（将学习率放入 $v_{t+1}$ 的计算式中），<br>$$\\theta_{t+1}=\\theta_t+v_{t+1}$$<br>这里，$v_{t+1}$ 的定义有很多种，例如经典 momentum，NAG (Nesterov Accelerated Gradient) 等。</p>\n<h3 id=\"1-3-1-经典-momentum（CE）\"><a href=\"#1-3-1-经典-momentum（CE）\" class=\"headerlink\" title=\"1.3.1 经典 momentum（CE）\"></a>1.3.1 经典 momentum（CE）</h3><p>我们重写一遍经典 momentum 的参数更新公式（式 2），<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)<br>\\\\ \\theta_{t+1}&amp;=\\theta_t + \\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\\end{aligned}$$</p>\n<h3 id=\"1-3-2-NAG\"><a href=\"#1-3-2-NAG\" class=\"headerlink\" title=\"1.3.2 NAG\"></a>1.3.2 NAG</h3><p>NAG 一次迭代过程分为两步：</p>\n<ol>\n<li><p>梯度下降步骤</p>\n<p>$$\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t) \\qquad(4)$$</p>\n</li>\n<li><p>momentum</p>\n<p>$$ y_{t+1}=\\theta_{t+1} + \\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t) \\qquad(5)$$</p>\n</li>\n</ol>\n<p>初始时令 $y_0=\\theta_0$，即这两个变量起点相同。</p>\n<p>NAG 中我们对 $y$ 做梯度下降，得到的值为 $\\theta$ 的新值，而非 $y$ 新值，$y$ 的新值是在 $\\theta$ 的基础之上再增加 $\\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t)$ 这么多的更新量。如图 3，<br><img src=\"/images/pytorch/NAS_0.png\" alt=\"\"> <center>图 3. NAG 过程示意图</center><br>如果 $\\mu \\equiv 0$，NAG 就是普通的梯度下降 SD。</p>\n<p>注意，这里将 $\\mu, \\ \\epsilon$ 系数带上时刻下标。下面我们推导 velocity 的迭代计算式。</p>\n<h3 id=\"1-3-3-Sutskever-Nesterov-Momentum\"><a href=\"#1-3-3-Sutskever-Nesterov-Momentum\" class=\"headerlink\" title=\"1.3.3 Sutskever Nesterov Momentum\"></a>1.3.3 Sutskever Nesterov Momentum</h3><p>NAG 中参数 $\\theta$ 的更新在梯度下降之后，在 momentum 之前。现在我们根据 NAG 推导出 velocity 项。首先要说明的是，需要将 NAG 迭代过程的两个步骤顺序对换，即 <code>momentum-GD-momentum-GD-...</code> 的顺序。</p>\n<p>已知，<br>$$y_t=\\theta_t+\\mu_t \\cdot(\\theta_t-\\theta_{t-1})$$</p>\n<p>写成以下形式，<br>$$y_t=\\theta_t + \\mu_t \\cdot v_t$$</p>\n<p>可根据 (4) 式消去 $y_t$，需要注意的是，(4) 式表示 t 时刻迭代过程中的梯度下降步骤，到 Sutskever Nesterov Momentum 中则为 t-1 时刻迭代中的 momentum 步骤，即 $\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t)$，与上式联合可消去 $y_t$， 得<br>$$\\theta_{t+1} = \\theta_t+\\mu_t \\cdot v_t-\\epsilon_t \\cdot \\nabla f(\\theta_t + \\mu_t \\cdot v_t) \\qquad(6)$$</p>\n<p>于是，</p>\n<p>$$v_{t+1} = \\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\theta_t+\\mu_t \\cdot v_t)  \\qquad(7) $$</p>\n<p>图 4 是经典 momentum 与 NAG 方法的图示比较。<br><img src=\"/images/pytorch/NAG.png\" alt=\"\"> <center>图 4 </center></p>\n<h3 id=\"1-3-4-Bengio-Nesterov-Momentum\"><a href=\"#1-3-4-Bengio-Nesterov-Momentum\" class=\"headerlink\" title=\"1.3.4 Bengio Nesterov Momentum\"></a>1.3.4 Bengio Nesterov Momentum</h3><p>NAG 中我们的模型参数是 $\\theta$，但是其更新不是对自身做梯度下降，而是对 $y$ 做梯度下降，进一步地，$y$ 的更新则又反过来依赖于 $\\theta$ 的 momentum。</p>\n<p>定义一个新变量，表示经过 momentum 更新后的 $\\theta$ 值，或者更准确地讲，是 momentum 更新后的模型参数的值。</p>\n<p>$$\\Theta_{t-1}=\\theta_{t-1} + \\mu_{t-1} \\cdot v_{t-1}$$<br>这里可能感觉有点绕，一会 $\\theta$，一会 $\\Theta$，到底哪个是表示模型参数。我是这么理解的，初始时模型参数为 $\\theta_0$，此后更新迭代过程中，$\\Theta$ 才表示模型参数，$\\theta$ 只作为中间变量。</p>\n<p>根据 velocity 的定义 (7) 式，有<br>$$v_t=\\mu_{t-1} \\cdot v_{t-1} - \\epsilon_{t-1} \\cdot \\nabla f(\\Theta_{t-1})$$</p>\n<p>$v_t$ 依然是 $\\theta$ “中间”变量的更新量。</p>\n<p>根据 $\\Theta$ 定义，<br>$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}= \\theta_{t+1}<br>\\\\ \\Theta_t-\\mu_t \\cdot v_t= \\theta_t$$<br>以及 (6) 式，有<br>$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}=\\Theta_t-\\mu_t \\cdot v_t+\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)$$<br>化简得，<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot v_{t+1}-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$</p>\n<p>继续代入 (7) 式，有<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot[\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)]-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$</p>\n<p>展开得，<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t) \\qquad(8)$$</p>\n<p>写成 $\\Theta_{t+1}=\\Theta_t + V_{t+1}$ 的形式，于是<br>$$V_{t+1}=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$<br> 就是 $\\Theta$ 的更新量，等价于 <code>(3&#39;)</code> 式中的 $v_{t+1}$，对应到 (1) 式中的 $v_{t+1}$ 的形式，去掉 $\\epsilon$，以及 $-$ 变成 $+$，易得，<br>$$\\begin{aligned} V_{t+1}&amp;=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t+\\mu_{t+1} \\cdot \\nabla f(\\Theta_t)+ \\nabla f(\\Theta_t)<br>\\\\ &amp;=\\mu_{t+1} \\cdot [\\mu_t \\cdot v_t+ \\nabla f(\\Theta_t)] + \\nabla f(\\Theta_t)  \\end{aligned} \\qquad(9)$$<br>此时 $\\Theta$ 的更新为<br>$$\\Theta_{t+1}=\\Theta_t - \\epsilon_t \\cdot V_{t+1} \\qquad(10)$$</p>\n<p><strong>(9) 和 (10) 式就对应 PyTorch 源码中 <code>SGD.step</code> 在 <code>nesterov=True</code> 时的计算过程。</strong></p>\n<h1 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h1><p>[1] On the importance of initialization and momentum in deep learning. Ilya Sutskever</p>\n<p>[2] <a href=\"https://jlmelville.github.io/mize/mesterov.html\" target=\"_blank\" rel=\"noopener\">Nesterov Accelerated Gradient and Momentum</a></p>\n<h1 id=\"更多阅读\"><a href=\"#更多阅读\" class=\"headerlink\" title=\"更多阅读\"></a>更多阅读</h1><p>[1] <a href=\"https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/\" target=\"_blank\" rel=\"noopener\">ORF523: Nesterov’s Accelerated Gradient Descent</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-SGD\"><a href=\"#1-SGD\" class=\"headerlink\" title=\"1. SGD\"></a>1. SGD</h1>","more":"<h2 id=\"1-1-weight-decay\"><a href=\"#1-1-weight-decay\" class=\"headerlink\" title=\"1.1 weight decay\"></a>1.1 weight decay</h2><p>为了过拟合，通常在损失函数中增加正则项，记原来损失（MSE 或者 CE 等）为 $L_0$，那么添加正则项后的损失为，</p>\n<p>$$L=L_0+\\frac 1 2 \\lambda \\cdot |\\mathbf \\theta|_2^2$$<br>如图 1 所示，<br><img src=\"/images/pytorch/overfitting.png\" alt=\"\"> <center>图 1 过拟合图示（来源《Deep Learning with PyTorch》）</center><br>下半部分展示了过拟合的图，这种曲线的多项式比上图曲线的多项式，其系数 $\\mathbf \\theta$ 的绝对值更大（曲线的部分段变化更快，说明斜率绝对值更大，也就是 $|\\mathbf \\theta_i|$ 更大），所以增加正则项作为惩罚，其中 $\\lambda$ 作为平衡因子，也称 <code>weight decay</code>。于是，求导时，损失对每个权重参数的梯度多了一项<br>$$\\frac {\\partial L}{\\partial \\mathbf \\theta_i}=\\frac {\\partial L_0}{\\partial \\mathbf \\theta_i}+\\lambda \\cdot \\mathbf \\theta_i$$</p>\n<h2 id=\"1-2-momentum\"><a href=\"#1-2-momentum\" class=\"headerlink\" title=\"1.2 momentum\"></a>1.2 momentum</h2><p>使用 SGD 训练时，有时候下降比较慢，甚至会陷入导局部最小值中，如图 2 所示，引入 momentum 可以加快收敛速度，我们知道 SGD 的参数更新公式为<br>$$\\theta_{t+1} = \\theta_t -\\epsilon \\cdot d\\theta_t$$<br>而使用 momentum 的更新公式为<br>$$\\begin{aligned} v_{t+1} &amp; = \\mu \\cdot v_t + d\\theta_t<br>\\\\ \\theta_{t+1} &amp;= \\theta_t - \\epsilon \\cdot v_{t+1}=\\theta_t-\\epsilon \\cdot \\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t \\end{aligned} \\qquad(1)$$<br>其中 $\\theta_0$ 为初始权值参数值，$v_0=0$，$\\epsilon$ 为学习率，$\\mu$ 为 momentum 系数。从上两式中可见，如果当前 velocity（v 值） 与梯度方向一致，那么将会加快权值参数的变化量。在局部最小值附近（见图 2），由于 velocity 累计了之前的梯度，所以有望冲出局部最小值区域。<br><img src=\"/images/pytorch/momentum.png\" alt=\"\"> <center>图 2</center><br>有的地方写成如下形式：<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - d\\theta_t<br>\\\\\\theta_{t+1}&amp;=\\theta_t+\\epsilon \\cdot v_{t+1}=\\theta_t +\\epsilon \\cdot \\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t \\end{aligned}\\qquad(2)$$<br>实际上，当 $v_0=0$ 时，这两组更新公式本质相同。</p>\n<p>caffe 框架以及在 Sutskever. [1] 中，更新公式为：<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t + \\epsilon \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}&amp;=\\theta_t - v_{t+1}=\\theta_t-\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3)$$</p>\n<p>或者<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - \\epsilon \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}&amp;=\\theta_t + v_{t+1}=\\theta_t+\\mu \\cdot v_t-\\epsilon \\cdot d\\theta_t\\end{aligned} \\qquad(3’)$$</p>\n<p>假设学习率 $\\epsilon$ 保持不变，那么在 $v_0=0$ 时，(3) 式中的 $v_{t+1}$ 是 (1) 式中的 $\\epsilon$ 倍（在其他变量均相同的情况下），<br>$$\\begin{aligned}v_1^{(3)}&amp;=\\epsilon \\cdot d\\theta_0 = \\epsilon \\cdot v_1^{(1)}<br>\\\\ v_2^{(3)}&amp; = \\mu \\cdot v_1^{(3)}+\\epsilon \\cdot d\\theta_1=\\epsilon \\cdot [\\mu \\cdot v_1^{(1)}+d\\theta_1]=\\epsilon \\cdot v_2^{(1)}<br>\\\\ &amp;\\cdots \\end{aligned}$$</p>\n<p>所以 (1) 式中更新 $\\theta_{t+1}$ 时 $v_{t+1}$ 前面添加了系数 $\\epsilon$ 后，(1) 和 (3) 也是等价的，但是前提条件是: 1. $v_0=0$；2. 学习率 $\\epsilon$ 保持不变。</p>\n<p>随着训练 epoch 增加，学习率可能会衰减，例如衰减为 10%，那么 (1) 和 (3) 会出现不同，我们继续写下迭代计算过程以探究为何会发生不同。记 在 t+1 时刻发生 $\\epsilon$ 衰减，衰减前后分别为 $\\epsilon_1, \\ \\epsilon_2$，且 $\\epsilon_2 = 0.1 \\epsilon_1$，<br>$$\\begin{aligned}v_t^{(3)} &amp;= \\epsilon_1 \\cdot v_t^{(1)}<br>\\\\ \\theta_{t+1}^{(3)}&amp;=\\theta_t- \\mu \\cdot v_t^{(3)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t- \\epsilon_1 \\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t<br>\\\\ \\theta_{t+1}^{(1)}&amp;=\\theta_t-\\epsilon_2\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t=\\theta_t-0.1\\epsilon_1\\cdot \\mu \\cdot v_t^{(1)} - \\epsilon_2 \\cdot d\\theta_t\\end{aligned}$$</p>\n<p>显然，(1) 式的更新方式在学习率衰减时能够有更加明显的体现，参数更新量明显变小，而 (3) 式此时的 velocity 相对较大，参数更新的量没有明显变小。当然随着训练迭代的推进，(3) 式的参数更新量也会逐渐变小，这是因为 velocity 不断更新后，逐渐被较新的 $\\epsilon_2 \\cdot d\\theta$ 主导，而先前累积的 $v_t^{(3)}$ 占比会越来越小，<br>$$\\begin{aligned} v_{t+n}&amp;=\\mu \\cdot v_{t+n-1}+\\epsilon_2 d\\theta_{t+n-1}\\\\ &amp;=\\mu^2 \\cdot v_{t+n-2}+\\mu \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-2}+\\epsilon_2 \\cdot d\\theta_{t+n-1}<br>\\\\&amp;=\\cdots<br>\\\\&amp;=\\mu^n \\cdot v_t + \\mu^{n-1} \\cdot \\epsilon_2 \\cdot d\\theta_{t}+\\mu^{n-2} \\cdot \\epsilon_2 \\cdot  d\\theta_{t+1} + \\cdots + \\mu^0 \\cdot \\epsilon_2 \\cdot d\\theta_{t+n-1}\\end{aligned}$$<br>由于 $\\mu &lt;1$，当 $n$ 较大时，上式第一项即 $v_t$ 对 velocity 贡献可以忽略。</p>\n<h3 id=\"1-2-1-dampening\"><a href=\"#1-2-1-dampening\" class=\"headerlink\" title=\"1.2.1 dampening\"></a>1.2.1 dampening</h3><p>阅读 PyTorch 中这部分的<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71\" target=\"_blank\" rel=\"noopener\">源码</a>，发现还使用了一个参数 <code>dampening</code>，这个参数指使用 momentum 更新时，对当前梯度的抑制，即 (1) 式中 velocity 更新变为<br>$$v_{t+1} = \\mu \\cdot v_t + \\text{dampening} \\cdot d\\theta_t$$<br>其实很多地方写成 $v_{t+1} = \\mu \\cdot v_t + (1-\\mu) \\cdot d\\theta_t$，由于 $0 \\le \\mu &lt; 1$，这表示 $v_{t+1}$ 在 $\\min (v_t, d\\theta_t)$ 与 $\\max (v_t, d\\theta_t)$ 之间。不过，实际计算中，<code>dampening</code> 常使用默认值 <code>0</code>。</p>\n<h2 id=\"1-3-Nesterov\"><a href=\"#1-3-Nesterov\" class=\"headerlink\" title=\"1.3 Nesterov\"></a>1.3 Nesterov</h2><p>在 momentum 一节使用式 (1) (3) 进行介绍，其实是为了与 PyTorch <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L71\" target=\"_blank\" rel=\"noopener\">源码</a> 或者 <a href=\"https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\" target=\"_blank\" rel=\"noopener\">文档</a> 对应，但是很多论文或者博客中更常用式 (3’) 的形式（将学习率放入 $v_{t+1}$ 的计算式中），<br>$$\\theta_{t+1}=\\theta_t+v_{t+1}$$<br>这里，$v_{t+1}$ 的定义有很多种，例如经典 momentum，NAG (Nesterov Accelerated Gradient) 等。</p>\n<h3 id=\"1-3-1-经典-momentum（CE）\"><a href=\"#1-3-1-经典-momentum（CE）\" class=\"headerlink\" title=\"1.3.1 经典 momentum（CE）\"></a>1.3.1 经典 momentum（CE）</h3><p>我们重写一遍经典 momentum 的参数更新公式（式 2），<br>$$\\begin{aligned}v_{t+1}&amp;=\\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)<br>\\\\ \\theta_{t+1}&amp;=\\theta_t + \\mu \\cdot v_t - \\epsilon \\cdot \\nabla f(\\theta_t)\\end{aligned}$$</p>\n<h3 id=\"1-3-2-NAG\"><a href=\"#1-3-2-NAG\" class=\"headerlink\" title=\"1.3.2 NAG\"></a>1.3.2 NAG</h3><p>NAG 一次迭代过程分为两步：</p>\n<ol>\n<li><p>梯度下降步骤</p>\n<p>$$\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t) \\qquad(4)$$</p>\n</li>\n<li><p>momentum</p>\n<p>$$ y_{t+1}=\\theta_{t+1} + \\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t) \\qquad(5)$$</p>\n</li>\n</ol>\n<p>初始时令 $y_0=\\theta_0$，即这两个变量起点相同。</p>\n<p>NAG 中我们对 $y$ 做梯度下降，得到的值为 $\\theta$ 的新值，而非 $y$ 新值，$y$ 的新值是在 $\\theta$ 的基础之上再增加 $\\mu_{t+1} \\cdot (\\theta_{t+1}-\\theta_t)$ 这么多的更新量。如图 3，<br><img src=\"/images/pytorch/NAS_0.png\" alt=\"\"> <center>图 3. NAG 过程示意图</center><br>如果 $\\mu \\equiv 0$，NAG 就是普通的梯度下降 SD。</p>\n<p>注意，这里将 $\\mu, \\ \\epsilon$ 系数带上时刻下标。下面我们推导 velocity 的迭代计算式。</p>\n<h3 id=\"1-3-3-Sutskever-Nesterov-Momentum\"><a href=\"#1-3-3-Sutskever-Nesterov-Momentum\" class=\"headerlink\" title=\"1.3.3 Sutskever Nesterov Momentum\"></a>1.3.3 Sutskever Nesterov Momentum</h3><p>NAG 中参数 $\\theta$ 的更新在梯度下降之后，在 momentum 之前。现在我们根据 NAG 推导出 velocity 项。首先要说明的是，需要将 NAG 迭代过程的两个步骤顺序对换，即 <code>momentum-GD-momentum-GD-...</code> 的顺序。</p>\n<p>已知，<br>$$y_t=\\theta_t+\\mu_t \\cdot(\\theta_t-\\theta_{t-1})$$</p>\n<p>写成以下形式，<br>$$y_t=\\theta_t + \\mu_t \\cdot v_t$$</p>\n<p>可根据 (4) 式消去 $y_t$，需要注意的是，(4) 式表示 t 时刻迭代过程中的梯度下降步骤，到 Sutskever Nesterov Momentum 中则为 t-1 时刻迭代中的 momentum 步骤，即 $\\theta_{t+1} = y_t - \\epsilon_t \\cdot \\nabla f(y_t)$，与上式联合可消去 $y_t$， 得<br>$$\\theta_{t+1} = \\theta_t+\\mu_t \\cdot v_t-\\epsilon_t \\cdot \\nabla f(\\theta_t + \\mu_t \\cdot v_t) \\qquad(6)$$</p>\n<p>于是，</p>\n<p>$$v_{t+1} = \\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\theta_t+\\mu_t \\cdot v_t)  \\qquad(7) $$</p>\n<p>图 4 是经典 momentum 与 NAG 方法的图示比较。<br><img src=\"/images/pytorch/NAG.png\" alt=\"\"> <center>图 4 </center></p>\n<h3 id=\"1-3-4-Bengio-Nesterov-Momentum\"><a href=\"#1-3-4-Bengio-Nesterov-Momentum\" class=\"headerlink\" title=\"1.3.4 Bengio Nesterov Momentum\"></a>1.3.4 Bengio Nesterov Momentum</h3><p>NAG 中我们的模型参数是 $\\theta$，但是其更新不是对自身做梯度下降，而是对 $y$ 做梯度下降，进一步地，$y$ 的更新则又反过来依赖于 $\\theta$ 的 momentum。</p>\n<p>定义一个新变量，表示经过 momentum 更新后的 $\\theta$ 值，或者更准确地讲，是 momentum 更新后的模型参数的值。</p>\n<p>$$\\Theta_{t-1}=\\theta_{t-1} + \\mu_{t-1} \\cdot v_{t-1}$$<br>这里可能感觉有点绕，一会 $\\theta$，一会 $\\Theta$，到底哪个是表示模型参数。我是这么理解的，初始时模型参数为 $\\theta_0$，此后更新迭代过程中，$\\Theta$ 才表示模型参数，$\\theta$ 只作为中间变量。</p>\n<p>根据 velocity 的定义 (7) 式，有<br>$$v_t=\\mu_{t-1} \\cdot v_{t-1} - \\epsilon_{t-1} \\cdot \\nabla f(\\Theta_{t-1})$$</p>\n<p>$v_t$ 依然是 $\\theta$ “中间”变量的更新量。</p>\n<p>根据 $\\Theta$ 定义，<br>$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}= \\theta_{t+1}<br>\\\\ \\Theta_t-\\mu_t \\cdot v_t= \\theta_t$$<br>以及 (6) 式，有<br>$$\\Theta_{t+1}-\\mu_{t+1} \\cdot v_{t+1}=\\Theta_t-\\mu_t \\cdot v_t+\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)$$<br>化简得，<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot v_{t+1}-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$</p>\n<p>继续代入 (7) 式，有<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot[\\mu_t \\cdot v_t - \\epsilon_t \\cdot \\nabla f(\\Theta_t)]-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$</p>\n<p>展开得，<br>$$\\Theta_{t+1}=\\Theta_t+\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t) \\qquad(8)$$</p>\n<p>写成 $\\Theta_{t+1}=\\Theta_t + V_{t+1}$ 的形式，于是<br>$$V_{t+1}=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t-\\mu_{t+1} \\cdot \\epsilon_t \\cdot \\nabla f(\\Theta_t)-\\epsilon_t \\cdot \\nabla f(\\Theta_t)$$<br> 就是 $\\Theta$ 的更新量，等价于 <code>(3&#39;)</code> 式中的 $v_{t+1}$，对应到 (1) 式中的 $v_{t+1}$ 的形式，去掉 $\\epsilon$，以及 $-$ 变成 $+$，易得，<br>$$\\begin{aligned} V_{t+1}&amp;=\\mu_{t+1} \\cdot \\mu_t \\cdot v_t+\\mu_{t+1} \\cdot \\nabla f(\\Theta_t)+ \\nabla f(\\Theta_t)<br>\\\\ &amp;=\\mu_{t+1} \\cdot [\\mu_t \\cdot v_t+ \\nabla f(\\Theta_t)] + \\nabla f(\\Theta_t)  \\end{aligned} \\qquad(9)$$<br>此时 $\\Theta$ 的更新为<br>$$\\Theta_{t+1}=\\Theta_t - \\epsilon_t \\cdot V_{t+1} \\qquad(10)$$</p>\n<p><strong>(9) 和 (10) 式就对应 PyTorch 源码中 <code>SGD.step</code> 在 <code>nesterov=True</code> 时的计算过程。</strong></p>\n<h1 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h1><p>[1] On the importance of initialization and momentum in deep learning. Ilya Sutskever</p>\n<p>[2] <a href=\"https://jlmelville.github.io/mize/mesterov.html\" target=\"_blank\" rel=\"noopener\">Nesterov Accelerated Gradient and Momentum</a></p>\n<h1 id=\"更多阅读\"><a href=\"#更多阅读\" class=\"headerlink\" title=\"更多阅读\"></a>更多阅读</h1><p>[1] <a href=\"https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/\" target=\"_blank\" rel=\"noopener\">ORF523: Nesterov’s Accelerated Gradient Descent</a></p>"},{"title":"PyTorch-1","p":"pytorch/PyTorch-1","date":"2019-06-12T11:17:11.000Z","_content":"# 安装\n一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。\n<!-- more -->\n当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。\n\n下载源码\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n由于使用了子模块所以增加--recursive选项，记pytorch的root dir为$ROOT_DIR。\n\n根据安装步骤进行自上而下的阅读。Linux下安装使用命令\n```\ncd pytorch\npython setup.py install\n```\npytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库\n\n### build_deps()\n这个方法内部关键的一步为\n```\nbuild_caffe2(...)\n```\n查看这个方法的定义，发现build_caffe2做了如下几件事：\n1. run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为`$ROOD_DIR/build`， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt\n2. 在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）\n3. 将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成\n\n这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\n这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。\n\n另外，top level 中CMakeLists.txt中有这么一行\n```\ninclude(cmake/Dependencies.cmake)\n```\n这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如`$ROOT_DIR/third_party`或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：\n1. 非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\n这个find_package告诉我们去查看`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n这就相当于能生成-lopenblas这样的链接选项。\n\n我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\n其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中\n\n以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。\n\n### setup()\nsetup方法（可以参考[setup()](https://docs.python.org/3/distutils/apiref.html)），其中几个值得说明的参数：\n1. ext_modules 有5个扩展库分别如下：\n- torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir\n- torch._dl 非WINDOWS平台下才有，指定了C源文件\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\n后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。\n\n2. cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含\n\n- create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据`$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)`这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n其中每个{...}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。\n- run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑\n- build_extensions 生成由ext_modules指定的python扩展库所用的方法\n\next_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。\n\n然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为`$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是.../miniconda3/lib/python3.7/site-packages/caffe2/python/目录。\n\n3. packages 指定安装到python 的site-packages下的包\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\n由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含__init__.py，所以将caffe2和torch两个包安装到site-packages下。\n\n现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。\n\n### 整理\n\n以上就是pytorch安装过程，主要分为两部分:\n\n1. 使用CMake生成c++库，对应build_deps()这个方法执行\n2. 使用python的setup方法生成扩展库，主要是build_ext。\n\n根据上面两点，重新整理一遍。\n```\ntop-level的CMakeLists.txt中\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\n于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，\n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\n安装目录则寻找对应的install语句。此外，文件中还有一句\n```\nadd_subdirectory(../torch torch)\n```\n（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）\n\n于是查看torch目录下的CMakeLists.txt， 其中生成的库为\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n然后根据其中的\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\n有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：\n- 预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 `-I<include dir>flag`。\n- 本项目内包含的库。包括：\n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # 添加tbb库\n```\n(2) qnnpack\n```\n# 添加 qnnpack 库\n# source directory为${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory为${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。\n(3) nnpack\n```\n# 添加 nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\n跳至nnpack.cmake文件，发现其中包含\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。\n\n(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置\n\n(5) LMDB。使用如下语句\n```\nfind_package(LMDB)\n```\n所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\n类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。\n\n(6) pybind11。在Dependencies.cmake添加pybind11依赖，\n```\nfind_package(pybind11 CONFIG)# 配置模式下寻找，然而没有${pybind11_DIR}，也没有pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # 继续module模式下寻找\nendif()\n```\n虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\n如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDA。在Dependencies.cmake中有\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\n在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\n其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\n保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。\n\n(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。\n\nDependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. 生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。\n\n- torch._C 链接的两个库为\n```\nmain_libraries=['shm', 'torch_python']\n```\n显然前面已经生成了这两个库。而使用的源文件则为\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了<dlfcn.h>中的三个常量到torch._dl库中，如下\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。\n\n### 还有...\n可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。","source":"_posts/pytorch/PyTorch-1.md","raw":"---\ntitle: PyTorch-1\np: pytorch/PyTorch-1\ndate: 2019-06-12 19:17:11\ntags: PyTorch\ncategories: DL Framework\n---\n# 安装\n一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。\n<!-- more -->\n当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。\n\n下载源码\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n由于使用了子模块所以增加--recursive选项，记pytorch的root dir为$ROOT_DIR。\n\n根据安装步骤进行自上而下的阅读。Linux下安装使用命令\n```\ncd pytorch\npython setup.py install\n```\npytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库\n\n### build_deps()\n这个方法内部关键的一步为\n```\nbuild_caffe2(...)\n```\n查看这个方法的定义，发现build_caffe2做了如下几件事：\n1. run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为`$ROOD_DIR/build`， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt\n2. 在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）\n3. 将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成\n\n这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\n这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。\n\n另外，top level 中CMakeLists.txt中有这么一行\n```\ninclude(cmake/Dependencies.cmake)\n```\n这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如`$ROOT_DIR/third_party`或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：\n1. 非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\n这个find_package告诉我们去查看`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n这就相当于能生成-lopenblas这样的链接选项。\n\n我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\n其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中\n\n以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。\n\n### setup()\nsetup方法（可以参考[setup()](https://docs.python.org/3/distutils/apiref.html)），其中几个值得说明的参数：\n1. ext_modules 有5个扩展库分别如下：\n- torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir\n- torch._dl 非WINDOWS平台下才有，指定了C源文件\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\n后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。\n\n2. cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含\n\n- create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据`$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)`这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n其中每个{...}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。\n- run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑\n- build_extensions 生成由ext_modules指定的python扩展库所用的方法\n\next_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。\n\n然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为`$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是.../miniconda3/lib/python3.7/site-packages/caffe2/python/目录。\n\n3. packages 指定安装到python 的site-packages下的包\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\n由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含__init__.py，所以将caffe2和torch两个包安装到site-packages下。\n\n现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。\n\n### 整理\n\n以上就是pytorch安装过程，主要分为两部分:\n\n1. 使用CMake生成c++库，对应build_deps()这个方法执行\n2. 使用python的setup方法生成扩展库，主要是build_ext。\n\n根据上面两点，重新整理一遍。\n```\ntop-level的CMakeLists.txt中\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\n于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，\n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\n安装目录则寻找对应的install语句。此外，文件中还有一句\n```\nadd_subdirectory(../torch torch)\n```\n（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）\n\n于是查看torch目录下的CMakeLists.txt， 其中生成的库为\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n然后根据其中的\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\n有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：\n- 预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 `-I<include dir>flag`。\n- 本项目内包含的库。包括：\n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # 添加tbb库\n```\n(2) qnnpack\n```\n# 添加 qnnpack 库\n# source directory为${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory为${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。\n(3) nnpack\n```\n# 添加 nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\n跳至nnpack.cmake文件，发现其中包含\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。\n\n(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置\n\n(5) LMDB。使用如下语句\n```\nfind_package(LMDB)\n```\n所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\n类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。\n\n(6) pybind11。在Dependencies.cmake添加pybind11依赖，\n```\nfind_package(pybind11 CONFIG)# 配置模式下寻找，然而没有${pybind11_DIR}，也没有pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # 继续module模式下寻找\nendif()\n```\n虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\n如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDA。在Dependencies.cmake中有\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\n在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\n其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\n保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。\n\n(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。\n\nDependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. 生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。\n\n- torch._C 链接的两个库为\n```\nmain_libraries=['shm', 'torch_python']\n```\n显然前面已经生成了这两个库。而使用的源文件则为\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了<dlfcn.h>中的三个常量到torch._dl库中，如下\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。\n\n### 还有...\n可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。","slug":"pytorch/PyTorch-1","published":1,"updated":"2020-04-24T10:34:18.685Z","_id":"ck9dzcj4w002mgga63195h8sz","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h1><p>一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。</p>\n<a id=\"more\"></a>\n<p>当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。</p>\n<p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch</span><br></pre></td></tr></table></figure>\n\n<p>由于使用了子模块所以增加–recursive选项，记pytorch的root dir为$ROOT_DIR。</p>\n<p>根据安装步骤进行自上而下的阅读。Linux下安装使用命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<p>pytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库</p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p>这个方法内部关键的一步为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n<p>查看这个方法的定义，发现build_caffe2做了如下几件事：</p>\n<ol>\n<li>run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为<code>$ROOD_DIR/build</code>， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt</li>\n<li>在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）</li>\n<li>将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成</li>\n</ol>\n<p>这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n<p>这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。</p>\n<p>另外，top level 中CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake&#x2F;Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n<p>这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如<code>$ROOT_DIR/third_party</code>或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：</p>\n<ol>\n<li>非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n这个find_package告诉我们去查看<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n这就相当于能生成-lopenblas这样的链接选项。</li>\n</ol>\n<p>我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;&#x2F;caffe2&#x2F;python&quot;)</span><br></pre></td></tr></table></figure>\n<p>其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中</p>\n<p>以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup方法（可以参考<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a>），其中几个值得说明的参数：</p>\n<ol>\n<li>ext_modules 有5个扩展库分别如下：</li>\n</ol>\n<ul>\n<li>torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir</li>\n<li>torch._dl 非WINDOWS平台下才有，指定了C源文件</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。</p>\n<ol start=\"2\">\n<li>cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含</li>\n</ol>\n<ul>\n<li>create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据<code>$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path&#x2F;to&#x2F;root&gt;build&#x2F;third_party&#x2F;protobuf&#x2F;cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;c++ ... -I&lt;path&#x2F;to&#x2F;root&gt;&#x2F;third_party&#x2F;protobuf&#x2F;src ... </span><br><span class=\"line\">                -o CMakeFiles&#x2F;libprotobuf.dir&#x2F;__&#x2F;src&#x2F;google&#x2F;protobuf&#x2F;arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path&#x2F;to&#x2F;root&gt;&#x2F;third_party&#x2F;protobuf&#x2F;src&#x2F;google&#x2F;protobuf&#x2F;arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n其中每个{…}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。</li>\n<li>run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑</li>\n<li>build_extensions 生成由ext_modules指定的python扩展库所用的方法</li>\n</ul>\n<p>ext_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。</p>\n<p>然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为<code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是…/miniconda3/lib/python3.7/site-packages/caffe2/python/目录。</p>\n<ol start=\"3\">\n<li>packages 指定安装到python 的site-packages下的包<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages &#x3D; find_packages(exclude&#x3D;[&#39;tools&#39;, &#39;tools.*&#39;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含<strong>init</strong>.py，所以将caffe2和torch两个包安装到site-packages下。</p>\n<p>现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。</p>\n<h3 id=\"整理\"><a href=\"#整理\" class=\"headerlink\" title=\"整理\"></a>整理</h3><p>以上就是pytorch安装过程，主要分为两部分:</p>\n<ol>\n<li>使用CMake生成c++库，对应build_deps()这个方法执行</li>\n<li>使用python的setup方法生成扩展库，主要是build_ext。</li>\n</ol>\n<p>根据上面两点，重新整理一遍。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-level的CMakeLists.txt中</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n<p>于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;&#x2F;csrc&#x2F;jit&#x2F;fuser&#x2F;cuda&#x2F;thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>安装目录则寻找对应的install语句。此外，文件中还有一句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(..&#x2F;torch torch)</span><br></pre></td></tr></table></figure>\n<p>（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）</p>\n<p>于是查看torch目录下的CMakeLists.txt， 其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>然后根据其中的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;&#x2F;lib&#x2F;$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n<p>继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n<p>有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：</p>\n<ul>\n<li>预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 <code>-I&lt;include dir&gt;flag</code>。</li>\n<li>本项目内包含的库。包括：<br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;cpu&#x2F;tbb)    # 添加tbb库</span><br></pre></td></tr></table></figure>\n(2) qnnpack<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 qnnpack 库</span><br><span class=\"line\"># source directory为$&#123;PROJECT_SOURCE_DIR&#125;&#x2F;third_party&#x2F;QNNPACK</span><br><span class=\"line\"># output directory为$&#123;PROJECT_BINARY_DIR&#125;&#x2F;confu-deps&#x2F;QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;&#x2F;QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。<br>(3) nnpack<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;External&#x2F;nnpack.cmake)</span><br></pre></td></tr></table></figure>\n跳至nnpack.cmake文件，发现其中包含<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;&#x2F;NNPACK)</span><br></pre></td></tr></table></figure>\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。</li>\n</ul>\n<p>(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置</p>\n<p>(5) LMDB。使用如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n<p>所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n<p>类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。</p>\n<p>(6) pybind11。在Dependencies.cmake添加pybind11依赖，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# 配置模式下寻找，然而没有$&#123;pybind11_DIR&#125;，也没有pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # 继续module模式下寻找</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n<p>虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;third_party&#x2F;pybind11&#x2F;include)</span><br></pre></td></tr></table></figure>\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n<p>如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n<p>(8) CUDA。在Dependencies.cmake中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;public&#x2F;cuda.cmake)</span><br></pre></td></tr></table></figure>\n<p>在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n<p>其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n<p>保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。</p>\n<p>(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。</p>\n<p>Dependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。</li>\n</ol>\n<ul>\n<li>torch._C 链接的两个库为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries&#x3D;[&#39;shm&#39;, &#39;torch_python&#39;]</span><br></pre></td></tr></table></figure>\n显然前面已经生成了这两个库。而使用的源文件则为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources&#x3D;[&quot;torch&#x2F;csrc&#x2F;stub.cpp&quot;]</span><br></pre></td></tr></table></figure></li>\n<li>torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了&lt;dlfcn.h&gt;中的三个常量到torch._dl库中，如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL&#x3D;0x100</span><br><span class=\"line\">RTLD_NOW   &#x3D;0x2</span><br><span class=\"line\">RTLD_LAZY  &#x3D;0x1</span><br></pre></td></tr></table></figure>\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。</li>\n</ul>\n<h3 id=\"还有…\"><a href=\"#还有…\" class=\"headerlink\" title=\"还有…\"></a>还有…</h3><p>可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h1><p>一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。</p>","more":"<p>当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。</p>\n<p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch</span><br></pre></td></tr></table></figure>\n\n<p>由于使用了子模块所以增加–recursive选项，记pytorch的root dir为$ROOT_DIR。</p>\n<p>根据安装步骤进行自上而下的阅读。Linux下安装使用命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<p>pytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库</p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p>这个方法内部关键的一步为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n<p>查看这个方法的定义，发现build_caffe2做了如下几件事：</p>\n<ol>\n<li>run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为<code>$ROOD_DIR/build</code>， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt</li>\n<li>在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）</li>\n<li>将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成</li>\n</ol>\n<p>这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n<p>这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。</p>\n<p>另外，top level 中CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake&#x2F;Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n<p>这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如<code>$ROOT_DIR/third_party</code>或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：</p>\n<ol>\n<li>非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n这个find_package告诉我们去查看<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n这就相当于能生成-lopenblas这样的链接选项。</li>\n</ol>\n<p>我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;&#x2F;caffe2&#x2F;python&quot;)</span><br></pre></td></tr></table></figure>\n<p>其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中</p>\n<p>以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup方法（可以参考<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a>），其中几个值得说明的参数：</p>\n<ol>\n<li>ext_modules 有5个扩展库分别如下：</li>\n</ol>\n<ul>\n<li>torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir</li>\n<li>torch._dl 非WINDOWS平台下才有，指定了C源文件</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。</p>\n<ol start=\"2\">\n<li>cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含</li>\n</ol>\n<ul>\n<li>create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据<code>$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path&#x2F;to&#x2F;root&gt;build&#x2F;third_party&#x2F;protobuf&#x2F;cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;c++ ... -I&lt;path&#x2F;to&#x2F;root&gt;&#x2F;third_party&#x2F;protobuf&#x2F;src ... </span><br><span class=\"line\">                -o CMakeFiles&#x2F;libprotobuf.dir&#x2F;__&#x2F;src&#x2F;google&#x2F;protobuf&#x2F;arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path&#x2F;to&#x2F;root&gt;&#x2F;third_party&#x2F;protobuf&#x2F;src&#x2F;google&#x2F;protobuf&#x2F;arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n其中每个{…}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。</li>\n<li>run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑</li>\n<li>build_extensions 生成由ext_modules指定的python扩展库所用的方法</li>\n</ul>\n<p>ext_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。</p>\n<p>然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为<code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是…/miniconda3/lib/python3.7/site-packages/caffe2/python/目录。</p>\n<ol start=\"3\">\n<li>packages 指定安装到python 的site-packages下的包<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages &#x3D; find_packages(exclude&#x3D;[&#39;tools&#39;, &#39;tools.*&#39;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含<strong>init</strong>.py，所以将caffe2和torch两个包安装到site-packages下。</p>\n<p>现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。</p>\n<h3 id=\"整理\"><a href=\"#整理\" class=\"headerlink\" title=\"整理\"></a>整理</h3><p>以上就是pytorch安装过程，主要分为两部分:</p>\n<ol>\n<li>使用CMake生成c++库，对应build_deps()这个方法执行</li>\n<li>使用python的setup方法生成扩展库，主要是build_ext。</li>\n</ol>\n<p>根据上面两点，重新整理一遍。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-level的CMakeLists.txt中</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n<p>于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;&#x2F;csrc&#x2F;jit&#x2F;fuser&#x2F;cuda&#x2F;thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>安装目录则寻找对应的install语句。此外，文件中还有一句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(..&#x2F;torch torch)</span><br></pre></td></tr></table></figure>\n<p>（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）</p>\n<p>于是查看torch目录下的CMakeLists.txt， 其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>然后根据其中的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;&#x2F;lib&#x2F;$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n<p>继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n<p>有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：</p>\n<ul>\n<li>预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 <code>-I&lt;include dir&gt;flag</code>。</li>\n<li>本项目内包含的库。包括：<br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;cpu&#x2F;tbb)    # 添加tbb库</span><br></pre></td></tr></table></figure>\n(2) qnnpack<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 qnnpack 库</span><br><span class=\"line\"># source directory为$&#123;PROJECT_SOURCE_DIR&#125;&#x2F;third_party&#x2F;QNNPACK</span><br><span class=\"line\"># output directory为$&#123;PROJECT_BINARY_DIR&#125;&#x2F;confu-deps&#x2F;QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;&#x2F;QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。<br>(3) nnpack<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;External&#x2F;nnpack.cmake)</span><br></pre></td></tr></table></figure>\n跳至nnpack.cmake文件，发现其中包含<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;&#x2F;NNPACK)</span><br></pre></td></tr></table></figure>\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。</li>\n</ul>\n<p>(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置</p>\n<p>(5) LMDB。使用如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n<p>所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n<p>类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。</p>\n<p>(6) pybind11。在Dependencies.cmake添加pybind11依赖，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# 配置模式下寻找，然而没有$&#123;pybind11_DIR&#125;，也没有pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # 继续module模式下寻找</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n<p>虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;third_party&#x2F;pybind11&#x2F;include)</span><br></pre></td></tr></table></figure>\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n<p>如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n<p>(8) CUDA。在Dependencies.cmake中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;public&#x2F;cuda.cmake)</span><br></pre></td></tr></table></figure>\n<p>在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n<p>其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n<p>保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。</p>\n<p>(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。</p>\n<p>Dependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。</li>\n</ol>\n<ul>\n<li>torch._C 链接的两个库为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries&#x3D;[&#39;shm&#39;, &#39;torch_python&#39;]</span><br></pre></td></tr></table></figure>\n显然前面已经生成了这两个库。而使用的源文件则为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources&#x3D;[&quot;torch&#x2F;csrc&#x2F;stub.cpp&quot;]</span><br></pre></td></tr></table></figure></li>\n<li>torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了&lt;dlfcn.h&gt;中的三个常量到torch._dl库中，如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL&#x3D;0x100</span><br><span class=\"line\">RTLD_NOW   &#x3D;0x2</span><br><span class=\"line\">RTLD_LAZY  &#x3D;0x1</span><br></pre></td></tr></table></figure>\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。</li>\n</ul>\n<h3 id=\"还有…\"><a href=\"#还有…\" class=\"headerlink\" title=\"还有…\"></a>还有…</h3><p>可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。</p>"},{"title":"PyTorch 方法总结","p":"pytorch/PyTorch-mtd","date":"2019-11-01T03:26:25.000Z","mathjax":true,"_content":"\n# 1. Fold / Unfold\n## Fold\n这是 torch.nn.Fold 类。\n\n首先我们来复习一下卷积过程，设输入 size 为 `(N,C,H,W)`，卷积 kernel size 为 `(C,h,w)`，碰撞系数为 `d`，padding 为 `p`，stride 记为 `s`，那么整个过程相当于将 `Cxhxw` 大小的数据块在输入数据中滑动，每一次滑动做一次卷积，记共有 `L` 次卷积，即，从输入数据中切分出 `L` 个数据块与卷积核做卷积，当然每个数据块的大小与卷积核相同，为 `(C,h,w)`，最后得到的输出 map 大小为\n<!-- more -->\n$$H_o= \\frac {H - [d(h-1)+1] + 2p} {s}+1\n\\\\\\\\ W_o= \\frac {W - [d(w-1)+1] + 2p} {s}+1$$\n因为每一次卷积得到的值均作为输出 map 上的一点，故 `L` 为\n$$L=H_o * W_o=\\left(\\frac {H - [d(h-1)+1] + 2p} {s}+1\\right) \\left(\\frac {W - [d(w-1)+1] + 2p} {s}+1\\right)$$\n\n好了，现在 Fold 要做的事情是反过来的，已知 fold 的输入为 `L` 个数据块，大小为 `(N,C*h*w,L)`，有关的构造参数为卷积核 size `(h,w)`，dilation，padding，stride，以及，指定最终的（Fold）输出大小 `(H,W)`，注意，Fold 做的事情是反过来的，也就是说，从 `L` 个数据块中恢复出原来普通卷积的输入 map 的大小，即 `(H,W)`，不是做完卷积之后的输出 map 的大小，记住，__Fold 的输出是普通卷积的输入__。\n\nFold 的这些构造参数指明了卷积核大小，以及卷积输入的大小，然后根据其（这里指 Fold）输入 `L` 个数据块的 tensor，size 为 `(N,C*h*w,L)`，恢复出卷积输入的 tensor，因为 Fold 的构造参数中指定了 卷据输入 map 的 `(H,W)`，而批大小 `N` 也已知，所以要求出通道 `C`，根据 Fold 输入 tensor 的 第二个维度值 `C*h*w` 以及 Fold 的构造参数中卷积核大小 `(h,w)` 很容易得到通道 `C`。\n\n先使用 PyTorch 文档中的例子加以说明，\n```python\n>>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))  # (H,W)=(4,5), (h,w)=(2,2)\n>>> input = torch.randn(1, 3 * 2 * 2, 12)   # (N, C*h*w,L)\n>>> output = fold(input)\n>>> output.size()\ntorch.Size([1, 3, 4, 5])    # (N,C,H,W)\n```\n\n将数据维度从 `H,W` 扩展到更多维度，就是 PyTorch 文档中关于 `L` 的计算式了，如下\n$$L=\\prod_d \\lfloor \\frac {\\text{output\\_size} [d] + 2 \\times \\text{padding}[d]-\\text{dilation}[d] \\times (\\text{kernel\\_size}[d]-1) -1} {\\text{stride}[d]} +1 \\rfloor$$\n\n__总结：__\n\nFold 的输入 size 为 $(N, C \\times \\prod(\\text{kernel\\_size}), L)$，输出 size 为 $(N,C, \\text{output\\_size}[0], \\text{output\\_size}[1], ...)$\n\n## Unfold\n这是 torch.nn.Unfold 类，所做的事情与 Fold 相反，根据普通卷积的输入 tensor 以及卷积核大小，dilation，padding 和 stride 等计算得到 `L` 个与卷积核做卷积操作的数据块。`L` 计算方式如上。Unfold 的输入 size 为 $(N,C,*)$，其中 * 表示多维数据，输出 size 为 $(N,C \\times \\prod(\\text{kernel\\_size}), L)$。\n\n引用PyTorch 文档中的例子，\n```python\n>>> unfold = nn.Unfold(kernel_size=(2, 3))\n>>> input = torch.randn(2, 5, 3, 4)\n>>> output = unfold(input)\n>>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels)\n>>> # 4 blocks (2x3 kernels) in total in the 3x4 input\n>>> output.size()\ntorch.Size([2, 30, 4])\n```\n\n# 2. Normalization\n## BatchNorm\n批归一化是针对一个 mini-batch 内的数据进行归一化。首先给出归一化公式：\n$$y=\\frac {x-E[x]} {\\sqrt{V[x]+\\epsilon}} * \\gamma + \\beta$$\n\n批归一化过程为：\nBatchNorm Layer 的输入（mini-batch）为 $\\mathcal B=\\{x_{1...m}\\}$，可学习参数为 $\\gamma, \\beta$。计算 mini-batch 的均值，方差\n$$\\mu_{\\mathcal B} = \\frac 1 m \\sum_{i=1}^m x_i, \\quad \\sigma_{\\mathcal B}^2=\\frac 1 m \\sum_{i=1}^m(x_i - \\mu_{\\mathcal B})^2$$\n然后计算归一化后的值\n$$\\hat x_i = \\frac {x_i - \\mu_{\\mathcal B}} {\\sqrt {\\sigma_{\\mathcal B}^2+ \\epsilon}}$$\n最后进行 scale 和 shift，\n$$y_i=\\hat x_i \\cdot \\gamma + \\beta$$\n\n__小结：__ 沿着 batch 方向进行归一化\n\n## LayerNorm\nLayer 归一化是针对某个数据（样本）内部进行归一化，假设某个数据样本到达 LayerNorm 层为 $x$，无论 $x$ 是多少维的 tensor，均可以看作是 1D vector，即 $x=(x_1,...x_H)$，$H$ 是 LayerNorm 层的单元数（也是 $x$ 的特征数），于是 LayerNorm 过程为\n$$\\mu=\\frac 1 H \\sum_{i=1}^H x_i, \\quad \\sigma^2=\\frac 1 H \\sum_{i=1}^H (x_i-\\mu)^2$$\n于是 LayerNorm 后的值为\n$$y=\\frac {x-\\mu} {\\sqrt {\\sigma^2+\\epsilon}} \\cdot \\gamma + \\beta$$\n\n__小结：__ 沿着特征方向进行归一化（特征包含了除 batch 维度外的其他所有维度）\n\n\n有了前面的归一化介绍，我们知道归一化过程都很类似，区别在于如何计算 $\\mu, \\sigma$，或者说沿着什么方向进行归一化。\n\n## InstanceNorm\n对于每个样例的每个 channel 分别计算 $\\mu, \\sigma$。假设输入为 $(N,C,H,W)$，那么沿着 $(H,W)$ 方向做归一化。\n\n## GroupNorm\nGroupNorm 是选择一组 channels 进行归一化，所以是介于 InstanceNorm（单个channel）和 LayerNorm （全部 channels）之间的。\n\n# 3. Pool\n池化操作都比较简单易懂，这里介绍几个非常规的池化操作。\n## FractionalMaxPool2d\n引用论文 [Fractional MaxPooling](https://arxiv.org/abs/1412.6071)。\n\npool 操作通常是用于降低 feature map 的大小，以常规的 `2x2` max-pooling 为例，记输入大小为 $N_{in} \\times N_{in}$，输出大小为 $N_{out} \\times N_{out}$，那么有\n$$N_{out}=\\frac {N_{in}-k+2p} {s} + 1= N_{in} / 2 \\Rightarrow N_{in} /N_{out} = 2$$\n\n将 $N_{in} \\times N_{in}$ 的 feature map 划分出 $N_{out}^2$ 个 pooling 区域 $(P_{i,j})$。我们用 $\\{1,2,...,N_{in}\\}^2$ （或 $[1,N_{in}]^2$）表示输入 feature map，pixel 使用坐标点表示，显然 pooling 区域满足\n$$P_{i,j} \\subset \\{1,2,...,N_{in}\\}, \\quad (i,j) \\in \\{1,...,N_{out}\\}^2$$\n\n现在，我们想让 $N_{in} / N_{out} \\in (1,2)$，或者为了提高速度，让 $N_{in} / N_{out} \\in (2,3)$，反正，这个比例不再是整数，这就是 Fractional max-pooling（FMP）。\n\n那么，FMP 具体是如何实现的呢？\n\n令两个递增序列 $(a_i)_{i=0}^{N_{out}}, \\ (b_i)_{i=0}^{N_{out}}$ 均以 `1` 开始，$N_{in}$ 结尾，递增量为 `1` 或者 `2`，即 $\\forall i,\\ a_{i+1}-a_{i} \\in \\{1,2\\}$，那么 pooling 区域可以有如下两种表示：\n$$P_{i,j}=[a_{i-1}, a_i-1] \\times [b_{j-1},b_j-1], \\quad i,j \\in \\{1,...,N_{out}\\}\n\\\\\\\\ P_{i,j}=[a_{i-1}, a_i] \\times [b_{j-1},b_j], \\quad i,j \\in \\{1,...,N_{out}\\}$$\n注意下标 `i,j` 的范围。\n\n第一种是 `disjoint` 表示，第二种是 `overlapping` 表示。显然使用第二种表示，相邻两个 pooling 区域是有重叠的，而第一种表示则不会。\n\n记下采样率 $\\alpha = N_{in} / N_{out}$，有如下两种方法得到 $(a_i)_{i=0}^{N_{out}}$\n1. `random` 方法\n   \n   当 $\\alpha$ 给定，那么 $(a_i-a_{i-1})_{i=1}^{N_{out}}$ 这个序列中有多少个 `1` 和多少个 `2` 已经是确定的了，将适量的 `1` 和 `2` shuffle 或者 random permutation，然后可可到 $\\alpha = N_{in} / N_{out}$\n\n2. `pseudorandom` 方法\n   \n   经过 $(0,0), \\ (N_{out}, N_{in}-1)$ 的直线，其斜率为 $\\alpha$（实际上是比下采样率小一点点，但是没关系，这两个值只要同时位于 $(1,2)$ 之间即可），将这个直线沿 y 轴 平移 $\\alpha \\cdot u$，其中 $\\alpha \\in (1,2), \\ u \\in (0,1), \\alpha \\cdot u \\in (0,1)$，即\n   $$y=\\alpha(i+u)$$\n   在此直线上取点，x 值依次为 $0,1,2,...,N_{out}$，对 y 值在应用 ceiling 函数，作为 $a_i$ 的值，\n   $$a_i=\\text{ceiling}(\\alpha(i+u)), \\quad i=0,1,2,...,N_{out}$$\n   验证一下 $\\{a_i\\}$ 序列是否满足上述条件：\n   - $i=0$，$a_0=\\text{ceiling}(\\alpha \\cdot u)$，由于 $\\alpha \\cdot u \\in (0,1)$，故 $a_0=1$\n   - $i=N_{out}$，$a_{N_{out}}=\\text{ceiling}(N_{in}-1+\\alpha \\cdot u)=N_{in}$\n   - `otherwise：` $a_{i+1}-a_i=\\text{ceiling}(\\alpha \\cdot i+\\alpha+\\alpha \\cdot u)-\\text{ceiling}(\\alpha \\cdot i+\\alpha \\cdot u)$。验证说明如下。\n\n   下面验证最后一种情况：\n   \n   记 $\\alpha \\cdot i+\\alpha \\cdot u=f \\in [k,k+1)$，k 是某个整数，那么当\n   - `f=k` 时，\n  \n        $a_{i+1}-a_i=\\text{ceiling}(k+\\alpha)-k=k+\\text{ceiling}(\\alpha)-k=\\text{ceiling}(\\alpha)=2$\n\n   - `k<f<k+1` 时，\n\n        $k+1<f+\\alpha<k+3$\n  \n        $a_{i+1}-a_i=\\text{ceiling}(f+\\alpha)-k-1 \\in \\{1,2\\}$\n\n   至此，验证了 $(a_i)$ 序列满足条件。显然，基于直线取离散点然后应用 ceiling 函数得到的是一种伪随机序列。\n\n# 4. ConvTranspose\n\n## 输出大小\n转置卷积，通常又称反卷积、逆卷积，然而转置卷积并非卷积的逆过程，并且转置卷积其实也是一种卷积，只不过与卷积相反的是，输出平面的大小通常不是变小而是变大。对于普通卷积，设输入平面边长为 $L_{in}$，输出平面边长为 $L_{out}$，卷积核边长为 $k$，dilation 、stride 和 padding 分别为 $d, p, s$，那么有\n$$L_{out}=\\frac {L_{in}-(d(k-1)+1)+2p} s + 1 \\qquad (4-1)$$\n对于转置卷积，令 $L_{in}^{\\top}, \\ L_{out}^{\\top}$ 分别表示输入和输出的边长，于是有\n$$L_{out}^{\\top}=s(L_{in}^{\\top} - 1) +d(k-1)+1 - 2p \\qquad (4-2)$$\n可见，转置卷积的输入输出边长的关系与普通卷积是反过来的。\n\n## 转置卷积计算\n### 第一种方法\n回顾一下卷积过程，以二维卷积为例，假设输入大小为 $4 \\times 4$，卷积核 $3 \\times 3$，不考虑 padding，且 stride 为 1，那么根据 $(4-1)$ 式输出大小为 $2 \\times 2$，我们可以用卷积核在输入平面上滑窗并做卷积来理解卷积，实际计算则是根据输入矩阵得到 $4 \\times 9$ 的矩阵（__部分 element 用 0 填充__），然后将卷积核展开成 $9 \\times 1$ 的矩阵，然后进行卷积相乘得到 $4 \\times 1$ 的输出矩阵。\n\n我们再看转置卷积，输入大小为 $2 \\times 2$，卷积核大小为 $3 \\times 3$，同样地，不考虑 padding 且 stride 为 1，那么根据 $(4-2)$ 式输出大小为 $4 \\times 4$，实际的计算过程为：__由于转置卷积也是一个普通卷积__，所以先将输入矩阵 zero-padding 为 $6\\times 6$ 的矩阵（$6 \\times 6$ 的输入矩阵经过 $3 \\times 3$ 的卷积才能得到 $4 \\times 4$ 的输出大小），然后与普通卷积一样地得到为 $16 \\times 9$ 的矩阵，卷积核 __旋转 180°__，然后 reshape 为 $9 \\times 1$ 的矩阵，通过矩阵乘法，得到矩阵大小为 $16 \\times 1$，然后 reshape 为 $4 \\times 4$，此即输出矩阵。\n\n下面我们画图来展示卷积和转置卷积地过程：\n\n![普通卷积](/images/pytorch_mth_conv.png)<center>普通卷积</center>\n\n![转置卷积](/images/pytorch_mtd_conv_t.png)<center>转置卷积</center>\n\n\n使用 Pytorch 进行验证：\n\n```python\nimport torch\nfrom torch import nn\nconvt = nn.ConvTranspose2d(1,1,3)\nconvt.bias = nn.Parameter(torch.tensor([0.]))\nconvt.weight = nn.Parameter(torch.tensor([[[[0.,1,2],\n                                            [2,2,0],\n                                            [0,1,2]]]]))\ninput = torch.tensor([[[[12.,12],\n                        [10,17]]]])\noutput = convt(input)\noutput\n# tensor([[[[ 0., 12., 36., 24.],\n#           [24., 58., 61., 34.],\n#           [20., 66., 70., 24.],\n#           [ 0., 10., 37., 34.]]]])\n```\n\n### 第二种方法\n还有另一种方法来理解计算卷积和转置卷积。还是以上面的例子进行说明。\n\n普通卷积中，输入矩阵 reshape 为 $1 \\times 16$。因为有 4 个滑窗卷积动作，所以将卷积核分别以四种不同的 zero-padding 方式得到 4 个 $4 \\times 4$ 的矩阵（即，卷积核的 $3 \\times 4$ 部分位于 $4 \\times 4$ 矩阵的左上角、右上角，左下角和右下角，其他位置 zero-padding），然后 reshape 为 $16 \\times 4$，记这个 $16 \\times 4$ 的矩阵为 $K$， 得到 $1 \\times 4$ 矩阵，reshape 为 $2 \\times 2$ 即输出矩阵。\n\n转置卷积中，输入矩阵大小为 $2 \\times 2$（即 `[12,12,10,17]`），直接 reshape 为 $1 \\times 4$，将上面的矩阵 $K$ __转置__，得到 $4 \\times 16$ 的矩阵，然后矩阵相乘得到 $1 \\times 16$ 矩阵，最后 reshape 为 $4 \\times 4$ 即为输出矩阵。\n\n普通卷积的过程如下图示意，转置卷积非常简单，读者可以自己画图验证。\n\n![卷积的另一种计算过程](/images/pytorch_mtd_conv_t_1.png)<center>卷积的另一种计算过程</center>\n\n从转置卷积得到的结果来看，很明显，转置卷积不是普通卷积的逆过程。\n\n### dilation > 1\n现在，我们的讨论还未结束，来看 `dilation` 不为 1 的情况，例如 `dilation=2`，还是使用上面的例子，对于转置卷积，此时根据 $(4-2)$ 式得到输出矩阵大小为 $6 \\times 6$，将卷积核膨胀后得到 $5 \\times 5$ 矩阵（间隔填充 0），并 __旋转 180°__，由于转置卷积也是一种普通卷积，所以应该将输入矩阵 zero-padding 到 $10 \\times 10$ 大小才能得到 $6 \\times 6$ 的输出，也就是说，输入矩阵上下左右均进行 4 个单位的 zero-padding，\n\n记 `input` 为 $I$，zero-padding后，$I[4:6,4:6]=[[12.,12],[10,17]]$，其余位置为 `0`，膨胀后的卷积核 __旋转 180°__ 后为 $K'=[[2., 0, 1, 0, 0],[0,0,0,0,0],[0,0,2,0,2],[0,0,0,0,0],[2,0,1,0,0]]$，可以手动计算卷积后的输出矩阵，这里给出 python 代码计算示例，\n```python\nconvt1 = nn.ConvTranspose2d(1,1,3,dilation=2)\nconvt1.bias = nn.Parameter(torch.tensor([0.]))\nconvt1.weight = nn.Parameter(torch.tensor([[[[0.,1,2],[2,2,0],[0,1,2]]]]))\noutput1 = convt1(input)\noutput1\n```\n\n### stride > 1\n依然以上面的例子进行说明，假设现在 `stride=2`，根据式 $(4-2)$ 转置卷积的输出大小为 $5 \\times 5$。把转置卷积看作一种普通卷积，那么其输入大小应该为 $7 \\times 7$，由于 `stride=2`，所以先将 $2 \\times 2$ 输入矩阵膨胀为 $3 \\times 3$ 的矩阵（2*(2-1)+1=3），然后再 zero-padding 成 $7 \\times 7$ 的矩阵（上下左右 padding 的数量均为 (7-3)/2=2），经过这番处理，输入矩阵变为 $I[2,2]=I[2,4]=12, \\ I[4,2]=10, \\ I[4,4]=17$，其余位置均为 `0`，卷积核 __旋转 180°__ 后为 $K'=[[2., 1, 0],[0,2,2],[2,1,0]]$，于是可以手动计算出卷积后的矩阵，这里给出 python 代码计算示例，\n```python\nconvt.stride = (2,2)\noutput = convt(input)\noutput\n```\n\n### padding > 0\n继续以上面的例子进行说明，假设现在 `padding=1`，根据式 $(4-2)$ 转置卷积的输出大小为 $2 \\times 2$。将输入矩阵上下左右均进行 1 单位的 zero-padding，得到矩阵大小 $4 \\times 4$，卷积核大小 $3 \\times 3$，计算过程还是将卷积核 __旋转 180°__，卷积计算过程略，不过相信这是足够简单的事情。\n\n以上 `dialtion > 1, stride > 1, padding > 0` 三种情况，除了可使用 python 程序验证，还可以使用 `第二种方法` 进行验证对输入矩阵以及卷积核的处理是正确的，并且，也可以使用 `第一种方法` 对输入矩阵和卷积核进行处理然后进行普通卷积计算得到输出矩阵。\n\n# 5. Upsample\n输入维度为 `minibatch x channels x [optional depth] x [optional height] x width`，即，输入可以是 3D/4D/5D。可用的算法包括 `nearest neighbor, linear, bilinear, bicubic, trilinear`。\n\n## nearest neighbor\n顾名思义，就是使用原平面上最近的一点作为上采样后的值。例如原平面 size 为 $m \\times m$，在原平面上建立坐标系 S，上采样后的 size 为 $n \\times n, \\ n > m$，设其上点的坐标为 $(x,y), \\ x,y =0,1,...,n-1$。将上采样后平面点映射到 S 中，对应坐标记为 $(x',y')$，那么有\n\n$$\\frac {x-0} {n-1-0}= \\frac {x'-0}{m-1-0} \\Rightarrow x' = \\frac {m-1} {n-1} \\cdot x$$\n同理有 $y' = \\frac {m-1} {n-1} \\cdot y$，然后找出与点 $(i',j')$ 最近的那个整数坐标点，显然必然在以下四个点中产生 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$ （这四个点可能有重合），分别计算 $(x',y')$ 与这四个点的距离，距离最小的那个点的值即作为 $(x,y)$ 上采样后的值。（使用哪种距离指标，可以查看 PyTorch 底层实现代码，这里本人尚未去查看。）\n\n## bilinear\n输入必须是 4D。\n### align_corners=True\n双线性插值。记四个顶点为 $(x_1,y_1), \\ (x_1,y_2), \\ (x_2,y_1), \\ (x_2,y_2)$，然后求目标点 $(x,y), \\ x_1 \\le x \\le x_2, \\ y_1 \\le y \\le y_2$ 的值。沿 x 轴线性插值，\n$$f(x,y_1)=\\frac {f_{21}-f_{11}} {x_2-x_1} \\cdot (x-x_1)+f_{11}\n\\\\\\\\ f(x,y_2)=\\frac {f_{22}-f_{12}} {x_2-x_1} \\cdot (x-x_1)+f_{12}\n\\\\\\\\ f(x,y)=\\frac {f_(x,y_2)-f(x,y_1)} {y_2-y_1} \\cdot (y-y_1)+f(x,y_1)$$\n\n与 `nearest neighbor` 中一样，首先将点 $(x,y)$ 映射到原平面上一点 $(x',y')$，然后四个顶点为 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$。用这种映射方法，显然原平面的四个 corners 和上采样后平面的四个 corners 分别对齐，这就是 `align_corners=True` 的由来。\n\n### align_corners=False\n如下图所示，显示了 `align_corners` 不同值的区别。\n![](/images/pytorch_mtd_aligncorners.png)<center>图源 [pytorch 论坛](https://discuss.pytorch.org/t/what-we-should-use-align-corners-false/22663/9)</center>\n\n从图中可以发现，映射回原平面坐标时，坐标计算方式不同，例如上菜以后平面上一点 $(x,y)$，映射回 S 中的坐标为\n$$x'=(x+0.5)/2-0.5\n\\\\\\\\ y'=(y+0.5)/2-0.5$$\n\n此后的插值方式一致（毕竟都是双线性插值），找到最近的 4 个点 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$ 进行双线性插值。\n## linear\n与 bilinear 类似，但是输入维度必须是 3D。\n\n## trilinear\n与 bilinear 类似，但是输入维度必须是 5D。\n\n","source":"_posts/pytorch/PyTorch-mtd.md","raw":"---\ntitle: PyTorch 方法总结\np: pytorch/PyTorch-mtd\ndate: 2019-11-01 11:26:25\ntags: PyTorch\nmathjax: true\n---\n\n# 1. Fold / Unfold\n## Fold\n这是 torch.nn.Fold 类。\n\n首先我们来复习一下卷积过程，设输入 size 为 `(N,C,H,W)`，卷积 kernel size 为 `(C,h,w)`，碰撞系数为 `d`，padding 为 `p`，stride 记为 `s`，那么整个过程相当于将 `Cxhxw` 大小的数据块在输入数据中滑动，每一次滑动做一次卷积，记共有 `L` 次卷积，即，从输入数据中切分出 `L` 个数据块与卷积核做卷积，当然每个数据块的大小与卷积核相同，为 `(C,h,w)`，最后得到的输出 map 大小为\n<!-- more -->\n$$H_o= \\frac {H - [d(h-1)+1] + 2p} {s}+1\n\\\\\\\\ W_o= \\frac {W - [d(w-1)+1] + 2p} {s}+1$$\n因为每一次卷积得到的值均作为输出 map 上的一点，故 `L` 为\n$$L=H_o * W_o=\\left(\\frac {H - [d(h-1)+1] + 2p} {s}+1\\right) \\left(\\frac {W - [d(w-1)+1] + 2p} {s}+1\\right)$$\n\n好了，现在 Fold 要做的事情是反过来的，已知 fold 的输入为 `L` 个数据块，大小为 `(N,C*h*w,L)`，有关的构造参数为卷积核 size `(h,w)`，dilation，padding，stride，以及，指定最终的（Fold）输出大小 `(H,W)`，注意，Fold 做的事情是反过来的，也就是说，从 `L` 个数据块中恢复出原来普通卷积的输入 map 的大小，即 `(H,W)`，不是做完卷积之后的输出 map 的大小，记住，__Fold 的输出是普通卷积的输入__。\n\nFold 的这些构造参数指明了卷积核大小，以及卷积输入的大小，然后根据其（这里指 Fold）输入 `L` 个数据块的 tensor，size 为 `(N,C*h*w,L)`，恢复出卷积输入的 tensor，因为 Fold 的构造参数中指定了 卷据输入 map 的 `(H,W)`，而批大小 `N` 也已知，所以要求出通道 `C`，根据 Fold 输入 tensor 的 第二个维度值 `C*h*w` 以及 Fold 的构造参数中卷积核大小 `(h,w)` 很容易得到通道 `C`。\n\n先使用 PyTorch 文档中的例子加以说明，\n```python\n>>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))  # (H,W)=(4,5), (h,w)=(2,2)\n>>> input = torch.randn(1, 3 * 2 * 2, 12)   # (N, C*h*w,L)\n>>> output = fold(input)\n>>> output.size()\ntorch.Size([1, 3, 4, 5])    # (N,C,H,W)\n```\n\n将数据维度从 `H,W` 扩展到更多维度，就是 PyTorch 文档中关于 `L` 的计算式了，如下\n$$L=\\prod_d \\lfloor \\frac {\\text{output\\_size} [d] + 2 \\times \\text{padding}[d]-\\text{dilation}[d] \\times (\\text{kernel\\_size}[d]-1) -1} {\\text{stride}[d]} +1 \\rfloor$$\n\n__总结：__\n\nFold 的输入 size 为 $(N, C \\times \\prod(\\text{kernel\\_size}), L)$，输出 size 为 $(N,C, \\text{output\\_size}[0], \\text{output\\_size}[1], ...)$\n\n## Unfold\n这是 torch.nn.Unfold 类，所做的事情与 Fold 相反，根据普通卷积的输入 tensor 以及卷积核大小，dilation，padding 和 stride 等计算得到 `L` 个与卷积核做卷积操作的数据块。`L` 计算方式如上。Unfold 的输入 size 为 $(N,C,*)$，其中 * 表示多维数据，输出 size 为 $(N,C \\times \\prod(\\text{kernel\\_size}), L)$。\n\n引用PyTorch 文档中的例子，\n```python\n>>> unfold = nn.Unfold(kernel_size=(2, 3))\n>>> input = torch.randn(2, 5, 3, 4)\n>>> output = unfold(input)\n>>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels)\n>>> # 4 blocks (2x3 kernels) in total in the 3x4 input\n>>> output.size()\ntorch.Size([2, 30, 4])\n```\n\n# 2. Normalization\n## BatchNorm\n批归一化是针对一个 mini-batch 内的数据进行归一化。首先给出归一化公式：\n$$y=\\frac {x-E[x]} {\\sqrt{V[x]+\\epsilon}} * \\gamma + \\beta$$\n\n批归一化过程为：\nBatchNorm Layer 的输入（mini-batch）为 $\\mathcal B=\\{x_{1...m}\\}$，可学习参数为 $\\gamma, \\beta$。计算 mini-batch 的均值，方差\n$$\\mu_{\\mathcal B} = \\frac 1 m \\sum_{i=1}^m x_i, \\quad \\sigma_{\\mathcal B}^2=\\frac 1 m \\sum_{i=1}^m(x_i - \\mu_{\\mathcal B})^2$$\n然后计算归一化后的值\n$$\\hat x_i = \\frac {x_i - \\mu_{\\mathcal B}} {\\sqrt {\\sigma_{\\mathcal B}^2+ \\epsilon}}$$\n最后进行 scale 和 shift，\n$$y_i=\\hat x_i \\cdot \\gamma + \\beta$$\n\n__小结：__ 沿着 batch 方向进行归一化\n\n## LayerNorm\nLayer 归一化是针对某个数据（样本）内部进行归一化，假设某个数据样本到达 LayerNorm 层为 $x$，无论 $x$ 是多少维的 tensor，均可以看作是 1D vector，即 $x=(x_1,...x_H)$，$H$ 是 LayerNorm 层的单元数（也是 $x$ 的特征数），于是 LayerNorm 过程为\n$$\\mu=\\frac 1 H \\sum_{i=1}^H x_i, \\quad \\sigma^2=\\frac 1 H \\sum_{i=1}^H (x_i-\\mu)^2$$\n于是 LayerNorm 后的值为\n$$y=\\frac {x-\\mu} {\\sqrt {\\sigma^2+\\epsilon}} \\cdot \\gamma + \\beta$$\n\n__小结：__ 沿着特征方向进行归一化（特征包含了除 batch 维度外的其他所有维度）\n\n\n有了前面的归一化介绍，我们知道归一化过程都很类似，区别在于如何计算 $\\mu, \\sigma$，或者说沿着什么方向进行归一化。\n\n## InstanceNorm\n对于每个样例的每个 channel 分别计算 $\\mu, \\sigma$。假设输入为 $(N,C,H,W)$，那么沿着 $(H,W)$ 方向做归一化。\n\n## GroupNorm\nGroupNorm 是选择一组 channels 进行归一化，所以是介于 InstanceNorm（单个channel）和 LayerNorm （全部 channels）之间的。\n\n# 3. Pool\n池化操作都比较简单易懂，这里介绍几个非常规的池化操作。\n## FractionalMaxPool2d\n引用论文 [Fractional MaxPooling](https://arxiv.org/abs/1412.6071)。\n\npool 操作通常是用于降低 feature map 的大小，以常规的 `2x2` max-pooling 为例，记输入大小为 $N_{in} \\times N_{in}$，输出大小为 $N_{out} \\times N_{out}$，那么有\n$$N_{out}=\\frac {N_{in}-k+2p} {s} + 1= N_{in} / 2 \\Rightarrow N_{in} /N_{out} = 2$$\n\n将 $N_{in} \\times N_{in}$ 的 feature map 划分出 $N_{out}^2$ 个 pooling 区域 $(P_{i,j})$。我们用 $\\{1,2,...,N_{in}\\}^2$ （或 $[1,N_{in}]^2$）表示输入 feature map，pixel 使用坐标点表示，显然 pooling 区域满足\n$$P_{i,j} \\subset \\{1,2,...,N_{in}\\}, \\quad (i,j) \\in \\{1,...,N_{out}\\}^2$$\n\n现在，我们想让 $N_{in} / N_{out} \\in (1,2)$，或者为了提高速度，让 $N_{in} / N_{out} \\in (2,3)$，反正，这个比例不再是整数，这就是 Fractional max-pooling（FMP）。\n\n那么，FMP 具体是如何实现的呢？\n\n令两个递增序列 $(a_i)_{i=0}^{N_{out}}, \\ (b_i)_{i=0}^{N_{out}}$ 均以 `1` 开始，$N_{in}$ 结尾，递增量为 `1` 或者 `2`，即 $\\forall i,\\ a_{i+1}-a_{i} \\in \\{1,2\\}$，那么 pooling 区域可以有如下两种表示：\n$$P_{i,j}=[a_{i-1}, a_i-1] \\times [b_{j-1},b_j-1], \\quad i,j \\in \\{1,...,N_{out}\\}\n\\\\\\\\ P_{i,j}=[a_{i-1}, a_i] \\times [b_{j-1},b_j], \\quad i,j \\in \\{1,...,N_{out}\\}$$\n注意下标 `i,j` 的范围。\n\n第一种是 `disjoint` 表示，第二种是 `overlapping` 表示。显然使用第二种表示，相邻两个 pooling 区域是有重叠的，而第一种表示则不会。\n\n记下采样率 $\\alpha = N_{in} / N_{out}$，有如下两种方法得到 $(a_i)_{i=0}^{N_{out}}$\n1. `random` 方法\n   \n   当 $\\alpha$ 给定，那么 $(a_i-a_{i-1})_{i=1}^{N_{out}}$ 这个序列中有多少个 `1` 和多少个 `2` 已经是确定的了，将适量的 `1` 和 `2` shuffle 或者 random permutation，然后可可到 $\\alpha = N_{in} / N_{out}$\n\n2. `pseudorandom` 方法\n   \n   经过 $(0,0), \\ (N_{out}, N_{in}-1)$ 的直线，其斜率为 $\\alpha$（实际上是比下采样率小一点点，但是没关系，这两个值只要同时位于 $(1,2)$ 之间即可），将这个直线沿 y 轴 平移 $\\alpha \\cdot u$，其中 $\\alpha \\in (1,2), \\ u \\in (0,1), \\alpha \\cdot u \\in (0,1)$，即\n   $$y=\\alpha(i+u)$$\n   在此直线上取点，x 值依次为 $0,1,2,...,N_{out}$，对 y 值在应用 ceiling 函数，作为 $a_i$ 的值，\n   $$a_i=\\text{ceiling}(\\alpha(i+u)), \\quad i=0,1,2,...,N_{out}$$\n   验证一下 $\\{a_i\\}$ 序列是否满足上述条件：\n   - $i=0$，$a_0=\\text{ceiling}(\\alpha \\cdot u)$，由于 $\\alpha \\cdot u \\in (0,1)$，故 $a_0=1$\n   - $i=N_{out}$，$a_{N_{out}}=\\text{ceiling}(N_{in}-1+\\alpha \\cdot u)=N_{in}$\n   - `otherwise：` $a_{i+1}-a_i=\\text{ceiling}(\\alpha \\cdot i+\\alpha+\\alpha \\cdot u)-\\text{ceiling}(\\alpha \\cdot i+\\alpha \\cdot u)$。验证说明如下。\n\n   下面验证最后一种情况：\n   \n   记 $\\alpha \\cdot i+\\alpha \\cdot u=f \\in [k,k+1)$，k 是某个整数，那么当\n   - `f=k` 时，\n  \n        $a_{i+1}-a_i=\\text{ceiling}(k+\\alpha)-k=k+\\text{ceiling}(\\alpha)-k=\\text{ceiling}(\\alpha)=2$\n\n   - `k<f<k+1` 时，\n\n        $k+1<f+\\alpha<k+3$\n  \n        $a_{i+1}-a_i=\\text{ceiling}(f+\\alpha)-k-1 \\in \\{1,2\\}$\n\n   至此，验证了 $(a_i)$ 序列满足条件。显然，基于直线取离散点然后应用 ceiling 函数得到的是一种伪随机序列。\n\n# 4. ConvTranspose\n\n## 输出大小\n转置卷积，通常又称反卷积、逆卷积，然而转置卷积并非卷积的逆过程，并且转置卷积其实也是一种卷积，只不过与卷积相反的是，输出平面的大小通常不是变小而是变大。对于普通卷积，设输入平面边长为 $L_{in}$，输出平面边长为 $L_{out}$，卷积核边长为 $k$，dilation 、stride 和 padding 分别为 $d, p, s$，那么有\n$$L_{out}=\\frac {L_{in}-(d(k-1)+1)+2p} s + 1 \\qquad (4-1)$$\n对于转置卷积，令 $L_{in}^{\\top}, \\ L_{out}^{\\top}$ 分别表示输入和输出的边长，于是有\n$$L_{out}^{\\top}=s(L_{in}^{\\top} - 1) +d(k-1)+1 - 2p \\qquad (4-2)$$\n可见，转置卷积的输入输出边长的关系与普通卷积是反过来的。\n\n## 转置卷积计算\n### 第一种方法\n回顾一下卷积过程，以二维卷积为例，假设输入大小为 $4 \\times 4$，卷积核 $3 \\times 3$，不考虑 padding，且 stride 为 1，那么根据 $(4-1)$ 式输出大小为 $2 \\times 2$，我们可以用卷积核在输入平面上滑窗并做卷积来理解卷积，实际计算则是根据输入矩阵得到 $4 \\times 9$ 的矩阵（__部分 element 用 0 填充__），然后将卷积核展开成 $9 \\times 1$ 的矩阵，然后进行卷积相乘得到 $4 \\times 1$ 的输出矩阵。\n\n我们再看转置卷积，输入大小为 $2 \\times 2$，卷积核大小为 $3 \\times 3$，同样地，不考虑 padding 且 stride 为 1，那么根据 $(4-2)$ 式输出大小为 $4 \\times 4$，实际的计算过程为：__由于转置卷积也是一个普通卷积__，所以先将输入矩阵 zero-padding 为 $6\\times 6$ 的矩阵（$6 \\times 6$ 的输入矩阵经过 $3 \\times 3$ 的卷积才能得到 $4 \\times 4$ 的输出大小），然后与普通卷积一样地得到为 $16 \\times 9$ 的矩阵，卷积核 __旋转 180°__，然后 reshape 为 $9 \\times 1$ 的矩阵，通过矩阵乘法，得到矩阵大小为 $16 \\times 1$，然后 reshape 为 $4 \\times 4$，此即输出矩阵。\n\n下面我们画图来展示卷积和转置卷积地过程：\n\n![普通卷积](/images/pytorch_mth_conv.png)<center>普通卷积</center>\n\n![转置卷积](/images/pytorch_mtd_conv_t.png)<center>转置卷积</center>\n\n\n使用 Pytorch 进行验证：\n\n```python\nimport torch\nfrom torch import nn\nconvt = nn.ConvTranspose2d(1,1,3)\nconvt.bias = nn.Parameter(torch.tensor([0.]))\nconvt.weight = nn.Parameter(torch.tensor([[[[0.,1,2],\n                                            [2,2,0],\n                                            [0,1,2]]]]))\ninput = torch.tensor([[[[12.,12],\n                        [10,17]]]])\noutput = convt(input)\noutput\n# tensor([[[[ 0., 12., 36., 24.],\n#           [24., 58., 61., 34.],\n#           [20., 66., 70., 24.],\n#           [ 0., 10., 37., 34.]]]])\n```\n\n### 第二种方法\n还有另一种方法来理解计算卷积和转置卷积。还是以上面的例子进行说明。\n\n普通卷积中，输入矩阵 reshape 为 $1 \\times 16$。因为有 4 个滑窗卷积动作，所以将卷积核分别以四种不同的 zero-padding 方式得到 4 个 $4 \\times 4$ 的矩阵（即，卷积核的 $3 \\times 4$ 部分位于 $4 \\times 4$ 矩阵的左上角、右上角，左下角和右下角，其他位置 zero-padding），然后 reshape 为 $16 \\times 4$，记这个 $16 \\times 4$ 的矩阵为 $K$， 得到 $1 \\times 4$ 矩阵，reshape 为 $2 \\times 2$ 即输出矩阵。\n\n转置卷积中，输入矩阵大小为 $2 \\times 2$（即 `[12,12,10,17]`），直接 reshape 为 $1 \\times 4$，将上面的矩阵 $K$ __转置__，得到 $4 \\times 16$ 的矩阵，然后矩阵相乘得到 $1 \\times 16$ 矩阵，最后 reshape 为 $4 \\times 4$ 即为输出矩阵。\n\n普通卷积的过程如下图示意，转置卷积非常简单，读者可以自己画图验证。\n\n![卷积的另一种计算过程](/images/pytorch_mtd_conv_t_1.png)<center>卷积的另一种计算过程</center>\n\n从转置卷积得到的结果来看，很明显，转置卷积不是普通卷积的逆过程。\n\n### dilation > 1\n现在，我们的讨论还未结束，来看 `dilation` 不为 1 的情况，例如 `dilation=2`，还是使用上面的例子，对于转置卷积，此时根据 $(4-2)$ 式得到输出矩阵大小为 $6 \\times 6$，将卷积核膨胀后得到 $5 \\times 5$ 矩阵（间隔填充 0），并 __旋转 180°__，由于转置卷积也是一种普通卷积，所以应该将输入矩阵 zero-padding 到 $10 \\times 10$ 大小才能得到 $6 \\times 6$ 的输出，也就是说，输入矩阵上下左右均进行 4 个单位的 zero-padding，\n\n记 `input` 为 $I$，zero-padding后，$I[4:6,4:6]=[[12.,12],[10,17]]$，其余位置为 `0`，膨胀后的卷积核 __旋转 180°__ 后为 $K'=[[2., 0, 1, 0, 0],[0,0,0,0,0],[0,0,2,0,2],[0,0,0,0,0],[2,0,1,0,0]]$，可以手动计算卷积后的输出矩阵，这里给出 python 代码计算示例，\n```python\nconvt1 = nn.ConvTranspose2d(1,1,3,dilation=2)\nconvt1.bias = nn.Parameter(torch.tensor([0.]))\nconvt1.weight = nn.Parameter(torch.tensor([[[[0.,1,2],[2,2,0],[0,1,2]]]]))\noutput1 = convt1(input)\noutput1\n```\n\n### stride > 1\n依然以上面的例子进行说明，假设现在 `stride=2`，根据式 $(4-2)$ 转置卷积的输出大小为 $5 \\times 5$。把转置卷积看作一种普通卷积，那么其输入大小应该为 $7 \\times 7$，由于 `stride=2`，所以先将 $2 \\times 2$ 输入矩阵膨胀为 $3 \\times 3$ 的矩阵（2*(2-1)+1=3），然后再 zero-padding 成 $7 \\times 7$ 的矩阵（上下左右 padding 的数量均为 (7-3)/2=2），经过这番处理，输入矩阵变为 $I[2,2]=I[2,4]=12, \\ I[4,2]=10, \\ I[4,4]=17$，其余位置均为 `0`，卷积核 __旋转 180°__ 后为 $K'=[[2., 1, 0],[0,2,2],[2,1,0]]$，于是可以手动计算出卷积后的矩阵，这里给出 python 代码计算示例，\n```python\nconvt.stride = (2,2)\noutput = convt(input)\noutput\n```\n\n### padding > 0\n继续以上面的例子进行说明，假设现在 `padding=1`，根据式 $(4-2)$ 转置卷积的输出大小为 $2 \\times 2$。将输入矩阵上下左右均进行 1 单位的 zero-padding，得到矩阵大小 $4 \\times 4$，卷积核大小 $3 \\times 3$，计算过程还是将卷积核 __旋转 180°__，卷积计算过程略，不过相信这是足够简单的事情。\n\n以上 `dialtion > 1, stride > 1, padding > 0` 三种情况，除了可使用 python 程序验证，还可以使用 `第二种方法` 进行验证对输入矩阵以及卷积核的处理是正确的，并且，也可以使用 `第一种方法` 对输入矩阵和卷积核进行处理然后进行普通卷积计算得到输出矩阵。\n\n# 5. Upsample\n输入维度为 `minibatch x channels x [optional depth] x [optional height] x width`，即，输入可以是 3D/4D/5D。可用的算法包括 `nearest neighbor, linear, bilinear, bicubic, trilinear`。\n\n## nearest neighbor\n顾名思义，就是使用原平面上最近的一点作为上采样后的值。例如原平面 size 为 $m \\times m$，在原平面上建立坐标系 S，上采样后的 size 为 $n \\times n, \\ n > m$，设其上点的坐标为 $(x,y), \\ x,y =0,1,...,n-1$。将上采样后平面点映射到 S 中，对应坐标记为 $(x',y')$，那么有\n\n$$\\frac {x-0} {n-1-0}= \\frac {x'-0}{m-1-0} \\Rightarrow x' = \\frac {m-1} {n-1} \\cdot x$$\n同理有 $y' = \\frac {m-1} {n-1} \\cdot y$，然后找出与点 $(i',j')$ 最近的那个整数坐标点，显然必然在以下四个点中产生 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$ （这四个点可能有重合），分别计算 $(x',y')$ 与这四个点的距离，距离最小的那个点的值即作为 $(x,y)$ 上采样后的值。（使用哪种距离指标，可以查看 PyTorch 底层实现代码，这里本人尚未去查看。）\n\n## bilinear\n输入必须是 4D。\n### align_corners=True\n双线性插值。记四个顶点为 $(x_1,y_1), \\ (x_1,y_2), \\ (x_2,y_1), \\ (x_2,y_2)$，然后求目标点 $(x,y), \\ x_1 \\le x \\le x_2, \\ y_1 \\le y \\le y_2$ 的值。沿 x 轴线性插值，\n$$f(x,y_1)=\\frac {f_{21}-f_{11}} {x_2-x_1} \\cdot (x-x_1)+f_{11}\n\\\\\\\\ f(x,y_2)=\\frac {f_{22}-f_{12}} {x_2-x_1} \\cdot (x-x_1)+f_{12}\n\\\\\\\\ f(x,y)=\\frac {f_(x,y_2)-f(x,y_1)} {y_2-y_1} \\cdot (y-y_1)+f(x,y_1)$$\n\n与 `nearest neighbor` 中一样，首先将点 $(x,y)$ 映射到原平面上一点 $(x',y')$，然后四个顶点为 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$。用这种映射方法，显然原平面的四个 corners 和上采样后平面的四个 corners 分别对齐，这就是 `align_corners=True` 的由来。\n\n### align_corners=False\n如下图所示，显示了 `align_corners` 不同值的区别。\n![](/images/pytorch_mtd_aligncorners.png)<center>图源 [pytorch 论坛](https://discuss.pytorch.org/t/what-we-should-use-align-corners-false/22663/9)</center>\n\n从图中可以发现，映射回原平面坐标时，坐标计算方式不同，例如上菜以后平面上一点 $(x,y)$，映射回 S 中的坐标为\n$$x'=(x+0.5)/2-0.5\n\\\\\\\\ y'=(y+0.5)/2-0.5$$\n\n此后的插值方式一致（毕竟都是双线性插值），找到最近的 4 个点 $(\\lfloor x'\\rfloor, \\lfloor y' \\rfloor), \\ (\\lfloor x'\\rfloor, \\lceil y' \\rceil), \\ (\\lceil x'\\rceil, \\lfloor y' \\rfloor), \\ (\\lceil x'\\rceil, \\lceil y' \\rceil)$ 进行双线性插值。\n## linear\n与 bilinear 类似，但是输入维度必须是 3D。\n\n## trilinear\n与 bilinear 类似，但是输入维度必须是 5D。\n\n","slug":"pytorch/PyTorch-mtd","published":1,"updated":"2020-04-24T10:35:09.355Z","_id":"ck9dzcj4y002ngga6gsbm1qbo","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"1-Fold-Unfold\"><a href=\"#1-Fold-Unfold\" class=\"headerlink\" title=\"1. Fold / Unfold\"></a>1. Fold / Unfold</h1><h2 id=\"Fold\"><a href=\"#Fold\" class=\"headerlink\" title=\"Fold\"></a>Fold</h2><p>这是 torch.nn.Fold 类。</p>\n<p>首先我们来复习一下卷积过程，设输入 size 为 <code>(N,C,H,W)</code>，卷积 kernel size 为 <code>(C,h,w)</code>，碰撞系数为 <code>d</code>，padding 为 <code>p</code>，stride 记为 <code>s</code>，那么整个过程相当于将 <code>Cxhxw</code> 大小的数据块在输入数据中滑动，每一次滑动做一次卷积，记共有 <code>L</code> 次卷积，即，从输入数据中切分出 <code>L</code> 个数据块与卷积核做卷积，当然每个数据块的大小与卷积核相同，为 <code>(C,h,w)</code>，最后得到的输出 map 大小为</p>\n<a id=\"more\"></a>\n<p>$$H_o= \\frac {H - [d(h-1)+1] + 2p} {s}+1<br>\\\\ W_o= \\frac {W - [d(w-1)+1] + 2p} {s}+1$$<br>因为每一次卷积得到的值均作为输出 map 上的一点，故 <code>L</code> 为<br>$$L=H_o * W_o=\\left(\\frac {H - [d(h-1)+1] + 2p} {s}+1\\right) \\left(\\frac {W - [d(w-1)+1] + 2p} {s}+1\\right)$$</p>\n<p>好了，现在 Fold 要做的事情是反过来的，已知 fold 的输入为 <code>L</code> 个数据块，大小为 <code>(N,C*h*w,L)</code>，有关的构造参数为卷积核 size <code>(h,w)</code>，dilation，padding，stride，以及，指定最终的（Fold）输出大小 <code>(H,W)</code>，注意，Fold 做的事情是反过来的，也就是说，从 <code>L</code> 个数据块中恢复出原来普通卷积的输入 map 的大小，即 <code>(H,W)</code>，不是做完卷积之后的输出 map 的大小，记住，<strong>Fold 的输出是普通卷积的输入</strong>。</p>\n<p>Fold 的这些构造参数指明了卷积核大小，以及卷积输入的大小，然后根据其（这里指 Fold）输入 <code>L</code> 个数据块的 tensor，size 为 <code>(N,C*h*w,L)</code>，恢复出卷积输入的 tensor，因为 Fold 的构造参数中指定了 卷据输入 map 的 <code>(H,W)</code>，而批大小 <code>N</code> 也已知，所以要求出通道 <code>C</code>，根据 Fold 输入 tensor 的 第二个维度值 <code>C*h*w</code> 以及 Fold 的构造参数中卷积核大小 <code>(h,w)</code> 很容易得到通道 <code>C</code>。</p>\n<p>先使用 PyTorch 文档中的例子加以说明，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>fold = nn.Fold(output_size=(<span class=\"number\">4</span>, <span class=\"number\">5</span>), kernel_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))  <span class=\"comment\"># (H,W)=(4,5), (h,w)=(2,2)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>input = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">3</span> * <span class=\"number\">2</span> * <span class=\"number\">2</span>, <span class=\"number\">12</span>)   <span class=\"comment\"># (N, C*h*w,L)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output = fold(input)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>])    <span class=\"comment\"># (N,C,H,W)</span></span><br></pre></td></tr></table></figure>\n\n<p>将数据维度从 <code>H,W</code> 扩展到更多维度，就是 PyTorch 文档中关于 <code>L</code> 的计算式了，如下<br>$$L=\\prod_d \\lfloor \\frac {\\text{output_size} [d] + 2 \\times \\text{padding}[d]-\\text{dilation}[d] \\times (\\text{kernel_size}[d]-1) -1} {\\text{stride}[d]} +1 \\rfloor$$</p>\n<p><strong>总结：</strong></p>\n<p>Fold 的输入 size 为 $(N, C \\times \\prod(\\text{kernel_size}), L)$，输出 size 为 $(N,C, \\text{output_size}[0], \\text{output_size}[1], …)$</p>\n<h2 id=\"Unfold\"><a href=\"#Unfold\" class=\"headerlink\" title=\"Unfold\"></a>Unfold</h2><p>这是 torch.nn.Unfold 类，所做的事情与 Fold 相反，根据普通卷积的输入 tensor 以及卷积核大小，dilation，padding 和 stride 等计算得到 <code>L</code> 个与卷积核做卷积操作的数据块。<code>L</code> 计算方式如上。Unfold 的输入 size 为 $(N,C,*)$，其中 * 表示多维数据，输出 size 为 $(N,C \\times \\prod(\\text{kernel_size}), L)$。</p>\n<p>引用PyTorch 文档中的例子，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>unfold = nn.Unfold(kernel_size=(<span class=\"number\">2</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>input = torch.randn(<span class=\"number\">2</span>, <span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output = unfold(input)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"comment\"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"comment\"># 4 blocks (2x3 kernels) in total in the 3x4 input</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">2</span>, <span class=\"number\">30</span>, <span class=\"number\">4</span>])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-Normalization\"><a href=\"#2-Normalization\" class=\"headerlink\" title=\"2. Normalization\"></a>2. Normalization</h1><h2 id=\"BatchNorm\"><a href=\"#BatchNorm\" class=\"headerlink\" title=\"BatchNorm\"></a>BatchNorm</h2><p>批归一化是针对一个 mini-batch 内的数据进行归一化。首先给出归一化公式：<br>$$y=\\frac {x-E[x]} {\\sqrt{V[x]+\\epsilon}} * \\gamma + \\beta$$</p>\n<p>批归一化过程为：<br>BatchNorm Layer 的输入（mini-batch）为 $\\mathcal B={x_{1…m}}$，可学习参数为 $\\gamma, \\beta$。计算 mini-batch 的均值，方差<br>$$\\mu_{\\mathcal B} = \\frac 1 m \\sum_{i=1}^m x_i, \\quad \\sigma_{\\mathcal B}^2=\\frac 1 m \\sum_{i=1}^m(x_i - \\mu_{\\mathcal B})^2$$<br>然后计算归一化后的值<br>$$\\hat x_i = \\frac {x_i - \\mu_{\\mathcal B}} {\\sqrt {\\sigma_{\\mathcal B}^2+ \\epsilon}}$$<br>最后进行 scale 和 shift，<br>$$y_i=\\hat x_i \\cdot \\gamma + \\beta$$</p>\n<p><strong>小结：</strong> 沿着 batch 方向进行归一化</p>\n<h2 id=\"LayerNorm\"><a href=\"#LayerNorm\" class=\"headerlink\" title=\"LayerNorm\"></a>LayerNorm</h2><p>Layer 归一化是针对某个数据（样本）内部进行归一化，假设某个数据样本到达 LayerNorm 层为 $x$，无论 $x$ 是多少维的 tensor，均可以看作是 1D vector，即 $x=(x_1,…x_H)$，$H$ 是 LayerNorm 层的单元数（也是 $x$ 的特征数），于是 LayerNorm 过程为<br>$$\\mu=\\frac 1 H \\sum_{i=1}^H x_i, \\quad \\sigma^2=\\frac 1 H \\sum_{i=1}^H (x_i-\\mu)^2$$<br>于是 LayerNorm 后的值为<br>$$y=\\frac {x-\\mu} {\\sqrt {\\sigma^2+\\epsilon}} \\cdot \\gamma + \\beta$$</p>\n<p><strong>小结：</strong> 沿着特征方向进行归一化（特征包含了除 batch 维度外的其他所有维度）</p>\n<p>有了前面的归一化介绍，我们知道归一化过程都很类似，区别在于如何计算 $\\mu, \\sigma$，或者说沿着什么方向进行归一化。</p>\n<h2 id=\"InstanceNorm\"><a href=\"#InstanceNorm\" class=\"headerlink\" title=\"InstanceNorm\"></a>InstanceNorm</h2><p>对于每个样例的每个 channel 分别计算 $\\mu, \\sigma$。假设输入为 $(N,C,H,W)$，那么沿着 $(H,W)$ 方向做归一化。</p>\n<h2 id=\"GroupNorm\"><a href=\"#GroupNorm\" class=\"headerlink\" title=\"GroupNorm\"></a>GroupNorm</h2><p>GroupNorm 是选择一组 channels 进行归一化，所以是介于 InstanceNorm（单个channel）和 LayerNorm （全部 channels）之间的。</p>\n<h1 id=\"3-Pool\"><a href=\"#3-Pool\" class=\"headerlink\" title=\"3. Pool\"></a>3. Pool</h1><p>池化操作都比较简单易懂，这里介绍几个非常规的池化操作。</p>\n<h2 id=\"FractionalMaxPool2d\"><a href=\"#FractionalMaxPool2d\" class=\"headerlink\" title=\"FractionalMaxPool2d\"></a>FractionalMaxPool2d</h2><p>引用论文 <a href=\"https://arxiv.org/abs/1412.6071\" target=\"_blank\" rel=\"noopener\">Fractional MaxPooling</a>。</p>\n<p>pool 操作通常是用于降低 feature map 的大小，以常规的 <code>2x2</code> max-pooling 为例，记输入大小为 $N_{in} \\times N_{in}$，输出大小为 $N_{out} \\times N_{out}$，那么有<br>$$N_{out}=\\frac {N_{in}-k+2p} {s} + 1= N_{in} / 2 \\Rightarrow N_{in} /N_{out} = 2$$</p>\n<p>将 $N_{in} \\times N_{in}$ 的 feature map 划分出 $N_{out}^2$ 个 pooling 区域 $(P_{i,j})$。我们用 ${1,2,…,N_{in}}^2$ （或 $[1,N_{in}]^2$）表示输入 feature map，pixel 使用坐标点表示，显然 pooling 区域满足<br>$$P_{i,j} \\subset {1,2,…,N_{in}}, \\quad (i,j) \\in {1,…,N_{out}}^2$$</p>\n<p>现在，我们想让 $N_{in} / N_{out} \\in (1,2)$，或者为了提高速度，让 $N_{in} / N_{out} \\in (2,3)$，反正，这个比例不再是整数，这就是 Fractional max-pooling（FMP）。</p>\n<p>那么，FMP 具体是如何实现的呢？</p>\n<p>令两个递增序列 $(a_i)<em>{i=0}^{N</em>{out}}, \\ (b_i)<em>{i=0}^{N</em>{out}}$ 均以 <code>1</code> 开始，$N_{in}$ 结尾，递增量为 <code>1</code> 或者 <code>2</code>，即 $\\forall i,\\ a_{i+1}-a_{i} \\in {1,2}$，那么 pooling 区域可以有如下两种表示：<br>$$P_{i,j}=[a_{i-1}, a_i-1] \\times [b_{j-1},b_j-1], \\quad i,j \\in {1,…,N_{out}}<br>\\\\ P_{i,j}=[a_{i-1}, a_i] \\times [b_{j-1},b_j], \\quad i,j \\in {1,…,N_{out}}$$<br>注意下标 <code>i,j</code> 的范围。</p>\n<p>第一种是 <code>disjoint</code> 表示，第二种是 <code>overlapping</code> 表示。显然使用第二种表示，相邻两个 pooling 区域是有重叠的，而第一种表示则不会。</p>\n<p>记下采样率 $\\alpha = N_{in} / N_{out}$，有如下两种方法得到 $(a_i)<em>{i=0}^{N</em>{out}}$</p>\n<ol>\n<li><p><code>random</code> 方法</p>\n<p>当 $\\alpha$ 给定，那么 $(a_i-a_{i-1})<em>{i=1}^{N</em>{out}}$ 这个序列中有多少个 <code>1</code> 和多少个 <code>2</code> 已经是确定的了，将适量的 <code>1</code> 和 <code>2</code> shuffle 或者 random permutation，然后可可到 $\\alpha = N_{in} / N_{out}$</p>\n</li>\n<li><p><code>pseudorandom</code> 方法</p>\n<p>经过 $(0,0), \\ (N_{out}, N_{in}-1)$ 的直线，其斜率为 $\\alpha$（实际上是比下采样率小一点点，但是没关系，这两个值只要同时位于 $(1,2)$ 之间即可），将这个直线沿 y 轴 平移 $\\alpha \\cdot u$，其中 $\\alpha \\in (1,2), \\ u \\in (0,1), \\alpha \\cdot u \\in (0,1)$，即<br>$$y=\\alpha(i+u)$$<br>在此直线上取点，x 值依次为 $0,1,2,…,N_{out}$，对 y 值在应用 ceiling 函数，作为 $a_i$ 的值，<br>$$a_i=\\text{ceiling}(\\alpha(i+u)), \\quad i=0,1,2,…,N_{out}$$<br>验证一下 ${a_i}$ 序列是否满足上述条件：</p>\n<ul>\n<li>$i=0$，$a_0=\\text{ceiling}(\\alpha \\cdot u)$，由于 $\\alpha \\cdot u \\in (0,1)$，故 $a_0=1$</li>\n<li>$i=N_{out}$，$a_{N_{out}}=\\text{ceiling}(N_{in}-1+\\alpha \\cdot u)=N_{in}$</li>\n<li><code>otherwise：</code> $a_{i+1}-a_i=\\text{ceiling}(\\alpha \\cdot i+\\alpha+\\alpha \\cdot u)-\\text{ceiling}(\\alpha \\cdot i+\\alpha \\cdot u)$。验证说明如下。</li>\n</ul>\n<p>下面验证最后一种情况：</p>\n<p>记 $\\alpha \\cdot i+\\alpha \\cdot u=f \\in [k,k+1)$，k 是某个整数，那么当</p>\n<ul>\n<li><p><code>f=k</code> 时，</p>\n<p>   $a_{i+1}-a_i=\\text{ceiling}(k+\\alpha)-k=k+\\text{ceiling}(\\alpha)-k=\\text{ceiling}(\\alpha)=2$</p>\n</li>\n<li><p><code>k&lt;f&lt;k+1</code> 时，</p>\n<p>   $k+1&lt;f+\\alpha&lt;k+3$</p>\n<p>   $a_{i+1}-a_i=\\text{ceiling}(f+\\alpha)-k-1 \\in {1,2}$</p>\n</li>\n</ul>\n<p>至此，验证了 $(a_i)$ 序列满足条件。显然，基于直线取离散点然后应用 ceiling 函数得到的是一种伪随机序列。</p>\n</li>\n</ol>\n<h1 id=\"4-ConvTranspose\"><a href=\"#4-ConvTranspose\" class=\"headerlink\" title=\"4. ConvTranspose\"></a>4. ConvTranspose</h1><h2 id=\"输出大小\"><a href=\"#输出大小\" class=\"headerlink\" title=\"输出大小\"></a>输出大小</h2><p>转置卷积，通常又称反卷积、逆卷积，然而转置卷积并非卷积的逆过程，并且转置卷积其实也是一种卷积，只不过与卷积相反的是，输出平面的大小通常不是变小而是变大。对于普通卷积，设输入平面边长为 $L_{in}$，输出平面边长为 $L_{out}$，卷积核边长为 $k$，dilation 、stride 和 padding 分别为 $d, p, s$，那么有<br>$$L_{out}=\\frac {L_{in}-(d(k-1)+1)+2p} s + 1 \\qquad (4-1)$$<br>对于转置卷积，令 $L_{in}^{\\top}, \\ L_{out}^{\\top}$ 分别表示输入和输出的边长，于是有<br>$$L_{out}^{\\top}=s(L_{in}^{\\top} - 1) +d(k-1)+1 - 2p \\qquad (4-2)$$<br>可见，转置卷积的输入输出边长的关系与普通卷积是反过来的。</p>\n<h2 id=\"转置卷积计算\"><a href=\"#转置卷积计算\" class=\"headerlink\" title=\"转置卷积计算\"></a>转置卷积计算</h2><h3 id=\"第一种方法\"><a href=\"#第一种方法\" class=\"headerlink\" title=\"第一种方法\"></a>第一种方法</h3><p>回顾一下卷积过程，以二维卷积为例，假设输入大小为 $4 \\times 4$，卷积核 $3 \\times 3$，不考虑 padding，且 stride 为 1，那么根据 $(4-1)$ 式输出大小为 $2 \\times 2$，我们可以用卷积核在输入平面上滑窗并做卷积来理解卷积，实际计算则是根据输入矩阵得到 $4 \\times 9$ 的矩阵（<strong>部分 element 用 0 填充</strong>），然后将卷积核展开成 $9 \\times 1$ 的矩阵，然后进行卷积相乘得到 $4 \\times 1$ 的输出矩阵。</p>\n<p>我们再看转置卷积，输入大小为 $2 \\times 2$，卷积核大小为 $3 \\times 3$，同样地，不考虑 padding 且 stride 为 1，那么根据 $(4-2)$ 式输出大小为 $4 \\times 4$，实际的计算过程为：<strong>由于转置卷积也是一个普通卷积</strong>，所以先将输入矩阵 zero-padding 为 $6\\times 6$ 的矩阵（$6 \\times 6$ 的输入矩阵经过 $3 \\times 3$ 的卷积才能得到 $4 \\times 4$ 的输出大小），然后与普通卷积一样地得到为 $16 \\times 9$ 的矩阵，卷积核 <strong>旋转 180°</strong>，然后 reshape 为 $9 \\times 1$ 的矩阵，通过矩阵乘法，得到矩阵大小为 $16 \\times 1$，然后 reshape 为 $4 \\times 4$，此即输出矩阵。</p>\n<p>下面我们画图来展示卷积和转置卷积地过程：</p>\n<p><img src=\"/images/pytorch_mth_conv.png\" alt=\"普通卷积\"><center>普通卷积</center></p>\n<p><img src=\"/images/pytorch_mtd_conv_t.png\" alt=\"转置卷积\"><center>转置卷积</center></p>\n<p>使用 Pytorch 进行验证：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\">convt = nn.ConvTranspose2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">convt.bias = nn.Parameter(torch.tensor([<span class=\"number\">0.</span>]))</span><br><span class=\"line\">convt.weight = nn.Parameter(torch.tensor([[[[<span class=\"number\">0.</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>],</span><br><span class=\"line\">                                            [<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                                            [<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>]]]]))</span><br><span class=\"line\">input = torch.tensor([[[[<span class=\"number\">12.</span>,<span class=\"number\">12</span>],</span><br><span class=\"line\">                        [<span class=\"number\">10</span>,<span class=\"number\">17</span>]]]])</span><br><span class=\"line\">output = convt(input)</span><br><span class=\"line\">output</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[ 0., 12., 36., 24.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [24., 58., 61., 34.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [20., 66., 70., 24.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 0., 10., 37., 34.]]]])</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"第二种方法\"><a href=\"#第二种方法\" class=\"headerlink\" title=\"第二种方法\"></a>第二种方法</h3><p>还有另一种方法来理解计算卷积和转置卷积。还是以上面的例子进行说明。</p>\n<p>普通卷积中，输入矩阵 reshape 为 $1 \\times 16$。因为有 4 个滑窗卷积动作，所以将卷积核分别以四种不同的 zero-padding 方式得到 4 个 $4 \\times 4$ 的矩阵（即，卷积核的 $3 \\times 4$ 部分位于 $4 \\times 4$ 矩阵的左上角、右上角，左下角和右下角，其他位置 zero-padding），然后 reshape 为 $16 \\times 4$，记这个 $16 \\times 4$ 的矩阵为 $K$， 得到 $1 \\times 4$ 矩阵，reshape 为 $2 \\times 2$ 即输出矩阵。</p>\n<p>转置卷积中，输入矩阵大小为 $2 \\times 2$（即 <code>[12,12,10,17]</code>），直接 reshape 为 $1 \\times 4$，将上面的矩阵 $K$ <strong>转置</strong>，得到 $4 \\times 16$ 的矩阵，然后矩阵相乘得到 $1 \\times 16$ 矩阵，最后 reshape 为 $4 \\times 4$ 即为输出矩阵。</p>\n<p>普通卷积的过程如下图示意，转置卷积非常简单，读者可以自己画图验证。</p>\n<p><img src=\"/images/pytorch_mtd_conv_t_1.png\" alt=\"卷积的另一种计算过程\"><center>卷积的另一种计算过程</center></p>\n<p>从转置卷积得到的结果来看，很明显，转置卷积不是普通卷积的逆过程。</p>\n<h3 id=\"dilation-gt-1\"><a href=\"#dilation-gt-1\" class=\"headerlink\" title=\"dilation &gt; 1\"></a>dilation &gt; 1</h3><p>现在，我们的讨论还未结束，来看 <code>dilation</code> 不为 1 的情况，例如 <code>dilation=2</code>，还是使用上面的例子，对于转置卷积，此时根据 $(4-2)$ 式得到输出矩阵大小为 $6 \\times 6$，将卷积核膨胀后得到 $5 \\times 5$ 矩阵（间隔填充 0），并 <strong>旋转 180°</strong>，由于转置卷积也是一种普通卷积，所以应该将输入矩阵 zero-padding 到 $10 \\times 10$ 大小才能得到 $6 \\times 6$ 的输出，也就是说，输入矩阵上下左右均进行 4 个单位的 zero-padding，</p>\n<p>记 <code>input</code> 为 $I$，zero-padding后，$I[4:6,4:6]=[[12.,12],[10,17]]$，其余位置为 <code>0</code>，膨胀后的卷积核 <strong>旋转 180°</strong> 后为 $K’=[[2., 0, 1, 0, 0],[0,0,0,0,0],[0,0,2,0,2],[0,0,0,0,0],[2,0,1,0,0]]$，可以手动计算卷积后的输出矩阵，这里给出 python 代码计算示例，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">convt1 = nn.ConvTranspose2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>,dilation=<span class=\"number\">2</span>)</span><br><span class=\"line\">convt1.bias = nn.Parameter(torch.tensor([<span class=\"number\">0.</span>]))</span><br><span class=\"line\">convt1.weight = nn.Parameter(torch.tensor([[[[<span class=\"number\">0.</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>]]]]))</span><br><span class=\"line\">output1 = convt1(input)</span><br><span class=\"line\">output1</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"stride-gt-1\"><a href=\"#stride-gt-1\" class=\"headerlink\" title=\"stride &gt; 1\"></a>stride &gt; 1</h3><p>依然以上面的例子进行说明，假设现在 <code>stride=2</code>，根据式 $(4-2)$ 转置卷积的输出大小为 $5 \\times 5$。把转置卷积看作一种普通卷积，那么其输入大小应该为 $7 \\times 7$，由于 <code>stride=2</code>，所以先将 $2 \\times 2$ 输入矩阵膨胀为 $3 \\times 3$ 的矩阵（2*(2-1)+1=3），然后再 zero-padding 成 $7 \\times 7$ 的矩阵（上下左右 padding 的数量均为 (7-3)/2=2），经过这番处理，输入矩阵变为 $I[2,2]=I[2,4]=12, \\ I[4,2]=10, \\ I[4,4]=17$，其余位置均为 <code>0</code>，卷积核 <strong>旋转 180°</strong> 后为 $K’=[[2., 1, 0],[0,2,2],[2,1,0]]$，于是可以手动计算出卷积后的矩阵，这里给出 python 代码计算示例，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">convt.stride = (<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">output = convt(input)</span><br><span class=\"line\">output</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"padding-gt-0\"><a href=\"#padding-gt-0\" class=\"headerlink\" title=\"padding &gt; 0\"></a>padding &gt; 0</h3><p>继续以上面的例子进行说明，假设现在 <code>padding=1</code>，根据式 $(4-2)$ 转置卷积的输出大小为 $2 \\times 2$。将输入矩阵上下左右均进行 1 单位的 zero-padding，得到矩阵大小 $4 \\times 4$，卷积核大小 $3 \\times 3$，计算过程还是将卷积核 <strong>旋转 180°</strong>，卷积计算过程略，不过相信这是足够简单的事情。</p>\n<p>以上 <code>dialtion &gt; 1, stride &gt; 1, padding &gt; 0</code> 三种情况，除了可使用 python 程序验证，还可以使用 <code>第二种方法</code> 进行验证对输入矩阵以及卷积核的处理是正确的，并且，也可以使用 <code>第一种方法</code> 对输入矩阵和卷积核进行处理然后进行普通卷积计算得到输出矩阵。</p>\n<h1 id=\"5-Upsample\"><a href=\"#5-Upsample\" class=\"headerlink\" title=\"5. Upsample\"></a>5. Upsample</h1><p>输入维度为 <code>minibatch x channels x [optional depth] x [optional height] x width</code>，即，输入可以是 3D/4D/5D。可用的算法包括 <code>nearest neighbor, linear, bilinear, bicubic, trilinear</code>。</p>\n<h2 id=\"nearest-neighbor\"><a href=\"#nearest-neighbor\" class=\"headerlink\" title=\"nearest neighbor\"></a>nearest neighbor</h2><p>顾名思义，就是使用原平面上最近的一点作为上采样后的值。例如原平面 size 为 $m \\times m$，在原平面上建立坐标系 S，上采样后的 size 为 $n \\times n, \\ n &gt; m$，设其上点的坐标为 $(x,y), \\ x,y =0,1,…,n-1$。将上采样后平面点映射到 S 中，对应坐标记为 $(x’,y’)$，那么有</p>\n<p>$$\\frac {x-0} {n-1-0}= \\frac {x’-0}{m-1-0} \\Rightarrow x’ = \\frac {m-1} {n-1} \\cdot x$$<br>同理有 $y’ = \\frac {m-1} {n-1} \\cdot y$，然后找出与点 $(i’,j’)$ 最近的那个整数坐标点，显然必然在以下四个点中产生 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$ （这四个点可能有重合），分别计算 $(x’,y’)$ 与这四个点的距离，距离最小的那个点的值即作为 $(x,y)$ 上采样后的值。（使用哪种距离指标，可以查看 PyTorch 底层实现代码，这里本人尚未去查看。）</p>\n<h2 id=\"bilinear\"><a href=\"#bilinear\" class=\"headerlink\" title=\"bilinear\"></a>bilinear</h2><p>输入必须是 4D。</p>\n<h3 id=\"align-corners-True\"><a href=\"#align-corners-True\" class=\"headerlink\" title=\"align_corners=True\"></a>align_corners=True</h3><p>双线性插值。记四个顶点为 $(x_1,y_1), \\ (x_1,y_2), \\ (x_2,y_1), \\ (x_2,y_2)$，然后求目标点 $(x,y), \\ x_1 \\le x \\le x_2, \\ y_1 \\le y \\le y_2$ 的值。沿 x 轴线性插值，<br>$$f(x,y_1)=\\frac {f_{21}-f_{11}} {x_2-x_1} \\cdot (x-x_1)+f_{11}<br>\\\\ f(x,y_2)=\\frac {f_{22}-f_{12}} {x_2-x_1} \\cdot (x-x_1)+f_{12}<br>\\\\ f(x,y)=\\frac {f_(x,y_2)-f(x,y_1)} {y_2-y_1} \\cdot (y-y_1)+f(x,y_1)$$</p>\n<p>与 <code>nearest neighbor</code> 中一样，首先将点 $(x,y)$ 映射到原平面上一点 $(x’,y’)$，然后四个顶点为 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$。用这种映射方法，显然原平面的四个 corners 和上采样后平面的四个 corners 分别对齐，这就是 <code>align_corners=True</code> 的由来。</p>\n<h3 id=\"align-corners-False\"><a href=\"#align-corners-False\" class=\"headerlink\" title=\"align_corners=False\"></a>align_corners=False</h3><p>如下图所示，显示了 <code>align_corners</code> 不同值的区别。<br><img src=\"/images/pytorch_mtd_aligncorners.png\" alt=\"\"><center>图源 <a href=\"https://discuss.pytorch.org/t/what-we-should-use-align-corners-false/22663/9\" target=\"_blank\" rel=\"noopener\">pytorch 论坛</a></center></p>\n<p>从图中可以发现，映射回原平面坐标时，坐标计算方式不同，例如上菜以后平面上一点 $(x,y)$，映射回 S 中的坐标为<br>$$x’=(x+0.5)/2-0.5<br>\\\\ y’=(y+0.5)/2-0.5$$</p>\n<p>此后的插值方式一致（毕竟都是双线性插值），找到最近的 4 个点 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$ 进行双线性插值。</p>\n<h2 id=\"linear\"><a href=\"#linear\" class=\"headerlink\" title=\"linear\"></a>linear</h2><p>与 bilinear 类似，但是输入维度必须是 3D。</p>\n<h2 id=\"trilinear\"><a href=\"#trilinear\" class=\"headerlink\" title=\"trilinear\"></a>trilinear</h2><p>与 bilinear 类似，但是输入维度必须是 5D。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-Fold-Unfold\"><a href=\"#1-Fold-Unfold\" class=\"headerlink\" title=\"1. Fold / Unfold\"></a>1. Fold / Unfold</h1><h2 id=\"Fold\"><a href=\"#Fold\" class=\"headerlink\" title=\"Fold\"></a>Fold</h2><p>这是 torch.nn.Fold 类。</p>\n<p>首先我们来复习一下卷积过程，设输入 size 为 <code>(N,C,H,W)</code>，卷积 kernel size 为 <code>(C,h,w)</code>，碰撞系数为 <code>d</code>，padding 为 <code>p</code>，stride 记为 <code>s</code>，那么整个过程相当于将 <code>Cxhxw</code> 大小的数据块在输入数据中滑动，每一次滑动做一次卷积，记共有 <code>L</code> 次卷积，即，从输入数据中切分出 <code>L</code> 个数据块与卷积核做卷积，当然每个数据块的大小与卷积核相同，为 <code>(C,h,w)</code>，最后得到的输出 map 大小为</p>","more":"<p>$$H_o= \\frac {H - [d(h-1)+1] + 2p} {s}+1<br>\\\\ W_o= \\frac {W - [d(w-1)+1] + 2p} {s}+1$$<br>因为每一次卷积得到的值均作为输出 map 上的一点，故 <code>L</code> 为<br>$$L=H_o * W_o=\\left(\\frac {H - [d(h-1)+1] + 2p} {s}+1\\right) \\left(\\frac {W - [d(w-1)+1] + 2p} {s}+1\\right)$$</p>\n<p>好了，现在 Fold 要做的事情是反过来的，已知 fold 的输入为 <code>L</code> 个数据块，大小为 <code>(N,C*h*w,L)</code>，有关的构造参数为卷积核 size <code>(h,w)</code>，dilation，padding，stride，以及，指定最终的（Fold）输出大小 <code>(H,W)</code>，注意，Fold 做的事情是反过来的，也就是说，从 <code>L</code> 个数据块中恢复出原来普通卷积的输入 map 的大小，即 <code>(H,W)</code>，不是做完卷积之后的输出 map 的大小，记住，<strong>Fold 的输出是普通卷积的输入</strong>。</p>\n<p>Fold 的这些构造参数指明了卷积核大小，以及卷积输入的大小，然后根据其（这里指 Fold）输入 <code>L</code> 个数据块的 tensor，size 为 <code>(N,C*h*w,L)</code>，恢复出卷积输入的 tensor，因为 Fold 的构造参数中指定了 卷据输入 map 的 <code>(H,W)</code>，而批大小 <code>N</code> 也已知，所以要求出通道 <code>C</code>，根据 Fold 输入 tensor 的 第二个维度值 <code>C*h*w</code> 以及 Fold 的构造参数中卷积核大小 <code>(h,w)</code> 很容易得到通道 <code>C</code>。</p>\n<p>先使用 PyTorch 文档中的例子加以说明，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>fold = nn.Fold(output_size=(<span class=\"number\">4</span>, <span class=\"number\">5</span>), kernel_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))  <span class=\"comment\"># (H,W)=(4,5), (h,w)=(2,2)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>input = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">3</span> * <span class=\"number\">2</span> * <span class=\"number\">2</span>, <span class=\"number\">12</span>)   <span class=\"comment\"># (N, C*h*w,L)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output = fold(input)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>])    <span class=\"comment\"># (N,C,H,W)</span></span><br></pre></td></tr></table></figure>\n\n<p>将数据维度从 <code>H,W</code> 扩展到更多维度，就是 PyTorch 文档中关于 <code>L</code> 的计算式了，如下<br>$$L=\\prod_d \\lfloor \\frac {\\text{output_size} [d] + 2 \\times \\text{padding}[d]-\\text{dilation}[d] \\times (\\text{kernel_size}[d]-1) -1} {\\text{stride}[d]} +1 \\rfloor$$</p>\n<p><strong>总结：</strong></p>\n<p>Fold 的输入 size 为 $(N, C \\times \\prod(\\text{kernel_size}), L)$，输出 size 为 $(N,C, \\text{output_size}[0], \\text{output_size}[1], …)$</p>\n<h2 id=\"Unfold\"><a href=\"#Unfold\" class=\"headerlink\" title=\"Unfold\"></a>Unfold</h2><p>这是 torch.nn.Unfold 类，所做的事情与 Fold 相反，根据普通卷积的输入 tensor 以及卷积核大小，dilation，padding 和 stride 等计算得到 <code>L</code> 个与卷积核做卷积操作的数据块。<code>L</code> 计算方式如上。Unfold 的输入 size 为 $(N,C,*)$，其中 * 表示多维数据，输出 size 为 $(N,C \\times \\prod(\\text{kernel_size}), L)$。</p>\n<p>引用PyTorch 文档中的例子，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>unfold = nn.Unfold(kernel_size=(<span class=\"number\">2</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>input = torch.randn(<span class=\"number\">2</span>, <span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output = unfold(input)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"comment\"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"comment\"># 4 blocks (2x3 kernels) in total in the 3x4 input</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>output.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">2</span>, <span class=\"number\">30</span>, <span class=\"number\">4</span>])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-Normalization\"><a href=\"#2-Normalization\" class=\"headerlink\" title=\"2. Normalization\"></a>2. Normalization</h1><h2 id=\"BatchNorm\"><a href=\"#BatchNorm\" class=\"headerlink\" title=\"BatchNorm\"></a>BatchNorm</h2><p>批归一化是针对一个 mini-batch 内的数据进行归一化。首先给出归一化公式：<br>$$y=\\frac {x-E[x]} {\\sqrt{V[x]+\\epsilon}} * \\gamma + \\beta$$</p>\n<p>批归一化过程为：<br>BatchNorm Layer 的输入（mini-batch）为 $\\mathcal B={x_{1…m}}$，可学习参数为 $\\gamma, \\beta$。计算 mini-batch 的均值，方差<br>$$\\mu_{\\mathcal B} = \\frac 1 m \\sum_{i=1}^m x_i, \\quad \\sigma_{\\mathcal B}^2=\\frac 1 m \\sum_{i=1}^m(x_i - \\mu_{\\mathcal B})^2$$<br>然后计算归一化后的值<br>$$\\hat x_i = \\frac {x_i - \\mu_{\\mathcal B}} {\\sqrt {\\sigma_{\\mathcal B}^2+ \\epsilon}}$$<br>最后进行 scale 和 shift，<br>$$y_i=\\hat x_i \\cdot \\gamma + \\beta$$</p>\n<p><strong>小结：</strong> 沿着 batch 方向进行归一化</p>\n<h2 id=\"LayerNorm\"><a href=\"#LayerNorm\" class=\"headerlink\" title=\"LayerNorm\"></a>LayerNorm</h2><p>Layer 归一化是针对某个数据（样本）内部进行归一化，假设某个数据样本到达 LayerNorm 层为 $x$，无论 $x$ 是多少维的 tensor，均可以看作是 1D vector，即 $x=(x_1,…x_H)$，$H$ 是 LayerNorm 层的单元数（也是 $x$ 的特征数），于是 LayerNorm 过程为<br>$$\\mu=\\frac 1 H \\sum_{i=1}^H x_i, \\quad \\sigma^2=\\frac 1 H \\sum_{i=1}^H (x_i-\\mu)^2$$<br>于是 LayerNorm 后的值为<br>$$y=\\frac {x-\\mu} {\\sqrt {\\sigma^2+\\epsilon}} \\cdot \\gamma + \\beta$$</p>\n<p><strong>小结：</strong> 沿着特征方向进行归一化（特征包含了除 batch 维度外的其他所有维度）</p>\n<p>有了前面的归一化介绍，我们知道归一化过程都很类似，区别在于如何计算 $\\mu, \\sigma$，或者说沿着什么方向进行归一化。</p>\n<h2 id=\"InstanceNorm\"><a href=\"#InstanceNorm\" class=\"headerlink\" title=\"InstanceNorm\"></a>InstanceNorm</h2><p>对于每个样例的每个 channel 分别计算 $\\mu, \\sigma$。假设输入为 $(N,C,H,W)$，那么沿着 $(H,W)$ 方向做归一化。</p>\n<h2 id=\"GroupNorm\"><a href=\"#GroupNorm\" class=\"headerlink\" title=\"GroupNorm\"></a>GroupNorm</h2><p>GroupNorm 是选择一组 channels 进行归一化，所以是介于 InstanceNorm（单个channel）和 LayerNorm （全部 channels）之间的。</p>\n<h1 id=\"3-Pool\"><a href=\"#3-Pool\" class=\"headerlink\" title=\"3. Pool\"></a>3. Pool</h1><p>池化操作都比较简单易懂，这里介绍几个非常规的池化操作。</p>\n<h2 id=\"FractionalMaxPool2d\"><a href=\"#FractionalMaxPool2d\" class=\"headerlink\" title=\"FractionalMaxPool2d\"></a>FractionalMaxPool2d</h2><p>引用论文 <a href=\"https://arxiv.org/abs/1412.6071\" target=\"_blank\" rel=\"noopener\">Fractional MaxPooling</a>。</p>\n<p>pool 操作通常是用于降低 feature map 的大小，以常规的 <code>2x2</code> max-pooling 为例，记输入大小为 $N_{in} \\times N_{in}$，输出大小为 $N_{out} \\times N_{out}$，那么有<br>$$N_{out}=\\frac {N_{in}-k+2p} {s} + 1= N_{in} / 2 \\Rightarrow N_{in} /N_{out} = 2$$</p>\n<p>将 $N_{in} \\times N_{in}$ 的 feature map 划分出 $N_{out}^2$ 个 pooling 区域 $(P_{i,j})$。我们用 ${1,2,…,N_{in}}^2$ （或 $[1,N_{in}]^2$）表示输入 feature map，pixel 使用坐标点表示，显然 pooling 区域满足<br>$$P_{i,j} \\subset {1,2,…,N_{in}}, \\quad (i,j) \\in {1,…,N_{out}}^2$$</p>\n<p>现在，我们想让 $N_{in} / N_{out} \\in (1,2)$，或者为了提高速度，让 $N_{in} / N_{out} \\in (2,3)$，反正，这个比例不再是整数，这就是 Fractional max-pooling（FMP）。</p>\n<p>那么，FMP 具体是如何实现的呢？</p>\n<p>令两个递增序列 $(a_i)<em>{i=0}^{N</em>{out}}, \\ (b_i)<em>{i=0}^{N</em>{out}}$ 均以 <code>1</code> 开始，$N_{in}$ 结尾，递增量为 <code>1</code> 或者 <code>2</code>，即 $\\forall i,\\ a_{i+1}-a_{i} \\in {1,2}$，那么 pooling 区域可以有如下两种表示：<br>$$P_{i,j}=[a_{i-1}, a_i-1] \\times [b_{j-1},b_j-1], \\quad i,j \\in {1,…,N_{out}}<br>\\\\ P_{i,j}=[a_{i-1}, a_i] \\times [b_{j-1},b_j], \\quad i,j \\in {1,…,N_{out}}$$<br>注意下标 <code>i,j</code> 的范围。</p>\n<p>第一种是 <code>disjoint</code> 表示，第二种是 <code>overlapping</code> 表示。显然使用第二种表示，相邻两个 pooling 区域是有重叠的，而第一种表示则不会。</p>\n<p>记下采样率 $\\alpha = N_{in} / N_{out}$，有如下两种方法得到 $(a_i)<em>{i=0}^{N</em>{out}}$</p>\n<ol>\n<li><p><code>random</code> 方法</p>\n<p>当 $\\alpha$ 给定，那么 $(a_i-a_{i-1})<em>{i=1}^{N</em>{out}}$ 这个序列中有多少个 <code>1</code> 和多少个 <code>2</code> 已经是确定的了，将适量的 <code>1</code> 和 <code>2</code> shuffle 或者 random permutation，然后可可到 $\\alpha = N_{in} / N_{out}$</p>\n</li>\n<li><p><code>pseudorandom</code> 方法</p>\n<p>经过 $(0,0), \\ (N_{out}, N_{in}-1)$ 的直线，其斜率为 $\\alpha$（实际上是比下采样率小一点点，但是没关系，这两个值只要同时位于 $(1,2)$ 之间即可），将这个直线沿 y 轴 平移 $\\alpha \\cdot u$，其中 $\\alpha \\in (1,2), \\ u \\in (0,1), \\alpha \\cdot u \\in (0,1)$，即<br>$$y=\\alpha(i+u)$$<br>在此直线上取点，x 值依次为 $0,1,2,…,N_{out}$，对 y 值在应用 ceiling 函数，作为 $a_i$ 的值，<br>$$a_i=\\text{ceiling}(\\alpha(i+u)), \\quad i=0,1,2,…,N_{out}$$<br>验证一下 ${a_i}$ 序列是否满足上述条件：</p>\n<ul>\n<li>$i=0$，$a_0=\\text{ceiling}(\\alpha \\cdot u)$，由于 $\\alpha \\cdot u \\in (0,1)$，故 $a_0=1$</li>\n<li>$i=N_{out}$，$a_{N_{out}}=\\text{ceiling}(N_{in}-1+\\alpha \\cdot u)=N_{in}$</li>\n<li><code>otherwise：</code> $a_{i+1}-a_i=\\text{ceiling}(\\alpha \\cdot i+\\alpha+\\alpha \\cdot u)-\\text{ceiling}(\\alpha \\cdot i+\\alpha \\cdot u)$。验证说明如下。</li>\n</ul>\n<p>下面验证最后一种情况：</p>\n<p>记 $\\alpha \\cdot i+\\alpha \\cdot u=f \\in [k,k+1)$，k 是某个整数，那么当</p>\n<ul>\n<li><p><code>f=k</code> 时，</p>\n<p>   $a_{i+1}-a_i=\\text{ceiling}(k+\\alpha)-k=k+\\text{ceiling}(\\alpha)-k=\\text{ceiling}(\\alpha)=2$</p>\n</li>\n<li><p><code>k&lt;f&lt;k+1</code> 时，</p>\n<p>   $k+1&lt;f+\\alpha&lt;k+3$</p>\n<p>   $a_{i+1}-a_i=\\text{ceiling}(f+\\alpha)-k-1 \\in {1,2}$</p>\n</li>\n</ul>\n<p>至此，验证了 $(a_i)$ 序列满足条件。显然，基于直线取离散点然后应用 ceiling 函数得到的是一种伪随机序列。</p>\n</li>\n</ol>\n<h1 id=\"4-ConvTranspose\"><a href=\"#4-ConvTranspose\" class=\"headerlink\" title=\"4. ConvTranspose\"></a>4. ConvTranspose</h1><h2 id=\"输出大小\"><a href=\"#输出大小\" class=\"headerlink\" title=\"输出大小\"></a>输出大小</h2><p>转置卷积，通常又称反卷积、逆卷积，然而转置卷积并非卷积的逆过程，并且转置卷积其实也是一种卷积，只不过与卷积相反的是，输出平面的大小通常不是变小而是变大。对于普通卷积，设输入平面边长为 $L_{in}$，输出平面边长为 $L_{out}$，卷积核边长为 $k$，dilation 、stride 和 padding 分别为 $d, p, s$，那么有<br>$$L_{out}=\\frac {L_{in}-(d(k-1)+1)+2p} s + 1 \\qquad (4-1)$$<br>对于转置卷积，令 $L_{in}^{\\top}, \\ L_{out}^{\\top}$ 分别表示输入和输出的边长，于是有<br>$$L_{out}^{\\top}=s(L_{in}^{\\top} - 1) +d(k-1)+1 - 2p \\qquad (4-2)$$<br>可见，转置卷积的输入输出边长的关系与普通卷积是反过来的。</p>\n<h2 id=\"转置卷积计算\"><a href=\"#转置卷积计算\" class=\"headerlink\" title=\"转置卷积计算\"></a>转置卷积计算</h2><h3 id=\"第一种方法\"><a href=\"#第一种方法\" class=\"headerlink\" title=\"第一种方法\"></a>第一种方法</h3><p>回顾一下卷积过程，以二维卷积为例，假设输入大小为 $4 \\times 4$，卷积核 $3 \\times 3$，不考虑 padding，且 stride 为 1，那么根据 $(4-1)$ 式输出大小为 $2 \\times 2$，我们可以用卷积核在输入平面上滑窗并做卷积来理解卷积，实际计算则是根据输入矩阵得到 $4 \\times 9$ 的矩阵（<strong>部分 element 用 0 填充</strong>），然后将卷积核展开成 $9 \\times 1$ 的矩阵，然后进行卷积相乘得到 $4 \\times 1$ 的输出矩阵。</p>\n<p>我们再看转置卷积，输入大小为 $2 \\times 2$，卷积核大小为 $3 \\times 3$，同样地，不考虑 padding 且 stride 为 1，那么根据 $(4-2)$ 式输出大小为 $4 \\times 4$，实际的计算过程为：<strong>由于转置卷积也是一个普通卷积</strong>，所以先将输入矩阵 zero-padding 为 $6\\times 6$ 的矩阵（$6 \\times 6$ 的输入矩阵经过 $3 \\times 3$ 的卷积才能得到 $4 \\times 4$ 的输出大小），然后与普通卷积一样地得到为 $16 \\times 9$ 的矩阵，卷积核 <strong>旋转 180°</strong>，然后 reshape 为 $9 \\times 1$ 的矩阵，通过矩阵乘法，得到矩阵大小为 $16 \\times 1$，然后 reshape 为 $4 \\times 4$，此即输出矩阵。</p>\n<p>下面我们画图来展示卷积和转置卷积地过程：</p>\n<p><img src=\"/images/pytorch_mth_conv.png\" alt=\"普通卷积\"><center>普通卷积</center></p>\n<p><img src=\"/images/pytorch_mtd_conv_t.png\" alt=\"转置卷积\"><center>转置卷积</center></p>\n<p>使用 Pytorch 进行验证：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\">convt = nn.ConvTranspose2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">convt.bias = nn.Parameter(torch.tensor([<span class=\"number\">0.</span>]))</span><br><span class=\"line\">convt.weight = nn.Parameter(torch.tensor([[[[<span class=\"number\">0.</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>],</span><br><span class=\"line\">                                            [<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                                            [<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>]]]]))</span><br><span class=\"line\">input = torch.tensor([[[[<span class=\"number\">12.</span>,<span class=\"number\">12</span>],</span><br><span class=\"line\">                        [<span class=\"number\">10</span>,<span class=\"number\">17</span>]]]])</span><br><span class=\"line\">output = convt(input)</span><br><span class=\"line\">output</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[ 0., 12., 36., 24.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [24., 58., 61., 34.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [20., 66., 70., 24.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 0., 10., 37., 34.]]]])</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"第二种方法\"><a href=\"#第二种方法\" class=\"headerlink\" title=\"第二种方法\"></a>第二种方法</h3><p>还有另一种方法来理解计算卷积和转置卷积。还是以上面的例子进行说明。</p>\n<p>普通卷积中，输入矩阵 reshape 为 $1 \\times 16$。因为有 4 个滑窗卷积动作，所以将卷积核分别以四种不同的 zero-padding 方式得到 4 个 $4 \\times 4$ 的矩阵（即，卷积核的 $3 \\times 4$ 部分位于 $4 \\times 4$ 矩阵的左上角、右上角，左下角和右下角，其他位置 zero-padding），然后 reshape 为 $16 \\times 4$，记这个 $16 \\times 4$ 的矩阵为 $K$， 得到 $1 \\times 4$ 矩阵，reshape 为 $2 \\times 2$ 即输出矩阵。</p>\n<p>转置卷积中，输入矩阵大小为 $2 \\times 2$（即 <code>[12,12,10,17]</code>），直接 reshape 为 $1 \\times 4$，将上面的矩阵 $K$ <strong>转置</strong>，得到 $4 \\times 16$ 的矩阵，然后矩阵相乘得到 $1 \\times 16$ 矩阵，最后 reshape 为 $4 \\times 4$ 即为输出矩阵。</p>\n<p>普通卷积的过程如下图示意，转置卷积非常简单，读者可以自己画图验证。</p>\n<p><img src=\"/images/pytorch_mtd_conv_t_1.png\" alt=\"卷积的另一种计算过程\"><center>卷积的另一种计算过程</center></p>\n<p>从转置卷积得到的结果来看，很明显，转置卷积不是普通卷积的逆过程。</p>\n<h3 id=\"dilation-gt-1\"><a href=\"#dilation-gt-1\" class=\"headerlink\" title=\"dilation &gt; 1\"></a>dilation &gt; 1</h3><p>现在，我们的讨论还未结束，来看 <code>dilation</code> 不为 1 的情况，例如 <code>dilation=2</code>，还是使用上面的例子，对于转置卷积，此时根据 $(4-2)$ 式得到输出矩阵大小为 $6 \\times 6$，将卷积核膨胀后得到 $5 \\times 5$ 矩阵（间隔填充 0），并 <strong>旋转 180°</strong>，由于转置卷积也是一种普通卷积，所以应该将输入矩阵 zero-padding 到 $10 \\times 10$ 大小才能得到 $6 \\times 6$ 的输出，也就是说，输入矩阵上下左右均进行 4 个单位的 zero-padding，</p>\n<p>记 <code>input</code> 为 $I$，zero-padding后，$I[4:6,4:6]=[[12.,12],[10,17]]$，其余位置为 <code>0</code>，膨胀后的卷积核 <strong>旋转 180°</strong> 后为 $K’=[[2., 0, 1, 0, 0],[0,0,0,0,0],[0,0,2,0,2],[0,0,0,0,0],[2,0,1,0,0]]$，可以手动计算卷积后的输出矩阵，这里给出 python 代码计算示例，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">convt1 = nn.ConvTranspose2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>,dilation=<span class=\"number\">2</span>)</span><br><span class=\"line\">convt1.bias = nn.Parameter(torch.tensor([<span class=\"number\">0.</span>]))</span><br><span class=\"line\">convt1.weight = nn.Parameter(torch.tensor([[[[<span class=\"number\">0.</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>]]]]))</span><br><span class=\"line\">output1 = convt1(input)</span><br><span class=\"line\">output1</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"stride-gt-1\"><a href=\"#stride-gt-1\" class=\"headerlink\" title=\"stride &gt; 1\"></a>stride &gt; 1</h3><p>依然以上面的例子进行说明，假设现在 <code>stride=2</code>，根据式 $(4-2)$ 转置卷积的输出大小为 $5 \\times 5$。把转置卷积看作一种普通卷积，那么其输入大小应该为 $7 \\times 7$，由于 <code>stride=2</code>，所以先将 $2 \\times 2$ 输入矩阵膨胀为 $3 \\times 3$ 的矩阵（2*(2-1)+1=3），然后再 zero-padding 成 $7 \\times 7$ 的矩阵（上下左右 padding 的数量均为 (7-3)/2=2），经过这番处理，输入矩阵变为 $I[2,2]=I[2,4]=12, \\ I[4,2]=10, \\ I[4,4]=17$，其余位置均为 <code>0</code>，卷积核 <strong>旋转 180°</strong> 后为 $K’=[[2., 1, 0],[0,2,2],[2,1,0]]$，于是可以手动计算出卷积后的矩阵，这里给出 python 代码计算示例，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">convt.stride = (<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">output = convt(input)</span><br><span class=\"line\">output</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"padding-gt-0\"><a href=\"#padding-gt-0\" class=\"headerlink\" title=\"padding &gt; 0\"></a>padding &gt; 0</h3><p>继续以上面的例子进行说明，假设现在 <code>padding=1</code>，根据式 $(4-2)$ 转置卷积的输出大小为 $2 \\times 2$。将输入矩阵上下左右均进行 1 单位的 zero-padding，得到矩阵大小 $4 \\times 4$，卷积核大小 $3 \\times 3$，计算过程还是将卷积核 <strong>旋转 180°</strong>，卷积计算过程略，不过相信这是足够简单的事情。</p>\n<p>以上 <code>dialtion &gt; 1, stride &gt; 1, padding &gt; 0</code> 三种情况，除了可使用 python 程序验证，还可以使用 <code>第二种方法</code> 进行验证对输入矩阵以及卷积核的处理是正确的，并且，也可以使用 <code>第一种方法</code> 对输入矩阵和卷积核进行处理然后进行普通卷积计算得到输出矩阵。</p>\n<h1 id=\"5-Upsample\"><a href=\"#5-Upsample\" class=\"headerlink\" title=\"5. Upsample\"></a>5. Upsample</h1><p>输入维度为 <code>minibatch x channels x [optional depth] x [optional height] x width</code>，即，输入可以是 3D/4D/5D。可用的算法包括 <code>nearest neighbor, linear, bilinear, bicubic, trilinear</code>。</p>\n<h2 id=\"nearest-neighbor\"><a href=\"#nearest-neighbor\" class=\"headerlink\" title=\"nearest neighbor\"></a>nearest neighbor</h2><p>顾名思义，就是使用原平面上最近的一点作为上采样后的值。例如原平面 size 为 $m \\times m$，在原平面上建立坐标系 S，上采样后的 size 为 $n \\times n, \\ n &gt; m$，设其上点的坐标为 $(x,y), \\ x,y =0,1,…,n-1$。将上采样后平面点映射到 S 中，对应坐标记为 $(x’,y’)$，那么有</p>\n<p>$$\\frac {x-0} {n-1-0}= \\frac {x’-0}{m-1-0} \\Rightarrow x’ = \\frac {m-1} {n-1} \\cdot x$$<br>同理有 $y’ = \\frac {m-1} {n-1} \\cdot y$，然后找出与点 $(i’,j’)$ 最近的那个整数坐标点，显然必然在以下四个点中产生 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$ （这四个点可能有重合），分别计算 $(x’,y’)$ 与这四个点的距离，距离最小的那个点的值即作为 $(x,y)$ 上采样后的值。（使用哪种距离指标，可以查看 PyTorch 底层实现代码，这里本人尚未去查看。）</p>\n<h2 id=\"bilinear\"><a href=\"#bilinear\" class=\"headerlink\" title=\"bilinear\"></a>bilinear</h2><p>输入必须是 4D。</p>\n<h3 id=\"align-corners-True\"><a href=\"#align-corners-True\" class=\"headerlink\" title=\"align_corners=True\"></a>align_corners=True</h3><p>双线性插值。记四个顶点为 $(x_1,y_1), \\ (x_1,y_2), \\ (x_2,y_1), \\ (x_2,y_2)$，然后求目标点 $(x,y), \\ x_1 \\le x \\le x_2, \\ y_1 \\le y \\le y_2$ 的值。沿 x 轴线性插值，<br>$$f(x,y_1)=\\frac {f_{21}-f_{11}} {x_2-x_1} \\cdot (x-x_1)+f_{11}<br>\\\\ f(x,y_2)=\\frac {f_{22}-f_{12}} {x_2-x_1} \\cdot (x-x_1)+f_{12}<br>\\\\ f(x,y)=\\frac {f_(x,y_2)-f(x,y_1)} {y_2-y_1} \\cdot (y-y_1)+f(x,y_1)$$</p>\n<p>与 <code>nearest neighbor</code> 中一样，首先将点 $(x,y)$ 映射到原平面上一点 $(x’,y’)$，然后四个顶点为 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$。用这种映射方法，显然原平面的四个 corners 和上采样后平面的四个 corners 分别对齐，这就是 <code>align_corners=True</code> 的由来。</p>\n<h3 id=\"align-corners-False\"><a href=\"#align-corners-False\" class=\"headerlink\" title=\"align_corners=False\"></a>align_corners=False</h3><p>如下图所示，显示了 <code>align_corners</code> 不同值的区别。<br><img src=\"/images/pytorch_mtd_aligncorners.png\" alt=\"\"><center>图源 <a href=\"https://discuss.pytorch.org/t/what-we-should-use-align-corners-false/22663/9\" target=\"_blank\" rel=\"noopener\">pytorch 论坛</a></center></p>\n<p>从图中可以发现，映射回原平面坐标时，坐标计算方式不同，例如上菜以后平面上一点 $(x,y)$，映射回 S 中的坐标为<br>$$x’=(x+0.5)/2-0.5<br>\\\\ y’=(y+0.5)/2-0.5$$</p>\n<p>此后的插值方式一致（毕竟都是双线性插值），找到最近的 4 个点 $(\\lfloor x’\\rfloor, \\lfloor y’ \\rfloor), \\ (\\lfloor x’\\rfloor, \\lceil y’ \\rceil), \\ (\\lceil x’\\rceil, \\lfloor y’ \\rfloor), \\ (\\lceil x’\\rceil, \\lceil y’ \\rceil)$ 进行双线性插值。</p>\n<h2 id=\"linear\"><a href=\"#linear\" class=\"headerlink\" title=\"linear\"></a>linear</h2><p>与 bilinear 类似，但是输入维度必须是 3D。</p>\n<h2 id=\"trilinear\"><a href=\"#trilinear\" class=\"headerlink\" title=\"trilinear\"></a>trilinear</h2><p>与 bilinear 类似，但是输入维度必须是 5D。</p>"},{"title":"Dynamic Programming (2)","date":"2019-08-14T06:36:05.000Z","p":"dp/DP2","mathjax":true,"_content":"上一篇文章 [Dynamic Programming (1)](2019/08/07/DP1) 介绍了动态规划的原理，以及几种常见模型的 DPFE。这篇文章则主要介绍一些实际应用。\n\n## 最佳分配问题 ALLOT\n最佳分配问题简称为 ALLOT，描述了如何讲有限资源分配给一些用户，损失（或者利益，损失对应最小化，利益对应最大化）与用户以及分配到的资源量有关。ALLOT 也可以看作是背包问题 KSINT 的一个变种。\n\n<!-- more -->\n\n假设有 M 单位的资源要分配给 N 个用户，记 C(k,d) 表示分配 d 单位资源给用户 k 时的损失，分配决策按阶段进行，即第一阶段分配 $d_1$ 单位资源给用户 1，第二阶段分配 $d_2$ 单位资源给用户 2，依次进行，定义状态 (k,m) 为阶段 k 时剩余 m 单位资源，阶段 k 分配之前的资源量为 m，阶段 k 的分配损失为 C(k,d)，下一阶段状态为 (k+1,m-d)，于是根据 [Dynamic Programming (1)](2019/08/07/DP1) 中式 (1.19) 可知 DPFE 为\n$$f(k,m)=\\min_{d \\in \\{0,...,m\\}} \\{C(k,d)+f(k+1,m-d)\\} \\quad (2.1)$$\n目标是求 f(1,M)，基本条件为 f(N+1,m)=0，其中 $m \\ge 0$，这表示资源可以不用全部瓜分完。\n\n现在假设有 M=4，N=3，且\n$$(C_{k,d})_{k\\in \\{1,2,3\\};d\\in \\{0,...,4\\}}=\\begin{pmatrix}\\infty & 1.0 & 0.8& 0.4 & 0.0 \\\\\\\\ \\infty & 1.0& 0.5 & 0.0 & 0.0 \\\\\\\\ \\infty & 1.0 & 0.6 & 0.3 & 0.0 \\end{pmatrix}$$\n那么，f(1,M)=1.0+0.5+1.0=2.5，最佳分配序列为 $d_1=1,d_2=2,d_3=1$。我们可以根据式 (2.1) 逐步展开计算，下面是代码实现，\n```python\ndef allot(cache=True):\n    M=4\n    N=3\n    max_float=1e8\n    cost=[[max_float, 1.0, 0.8, 0.4, 0.0],\n          [max_float, 1.0, 0.5, 0.0, 0.0],\n          [max_float, 1.0, 0.6, 0.3, 0.0]]\n    \n    if cache:\n        cache_dict = {}\n    def allot_inner(k,m):\n        if cache and (k,m) in cache_dict:\n            return cache_dict[(k,m)]\n        if k>= N: return [], 0\n\n        min_f=max_float\n        min_ds = []\n        for d in range(m+1):\n            ds,f=allot_inner(k+1,m-d)\n            temp=cost[k][d]+f\n            if min_f > temp:\n                min_f = temp\n                min_ds = [d]+ds\n        if cache and k > 1:\n            cache_dict[(k,m)]=(min_ds,min_f)\n        return min_ds, min_f\n    ds, f=allot_inner(0,M)\n    print(\"min cost:\",f,\"opt allotments:\", ds)\n```\n\n\n\n## 所有结点对的最短路径问题 APSP\n\n在图中寻找从任一起点 s 到任一终点 t 之间的最短路径，记图中结点数量为 N，为简单起见，我们假设任意结点对之间没有非正长度的环，并且没有自环（self-loop）。\n\n可以利用 [Dynamic Programming (1)](2019/08/07/DP1) 中的最短路径模型，将 (s,t) 看作变量，求得一系列的最短路径值，然后再求最小值即可，但是这其中肯定存在一些重复计算，所以这里我们讨论更高效率的计算方法。\n\n__Relaxation__ \n\n定义 F(k,p,q) 为从 p 到 q 之间的最短路径长度，k 表示从 p 到 q 最多走 k 步（从一个节点到下一个节点为一步）。借助 [Dynamic Programming (1)](2019/08/07/DP1) 中式 (1.27)，DPFE 为\n$$F(k,p,q)=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\quad(2.2)$$\n其中 r 是 p 的直接后继节点。基本条件为:\n1. $F(k,p,q)=0, k \\ge 0, p=q$，表示当 p 就是 q 时，最多走 k 步，最短路径长度为 0。\n2. $F(0,p,q)=\\infty, p \\ne q$， 表示当 p 不为 q 时，最多走 0 步，最短路径长度为无穷大。\n\n虽然我们假定没有自环，但是我们依然可以令 $b(p,p)=0$（实际路径中我们可以去掉环即可），那么式 (2.2) 可简化为\n$$\\begin{aligned}F(k,p,q)&=\\min \\{F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\\\\\\\ &=\\min_{r \\in succ(p)\\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.3) \\end{aligned}$$\n\n\n__Floyd-Warshall__ \n\n式 (2.2) 这个 DPFE 是一种分而治之的思想：从 p 到 q 最多走 k 步的路径，可以分为从 p 走一步到 r 以及从 r 最多走 k-1 步到 q 两个子路径。还有一种替代方案是从 p 到 r 并且从 r 到 q，其中 p 到 r 的步数不再固定为 1，但是从 p 出发，到达 q 总共最多经过 k 个点，r 就是这 k 个中间点，不妨记这 k 个点为 $\\{1,2,...,k\\}$，那么求从 p 到 q 并使用 $\\{1,2,...,N\\}$ 作为可能的中间点的最短路径就是 p 到 q 的全局最短路径。DPFE 为\n$$F(k,p,q)=\\min \\{F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)\\} \\qquad(2.4)$$\n\n为了便于理解，我们作如下说明：\n1. 将 N 个节点编号为 $V=\\{1,2,...,N\\}$，$p,q \\in V$\n2. $F(k,p,q)$ 表示从 p 到 q 且以 $\\{1,2,...,k\\}$ 作为可能的中间节点\n3. 问题的求解目标为 $F(N,p,q)$\n4. 如何理解式 $(2.4)$？\n   - p 到 q 的路径不经过中间点 k，即，使用 $\\{1,2,...,k-1\\}$ 作为可能的中间节点\n   - 或者 p 到 q 的路径经过中间点 k，即，分为两个子路径 p 到 k 和 k 到 q，这两个子路径均使用 $\\{1,2,...,k-1\\}$ 作为可能的中间节点\n5. 式 $(2.4)$ 这个递归操作需要条件 k>0。k=0 时为基本条件 $F(0,p,q)=0, p=q$，以及 $F(0,p,q)=b(p,q), p\\ne q$。前者表示当 p q 为同一节点时，不使用任何中间节点，损失为 0；后者表示当 p q 不同时，不使用任何中间节点，损失为 $b(p,q)$，需要注意这里 q 是 p 的直接后继，如果不是，那么有 $F(0,p,q)=\\infty, p \\notin succ(p) \\cup \\{p\\}$\n6. 中间点序列 $\\{1,2,...,k\\}$ 可能会包含 p 和 q，如果包含了的话，由于我们假定所有的环都是正的，所以再求序列最小值的，带环的路径均会被过滤掉，而自环 $b(p,p)=0$，不会影响最短路径的长度，如果路径中出现自环，去掉即可（去掉连续重复的节点，只保留一个）。\n\n式 (2.2) 中 r 是 p 的后继节点，最多可取 $N-1$ 个节点（假设图中其他节点均为 p 的后继节点，就对应 $N-1$），k 最大为 $N-1$ 步，p 和 q 均各有 N 个取值，所以式 (2.2) 的时间复杂度为 $O(N^4)$，类似地，式 (2.4) 的时间复杂度为 $O(N^3)$。\n\n实际中要解决 APSP 问题，可以根据式 (2.2) 求出矩阵序列 $\\{F^{(1)},F^{(2)},...,F^{(N-1)}\\}$，任一矩阵 $F^{(k)}$ 维度为 $N \\times N$，$F_{p,q}^{k}$ 表示从 p 到 q 最多走 k 步的最短路径长度，然后求 $\\min_{p,q} F_{p,q}^{(N-1)}$ 就是 APSP 的解。\n\n__矩阵乘法__\n\n为了借鉴矩阵乘法的思想，我们首先将式 (2.2) 作变换，\n$$\\begin{aligned} F(k,p,q)&=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\}\n\\\\\\\\ &= \\min_{r \\in succ(p)} \\{b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in succ(p) \\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in \\{1,2,...,N\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.5) \\end{aligned}$$\n其中，$b(p,r)$ 是事先给定的任意两节点之间的距离，若两节点之间没有边 edge 相连，则距离为 $\\infty$，这里称所有节点对之间的距离组成的矩阵为权重矩阵 $W_{N \\times N}$。根据式 (2.2) 的基本条件，不难得知 $F^{(0)}$ 矩阵对角线全 0，其余元素均为 $\\infty$：\n\n$$F^{(0)}=\\begin{bmatrix}0 & \\infty & \\cdots & \\infty\n\\\\\\\\                    \\infty & 0  & \\cdots & \\infty\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    \\infty & \\infty & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n$$W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n根据式 (2.5)，已知 $F^{(k-1)}$ 求 $F^{(k)}$ 的代码为\n```python\nimport sys\nF_k=[[None]*N]*N\nfor p in range(0,N):\n  for q in range(0,N):\n    F_k[p][q]=sys.info.float_max\n    # F_k[p][q]=0\n    for r in range(0,N):\n      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])\n      # F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])\n```\n从上面代码片段可见，与矩阵乘法（注释部分）完全一个模样，而我们的目的是为了计算 $F^{(N-1)}$，中间的其他 $F^{(k)}$ 矩阵如无必要，可以不用计算出来，比如下面，\n$$\\begin{aligned} F^{(1)}&=W \\circ F^{(0)}=W\n\\\\\\\\ F^{(2)}&=W \\circ F^{(1)}=W^2\n\\\\\\\\ F^{(3)}&=W \\circ F^{(2)}=W^3\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{(N-1)}&=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$\n\n$\\circ$ 表示某种运算符，比如矩阵乘法或者这里的最小值计算，我们改为如下序列计算，\n$$\\begin{aligned} F^{(1)}&=W\n\\\\\\\\ F^{(2)}&=W^2=W \\circ W\n\\\\\\\\ F^{(4)}&=W^4=W^2 \\circ W^2\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$\n注意上面 $2^{\\lceil log(N-1) \\rceil}$ 中的向上取整 $\\lceil \\cdot \\rceil$ 很重要，这保证了 $2^{\\lceil log(N-1) \\rceil} \\ge N-1$，从而 $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ （element-wise comparison）。\n\n因为结合顺序无关紧要，才使得我们可以从式 (2.6) 可以改写为式 (2.7)，例如\n$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$\n将 $\\circ$ 替换为 $\\min$，即 $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$，注意这里的 $\\min$ 不是 element-wise operator，就跟矩阵乘法不是矩阵点乘一样。当然，以上内容只是帮助理解，不是式 (2.6) 可以变换为式 (2.7) 的严格证明。\n\n好了，有了式 (2.7) 就可以更快的计算出 $F^{(M)}, M \\ge N-1$，由于 $F^{(k)}$ 单调减，并收敛于 $F^{(N-1)}$，于是 $F^{(M)}$ 就是全局最短路径长度矩阵。\n\n使用矩阵乘法加速的算法代码为\n```python\nimport sys\n\ndef fast_apsp():\n  k=1\n  F_prev=W\n  while k<N-1:\n    F_next=[[sys.info.float_max]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        for r in range(0,N):\n          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])\n    F_prev=F_next\n    k*=2\n  return F_prev\n```\n\n__Floyd-Warshall__ 的代码\n\n考虑式 (2.4)，注意 $F^{(k)}$ 中的 k 表示路径可以经过中间节点 $\\{1,2,...,k\\}$，所以 $F^{(0)}$ 表示不经过任何中间节点的两点之间最短路径长度矩阵，所以根据基本条件不难得到\n$$F^{(0)}=W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n根据式 (2.4)，不难写出原始的 __Floyd-Warshall__ 算法的代码为\n```python\nF_prev=F_0\ndef floyd_warshall():\n  for k in range(0,N):\n    F_next=[[None]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])\n    F_prev=F_next\n  return F_prev\n```\n\n## 最优字母基数编码树问题 ARC\n\nARC 是霍夫曼编码树问题的一个变体。霍夫曼树要满足的条件是节点权值*路径长度，求和最小。比如常见的霍夫曼编码，节点权值对应词频，路径对应单个词的编码长度，加权求和就是编码后的序列总长度最小。构造霍夫曼树可以直接使用贪心算法，给定一个具有权值的节点序列，找出具有最小权值的两个节点，构造出一个子树，这两个节点为子树的子节点，子树的父节点权值为这两个子节点的权值之和，然后将这两个字节点从节点序列中移除，并插入新构造的父节点到节点序列，重复这个过程，直到节点序列中只剩一个节点。这是一个自底向上的构造过程。\n\nARC为，给定一个具有构造成本（权值）的节点序列，目标是构造一棵成本最小的树，整棵树的总成本为所有内部节点（internal nodes，包括 root）的成本之和，内部节点的损失为以此内部节点为根节点的子树中所有叶节点的成本之和。给定 $S=(w_0,w_1,...,w_{n-1})$ 为有序序列表示叶节点的权值，定义 $(i,j)$ 的态为 $(w_i,...,w_j)$，那么\n$$f(i,j)=\\min_{}\\{c(i,j,d)+f(i,d)+f(d+1,j)\\}, \\quad i<j \\qquad(2.8)$$\n其中 $f(i,j)$ 表示以节点 $(i,...,j)$ 为叶节点的子树总成本， $c(i,j,d)=\\sum_{k=i}^j w_k$ 表示这个子树的根节点的成本，这里我们限制为二叉树，所以不难理解式 (2.8)，树的总成本为左子树的总成本与右子树的总成本以及树根节点成本三者之和。d 用于划分，其中 $(i,...,d)$ 为左子树的叶节点，$(d+1,...,j)$ 为右子树的叶节点，无论怎么划分（即选择任何 d 的有效值），以 $(i,...,j)$ 为叶节点的子树的根节点成本 $c(i,j,d)$ 均保持不变。\n\n最后，目标是计算 $f(0,n-1)$，基本条件为 $f(i,i)=0, \\ \\forall i \\in \\{0,1,...,n-1\\}$。\n\n构造 ARC 比构造 Huffman 树多一个限制，那就是给定节点序列后，只能结合两个相邻节点构造一个子树。例如，$S=(1,2,3,4)$，最优树为 $(((1,2),3),4)$，$f(S)=3+6+10=19$（三个内部节点成本之和），再例如 $S=(2,3,3,4)$，那么最优树为 $((2,3),(3,4))$，$f(S)=5+7+12=24$。\n\n  \n## 装配线平衡问题 ASMBAL\n一个产品装配需要经过一系列的处理，每个处理步骤均有代价/损失，并且从一个处理站到另一个处理站之间也存在代价。这里举一个简单的例子进行说明，一个处理站看作一条处理线，一次处理为一个阶段 stage，转移发生在从 阶段 k，处理线 i 的节点到另一个位于阶段 k+1 处理线 j 的节点上。处理线之间的切换损失为 c(k,i,j)，通常 c(k,i,i)=0，即不切换处理线，那么切换损失应为 0，除了切换损失，每个节点自身还存在损失，初始状态 s 和终止状态 t 处于标记为 0 的处理线上，这只是为了说明初始时刻和终止时刻产品不应该在任何处理线上，初始决策的损失为 c(0,0,j)，最终决策的损失为 c(N,j,0)（j 为某个处理线）。 \n\n某种可行的处理使用一个节点表示，记节点标号为 0~13 共 14 个节点，其中特别地 0 和 13 分别表示起始状态和终止状态，即表示不用处理。假设 14 个节点自身的损失分别为\n$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$\n处理线之间的切换损失如下图，\n![](/images/DP2_fig1.png)\n\n也可以写成 14x14 的毗邻矩阵。可见，总共有两条处理线。\n\n在阶段 k，从 i 线到 j 线的损失由两部分组成 $R(k,i,j)=v(k,i)+c(k,i,j)$，DPFE 为\n$$f(k,i)=\\min_j \\{R(k,i,j)+f(k+1,j)\\}$$\n其中 $f(k,i)$ 表示从阶段 k 并处于线 i 上开始到最终态的损失，问题的目标就是求 $f(0,0)$，基本条件为 $f(k,i)=0, k > N$，N 为总阶段数，图中为 N=6。计算过程如下：\n$$\\begin{aligned} f(0,0)=\\min \\{R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)\\}\n\\\\\\\\ f(1,0)=\\min \\{R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)\\}\n\\\\\\\\ f(1,1)=\\min \\{R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)\\}\n\\\\\\\\ \\cdots \\ (omitted)\n\\end{aligned}$$\n\n## 最佳分派问题 ASSIGN\n集合 B 中的每个成员需要唯一地被赋值为集合 A 中的成员，如何 A 是有序的，那么这个过程也可以看作是 A 的一种排列，例如 $\\{1,2,3\\}$ 的一种排列为 $\\{3,2,1\\}$（总共有 3! 种排列）。集合 $A=(a_0,a_1,...,a_{n-1})$ 的某个排列 $B=(b_0,b_1,...,b_{n-1})$ 可按如下过程得到：在阶段 i，赋值 $a_j$ 给 $b_i$，对应的损失为 $c(i,j)$。由于赋值必须是唯一的，所以每个阶段需要跟踪集合 A 中还有多少尚未使用的成员。具体而言，我们记状态为 $(k,S)$，表示阶段 k 集合 A 中尚未使用的成员集合为 S，阶段 k 时选择成员 d，相应的损失记为 $C(k,S,d)$，下一状态为 $(k+1,S-\\{d\\})$，DPFE 为\n$$f(k,S)=\\min_{d \\in S} \\{C(k,S,d)+f(k+1,S-\\{d\\})\\}$$\n求解目标为 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$\n\n## 最佳二叉搜索树问题 BST\n假设包含 n 个数据的集合 $X=\\{x_0,...,x_{n-1}\\}$，并且是 __有序__ 的，每个数据 $x_i$ 的被访问概率为 $p(x_i)$，或简写为 $p_i$，且有 $\\sum_{i=0}^{n-1}p_i=1$，目标是建立一棵最小损失的二叉搜索树，其中树的损失定义为\n$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$\n$\\text{level}(x_i)$ 表示 $x_i$ 所在节点的深度。需要注意的是，节点不仅可存储在叶节点中，还可以存储在内部节点中。下面我们讨论如何使用 DP 来解决这个问题。\n\n### 方法一\n定义状态 S 为待安排到树中的元素集合，由于构造树是一个递归过程，易知 DPFE 为\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & S \\ne \\emptyset\n\\\\\\\\ 0 & S=\\emptyset \\end{cases}$$\n其中 $S_l = \\{x \\in S: x < \\alpha\\}, \\ S_r = \\{x \\in S: x > \\alpha\\}$，决策代价 $r(\\alpha, S)=\\sum_{x \\in S} p(x)$，根节点深度为 1。这个 DPFE 非常简单，不多说。\n\n以上 $S_l,\\ S_r$ 的划分保证了二叉树是 __有序__ 的。（若不需要保持有序，此问题与霍夫曼编码相同）\n\n上式可改写为\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & |S|>1\n\\\\\\\\ p(x) & S=\\{x\\} \\end{cases}$$\n\n### 方法二\n定义状态为一对整数 $(i,j)$，分别表示数据的起始下标和截止下标，在此范围内的数据是需要被安排进树的，已知数据集合为 $X=\\{x_0,...,x_{n-1}\\}$，那么 DPFE 为\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i \\le j\n\\\\\\\\ 0 & i > j \\end{cases}$$\n$(i,j)$ 表示待安排进树的节点下标的起止范围，即将从此范围中选择下标为 k 的数据作为当前节点，或称子树根节点，这个子树的所有节点对应数据下标范围为 $(i,j)$, 其中$(i,k-1)$ 用于被安排进当前子树的左子树，$(k+1,j)$ 用于被安排进当前子树的右子树，无论当前选择哪个决策 k，$(i,j)$ 范围内的数据的概率在当前层 level 均需被计算一次，因为是从上到下构造树的，所以每到达一个 level，所有到达这个 level 的数据其访问概率需要被计算一次，在 $f(i,j)$ 中，$(i,j)$ 范围内的数据均到达当前 level，属于待安排进树的数据，所以当前决策损失为累加所有到达当前 level 的数据的访问概率，即 $\\sum_{l=i}^j p_l$。\n\n与方法一同样地可改写上式为\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i < j\n\\\\\\\\ p_i & i = j \\end{cases}$$\n\n## 最佳覆盖问题 COV\n假设有 k 个不同大小的灌木需要在严寒中被保护起来，k 个灌木按大小排序，标号为 0 的灌木尺寸最小，假设灌木的保护措施就是制造覆盖，将其覆盖起来进行保护，大小为 i 的覆盖其制造成本为 $c_i$。由于技术限制，只能制造不超过 n 种尺寸的覆盖，且 $n \\le k$，已知大的覆盖能用于保护小的灌木，目标是选择 n 种覆盖尺寸，使得能覆盖所有的灌木，且成本最小。\n\n使用 DP 解决上述问题，为了方便，我们假设灌木尺寸按从小到达排好序，编号为 $0,1,...,k-1$，编号为 $l$ 的灌木尺寸为 $s_l$，其对应的覆盖制造成本为 $c_{s_l}$，简记为 $c_l$，这里为了使问题简单，我们不考虑使用多个较小尺寸的覆盖来保护较大尺寸的灌木，即，每个灌木只使用单个覆盖。一种显然的想法是，从大到小来制造覆盖，因为大尺寸的覆盖总能保护小尺寸的灌木，令 $j$ 表示当前还可以制造多少种尺寸的覆盖，$l$ 表示当前未得到保护的最大尺寸灌木编号，于是 DPFE 为\n$$f(j,l)=\\begin{cases} \\min_{d \\in \\{j-2,...,l-1\\}} \\{(l-d)c_l+f(j-1,d)\\} & j>1\n\\\\\\\\ (l+1)c_l & j=1 \\end{cases}$$\nd 表示灌木尺寸范围的起始下标（exclusive），也就是说当前决策是制造尺寸为 $s_l$ 的覆盖，用于保护 $\\{d+1,...,l\\}$ 范围内的灌木。显然 d 最大为 $l-1$，此时当前决策所制造的覆盖只用于保护编号为 $l$ 的灌木，d 最小为 $j-2$，这是因为当前决策所制造的覆盖最多能用于保护灌木的编号为 $\\{j-1,...,l\\}$，此时剩余尚未保护的灌木编号 $\\{0,1,...,j-2\\}$ 共 $j-1$ 种尺寸，正好剩余可以制造的覆盖尺寸也是 $j-1$ 种，两者一一对应。\n\n再看基本条件，$f(j,l)=(l+1)c_l, j=1$，表示当前只能制造一种尺寸的覆盖时，那只能制造当前尚未得到保护的最大灌木的尺寸的覆盖，当前最大尺寸的灌木编号为 $l$，由于只有这一种尺寸的覆盖可以制造，其必须保护 $0,...,l$ 范围内的灌木。\n\n## 时限调度问题 DEADLINE\n有一个进程集合，其中每个进程的执行时间均为单位时间，同时每个进程也各自有一个最后期限，如果在最后期限之前完成，则具有收益，否则收益为 0，现在要选择一个最优进程子集，放置在单处理器上执行，并且收益要最大。这个问题当然也可以使用贪心算法来解决，那些收益较大的进程我们总希望能被执行，无论是先执行还是后执行，反正只要在其最后期限之前被执行即可，那么不如优先执行这些收益最大的进程，即， __每一步优先执行收益最大的进程__。\n\n假设进程编号为 $S^{\\ast}=\\{0,1,2,3,4\\}$，对应收益为 $p=\\{10,15,20,1,5\\}$，截止期限为 $t=\\{1,2,2,3,3\\}$。贪心算法的代码如下：\n```python\nimport numpy as np\nt=np.array([1,2,2,3,3])\np=np.array([10,15,20,1,5])\nm=0   # 总收益\nn=t.shape[0]  # 最多决策数\n\nfor i in range(n):\n  idx=np.where(t>0)[0]\n  if idx.shape[0]==0:\n    break   # 全部超出最后期限，决策结束\n  c=np.max(p[idx])  # 尚未超出期限的进程的最大收益，当前最优决策的收益（贪心策略）\n  if c < 0:\n    break\n  m+=c\n  idx=np.argmax(p[idx])+idx[0]  # 当前决策的进程的编号\n  p[idx]=-1e8                   # 标记收益为负，表示后续决策不再考虑此进程\n  t-=1                          # 更新进程的最后期限\nprint('最大收益为', sep=' ')\nprint(m)    # 40\n```\n\n使用 DP 解决这个问题，定义状态为 $(k,S)$，其中 k 为阶段编号，S 为所有尚未执行的进程集合，每一步的决策 d 是从 S 中选出，决策后的下一状态为 $(k+1,S-\\{d\\})$，根据最后期限从近到远对进程进行排序，DPFE 为\n$$f(k,S)=\\max_{d \\in S}\\{c(d|S)+f(k+1,S-\\{d\\})\\}$$\n其中，当决策 d 的最后期限大于等于阶段 k 时（k 从 1 开始计算）$c(d|S)=w_d$ ，否则 $c(d|S)=0$。目标是求 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$，其中 n 为进程数。使用上述例子，代码如下\n```python\nt=[1,2,2,3,3]\np=[10,15,20,1,5]\nS=[0,1,2,3,4]\nn=len(t)\ndef profit(k,d):\n  return p[d] if t[d]>=k else 0\n\ndef deadline(k,S):\n  return 0 if len(S)==0 or k==n+1 else \\\n    max([profit(k,S[i])+deadline(k+1,S[:i]+S[i+1:]) for i in range(len(S))])\n\nprint(deadline(1, S))   # 40\n```\n\n## 折现利润问题 DPP\n假设一个湖中第一年开始时有 $b_1$ 条鱼，第 t 年开始时的鱼数为 $b_t$，在第 t 年中售出 $x_t$ 条鱼，收益为 $r(x_t)$，捕鱼成本为 $c(x_t,b_t)$，鱼可以再生，再生长率为 s，表示某一年年初的鱼数量是上一年年底鱼数量的 s 倍，计划展望期为 $1,...,T$，即最多 T 年卖完所有鱼，其中货币年贬值率为 y，决策为 $x_t$ 表示 第 t 年卖出多少鱼，要求最大净收益，状态为 $(t,b)$，其中 t 表示年份，b 表示第 t 年初的鱼数，DPFE 为\n$$f(t,b)=\\begin{cases} \\max_{x_t \\in \\{0,...,b\\}} \\{r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)\\} & t \\le T\n\\\\\\\\ 0 & t=T+1 \\end{cases}$$\n\n\n## 编辑长度问题 EDP\n编辑长度问题通常用于字符串的非精确匹配问题。记 $\\Sigma$ 为一有限字符集，给定两个字符串 $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$，或者写为 $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$，任务是将 x 转变为 y，并且只能使用如下三种编辑操作：\n- 删除操作 D。从某字符串中删除一个字符，损失为 $c(D)$\n- 插入操作 I。向某字符串中插入一个字符，损失为 $c(I)$\n- 替换操作 R。将某字符串中某个字符替换为另一个字符，损失为 $c(R)$，如果替换前后字符相同，则 $c(R)=0$\n  \n要求一个编辑序列，使得 x 转变为 y，且具有最小损失，这里损失为所有操作的损失之和。DPFE 为\n$$f(i,j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min \\{f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)\\} & i>0,j>0 \\end{cases}$$\n其中 $f(i,j)$ 表示从 $X_i$ 到 $Y_j$ 需要的操作损失，$X_i$ 为 x 的前 i 个字符形成的子串，$Y_j$ 为 y 的前 j 个字符形成的子串，当 i=0 时，向 $X_i$ 插入 j 个字符得到 $Y_j$，当 j=0 时，将 $X_i$ 删除 i 个字符得到 $Y_j$，当 i>0 且 j>0 时，从 $X_i$ 变换到 $Y_j$，总共可以有以下三种决策/操作，取损失最小的决策：\n- 删除 $X_i$ 的第 i 个字符，然后从 $X_{i-1}$ 变换到 $Y_j$，两部分损失分别为 $c(D)$ 和 $f(i-1,j)$\n- 从 $X_i$ 变换到 $Y_{j-1}$，然后再在末尾插入一个字符得到 $Y_j$，两部分的操作损失分别为 $f(i,j-1)$ 和 $c(I)$\n- 两个子串的最后一个字符，使用替换，$x_i \\rightarrow y_j$，然后再从 $X_{i-1}$ 变换到 $Y_{j-1}$ 即可得到 $X_i$ 变换到 $Y_j$ 的过程，两部分的损失分别为 $f(i-1,j-1)$ 和 $c(R)$\n\n还可以写成如下形式的 DPFE\n$$f(X_i,Y_j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min_{d \\in \\{D,I,R\\}} \\{f(t(X_i,Y_j,d))+c(d)\\} & i>0,j>0\n\\end{cases}$$\n其中变换定义为\n$$t(X_i,Y_j,D)=(X_{i-1},Y_j)\n\\\\\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})\n\\\\\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$\n","source":"_posts/dp/DP2.md","raw":"---\ntitle: Dynamic Programming (2)\ndate: 2019-08-14 14:36:05\np: dp/DP2\ntags: \n    - math\n    - DP\nmathjax: true\n---\n上一篇文章 [Dynamic Programming (1)](2019/08/07/DP1) 介绍了动态规划的原理，以及几种常见模型的 DPFE。这篇文章则主要介绍一些实际应用。\n\n## 最佳分配问题 ALLOT\n最佳分配问题简称为 ALLOT，描述了如何讲有限资源分配给一些用户，损失（或者利益，损失对应最小化，利益对应最大化）与用户以及分配到的资源量有关。ALLOT 也可以看作是背包问题 KSINT 的一个变种。\n\n<!-- more -->\n\n假设有 M 单位的资源要分配给 N 个用户，记 C(k,d) 表示分配 d 单位资源给用户 k 时的损失，分配决策按阶段进行，即第一阶段分配 $d_1$ 单位资源给用户 1，第二阶段分配 $d_2$ 单位资源给用户 2，依次进行，定义状态 (k,m) 为阶段 k 时剩余 m 单位资源，阶段 k 分配之前的资源量为 m，阶段 k 的分配损失为 C(k,d)，下一阶段状态为 (k+1,m-d)，于是根据 [Dynamic Programming (1)](2019/08/07/DP1) 中式 (1.19) 可知 DPFE 为\n$$f(k,m)=\\min_{d \\in \\{0,...,m\\}} \\{C(k,d)+f(k+1,m-d)\\} \\quad (2.1)$$\n目标是求 f(1,M)，基本条件为 f(N+1,m)=0，其中 $m \\ge 0$，这表示资源可以不用全部瓜分完。\n\n现在假设有 M=4，N=3，且\n$$(C_{k,d})_{k\\in \\{1,2,3\\};d\\in \\{0,...,4\\}}=\\begin{pmatrix}\\infty & 1.0 & 0.8& 0.4 & 0.0 \\\\\\\\ \\infty & 1.0& 0.5 & 0.0 & 0.0 \\\\\\\\ \\infty & 1.0 & 0.6 & 0.3 & 0.0 \\end{pmatrix}$$\n那么，f(1,M)=1.0+0.5+1.0=2.5，最佳分配序列为 $d_1=1,d_2=2,d_3=1$。我们可以根据式 (2.1) 逐步展开计算，下面是代码实现，\n```python\ndef allot(cache=True):\n    M=4\n    N=3\n    max_float=1e8\n    cost=[[max_float, 1.0, 0.8, 0.4, 0.0],\n          [max_float, 1.0, 0.5, 0.0, 0.0],\n          [max_float, 1.0, 0.6, 0.3, 0.0]]\n    \n    if cache:\n        cache_dict = {}\n    def allot_inner(k,m):\n        if cache and (k,m) in cache_dict:\n            return cache_dict[(k,m)]\n        if k>= N: return [], 0\n\n        min_f=max_float\n        min_ds = []\n        for d in range(m+1):\n            ds,f=allot_inner(k+1,m-d)\n            temp=cost[k][d]+f\n            if min_f > temp:\n                min_f = temp\n                min_ds = [d]+ds\n        if cache and k > 1:\n            cache_dict[(k,m)]=(min_ds,min_f)\n        return min_ds, min_f\n    ds, f=allot_inner(0,M)\n    print(\"min cost:\",f,\"opt allotments:\", ds)\n```\n\n\n\n## 所有结点对的最短路径问题 APSP\n\n在图中寻找从任一起点 s 到任一终点 t 之间的最短路径，记图中结点数量为 N，为简单起见，我们假设任意结点对之间没有非正长度的环，并且没有自环（self-loop）。\n\n可以利用 [Dynamic Programming (1)](2019/08/07/DP1) 中的最短路径模型，将 (s,t) 看作变量，求得一系列的最短路径值，然后再求最小值即可，但是这其中肯定存在一些重复计算，所以这里我们讨论更高效率的计算方法。\n\n__Relaxation__ \n\n定义 F(k,p,q) 为从 p 到 q 之间的最短路径长度，k 表示从 p 到 q 最多走 k 步（从一个节点到下一个节点为一步）。借助 [Dynamic Programming (1)](2019/08/07/DP1) 中式 (1.27)，DPFE 为\n$$F(k,p,q)=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\quad(2.2)$$\n其中 r 是 p 的直接后继节点。基本条件为:\n1. $F(k,p,q)=0, k \\ge 0, p=q$，表示当 p 就是 q 时，最多走 k 步，最短路径长度为 0。\n2. $F(0,p,q)=\\infty, p \\ne q$， 表示当 p 不为 q 时，最多走 0 步，最短路径长度为无穷大。\n\n虽然我们假定没有自环，但是我们依然可以令 $b(p,p)=0$（实际路径中我们可以去掉环即可），那么式 (2.2) 可简化为\n$$\\begin{aligned}F(k,p,q)&=\\min \\{F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\\\\\\\ &=\\min_{r \\in succ(p)\\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.3) \\end{aligned}$$\n\n\n__Floyd-Warshall__ \n\n式 (2.2) 这个 DPFE 是一种分而治之的思想：从 p 到 q 最多走 k 步的路径，可以分为从 p 走一步到 r 以及从 r 最多走 k-1 步到 q 两个子路径。还有一种替代方案是从 p 到 r 并且从 r 到 q，其中 p 到 r 的步数不再固定为 1，但是从 p 出发，到达 q 总共最多经过 k 个点，r 就是这 k 个中间点，不妨记这 k 个点为 $\\{1,2,...,k\\}$，那么求从 p 到 q 并使用 $\\{1,2,...,N\\}$ 作为可能的中间点的最短路径就是 p 到 q 的全局最短路径。DPFE 为\n$$F(k,p,q)=\\min \\{F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)\\} \\qquad(2.4)$$\n\n为了便于理解，我们作如下说明：\n1. 将 N 个节点编号为 $V=\\{1,2,...,N\\}$，$p,q \\in V$\n2. $F(k,p,q)$ 表示从 p 到 q 且以 $\\{1,2,...,k\\}$ 作为可能的中间节点\n3. 问题的求解目标为 $F(N,p,q)$\n4. 如何理解式 $(2.4)$？\n   - p 到 q 的路径不经过中间点 k，即，使用 $\\{1,2,...,k-1\\}$ 作为可能的中间节点\n   - 或者 p 到 q 的路径经过中间点 k，即，分为两个子路径 p 到 k 和 k 到 q，这两个子路径均使用 $\\{1,2,...,k-1\\}$ 作为可能的中间节点\n5. 式 $(2.4)$ 这个递归操作需要条件 k>0。k=0 时为基本条件 $F(0,p,q)=0, p=q$，以及 $F(0,p,q)=b(p,q), p\\ne q$。前者表示当 p q 为同一节点时，不使用任何中间节点，损失为 0；后者表示当 p q 不同时，不使用任何中间节点，损失为 $b(p,q)$，需要注意这里 q 是 p 的直接后继，如果不是，那么有 $F(0,p,q)=\\infty, p \\notin succ(p) \\cup \\{p\\}$\n6. 中间点序列 $\\{1,2,...,k\\}$ 可能会包含 p 和 q，如果包含了的话，由于我们假定所有的环都是正的，所以再求序列最小值的，带环的路径均会被过滤掉，而自环 $b(p,p)=0$，不会影响最短路径的长度，如果路径中出现自环，去掉即可（去掉连续重复的节点，只保留一个）。\n\n式 (2.2) 中 r 是 p 的后继节点，最多可取 $N-1$ 个节点（假设图中其他节点均为 p 的后继节点，就对应 $N-1$），k 最大为 $N-1$ 步，p 和 q 均各有 N 个取值，所以式 (2.2) 的时间复杂度为 $O(N^4)$，类似地，式 (2.4) 的时间复杂度为 $O(N^3)$。\n\n实际中要解决 APSP 问题，可以根据式 (2.2) 求出矩阵序列 $\\{F^{(1)},F^{(2)},...,F^{(N-1)}\\}$，任一矩阵 $F^{(k)}$ 维度为 $N \\times N$，$F_{p,q}^{k}$ 表示从 p 到 q 最多走 k 步的最短路径长度，然后求 $\\min_{p,q} F_{p,q}^{(N-1)}$ 就是 APSP 的解。\n\n__矩阵乘法__\n\n为了借鉴矩阵乘法的思想，我们首先将式 (2.2) 作变换，\n$$\\begin{aligned} F(k,p,q)&=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\}\n\\\\\\\\ &= \\min_{r \\in succ(p)} \\{b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in succ(p) \\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in \\{1,2,...,N\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.5) \\end{aligned}$$\n其中，$b(p,r)$ 是事先给定的任意两节点之间的距离，若两节点之间没有边 edge 相连，则距离为 $\\infty$，这里称所有节点对之间的距离组成的矩阵为权重矩阵 $W_{N \\times N}$。根据式 (2.2) 的基本条件，不难得知 $F^{(0)}$ 矩阵对角线全 0，其余元素均为 $\\infty$：\n\n$$F^{(0)}=\\begin{bmatrix}0 & \\infty & \\cdots & \\infty\n\\\\\\\\                    \\infty & 0  & \\cdots & \\infty\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    \\infty & \\infty & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n$$W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n根据式 (2.5)，已知 $F^{(k-1)}$ 求 $F^{(k)}$ 的代码为\n```python\nimport sys\nF_k=[[None]*N]*N\nfor p in range(0,N):\n  for q in range(0,N):\n    F_k[p][q]=sys.info.float_max\n    # F_k[p][q]=0\n    for r in range(0,N):\n      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])\n      # F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])\n```\n从上面代码片段可见，与矩阵乘法（注释部分）完全一个模样，而我们的目的是为了计算 $F^{(N-1)}$，中间的其他 $F^{(k)}$ 矩阵如无必要，可以不用计算出来，比如下面，\n$$\\begin{aligned} F^{(1)}&=W \\circ F^{(0)}=W\n\\\\\\\\ F^{(2)}&=W \\circ F^{(1)}=W^2\n\\\\\\\\ F^{(3)}&=W \\circ F^{(2)}=W^3\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{(N-1)}&=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$\n\n$\\circ$ 表示某种运算符，比如矩阵乘法或者这里的最小值计算，我们改为如下序列计算，\n$$\\begin{aligned} F^{(1)}&=W\n\\\\\\\\ F^{(2)}&=W^2=W \\circ W\n\\\\\\\\ F^{(4)}&=W^4=W^2 \\circ W^2\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$\n注意上面 $2^{\\lceil log(N-1) \\rceil}$ 中的向上取整 $\\lceil \\cdot \\rceil$ 很重要，这保证了 $2^{\\lceil log(N-1) \\rceil} \\ge N-1$，从而 $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ （element-wise comparison）。\n\n因为结合顺序无关紧要，才使得我们可以从式 (2.6) 可以改写为式 (2.7)，例如\n$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$\n将 $\\circ$ 替换为 $\\min$，即 $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$，注意这里的 $\\min$ 不是 element-wise operator，就跟矩阵乘法不是矩阵点乘一样。当然，以上内容只是帮助理解，不是式 (2.6) 可以变换为式 (2.7) 的严格证明。\n\n好了，有了式 (2.7) 就可以更快的计算出 $F^{(M)}, M \\ge N-1$，由于 $F^{(k)}$ 单调减，并收敛于 $F^{(N-1)}$，于是 $F^{(M)}$ 就是全局最短路径长度矩阵。\n\n使用矩阵乘法加速的算法代码为\n```python\nimport sys\n\ndef fast_apsp():\n  k=1\n  F_prev=W\n  while k<N-1:\n    F_next=[[sys.info.float_max]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        for r in range(0,N):\n          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])\n    F_prev=F_next\n    k*=2\n  return F_prev\n```\n\n__Floyd-Warshall__ 的代码\n\n考虑式 (2.4)，注意 $F^{(k)}$ 中的 k 表示路径可以经过中间节点 $\\{1,2,...,k\\}$，所以 $F^{(0)}$ 表示不经过任何中间节点的两点之间最短路径长度矩阵，所以根据基本条件不难得到\n$$F^{(0)}=W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n根据式 (2.4)，不难写出原始的 __Floyd-Warshall__ 算法的代码为\n```python\nF_prev=F_0\ndef floyd_warshall():\n  for k in range(0,N):\n    F_next=[[None]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])\n    F_prev=F_next\n  return F_prev\n```\n\n## 最优字母基数编码树问题 ARC\n\nARC 是霍夫曼编码树问题的一个变体。霍夫曼树要满足的条件是节点权值*路径长度，求和最小。比如常见的霍夫曼编码，节点权值对应词频，路径对应单个词的编码长度，加权求和就是编码后的序列总长度最小。构造霍夫曼树可以直接使用贪心算法，给定一个具有权值的节点序列，找出具有最小权值的两个节点，构造出一个子树，这两个节点为子树的子节点，子树的父节点权值为这两个子节点的权值之和，然后将这两个字节点从节点序列中移除，并插入新构造的父节点到节点序列，重复这个过程，直到节点序列中只剩一个节点。这是一个自底向上的构造过程。\n\nARC为，给定一个具有构造成本（权值）的节点序列，目标是构造一棵成本最小的树，整棵树的总成本为所有内部节点（internal nodes，包括 root）的成本之和，内部节点的损失为以此内部节点为根节点的子树中所有叶节点的成本之和。给定 $S=(w_0,w_1,...,w_{n-1})$ 为有序序列表示叶节点的权值，定义 $(i,j)$ 的态为 $(w_i,...,w_j)$，那么\n$$f(i,j)=\\min_{}\\{c(i,j,d)+f(i,d)+f(d+1,j)\\}, \\quad i<j \\qquad(2.8)$$\n其中 $f(i,j)$ 表示以节点 $(i,...,j)$ 为叶节点的子树总成本， $c(i,j,d)=\\sum_{k=i}^j w_k$ 表示这个子树的根节点的成本，这里我们限制为二叉树，所以不难理解式 (2.8)，树的总成本为左子树的总成本与右子树的总成本以及树根节点成本三者之和。d 用于划分，其中 $(i,...,d)$ 为左子树的叶节点，$(d+1,...,j)$ 为右子树的叶节点，无论怎么划分（即选择任何 d 的有效值），以 $(i,...,j)$ 为叶节点的子树的根节点成本 $c(i,j,d)$ 均保持不变。\n\n最后，目标是计算 $f(0,n-1)$，基本条件为 $f(i,i)=0, \\ \\forall i \\in \\{0,1,...,n-1\\}$。\n\n构造 ARC 比构造 Huffman 树多一个限制，那就是给定节点序列后，只能结合两个相邻节点构造一个子树。例如，$S=(1,2,3,4)$，最优树为 $(((1,2),3),4)$，$f(S)=3+6+10=19$（三个内部节点成本之和），再例如 $S=(2,3,3,4)$，那么最优树为 $((2,3),(3,4))$，$f(S)=5+7+12=24$。\n\n  \n## 装配线平衡问题 ASMBAL\n一个产品装配需要经过一系列的处理，每个处理步骤均有代价/损失，并且从一个处理站到另一个处理站之间也存在代价。这里举一个简单的例子进行说明，一个处理站看作一条处理线，一次处理为一个阶段 stage，转移发生在从 阶段 k，处理线 i 的节点到另一个位于阶段 k+1 处理线 j 的节点上。处理线之间的切换损失为 c(k,i,j)，通常 c(k,i,i)=0，即不切换处理线，那么切换损失应为 0，除了切换损失，每个节点自身还存在损失，初始状态 s 和终止状态 t 处于标记为 0 的处理线上，这只是为了说明初始时刻和终止时刻产品不应该在任何处理线上，初始决策的损失为 c(0,0,j)，最终决策的损失为 c(N,j,0)（j 为某个处理线）。 \n\n某种可行的处理使用一个节点表示，记节点标号为 0~13 共 14 个节点，其中特别地 0 和 13 分别表示起始状态和终止状态，即表示不用处理。假设 14 个节点自身的损失分别为\n$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$\n处理线之间的切换损失如下图，\n![](/images/DP2_fig1.png)\n\n也可以写成 14x14 的毗邻矩阵。可见，总共有两条处理线。\n\n在阶段 k，从 i 线到 j 线的损失由两部分组成 $R(k,i,j)=v(k,i)+c(k,i,j)$，DPFE 为\n$$f(k,i)=\\min_j \\{R(k,i,j)+f(k+1,j)\\}$$\n其中 $f(k,i)$ 表示从阶段 k 并处于线 i 上开始到最终态的损失，问题的目标就是求 $f(0,0)$，基本条件为 $f(k,i)=0, k > N$，N 为总阶段数，图中为 N=6。计算过程如下：\n$$\\begin{aligned} f(0,0)=\\min \\{R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)\\}\n\\\\\\\\ f(1,0)=\\min \\{R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)\\}\n\\\\\\\\ f(1,1)=\\min \\{R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)\\}\n\\\\\\\\ \\cdots \\ (omitted)\n\\end{aligned}$$\n\n## 最佳分派问题 ASSIGN\n集合 B 中的每个成员需要唯一地被赋值为集合 A 中的成员，如何 A 是有序的，那么这个过程也可以看作是 A 的一种排列，例如 $\\{1,2,3\\}$ 的一种排列为 $\\{3,2,1\\}$（总共有 3! 种排列）。集合 $A=(a_0,a_1,...,a_{n-1})$ 的某个排列 $B=(b_0,b_1,...,b_{n-1})$ 可按如下过程得到：在阶段 i，赋值 $a_j$ 给 $b_i$，对应的损失为 $c(i,j)$。由于赋值必须是唯一的，所以每个阶段需要跟踪集合 A 中还有多少尚未使用的成员。具体而言，我们记状态为 $(k,S)$，表示阶段 k 集合 A 中尚未使用的成员集合为 S，阶段 k 时选择成员 d，相应的损失记为 $C(k,S,d)$，下一状态为 $(k+1,S-\\{d\\})$，DPFE 为\n$$f(k,S)=\\min_{d \\in S} \\{C(k,S,d)+f(k+1,S-\\{d\\})\\}$$\n求解目标为 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$\n\n## 最佳二叉搜索树问题 BST\n假设包含 n 个数据的集合 $X=\\{x_0,...,x_{n-1}\\}$，并且是 __有序__ 的，每个数据 $x_i$ 的被访问概率为 $p(x_i)$，或简写为 $p_i$，且有 $\\sum_{i=0}^{n-1}p_i=1$，目标是建立一棵最小损失的二叉搜索树，其中树的损失定义为\n$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$\n$\\text{level}(x_i)$ 表示 $x_i$ 所在节点的深度。需要注意的是，节点不仅可存储在叶节点中，还可以存储在内部节点中。下面我们讨论如何使用 DP 来解决这个问题。\n\n### 方法一\n定义状态 S 为待安排到树中的元素集合，由于构造树是一个递归过程，易知 DPFE 为\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & S \\ne \\emptyset\n\\\\\\\\ 0 & S=\\emptyset \\end{cases}$$\n其中 $S_l = \\{x \\in S: x < \\alpha\\}, \\ S_r = \\{x \\in S: x > \\alpha\\}$，决策代价 $r(\\alpha, S)=\\sum_{x \\in S} p(x)$，根节点深度为 1。这个 DPFE 非常简单，不多说。\n\n以上 $S_l,\\ S_r$ 的划分保证了二叉树是 __有序__ 的。（若不需要保持有序，此问题与霍夫曼编码相同）\n\n上式可改写为\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & |S|>1\n\\\\\\\\ p(x) & S=\\{x\\} \\end{cases}$$\n\n### 方法二\n定义状态为一对整数 $(i,j)$，分别表示数据的起始下标和截止下标，在此范围内的数据是需要被安排进树的，已知数据集合为 $X=\\{x_0,...,x_{n-1}\\}$，那么 DPFE 为\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i \\le j\n\\\\\\\\ 0 & i > j \\end{cases}$$\n$(i,j)$ 表示待安排进树的节点下标的起止范围，即将从此范围中选择下标为 k 的数据作为当前节点，或称子树根节点，这个子树的所有节点对应数据下标范围为 $(i,j)$, 其中$(i,k-1)$ 用于被安排进当前子树的左子树，$(k+1,j)$ 用于被安排进当前子树的右子树，无论当前选择哪个决策 k，$(i,j)$ 范围内的数据的概率在当前层 level 均需被计算一次，因为是从上到下构造树的，所以每到达一个 level，所有到达这个 level 的数据其访问概率需要被计算一次，在 $f(i,j)$ 中，$(i,j)$ 范围内的数据均到达当前 level，属于待安排进树的数据，所以当前决策损失为累加所有到达当前 level 的数据的访问概率，即 $\\sum_{l=i}^j p_l$。\n\n与方法一同样地可改写上式为\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i < j\n\\\\\\\\ p_i & i = j \\end{cases}$$\n\n## 最佳覆盖问题 COV\n假设有 k 个不同大小的灌木需要在严寒中被保护起来，k 个灌木按大小排序，标号为 0 的灌木尺寸最小，假设灌木的保护措施就是制造覆盖，将其覆盖起来进行保护，大小为 i 的覆盖其制造成本为 $c_i$。由于技术限制，只能制造不超过 n 种尺寸的覆盖，且 $n \\le k$，已知大的覆盖能用于保护小的灌木，目标是选择 n 种覆盖尺寸，使得能覆盖所有的灌木，且成本最小。\n\n使用 DP 解决上述问题，为了方便，我们假设灌木尺寸按从小到达排好序，编号为 $0,1,...,k-1$，编号为 $l$ 的灌木尺寸为 $s_l$，其对应的覆盖制造成本为 $c_{s_l}$，简记为 $c_l$，这里为了使问题简单，我们不考虑使用多个较小尺寸的覆盖来保护较大尺寸的灌木，即，每个灌木只使用单个覆盖。一种显然的想法是，从大到小来制造覆盖，因为大尺寸的覆盖总能保护小尺寸的灌木，令 $j$ 表示当前还可以制造多少种尺寸的覆盖，$l$ 表示当前未得到保护的最大尺寸灌木编号，于是 DPFE 为\n$$f(j,l)=\\begin{cases} \\min_{d \\in \\{j-2,...,l-1\\}} \\{(l-d)c_l+f(j-1,d)\\} & j>1\n\\\\\\\\ (l+1)c_l & j=1 \\end{cases}$$\nd 表示灌木尺寸范围的起始下标（exclusive），也就是说当前决策是制造尺寸为 $s_l$ 的覆盖，用于保护 $\\{d+1,...,l\\}$ 范围内的灌木。显然 d 最大为 $l-1$，此时当前决策所制造的覆盖只用于保护编号为 $l$ 的灌木，d 最小为 $j-2$，这是因为当前决策所制造的覆盖最多能用于保护灌木的编号为 $\\{j-1,...,l\\}$，此时剩余尚未保护的灌木编号 $\\{0,1,...,j-2\\}$ 共 $j-1$ 种尺寸，正好剩余可以制造的覆盖尺寸也是 $j-1$ 种，两者一一对应。\n\n再看基本条件，$f(j,l)=(l+1)c_l, j=1$，表示当前只能制造一种尺寸的覆盖时，那只能制造当前尚未得到保护的最大灌木的尺寸的覆盖，当前最大尺寸的灌木编号为 $l$，由于只有这一种尺寸的覆盖可以制造，其必须保护 $0,...,l$ 范围内的灌木。\n\n## 时限调度问题 DEADLINE\n有一个进程集合，其中每个进程的执行时间均为单位时间，同时每个进程也各自有一个最后期限，如果在最后期限之前完成，则具有收益，否则收益为 0，现在要选择一个最优进程子集，放置在单处理器上执行，并且收益要最大。这个问题当然也可以使用贪心算法来解决，那些收益较大的进程我们总希望能被执行，无论是先执行还是后执行，反正只要在其最后期限之前被执行即可，那么不如优先执行这些收益最大的进程，即， __每一步优先执行收益最大的进程__。\n\n假设进程编号为 $S^{\\ast}=\\{0,1,2,3,4\\}$，对应收益为 $p=\\{10,15,20,1,5\\}$，截止期限为 $t=\\{1,2,2,3,3\\}$。贪心算法的代码如下：\n```python\nimport numpy as np\nt=np.array([1,2,2,3,3])\np=np.array([10,15,20,1,5])\nm=0   # 总收益\nn=t.shape[0]  # 最多决策数\n\nfor i in range(n):\n  idx=np.where(t>0)[0]\n  if idx.shape[0]==0:\n    break   # 全部超出最后期限，决策结束\n  c=np.max(p[idx])  # 尚未超出期限的进程的最大收益，当前最优决策的收益（贪心策略）\n  if c < 0:\n    break\n  m+=c\n  idx=np.argmax(p[idx])+idx[0]  # 当前决策的进程的编号\n  p[idx]=-1e8                   # 标记收益为负，表示后续决策不再考虑此进程\n  t-=1                          # 更新进程的最后期限\nprint('最大收益为', sep=' ')\nprint(m)    # 40\n```\n\n使用 DP 解决这个问题，定义状态为 $(k,S)$，其中 k 为阶段编号，S 为所有尚未执行的进程集合，每一步的决策 d 是从 S 中选出，决策后的下一状态为 $(k+1,S-\\{d\\})$，根据最后期限从近到远对进程进行排序，DPFE 为\n$$f(k,S)=\\max_{d \\in S}\\{c(d|S)+f(k+1,S-\\{d\\})\\}$$\n其中，当决策 d 的最后期限大于等于阶段 k 时（k 从 1 开始计算）$c(d|S)=w_d$ ，否则 $c(d|S)=0$。目标是求 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$，其中 n 为进程数。使用上述例子，代码如下\n```python\nt=[1,2,2,3,3]\np=[10,15,20,1,5]\nS=[0,1,2,3,4]\nn=len(t)\ndef profit(k,d):\n  return p[d] if t[d]>=k else 0\n\ndef deadline(k,S):\n  return 0 if len(S)==0 or k==n+1 else \\\n    max([profit(k,S[i])+deadline(k+1,S[:i]+S[i+1:]) for i in range(len(S))])\n\nprint(deadline(1, S))   # 40\n```\n\n## 折现利润问题 DPP\n假设一个湖中第一年开始时有 $b_1$ 条鱼，第 t 年开始时的鱼数为 $b_t$，在第 t 年中售出 $x_t$ 条鱼，收益为 $r(x_t)$，捕鱼成本为 $c(x_t,b_t)$，鱼可以再生，再生长率为 s，表示某一年年初的鱼数量是上一年年底鱼数量的 s 倍，计划展望期为 $1,...,T$，即最多 T 年卖完所有鱼，其中货币年贬值率为 y，决策为 $x_t$ 表示 第 t 年卖出多少鱼，要求最大净收益，状态为 $(t,b)$，其中 t 表示年份，b 表示第 t 年初的鱼数，DPFE 为\n$$f(t,b)=\\begin{cases} \\max_{x_t \\in \\{0,...,b\\}} \\{r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)\\} & t \\le T\n\\\\\\\\ 0 & t=T+1 \\end{cases}$$\n\n\n## 编辑长度问题 EDP\n编辑长度问题通常用于字符串的非精确匹配问题。记 $\\Sigma$ 为一有限字符集，给定两个字符串 $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$，或者写为 $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$，任务是将 x 转变为 y，并且只能使用如下三种编辑操作：\n- 删除操作 D。从某字符串中删除一个字符，损失为 $c(D)$\n- 插入操作 I。向某字符串中插入一个字符，损失为 $c(I)$\n- 替换操作 R。将某字符串中某个字符替换为另一个字符，损失为 $c(R)$，如果替换前后字符相同，则 $c(R)=0$\n  \n要求一个编辑序列，使得 x 转变为 y，且具有最小损失，这里损失为所有操作的损失之和。DPFE 为\n$$f(i,j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min \\{f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)\\} & i>0,j>0 \\end{cases}$$\n其中 $f(i,j)$ 表示从 $X_i$ 到 $Y_j$ 需要的操作损失，$X_i$ 为 x 的前 i 个字符形成的子串，$Y_j$ 为 y 的前 j 个字符形成的子串，当 i=0 时，向 $X_i$ 插入 j 个字符得到 $Y_j$，当 j=0 时，将 $X_i$ 删除 i 个字符得到 $Y_j$，当 i>0 且 j>0 时，从 $X_i$ 变换到 $Y_j$，总共可以有以下三种决策/操作，取损失最小的决策：\n- 删除 $X_i$ 的第 i 个字符，然后从 $X_{i-1}$ 变换到 $Y_j$，两部分损失分别为 $c(D)$ 和 $f(i-1,j)$\n- 从 $X_i$ 变换到 $Y_{j-1}$，然后再在末尾插入一个字符得到 $Y_j$，两部分的操作损失分别为 $f(i,j-1)$ 和 $c(I)$\n- 两个子串的最后一个字符，使用替换，$x_i \\rightarrow y_j$，然后再从 $X_{i-1}$ 变换到 $Y_{j-1}$ 即可得到 $X_i$ 变换到 $Y_j$ 的过程，两部分的损失分别为 $f(i-1,j-1)$ 和 $c(R)$\n\n还可以写成如下形式的 DPFE\n$$f(X_i,Y_j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min_{d \\in \\{D,I,R\\}} \\{f(t(X_i,Y_j,d))+c(d)\\} & i>0,j>0\n\\end{cases}$$\n其中变换定义为\n$$t(X_i,Y_j,D)=(X_{i-1},Y_j)\n\\\\\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})\n\\\\\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$\n","slug":"dp/DP2","published":1,"updated":"2020-04-24T10:31:14.130Z","_id":"ck9dzcjg3002rgga6dmdfbe4b","comments":1,"layout":"post","photos":[],"link":"","content":"<p>上一篇文章 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 介绍了动态规划的原理，以及几种常见模型的 DPFE。这篇文章则主要介绍一些实际应用。</p>\n<h2 id=\"最佳分配问题-ALLOT\"><a href=\"#最佳分配问题-ALLOT\" class=\"headerlink\" title=\"最佳分配问题 ALLOT\"></a>最佳分配问题 ALLOT</h2><p>最佳分配问题简称为 ALLOT，描述了如何讲有限资源分配给一些用户，损失（或者利益，损失对应最小化，利益对应最大化）与用户以及分配到的资源量有关。ALLOT 也可以看作是背包问题 KSINT 的一个变种。</p>\n<a id=\"more\"></a>\n\n<p>假设有 M 单位的资源要分配给 N 个用户，记 C(k,d) 表示分配 d 单位资源给用户 k 时的损失，分配决策按阶段进行，即第一阶段分配 $d_1$ 单位资源给用户 1，第二阶段分配 $d_2$ 单位资源给用户 2，依次进行，定义状态 (k,m) 为阶段 k 时剩余 m 单位资源，阶段 k 分配之前的资源量为 m，阶段 k 的分配损失为 C(k,d)，下一阶段状态为 (k+1,m-d)，于是根据 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中式 (1.19) 可知 DPFE 为<br>$$f(k,m)=\\min_{d \\in {0,…,m}} {C(k,d)+f(k+1,m-d)} \\quad (2.1)$$<br>目标是求 f(1,M)，基本条件为 f(N+1,m)=0，其中 $m \\ge 0$，这表示资源可以不用全部瓜分完。</p>\n<p>现在假设有 M=4，N=3，且<br>$$(C_{k,d})_{k\\in {1,2,3};d\\in {0,…,4}}=\\begin{pmatrix}\\infty &amp; 1.0 &amp; 0.8&amp; 0.4 &amp; 0.0 \\\\ \\infty &amp; 1.0&amp; 0.5 &amp; 0.0 &amp; 0.0 \\\\ \\infty &amp; 1.0 &amp; 0.6 &amp; 0.3 &amp; 0.0 \\end{pmatrix}$$<br>那么，f(1,M)=1.0+0.5+1.0=2.5，最佳分配序列为 $d_1=1,d_2=2,d_3=1$。我们可以根据式 (2.1) 逐步展开计算，下面是代码实现，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot</span><span class=\"params\">(cache=True)</span>:</span></span><br><span class=\"line\">    M=<span class=\"number\">4</span></span><br><span class=\"line\">    N=<span class=\"number\">3</span></span><br><span class=\"line\">    max_float=<span class=\"number\">1e8</span></span><br><span class=\"line\">    cost=[[max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.0</span>]]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> cache:</span><br><span class=\"line\">        cache_dict = &#123;&#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot_inner</span><span class=\"params\">(k,m)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> (k,m) <span class=\"keyword\">in</span> cache_dict:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> cache_dict[(k,m)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;= N: <span class=\"keyword\">return</span> [], <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        min_f=max_float</span><br><span class=\"line\">        min_ds = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> range(m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ds,f=allot_inner(k+<span class=\"number\">1</span>,m-d)</span><br><span class=\"line\">            temp=cost[k][d]+f</span><br><span class=\"line\">            <span class=\"keyword\">if</span> min_f &gt; temp:</span><br><span class=\"line\">                min_f = temp</span><br><span class=\"line\">                min_ds = [d]+ds</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> k &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            cache_dict[(k,m)]=(min_ds,min_f)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> min_ds, min_f</span><br><span class=\"line\">    ds, f=allot_inner(<span class=\"number\">0</span>,M)</span><br><span class=\"line\">    print(<span class=\"string\">\"min cost:\"</span>,f,<span class=\"string\">\"opt allotments:\"</span>, ds)</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"所有结点对的最短路径问题-APSP\"><a href=\"#所有结点对的最短路径问题-APSP\" class=\"headerlink\" title=\"所有结点对的最短路径问题 APSP\"></a>所有结点对的最短路径问题 APSP</h2><p>在图中寻找从任一起点 s 到任一终点 t 之间的最短路径，记图中结点数量为 N，为简单起见，我们假设任意结点对之间没有非正长度的环，并且没有自环（self-loop）。</p>\n<p>可以利用 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中的最短路径模型，将 (s,t) 看作变量，求得一系列的最短路径值，然后再求最小值即可，但是这其中肯定存在一些重复计算，所以这里我们讨论更高效率的计算方法。</p>\n<p><strong>Relaxation</strong> </p>\n<p>定义 F(k,p,q) 为从 p 到 q 之间的最短路径长度，k 表示从 p 到 q 最多走 k 步（从一个节点到下一个节点为一步）。借助 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中式 (1.27)，DPFE 为<br>$$F(k,p,q)=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\quad(2.2)$$<br>其中 r 是 p 的直接后继节点。基本条件为:</p>\n<ol>\n<li>$F(k,p,q)=0, k \\ge 0, p=q$，表示当 p 就是 q 时，最多走 k 步，最短路径长度为 0。</li>\n<li>$F(0,p,q)=\\infty, p \\ne q$， 表示当 p 不为 q 时，最多走 0 步，最短路径长度为无穷大。</li>\n</ol>\n<p>虽然我们假定没有自环，但是我们依然可以令 $b(p,p)=0$（实际路径中我们可以去掉环即可），那么式 (2.2) 可简化为<br>$$\\begin{aligned}F(k,p,q)&amp;=\\min {F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\\\ &amp;=\\min_{r \\in succ(p)\\cup {p}} {b(p,r)+F(k-1,r,q)} \\qquad(2.3) \\end{aligned}$$</p>\n<p><strong>Floyd-Warshall</strong> </p>\n<p>式 (2.2) 这个 DPFE 是一种分而治之的思想：从 p 到 q 最多走 k 步的路径，可以分为从 p 走一步到 r 以及从 r 最多走 k-1 步到 q 两个子路径。还有一种替代方案是从 p 到 r 并且从 r 到 q，其中 p 到 r 的步数不再固定为 1，但是从 p 出发，到达 q 总共最多经过 k 个点，r 就是这 k 个中间点，不妨记这 k 个点为 ${1,2,…,k}$，那么求从 p 到 q 并使用 ${1,2,…,N}$ 作为可能的中间点的最短路径就是 p 到 q 的全局最短路径。DPFE 为<br>$$F(k,p,q)=\\min {F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)} \\qquad(2.4)$$</p>\n<p>为了便于理解，我们作如下说明：</p>\n<ol>\n<li>将 N 个节点编号为 $V={1,2,…,N}$，$p,q \\in V$</li>\n<li>$F(k,p,q)$ 表示从 p 到 q 且以 ${1,2,…,k}$ 作为可能的中间节点</li>\n<li>问题的求解目标为 $F(N,p,q)$</li>\n<li>如何理解式 $(2.4)$？<ul>\n<li>p 到 q 的路径不经过中间点 k，即，使用 ${1,2,…,k-1}$ 作为可能的中间节点</li>\n<li>或者 p 到 q 的路径经过中间点 k，即，分为两个子路径 p 到 k 和 k 到 q，这两个子路径均使用 ${1,2,…,k-1}$ 作为可能的中间节点</li>\n</ul>\n</li>\n<li>式 $(2.4)$ 这个递归操作需要条件 k&gt;0。k=0 时为基本条件 $F(0,p,q)=0, p=q$，以及 $F(0,p,q)=b(p,q), p\\ne q$。前者表示当 p q 为同一节点时，不使用任何中间节点，损失为 0；后者表示当 p q 不同时，不使用任何中间节点，损失为 $b(p,q)$，需要注意这里 q 是 p 的直接后继，如果不是，那么有 $F(0,p,q)=\\infty, p \\notin succ(p) \\cup {p}$</li>\n<li>中间点序列 ${1,2,…,k}$ 可能会包含 p 和 q，如果包含了的话，由于我们假定所有的环都是正的，所以再求序列最小值的，带环的路径均会被过滤掉，而自环 $b(p,p)=0$，不会影响最短路径的长度，如果路径中出现自环，去掉即可（去掉连续重复的节点，只保留一个）。</li>\n</ol>\n<p>式 (2.2) 中 r 是 p 的后继节点，最多可取 $N-1$ 个节点（假设图中其他节点均为 p 的后继节点，就对应 $N-1$），k 最大为 $N-1$ 步，p 和 q 均各有 N 个取值，所以式 (2.2) 的时间复杂度为 $O(N^4)$，类似地，式 (2.4) 的时间复杂度为 $O(N^3)$。</p>\n<p>实际中要解决 APSP 问题，可以根据式 (2.2) 求出矩阵序列 ${F^{(1)},F^{(2)},…,F^{(N-1)}}$，任一矩阵 $F^{(k)}$ 维度为 $N \\times N$，$F_{p,q}^{k}$ 表示从 p 到 q 最多走 k 步的最短路径长度，然后求 $\\min_{p,q} F_{p,q}^{(N-1)}$ 就是 APSP 的解。</p>\n<p><strong>矩阵乘法</strong></p>\n<p>为了借鉴矩阵乘法的思想，我们首先将式 (2.2) 作变换，<br>$$\\begin{aligned} F(k,p,q)&amp;=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}}<br>\\\\ &amp;= \\min_{r \\in succ(p)} {b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in succ(p) \\cup {p}} {b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in {1,2,…,N}} {b(p,r)+F(k-1,r,q)} \\qquad(2.5) \\end{aligned}$$<br>其中，$b(p,r)$ 是事先给定的任意两节点之间的距离，若两节点之间没有边 edge 相连，则距离为 $\\infty$，这里称所有节点对之间的距离组成的矩阵为权重矩阵 $W_{N \\times N}$。根据式 (2.2) 的基本条件，不难得知 $F^{(0)}$ 矩阵对角线全 0，其余元素均为 $\\infty$：</p>\n<p>$$F^{(0)}=\\begin{bmatrix}0 &amp; \\infty &amp; \\cdots &amp; \\infty<br>\\\\                    \\infty &amp; 0  &amp; \\cdots &amp; \\infty<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    \\infty &amp; \\infty &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>$$W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>根据式 (2.5)，已知 $F^{(k-1)}$ 求 $F^{(k)}$ 的代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">F_k=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">  <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_k[p][q]=sys.info.float_max</span><br><span class=\"line\">    <span class=\"comment\"># F_k[p][q]=0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])</span><br><span class=\"line\">      <span class=\"comment\"># F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])</span></span><br></pre></td></tr></table></figure>\n<p>从上面代码片段可见，与矩阵乘法（注释部分）完全一个模样，而我们的目的是为了计算 $F^{(N-1)}$，中间的其他 $F^{(k)}$ 矩阵如无必要，可以不用计算出来，比如下面，<br>$$\\begin{aligned} F^{(1)}&amp;=W \\circ F^{(0)}=W<br>\\\\ F^{(2)}&amp;=W \\circ F^{(1)}=W^2<br>\\\\ F^{(3)}&amp;=W \\circ F^{(2)}=W^3<br>\\\\ &amp;\\vdots<br>\\\\ F^{(N-1)}&amp;=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$</p>\n<p>$\\circ$ 表示某种运算符，比如矩阵乘法或者这里的最小值计算，我们改为如下序列计算，<br>$$\\begin{aligned} F^{(1)}&amp;=W<br>\\\\ F^{(2)}&amp;=W^2=W \\circ W<br>\\\\ F^{(4)}&amp;=W^4=W^2 \\circ W^2<br>\\\\ &amp;\\vdots<br>\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&amp;=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$<br>注意上面 $2^{\\lceil log(N-1) \\rceil}$ 中的向上取整 $\\lceil \\cdot \\rceil$ 很重要，这保证了 $2^{\\lceil log(N-1) \\rceil} \\ge N-1$，从而 $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ （element-wise comparison）。</p>\n<p>因为结合顺序无关紧要，才使得我们可以从式 (2.6) 可以改写为式 (2.7)，例如<br>$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$<br>将 $\\circ$ 替换为 $\\min$，即 $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$，注意这里的 $\\min$ 不是 element-wise operator，就跟矩阵乘法不是矩阵点乘一样。当然，以上内容只是帮助理解，不是式 (2.6) 可以变换为式 (2.7) 的严格证明。</p>\n<p>好了，有了式 (2.7) 就可以更快的计算出 $F^{(M)}, M \\ge N-1$，由于 $F^{(k)}$ 单调减，并收敛于 $F^{(N-1)}$，于是 $F^{(M)}$ 就是全局最短路径长度矩阵。</p>\n<p>使用矩阵乘法加速的算法代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fast_apsp</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  k=<span class=\"number\">1</span></span><br><span class=\"line\">  F_prev=W</span><br><span class=\"line\">  <span class=\"keyword\">while</span> k&lt;N<span class=\"number\">-1</span>:</span><br><span class=\"line\">    F_next=[[sys.info.float_max]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">    k*=<span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<p><strong>Floyd-Warshall</strong> 的代码</p>\n<p>考虑式 (2.4)，注意 $F^{(k)}$ 中的 k 表示路径可以经过中间节点 ${1,2,…,k}$，所以 $F^{(0)}$ 表示不经过任何中间节点的两点之间最短路径长度矩阵，所以根据基本条件不难得到<br>$$F^{(0)}=W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>根据式 (2.4)，不难写出原始的 <strong>Floyd-Warshall</strong> 算法的代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F_prev=F_0</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">floyd_warshall</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_next=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"最优字母基数编码树问题-ARC\"><a href=\"#最优字母基数编码树问题-ARC\" class=\"headerlink\" title=\"最优字母基数编码树问题 ARC\"></a>最优字母基数编码树问题 ARC</h2><p>ARC 是霍夫曼编码树问题的一个变体。霍夫曼树要满足的条件是节点权值*路径长度，求和最小。比如常见的霍夫曼编码，节点权值对应词频，路径对应单个词的编码长度，加权求和就是编码后的序列总长度最小。构造霍夫曼树可以直接使用贪心算法，给定一个具有权值的节点序列，找出具有最小权值的两个节点，构造出一个子树，这两个节点为子树的子节点，子树的父节点权值为这两个子节点的权值之和，然后将这两个字节点从节点序列中移除，并插入新构造的父节点到节点序列，重复这个过程，直到节点序列中只剩一个节点。这是一个自底向上的构造过程。</p>\n<p>ARC为，给定一个具有构造成本（权值）的节点序列，目标是构造一棵成本最小的树，整棵树的总成本为所有内部节点（internal nodes，包括 root）的成本之和，内部节点的损失为以此内部节点为根节点的子树中所有叶节点的成本之和。给定 $S=(w_0,w_1,…,w_{n-1})$ 为有序序列表示叶节点的权值，定义 $(i,j)$ 的态为 $(w_i,…,w_j)$，那么<br>$$f(i,j)=\\min_{}{c(i,j,d)+f(i,d)+f(d+1,j)}, \\quad i&lt;j \\qquad(2.8)$$<br>其中 $f(i,j)$ 表示以节点 $(i,…,j)$ 为叶节点的子树总成本， $c(i,j,d)=\\sum_{k=i}^j w_k$ 表示这个子树的根节点的成本，这里我们限制为二叉树，所以不难理解式 (2.8)，树的总成本为左子树的总成本与右子树的总成本以及树根节点成本三者之和。d 用于划分，其中 $(i,…,d)$ 为左子树的叶节点，$(d+1,…,j)$ 为右子树的叶节点，无论怎么划分（即选择任何 d 的有效值），以 $(i,…,j)$ 为叶节点的子树的根节点成本 $c(i,j,d)$ 均保持不变。</p>\n<p>最后，目标是计算 $f(0,n-1)$，基本条件为 $f(i,i)=0, \\ \\forall i \\in {0,1,…,n-1}$。</p>\n<p>构造 ARC 比构造 Huffman 树多一个限制，那就是给定节点序列后，只能结合两个相邻节点构造一个子树。例如，$S=(1,2,3,4)$，最优树为 $(((1,2),3),4)$，$f(S)=3+6+10=19$（三个内部节点成本之和），再例如 $S=(2,3,3,4)$，那么最优树为 $((2,3),(3,4))$，$f(S)=5+7+12=24$。</p>\n<h2 id=\"装配线平衡问题-ASMBAL\"><a href=\"#装配线平衡问题-ASMBAL\" class=\"headerlink\" title=\"装配线平衡问题 ASMBAL\"></a>装配线平衡问题 ASMBAL</h2><p>一个产品装配需要经过一系列的处理，每个处理步骤均有代价/损失，并且从一个处理站到另一个处理站之间也存在代价。这里举一个简单的例子进行说明，一个处理站看作一条处理线，一次处理为一个阶段 stage，转移发生在从 阶段 k，处理线 i 的节点到另一个位于阶段 k+1 处理线 j 的节点上。处理线之间的切换损失为 c(k,i,j)，通常 c(k,i,i)=0，即不切换处理线，那么切换损失应为 0，除了切换损失，每个节点自身还存在损失，初始状态 s 和终止状态 t 处于标记为 0 的处理线上，这只是为了说明初始时刻和终止时刻产品不应该在任何处理线上，初始决策的损失为 c(0,0,j)，最终决策的损失为 c(N,j,0)（j 为某个处理线）。 </p>\n<p>某种可行的处理使用一个节点表示，记节点标号为 0~13 共 14 个节点，其中特别地 0 和 13 分别表示起始状态和终止状态，即表示不用处理。假设 14 个节点自身的损失分别为<br>$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$<br>处理线之间的切换损失如下图，<br><img src=\"/images/DP2_fig1.png\" alt=\"\"></p>\n<p>也可以写成 14x14 的毗邻矩阵。可见，总共有两条处理线。</p>\n<p>在阶段 k，从 i 线到 j 线的损失由两部分组成 $R(k,i,j)=v(k,i)+c(k,i,j)$，DPFE 为<br>$$f(k,i)=\\min_j {R(k,i,j)+f(k+1,j)}$$<br>其中 $f(k,i)$ 表示从阶段 k 并处于线 i 上开始到最终态的损失，问题的目标就是求 $f(0,0)$，基本条件为 $f(k,i)=0, k &gt; N$，N 为总阶段数，图中为 N=6。计算过程如下：<br>$$\\begin{aligned} f(0,0)=\\min {R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)}<br>\\\\ f(1,0)=\\min {R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)}<br>\\\\ f(1,1)=\\min {R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)}<br>\\\\ \\cdots \\ (omitted)<br>\\end{aligned}$$</p>\n<h2 id=\"最佳分派问题-ASSIGN\"><a href=\"#最佳分派问题-ASSIGN\" class=\"headerlink\" title=\"最佳分派问题 ASSIGN\"></a>最佳分派问题 ASSIGN</h2><p>集合 B 中的每个成员需要唯一地被赋值为集合 A 中的成员，如何 A 是有序的，那么这个过程也可以看作是 A 的一种排列，例如 ${1,2,3}$ 的一种排列为 ${3,2,1}$（总共有 3! 种排列）。集合 $A=(a_0,a_1,…,a_{n-1})$ 的某个排列 $B=(b_0,b_1,…,b_{n-1})$ 可按如下过程得到：在阶段 i，赋值 $a_j$ 给 $b_i$，对应的损失为 $c(i,j)$。由于赋值必须是唯一的，所以每个阶段需要跟踪集合 A 中还有多少尚未使用的成员。具体而言，我们记状态为 $(k,S)$，表示阶段 k 集合 A 中尚未使用的成员集合为 S，阶段 k 时选择成员 d，相应的损失记为 $C(k,S,d)$，下一状态为 $(k+1,S-{d})$，DPFE 为<br>$$f(k,S)=\\min_{d \\in S} {C(k,S,d)+f(k+1,S-{d})}$$<br>求解目标为 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$</p>\n<h2 id=\"最佳二叉搜索树问题-BST\"><a href=\"#最佳二叉搜索树问题-BST\" class=\"headerlink\" title=\"最佳二叉搜索树问题 BST\"></a>最佳二叉搜索树问题 BST</h2><p>假设包含 n 个数据的集合 $X={x_0,…,x_{n-1}}$，并且是 <strong>有序</strong> 的，每个数据 $x_i$ 的被访问概率为 $p(x_i)$，或简写为 $p_i$，且有 $\\sum_{i=0}^{n-1}p_i=1$，目标是建立一棵最小损失的二叉搜索树，其中树的损失定义为<br>$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$<br>$\\text{level}(x_i)$ 表示 $x_i$ 所在节点的深度。需要注意的是，节点不仅可存储在叶节点中，还可以存储在内部节点中。下面我们讨论如何使用 DP 来解决这个问题。</p>\n<h3 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h3><p>定义状态 S 为待安排到树中的元素集合，由于构造树是一个递归过程，易知 DPFE 为<br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; S \\ne \\emptyset<br>\\\\ 0 &amp; S=\\emptyset \\end{cases}$$<br>其中 $S_l = {x \\in S: x &lt; \\alpha}, \\ S_r = {x \\in S: x &gt; \\alpha}$，决策代价 $r(\\alpha, S)=\\sum_{x \\in S} p(x)$，根节点深度为 1。这个 DPFE 非常简单，不多说。</p>\n<p>以上 $S_l,\\ S_r$ 的划分保证了二叉树是 <strong>有序</strong> 的。（若不需要保持有序，此问题与霍夫曼编码相同）</p>\n<p>上式可改写为<br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; |S|&gt;1<br>\\\\ p(x) &amp; S={x} \\end{cases}$$</p>\n<h3 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h3><p>定义状态为一对整数 $(i,j)$，分别表示数据的起始下标和截止下标，在此范围内的数据是需要被安排进树的，已知数据集合为 $X={x_0,…,x_{n-1}}$，那么 DPFE 为<br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,…,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i \\le j<br>\\\\ 0 &amp; i &gt; j \\end{cases}$$<br>$(i,j)$ 表示待安排进树的节点下标的起止范围，即将从此范围中选择下标为 k 的数据作为当前节点，或称子树根节点，这个子树的所有节点对应数据下标范围为 $(i,j)$, 其中$(i,k-1)$ 用于被安排进当前子树的左子树，$(k+1,j)$ 用于被安排进当前子树的右子树，无论当前选择哪个决策 k，$(i,j)$ 范围内的数据的概率在当前层 level 均需被计算一次，因为是从上到下构造树的，所以每到达一个 level，所有到达这个 level 的数据其访问概率需要被计算一次，在 $f(i,j)$ 中，$(i,j)$ 范围内的数据均到达当前 level，属于待安排进树的数据，所以当前决策损失为累加所有到达当前 level 的数据的访问概率，即 $\\sum_{l=i}^j p_l$。</p>\n<p>与方法一同样地可改写上式为<br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,…,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i &lt; j<br>\\\\ p_i &amp; i = j \\end{cases}$$</p>\n<h2 id=\"最佳覆盖问题-COV\"><a href=\"#最佳覆盖问题-COV\" class=\"headerlink\" title=\"最佳覆盖问题 COV\"></a>最佳覆盖问题 COV</h2><p>假设有 k 个不同大小的灌木需要在严寒中被保护起来，k 个灌木按大小排序，标号为 0 的灌木尺寸最小，假设灌木的保护措施就是制造覆盖，将其覆盖起来进行保护，大小为 i 的覆盖其制造成本为 $c_i$。由于技术限制，只能制造不超过 n 种尺寸的覆盖，且 $n \\le k$，已知大的覆盖能用于保护小的灌木，目标是选择 n 种覆盖尺寸，使得能覆盖所有的灌木，且成本最小。</p>\n<p>使用 DP 解决上述问题，为了方便，我们假设灌木尺寸按从小到达排好序，编号为 $0,1,…,k-1$，编号为 $l$ 的灌木尺寸为 $s_l$，其对应的覆盖制造成本为 $c_{s_l}$，简记为 $c_l$，这里为了使问题简单，我们不考虑使用多个较小尺寸的覆盖来保护较大尺寸的灌木，即，每个灌木只使用单个覆盖。一种显然的想法是，从大到小来制造覆盖，因为大尺寸的覆盖总能保护小尺寸的灌木，令 $j$ 表示当前还可以制造多少种尺寸的覆盖，$l$ 表示当前未得到保护的最大尺寸灌木编号，于是 DPFE 为<br>$$f(j,l)=\\begin{cases} \\min_{d \\in {j-2,…,l-1}} {(l-d)c_l+f(j-1,d)} &amp; j&gt;1<br>\\\\ (l+1)c_l &amp; j=1 \\end{cases}$$<br>d 表示灌木尺寸范围的起始下标（exclusive），也就是说当前决策是制造尺寸为 $s_l$ 的覆盖，用于保护 ${d+1,…,l}$ 范围内的灌木。显然 d 最大为 $l-1$，此时当前决策所制造的覆盖只用于保护编号为 $l$ 的灌木，d 最小为 $j-2$，这是因为当前决策所制造的覆盖最多能用于保护灌木的编号为 ${j-1,…,l}$，此时剩余尚未保护的灌木编号 ${0,1,…,j-2}$ 共 $j-1$ 种尺寸，正好剩余可以制造的覆盖尺寸也是 $j-1$ 种，两者一一对应。</p>\n<p>再看基本条件，$f(j,l)=(l+1)c_l, j=1$，表示当前只能制造一种尺寸的覆盖时，那只能制造当前尚未得到保护的最大灌木的尺寸的覆盖，当前最大尺寸的灌木编号为 $l$，由于只有这一种尺寸的覆盖可以制造，其必须保护 $0,…,l$ 范围内的灌木。</p>\n<h2 id=\"时限调度问题-DEADLINE\"><a href=\"#时限调度问题-DEADLINE\" class=\"headerlink\" title=\"时限调度问题 DEADLINE\"></a>时限调度问题 DEADLINE</h2><p>有一个进程集合，其中每个进程的执行时间均为单位时间，同时每个进程也各自有一个最后期限，如果在最后期限之前完成，则具有收益，否则收益为 0，现在要选择一个最优进程子集，放置在单处理器上执行，并且收益要最大。这个问题当然也可以使用贪心算法来解决，那些收益较大的进程我们总希望能被执行，无论是先执行还是后执行，反正只要在其最后期限之前被执行即可，那么不如优先执行这些收益最大的进程，即， <strong>每一步优先执行收益最大的进程</strong>。</p>\n<p>假设进程编号为 $S^{\\ast}={0,1,2,3,4}$，对应收益为 $p={10,15,20,1,5}$，截止期限为 $t={1,2,2,3,3}$。贪心算法的代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">t=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">p=np.array([<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">m=<span class=\"number\">0</span>   <span class=\"comment\"># 总收益</span></span><br><span class=\"line\">n=t.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 最多决策数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">  idx=np.where(t&gt;<span class=\"number\">0</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> idx.shape[<span class=\"number\">0</span>]==<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span>   <span class=\"comment\"># 全部超出最后期限，决策结束</span></span><br><span class=\"line\">  c=np.max(p[idx])  <span class=\"comment\"># 尚未超出期限的进程的最大收益，当前最优决策的收益（贪心策略）</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> c &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br><span class=\"line\">  m+=c</span><br><span class=\"line\">  idx=np.argmax(p[idx])+idx[<span class=\"number\">0</span>]  <span class=\"comment\"># 当前决策的进程的编号</span></span><br><span class=\"line\">  p[idx]=<span class=\"number\">-1e8</span>                   <span class=\"comment\"># 标记收益为负，表示后续决策不再考虑此进程</span></span><br><span class=\"line\">  t-=<span class=\"number\">1</span>                          <span class=\"comment\"># 更新进程的最后期限</span></span><br><span class=\"line\">print(<span class=\"string\">'最大收益为'</span>, sep=<span class=\"string\">' '</span>)</span><br><span class=\"line\">print(m)    <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<p>使用 DP 解决这个问题，定义状态为 $(k,S)$，其中 k 为阶段编号，S 为所有尚未执行的进程集合，每一步的决策 d 是从 S 中选出，决策后的下一状态为 $(k+1,S-{d})$，根据最后期限从近到远对进程进行排序，DPFE 为<br>$$f(k,S)=\\max_{d \\in S}{c(d|S)+f(k+1,S-{d})}$$<br>其中，当决策 d 的最后期限大于等于阶段 k 时（k 从 1 开始计算）$c(d|S)=w_d$ ，否则 $c(d|S)=0$。目标是求 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$，其中 n 为进程数。使用上述例子，代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">S=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">n=len(t)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">profit</span><span class=\"params\">(k,d)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> p[d] <span class=\"keyword\">if</span> t[d]&gt;=k <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deadline</span><span class=\"params\">(k,S)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span> <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> k==n+<span class=\"number\">1</span> <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    max([profit(k,S[i])+deadline(k+<span class=\"number\">1</span>,S[:i]+S[i+<span class=\"number\">1</span>:]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(S))])</span><br><span class=\"line\"></span><br><span class=\"line\">print(deadline(<span class=\"number\">1</span>, S))   <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"折现利润问题-DPP\"><a href=\"#折现利润问题-DPP\" class=\"headerlink\" title=\"折现利润问题 DPP\"></a>折现利润问题 DPP</h2><p>假设一个湖中第一年开始时有 $b_1$ 条鱼，第 t 年开始时的鱼数为 $b_t$，在第 t 年中售出 $x_t$ 条鱼，收益为 $r(x_t)$，捕鱼成本为 $c(x_t,b_t)$，鱼可以再生，再生长率为 s，表示某一年年初的鱼数量是上一年年底鱼数量的 s 倍，计划展望期为 $1,…,T$，即最多 T 年卖完所有鱼，其中货币年贬值率为 y，决策为 $x_t$ 表示 第 t 年卖出多少鱼，要求最大净收益，状态为 $(t,b)$，其中 t 表示年份，b 表示第 t 年初的鱼数，DPFE 为<br>$$f(t,b)=\\begin{cases} \\max_{x_t \\in {0,…,b}} {r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)} &amp; t \\le T<br>\\\\ 0 &amp; t=T+1 \\end{cases}$$</p>\n<h2 id=\"编辑长度问题-EDP\"><a href=\"#编辑长度问题-EDP\" class=\"headerlink\" title=\"编辑长度问题 EDP\"></a>编辑长度问题 EDP</h2><p>编辑长度问题通常用于字符串的非精确匹配问题。记 $\\Sigma$ 为一有限字符集，给定两个字符串 $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$，或者写为 $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$，任务是将 x 转变为 y，并且只能使用如下三种编辑操作：</p>\n<ul>\n<li>删除操作 D。从某字符串中删除一个字符，损失为 $c(D)$</li>\n<li>插入操作 I。向某字符串中插入一个字符，损失为 $c(I)$</li>\n<li>替换操作 R。将某字符串中某个字符替换为另一个字符，损失为 $c(R)$，如果替换前后字符相同，则 $c(R)=0$</li>\n</ul>\n<p>要求一个编辑序列，使得 x 转变为 y，且具有最小损失，这里损失为所有操作的损失之和。DPFE 为<br>$$f(i,j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min {f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)} &amp; i&gt;0,j&gt;0 \\end{cases}$$<br>其中 $f(i,j)$ 表示从 $X_i$ 到 $Y_j$ 需要的操作损失，$X_i$ 为 x 的前 i 个字符形成的子串，$Y_j$ 为 y 的前 j 个字符形成的子串，当 i=0 时，向 $X_i$ 插入 j 个字符得到 $Y_j$，当 j=0 时，将 $X_i$ 删除 i 个字符得到 $Y_j$，当 i&gt;0 且 j&gt;0 时，从 $X_i$ 变换到 $Y_j$，总共可以有以下三种决策/操作，取损失最小的决策：</p>\n<ul>\n<li>删除 $X_i$ 的第 i 个字符，然后从 $X_{i-1}$ 变换到 $Y_j$，两部分损失分别为 $c(D)$ 和 $f(i-1,j)$</li>\n<li>从 $X_i$ 变换到 $Y_{j-1}$，然后再在末尾插入一个字符得到 $Y_j$，两部分的操作损失分别为 $f(i,j-1)$ 和 $c(I)$</li>\n<li>两个子串的最后一个字符，使用替换，$x_i \\rightarrow y_j$，然后再从 $X_{i-1}$ 变换到 $Y_{j-1}$ 即可得到 $X_i$ 变换到 $Y_j$ 的过程，两部分的损失分别为 $f(i-1,j-1)$ 和 $c(R)$</li>\n</ul>\n<p>还可以写成如下形式的 DPFE<br>$$f(X_i,Y_j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min_{d \\in {D,I,R}} {f(t(X_i,Y_j,d))+c(d)} &amp; i&gt;0,j&gt;0<br>\\end{cases}$$<br>其中变换定义为<br>$$t(X_i,Y_j,D)=(X_{i-1},Y_j)<br>\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})<br>\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$</p>\n","site":{"data":{}},"excerpt":"<p>上一篇文章 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 介绍了动态规划的原理，以及几种常见模型的 DPFE。这篇文章则主要介绍一些实际应用。</p>\n<h2 id=\"最佳分配问题-ALLOT\"><a href=\"#最佳分配问题-ALLOT\" class=\"headerlink\" title=\"最佳分配问题 ALLOT\"></a>最佳分配问题 ALLOT</h2><p>最佳分配问题简称为 ALLOT，描述了如何讲有限资源分配给一些用户，损失（或者利益，损失对应最小化，利益对应最大化）与用户以及分配到的资源量有关。ALLOT 也可以看作是背包问题 KSINT 的一个变种。</p>","more":"<p>假设有 M 单位的资源要分配给 N 个用户，记 C(k,d) 表示分配 d 单位资源给用户 k 时的损失，分配决策按阶段进行，即第一阶段分配 $d_1$ 单位资源给用户 1，第二阶段分配 $d_2$ 单位资源给用户 2，依次进行，定义状态 (k,m) 为阶段 k 时剩余 m 单位资源，阶段 k 分配之前的资源量为 m，阶段 k 的分配损失为 C(k,d)，下一阶段状态为 (k+1,m-d)，于是根据 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中式 (1.19) 可知 DPFE 为<br>$$f(k,m)=\\min_{d \\in {0,…,m}} {C(k,d)+f(k+1,m-d)} \\quad (2.1)$$<br>目标是求 f(1,M)，基本条件为 f(N+1,m)=0，其中 $m \\ge 0$，这表示资源可以不用全部瓜分完。</p>\n<p>现在假设有 M=4，N=3，且<br>$$(C_{k,d})_{k\\in {1,2,3};d\\in {0,…,4}}=\\begin{pmatrix}\\infty &amp; 1.0 &amp; 0.8&amp; 0.4 &amp; 0.0 \\\\ \\infty &amp; 1.0&amp; 0.5 &amp; 0.0 &amp; 0.0 \\\\ \\infty &amp; 1.0 &amp; 0.6 &amp; 0.3 &amp; 0.0 \\end{pmatrix}$$<br>那么，f(1,M)=1.0+0.5+1.0=2.5，最佳分配序列为 $d_1=1,d_2=2,d_3=1$。我们可以根据式 (2.1) 逐步展开计算，下面是代码实现，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot</span><span class=\"params\">(cache=True)</span>:</span></span><br><span class=\"line\">    M=<span class=\"number\">4</span></span><br><span class=\"line\">    N=<span class=\"number\">3</span></span><br><span class=\"line\">    max_float=<span class=\"number\">1e8</span></span><br><span class=\"line\">    cost=[[max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.0</span>]]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> cache:</span><br><span class=\"line\">        cache_dict = &#123;&#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot_inner</span><span class=\"params\">(k,m)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> (k,m) <span class=\"keyword\">in</span> cache_dict:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> cache_dict[(k,m)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;= N: <span class=\"keyword\">return</span> [], <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        min_f=max_float</span><br><span class=\"line\">        min_ds = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> range(m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ds,f=allot_inner(k+<span class=\"number\">1</span>,m-d)</span><br><span class=\"line\">            temp=cost[k][d]+f</span><br><span class=\"line\">            <span class=\"keyword\">if</span> min_f &gt; temp:</span><br><span class=\"line\">                min_f = temp</span><br><span class=\"line\">                min_ds = [d]+ds</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> k &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            cache_dict[(k,m)]=(min_ds,min_f)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> min_ds, min_f</span><br><span class=\"line\">    ds, f=allot_inner(<span class=\"number\">0</span>,M)</span><br><span class=\"line\">    print(<span class=\"string\">\"min cost:\"</span>,f,<span class=\"string\">\"opt allotments:\"</span>, ds)</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"所有结点对的最短路径问题-APSP\"><a href=\"#所有结点对的最短路径问题-APSP\" class=\"headerlink\" title=\"所有结点对的最短路径问题 APSP\"></a>所有结点对的最短路径问题 APSP</h2><p>在图中寻找从任一起点 s 到任一终点 t 之间的最短路径，记图中结点数量为 N，为简单起见，我们假设任意结点对之间没有非正长度的环，并且没有自环（self-loop）。</p>\n<p>可以利用 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中的最短路径模型，将 (s,t) 看作变量，求得一系列的最短路径值，然后再求最小值即可，但是这其中肯定存在一些重复计算，所以这里我们讨论更高效率的计算方法。</p>\n<p><strong>Relaxation</strong> </p>\n<p>定义 F(k,p,q) 为从 p 到 q 之间的最短路径长度，k 表示从 p 到 q 最多走 k 步（从一个节点到下一个节点为一步）。借助 <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a> 中式 (1.27)，DPFE 为<br>$$F(k,p,q)=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\quad(2.2)$$<br>其中 r 是 p 的直接后继节点。基本条件为:</p>\n<ol>\n<li>$F(k,p,q)=0, k \\ge 0, p=q$，表示当 p 就是 q 时，最多走 k 步，最短路径长度为 0。</li>\n<li>$F(0,p,q)=\\infty, p \\ne q$， 表示当 p 不为 q 时，最多走 0 步，最短路径长度为无穷大。</li>\n</ol>\n<p>虽然我们假定没有自环，但是我们依然可以令 $b(p,p)=0$（实际路径中我们可以去掉环即可），那么式 (2.2) 可简化为<br>$$\\begin{aligned}F(k,p,q)&amp;=\\min {F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\\\ &amp;=\\min_{r \\in succ(p)\\cup {p}} {b(p,r)+F(k-1,r,q)} \\qquad(2.3) \\end{aligned}$$</p>\n<p><strong>Floyd-Warshall</strong> </p>\n<p>式 (2.2) 这个 DPFE 是一种分而治之的思想：从 p 到 q 最多走 k 步的路径，可以分为从 p 走一步到 r 以及从 r 最多走 k-1 步到 q 两个子路径。还有一种替代方案是从 p 到 r 并且从 r 到 q，其中 p 到 r 的步数不再固定为 1，但是从 p 出发，到达 q 总共最多经过 k 个点，r 就是这 k 个中间点，不妨记这 k 个点为 ${1,2,…,k}$，那么求从 p 到 q 并使用 ${1,2,…,N}$ 作为可能的中间点的最短路径就是 p 到 q 的全局最短路径。DPFE 为<br>$$F(k,p,q)=\\min {F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)} \\qquad(2.4)$$</p>\n<p>为了便于理解，我们作如下说明：</p>\n<ol>\n<li>将 N 个节点编号为 $V={1,2,…,N}$，$p,q \\in V$</li>\n<li>$F(k,p,q)$ 表示从 p 到 q 且以 ${1,2,…,k}$ 作为可能的中间节点</li>\n<li>问题的求解目标为 $F(N,p,q)$</li>\n<li>如何理解式 $(2.4)$？<ul>\n<li>p 到 q 的路径不经过中间点 k，即，使用 ${1,2,…,k-1}$ 作为可能的中间节点</li>\n<li>或者 p 到 q 的路径经过中间点 k，即，分为两个子路径 p 到 k 和 k 到 q，这两个子路径均使用 ${1,2,…,k-1}$ 作为可能的中间节点</li>\n</ul>\n</li>\n<li>式 $(2.4)$ 这个递归操作需要条件 k&gt;0。k=0 时为基本条件 $F(0,p,q)=0, p=q$，以及 $F(0,p,q)=b(p,q), p\\ne q$。前者表示当 p q 为同一节点时，不使用任何中间节点，损失为 0；后者表示当 p q 不同时，不使用任何中间节点，损失为 $b(p,q)$，需要注意这里 q 是 p 的直接后继，如果不是，那么有 $F(0,p,q)=\\infty, p \\notin succ(p) \\cup {p}$</li>\n<li>中间点序列 ${1,2,…,k}$ 可能会包含 p 和 q，如果包含了的话，由于我们假定所有的环都是正的，所以再求序列最小值的，带环的路径均会被过滤掉，而自环 $b(p,p)=0$，不会影响最短路径的长度，如果路径中出现自环，去掉即可（去掉连续重复的节点，只保留一个）。</li>\n</ol>\n<p>式 (2.2) 中 r 是 p 的后继节点，最多可取 $N-1$ 个节点（假设图中其他节点均为 p 的后继节点，就对应 $N-1$），k 最大为 $N-1$ 步，p 和 q 均各有 N 个取值，所以式 (2.2) 的时间复杂度为 $O(N^4)$，类似地，式 (2.4) 的时间复杂度为 $O(N^3)$。</p>\n<p>实际中要解决 APSP 问题，可以根据式 (2.2) 求出矩阵序列 ${F^{(1)},F^{(2)},…,F^{(N-1)}}$，任一矩阵 $F^{(k)}$ 维度为 $N \\times N$，$F_{p,q}^{k}$ 表示从 p 到 q 最多走 k 步的最短路径长度，然后求 $\\min_{p,q} F_{p,q}^{(N-1)}$ 就是 APSP 的解。</p>\n<p><strong>矩阵乘法</strong></p>\n<p>为了借鉴矩阵乘法的思想，我们首先将式 (2.2) 作变换，<br>$$\\begin{aligned} F(k,p,q)&amp;=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}}<br>\\\\ &amp;= \\min_{r \\in succ(p)} {b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in succ(p) \\cup {p}} {b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in {1,2,…,N}} {b(p,r)+F(k-1,r,q)} \\qquad(2.5) \\end{aligned}$$<br>其中，$b(p,r)$ 是事先给定的任意两节点之间的距离，若两节点之间没有边 edge 相连，则距离为 $\\infty$，这里称所有节点对之间的距离组成的矩阵为权重矩阵 $W_{N \\times N}$。根据式 (2.2) 的基本条件，不难得知 $F^{(0)}$ 矩阵对角线全 0，其余元素均为 $\\infty$：</p>\n<p>$$F^{(0)}=\\begin{bmatrix}0 &amp; \\infty &amp; \\cdots &amp; \\infty<br>\\\\                    \\infty &amp; 0  &amp; \\cdots &amp; \\infty<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    \\infty &amp; \\infty &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>$$W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>根据式 (2.5)，已知 $F^{(k-1)}$ 求 $F^{(k)}$ 的代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">F_k=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">  <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_k[p][q]=sys.info.float_max</span><br><span class=\"line\">    <span class=\"comment\"># F_k[p][q]=0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])</span><br><span class=\"line\">      <span class=\"comment\"># F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])</span></span><br></pre></td></tr></table></figure>\n<p>从上面代码片段可见，与矩阵乘法（注释部分）完全一个模样，而我们的目的是为了计算 $F^{(N-1)}$，中间的其他 $F^{(k)}$ 矩阵如无必要，可以不用计算出来，比如下面，<br>$$\\begin{aligned} F^{(1)}&amp;=W \\circ F^{(0)}=W<br>\\\\ F^{(2)}&amp;=W \\circ F^{(1)}=W^2<br>\\\\ F^{(3)}&amp;=W \\circ F^{(2)}=W^3<br>\\\\ &amp;\\vdots<br>\\\\ F^{(N-1)}&amp;=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$</p>\n<p>$\\circ$ 表示某种运算符，比如矩阵乘法或者这里的最小值计算，我们改为如下序列计算，<br>$$\\begin{aligned} F^{(1)}&amp;=W<br>\\\\ F^{(2)}&amp;=W^2=W \\circ W<br>\\\\ F^{(4)}&amp;=W^4=W^2 \\circ W^2<br>\\\\ &amp;\\vdots<br>\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&amp;=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$<br>注意上面 $2^{\\lceil log(N-1) \\rceil}$ 中的向上取整 $\\lceil \\cdot \\rceil$ 很重要，这保证了 $2^{\\lceil log(N-1) \\rceil} \\ge N-1$，从而 $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ （element-wise comparison）。</p>\n<p>因为结合顺序无关紧要，才使得我们可以从式 (2.6) 可以改写为式 (2.7)，例如<br>$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$<br>将 $\\circ$ 替换为 $\\min$，即 $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$，注意这里的 $\\min$ 不是 element-wise operator，就跟矩阵乘法不是矩阵点乘一样。当然，以上内容只是帮助理解，不是式 (2.6) 可以变换为式 (2.7) 的严格证明。</p>\n<p>好了，有了式 (2.7) 就可以更快的计算出 $F^{(M)}, M \\ge N-1$，由于 $F^{(k)}$ 单调减，并收敛于 $F^{(N-1)}$，于是 $F^{(M)}$ 就是全局最短路径长度矩阵。</p>\n<p>使用矩阵乘法加速的算法代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fast_apsp</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  k=<span class=\"number\">1</span></span><br><span class=\"line\">  F_prev=W</span><br><span class=\"line\">  <span class=\"keyword\">while</span> k&lt;N<span class=\"number\">-1</span>:</span><br><span class=\"line\">    F_next=[[sys.info.float_max]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">    k*=<span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<p><strong>Floyd-Warshall</strong> 的代码</p>\n<p>考虑式 (2.4)，注意 $F^{(k)}$ 中的 k 表示路径可以经过中间节点 ${1,2,…,k}$，所以 $F^{(0)}$ 表示不经过任何中间节点的两点之间最短路径长度矩阵，所以根据基本条件不难得到<br>$$F^{(0)}=W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>根据式 (2.4)，不难写出原始的 <strong>Floyd-Warshall</strong> 算法的代码为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F_prev=F_0</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">floyd_warshall</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_next=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"最优字母基数编码树问题-ARC\"><a href=\"#最优字母基数编码树问题-ARC\" class=\"headerlink\" title=\"最优字母基数编码树问题 ARC\"></a>最优字母基数编码树问题 ARC</h2><p>ARC 是霍夫曼编码树问题的一个变体。霍夫曼树要满足的条件是节点权值*路径长度，求和最小。比如常见的霍夫曼编码，节点权值对应词频，路径对应单个词的编码长度，加权求和就是编码后的序列总长度最小。构造霍夫曼树可以直接使用贪心算法，给定一个具有权值的节点序列，找出具有最小权值的两个节点，构造出一个子树，这两个节点为子树的子节点，子树的父节点权值为这两个子节点的权值之和，然后将这两个字节点从节点序列中移除，并插入新构造的父节点到节点序列，重复这个过程，直到节点序列中只剩一个节点。这是一个自底向上的构造过程。</p>\n<p>ARC为，给定一个具有构造成本（权值）的节点序列，目标是构造一棵成本最小的树，整棵树的总成本为所有内部节点（internal nodes，包括 root）的成本之和，内部节点的损失为以此内部节点为根节点的子树中所有叶节点的成本之和。给定 $S=(w_0,w_1,…,w_{n-1})$ 为有序序列表示叶节点的权值，定义 $(i,j)$ 的态为 $(w_i,…,w_j)$，那么<br>$$f(i,j)=\\min_{}{c(i,j,d)+f(i,d)+f(d+1,j)}, \\quad i&lt;j \\qquad(2.8)$$<br>其中 $f(i,j)$ 表示以节点 $(i,…,j)$ 为叶节点的子树总成本， $c(i,j,d)=\\sum_{k=i}^j w_k$ 表示这个子树的根节点的成本，这里我们限制为二叉树，所以不难理解式 (2.8)，树的总成本为左子树的总成本与右子树的总成本以及树根节点成本三者之和。d 用于划分，其中 $(i,…,d)$ 为左子树的叶节点，$(d+1,…,j)$ 为右子树的叶节点，无论怎么划分（即选择任何 d 的有效值），以 $(i,…,j)$ 为叶节点的子树的根节点成本 $c(i,j,d)$ 均保持不变。</p>\n<p>最后，目标是计算 $f(0,n-1)$，基本条件为 $f(i,i)=0, \\ \\forall i \\in {0,1,…,n-1}$。</p>\n<p>构造 ARC 比构造 Huffman 树多一个限制，那就是给定节点序列后，只能结合两个相邻节点构造一个子树。例如，$S=(1,2,3,4)$，最优树为 $(((1,2),3),4)$，$f(S)=3+6+10=19$（三个内部节点成本之和），再例如 $S=(2,3,3,4)$，那么最优树为 $((2,3),(3,4))$，$f(S)=5+7+12=24$。</p>\n<h2 id=\"装配线平衡问题-ASMBAL\"><a href=\"#装配线平衡问题-ASMBAL\" class=\"headerlink\" title=\"装配线平衡问题 ASMBAL\"></a>装配线平衡问题 ASMBAL</h2><p>一个产品装配需要经过一系列的处理，每个处理步骤均有代价/损失，并且从一个处理站到另一个处理站之间也存在代价。这里举一个简单的例子进行说明，一个处理站看作一条处理线，一次处理为一个阶段 stage，转移发生在从 阶段 k，处理线 i 的节点到另一个位于阶段 k+1 处理线 j 的节点上。处理线之间的切换损失为 c(k,i,j)，通常 c(k,i,i)=0，即不切换处理线，那么切换损失应为 0，除了切换损失，每个节点自身还存在损失，初始状态 s 和终止状态 t 处于标记为 0 的处理线上，这只是为了说明初始时刻和终止时刻产品不应该在任何处理线上，初始决策的损失为 c(0,0,j)，最终决策的损失为 c(N,j,0)（j 为某个处理线）。 </p>\n<p>某种可行的处理使用一个节点表示，记节点标号为 0~13 共 14 个节点，其中特别地 0 和 13 分别表示起始状态和终止状态，即表示不用处理。假设 14 个节点自身的损失分别为<br>$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$<br>处理线之间的切换损失如下图，<br><img src=\"/images/DP2_fig1.png\" alt=\"\"></p>\n<p>也可以写成 14x14 的毗邻矩阵。可见，总共有两条处理线。</p>\n<p>在阶段 k，从 i 线到 j 线的损失由两部分组成 $R(k,i,j)=v(k,i)+c(k,i,j)$，DPFE 为<br>$$f(k,i)=\\min_j {R(k,i,j)+f(k+1,j)}$$<br>其中 $f(k,i)$ 表示从阶段 k 并处于线 i 上开始到最终态的损失，问题的目标就是求 $f(0,0)$，基本条件为 $f(k,i)=0, k &gt; N$，N 为总阶段数，图中为 N=6。计算过程如下：<br>$$\\begin{aligned} f(0,0)=\\min {R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)}<br>\\\\ f(1,0)=\\min {R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)}<br>\\\\ f(1,1)=\\min {R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)}<br>\\\\ \\cdots \\ (omitted)<br>\\end{aligned}$$</p>\n<h2 id=\"最佳分派问题-ASSIGN\"><a href=\"#最佳分派问题-ASSIGN\" class=\"headerlink\" title=\"最佳分派问题 ASSIGN\"></a>最佳分派问题 ASSIGN</h2><p>集合 B 中的每个成员需要唯一地被赋值为集合 A 中的成员，如何 A 是有序的，那么这个过程也可以看作是 A 的一种排列，例如 ${1,2,3}$ 的一种排列为 ${3,2,1}$（总共有 3! 种排列）。集合 $A=(a_0,a_1,…,a_{n-1})$ 的某个排列 $B=(b_0,b_1,…,b_{n-1})$ 可按如下过程得到：在阶段 i，赋值 $a_j$ 给 $b_i$，对应的损失为 $c(i,j)$。由于赋值必须是唯一的，所以每个阶段需要跟踪集合 A 中还有多少尚未使用的成员。具体而言，我们记状态为 $(k,S)$，表示阶段 k 集合 A 中尚未使用的成员集合为 S，阶段 k 时选择成员 d，相应的损失记为 $C(k,S,d)$，下一状态为 $(k+1,S-{d})$，DPFE 为<br>$$f(k,S)=\\min_{d \\in S} {C(k,S,d)+f(k+1,S-{d})}$$<br>求解目标为 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$</p>\n<h2 id=\"最佳二叉搜索树问题-BST\"><a href=\"#最佳二叉搜索树问题-BST\" class=\"headerlink\" title=\"最佳二叉搜索树问题 BST\"></a>最佳二叉搜索树问题 BST</h2><p>假设包含 n 个数据的集合 $X={x_0,…,x_{n-1}}$，并且是 <strong>有序</strong> 的，每个数据 $x_i$ 的被访问概率为 $p(x_i)$，或简写为 $p_i$，且有 $\\sum_{i=0}^{n-1}p_i=1$，目标是建立一棵最小损失的二叉搜索树，其中树的损失定义为<br>$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$<br>$\\text{level}(x_i)$ 表示 $x_i$ 所在节点的深度。需要注意的是，节点不仅可存储在叶节点中，还可以存储在内部节点中。下面我们讨论如何使用 DP 来解决这个问题。</p>\n<h3 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h3><p>定义状态 S 为待安排到树中的元素集合，由于构造树是一个递归过程，易知 DPFE 为<br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; S \\ne \\emptyset<br>\\\\ 0 &amp; S=\\emptyset \\end{cases}$$<br>其中 $S_l = {x \\in S: x &lt; \\alpha}, \\ S_r = {x \\in S: x &gt; \\alpha}$，决策代价 $r(\\alpha, S)=\\sum_{x \\in S} p(x)$，根节点深度为 1。这个 DPFE 非常简单，不多说。</p>\n<p>以上 $S_l,\\ S_r$ 的划分保证了二叉树是 <strong>有序</strong> 的。（若不需要保持有序，此问题与霍夫曼编码相同）</p>\n<p>上式可改写为<br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; |S|&gt;1<br>\\\\ p(x) &amp; S={x} \\end{cases}$$</p>\n<h3 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h3><p>定义状态为一对整数 $(i,j)$，分别表示数据的起始下标和截止下标，在此范围内的数据是需要被安排进树的，已知数据集合为 $X={x_0,…,x_{n-1}}$，那么 DPFE 为<br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,…,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i \\le j<br>\\\\ 0 &amp; i &gt; j \\end{cases}$$<br>$(i,j)$ 表示待安排进树的节点下标的起止范围，即将从此范围中选择下标为 k 的数据作为当前节点，或称子树根节点，这个子树的所有节点对应数据下标范围为 $(i,j)$, 其中$(i,k-1)$ 用于被安排进当前子树的左子树，$(k+1,j)$ 用于被安排进当前子树的右子树，无论当前选择哪个决策 k，$(i,j)$ 范围内的数据的概率在当前层 level 均需被计算一次，因为是从上到下构造树的，所以每到达一个 level，所有到达这个 level 的数据其访问概率需要被计算一次，在 $f(i,j)$ 中，$(i,j)$ 范围内的数据均到达当前 level，属于待安排进树的数据，所以当前决策损失为累加所有到达当前 level 的数据的访问概率，即 $\\sum_{l=i}^j p_l$。</p>\n<p>与方法一同样地可改写上式为<br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,…,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i &lt; j<br>\\\\ p_i &amp; i = j \\end{cases}$$</p>\n<h2 id=\"最佳覆盖问题-COV\"><a href=\"#最佳覆盖问题-COV\" class=\"headerlink\" title=\"最佳覆盖问题 COV\"></a>最佳覆盖问题 COV</h2><p>假设有 k 个不同大小的灌木需要在严寒中被保护起来，k 个灌木按大小排序，标号为 0 的灌木尺寸最小，假设灌木的保护措施就是制造覆盖，将其覆盖起来进行保护，大小为 i 的覆盖其制造成本为 $c_i$。由于技术限制，只能制造不超过 n 种尺寸的覆盖，且 $n \\le k$，已知大的覆盖能用于保护小的灌木，目标是选择 n 种覆盖尺寸，使得能覆盖所有的灌木，且成本最小。</p>\n<p>使用 DP 解决上述问题，为了方便，我们假设灌木尺寸按从小到达排好序，编号为 $0,1,…,k-1$，编号为 $l$ 的灌木尺寸为 $s_l$，其对应的覆盖制造成本为 $c_{s_l}$，简记为 $c_l$，这里为了使问题简单，我们不考虑使用多个较小尺寸的覆盖来保护较大尺寸的灌木，即，每个灌木只使用单个覆盖。一种显然的想法是，从大到小来制造覆盖，因为大尺寸的覆盖总能保护小尺寸的灌木，令 $j$ 表示当前还可以制造多少种尺寸的覆盖，$l$ 表示当前未得到保护的最大尺寸灌木编号，于是 DPFE 为<br>$$f(j,l)=\\begin{cases} \\min_{d \\in {j-2,…,l-1}} {(l-d)c_l+f(j-1,d)} &amp; j&gt;1<br>\\\\ (l+1)c_l &amp; j=1 \\end{cases}$$<br>d 表示灌木尺寸范围的起始下标（exclusive），也就是说当前决策是制造尺寸为 $s_l$ 的覆盖，用于保护 ${d+1,…,l}$ 范围内的灌木。显然 d 最大为 $l-1$，此时当前决策所制造的覆盖只用于保护编号为 $l$ 的灌木，d 最小为 $j-2$，这是因为当前决策所制造的覆盖最多能用于保护灌木的编号为 ${j-1,…,l}$，此时剩余尚未保护的灌木编号 ${0,1,…,j-2}$ 共 $j-1$ 种尺寸，正好剩余可以制造的覆盖尺寸也是 $j-1$ 种，两者一一对应。</p>\n<p>再看基本条件，$f(j,l)=(l+1)c_l, j=1$，表示当前只能制造一种尺寸的覆盖时，那只能制造当前尚未得到保护的最大灌木的尺寸的覆盖，当前最大尺寸的灌木编号为 $l$，由于只有这一种尺寸的覆盖可以制造，其必须保护 $0,…,l$ 范围内的灌木。</p>\n<h2 id=\"时限调度问题-DEADLINE\"><a href=\"#时限调度问题-DEADLINE\" class=\"headerlink\" title=\"时限调度问题 DEADLINE\"></a>时限调度问题 DEADLINE</h2><p>有一个进程集合，其中每个进程的执行时间均为单位时间，同时每个进程也各自有一个最后期限，如果在最后期限之前完成，则具有收益，否则收益为 0，现在要选择一个最优进程子集，放置在单处理器上执行，并且收益要最大。这个问题当然也可以使用贪心算法来解决，那些收益较大的进程我们总希望能被执行，无论是先执行还是后执行，反正只要在其最后期限之前被执行即可，那么不如优先执行这些收益最大的进程，即， <strong>每一步优先执行收益最大的进程</strong>。</p>\n<p>假设进程编号为 $S^{\\ast}={0,1,2,3,4}$，对应收益为 $p={10,15,20,1,5}$，截止期限为 $t={1,2,2,3,3}$。贪心算法的代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">t=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">p=np.array([<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">m=<span class=\"number\">0</span>   <span class=\"comment\"># 总收益</span></span><br><span class=\"line\">n=t.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 最多决策数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">  idx=np.where(t&gt;<span class=\"number\">0</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> idx.shape[<span class=\"number\">0</span>]==<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span>   <span class=\"comment\"># 全部超出最后期限，决策结束</span></span><br><span class=\"line\">  c=np.max(p[idx])  <span class=\"comment\"># 尚未超出期限的进程的最大收益，当前最优决策的收益（贪心策略）</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> c &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br><span class=\"line\">  m+=c</span><br><span class=\"line\">  idx=np.argmax(p[idx])+idx[<span class=\"number\">0</span>]  <span class=\"comment\"># 当前决策的进程的编号</span></span><br><span class=\"line\">  p[idx]=<span class=\"number\">-1e8</span>                   <span class=\"comment\"># 标记收益为负，表示后续决策不再考虑此进程</span></span><br><span class=\"line\">  t-=<span class=\"number\">1</span>                          <span class=\"comment\"># 更新进程的最后期限</span></span><br><span class=\"line\">print(<span class=\"string\">'最大收益为'</span>, sep=<span class=\"string\">' '</span>)</span><br><span class=\"line\">print(m)    <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<p>使用 DP 解决这个问题，定义状态为 $(k,S)$，其中 k 为阶段编号，S 为所有尚未执行的进程集合，每一步的决策 d 是从 S 中选出，决策后的下一状态为 $(k+1,S-{d})$，根据最后期限从近到远对进程进行排序，DPFE 为<br>$$f(k,S)=\\max_{d \\in S}{c(d|S)+f(k+1,S-{d})}$$<br>其中，当决策 d 的最后期限大于等于阶段 k 时（k 从 1 开始计算）$c(d|S)=w_d$ ，否则 $c(d|S)=0$。目标是求 $f(1,S^{\\ast})$，基本条件为 $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$，其中 n 为进程数。使用上述例子，代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">S=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">n=len(t)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">profit</span><span class=\"params\">(k,d)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> p[d] <span class=\"keyword\">if</span> t[d]&gt;=k <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deadline</span><span class=\"params\">(k,S)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span> <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> k==n+<span class=\"number\">1</span> <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    max([profit(k,S[i])+deadline(k+<span class=\"number\">1</span>,S[:i]+S[i+<span class=\"number\">1</span>:]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(S))])</span><br><span class=\"line\"></span><br><span class=\"line\">print(deadline(<span class=\"number\">1</span>, S))   <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"折现利润问题-DPP\"><a href=\"#折现利润问题-DPP\" class=\"headerlink\" title=\"折现利润问题 DPP\"></a>折现利润问题 DPP</h2><p>假设一个湖中第一年开始时有 $b_1$ 条鱼，第 t 年开始时的鱼数为 $b_t$，在第 t 年中售出 $x_t$ 条鱼，收益为 $r(x_t)$，捕鱼成本为 $c(x_t,b_t)$，鱼可以再生，再生长率为 s，表示某一年年初的鱼数量是上一年年底鱼数量的 s 倍，计划展望期为 $1,…,T$，即最多 T 年卖完所有鱼，其中货币年贬值率为 y，决策为 $x_t$ 表示 第 t 年卖出多少鱼，要求最大净收益，状态为 $(t,b)$，其中 t 表示年份，b 表示第 t 年初的鱼数，DPFE 为<br>$$f(t,b)=\\begin{cases} \\max_{x_t \\in {0,…,b}} {r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)} &amp; t \\le T<br>\\\\ 0 &amp; t=T+1 \\end{cases}$$</p>\n<h2 id=\"编辑长度问题-EDP\"><a href=\"#编辑长度问题-EDP\" class=\"headerlink\" title=\"编辑长度问题 EDP\"></a>编辑长度问题 EDP</h2><p>编辑长度问题通常用于字符串的非精确匹配问题。记 $\\Sigma$ 为一有限字符集，给定两个字符串 $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$，或者写为 $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$，任务是将 x 转变为 y，并且只能使用如下三种编辑操作：</p>\n<ul>\n<li>删除操作 D。从某字符串中删除一个字符，损失为 $c(D)$</li>\n<li>插入操作 I。向某字符串中插入一个字符，损失为 $c(I)$</li>\n<li>替换操作 R。将某字符串中某个字符替换为另一个字符，损失为 $c(R)$，如果替换前后字符相同，则 $c(R)=0$</li>\n</ul>\n<p>要求一个编辑序列，使得 x 转变为 y，且具有最小损失，这里损失为所有操作的损失之和。DPFE 为<br>$$f(i,j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min {f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)} &amp; i&gt;0,j&gt;0 \\end{cases}$$<br>其中 $f(i,j)$ 表示从 $X_i$ 到 $Y_j$ 需要的操作损失，$X_i$ 为 x 的前 i 个字符形成的子串，$Y_j$ 为 y 的前 j 个字符形成的子串，当 i=0 时，向 $X_i$ 插入 j 个字符得到 $Y_j$，当 j=0 时，将 $X_i$ 删除 i 个字符得到 $Y_j$，当 i&gt;0 且 j&gt;0 时，从 $X_i$ 变换到 $Y_j$，总共可以有以下三种决策/操作，取损失最小的决策：</p>\n<ul>\n<li>删除 $X_i$ 的第 i 个字符，然后从 $X_{i-1}$ 变换到 $Y_j$，两部分损失分别为 $c(D)$ 和 $f(i-1,j)$</li>\n<li>从 $X_i$ 变换到 $Y_{j-1}$，然后再在末尾插入一个字符得到 $Y_j$，两部分的操作损失分别为 $f(i,j-1)$ 和 $c(I)$</li>\n<li>两个子串的最后一个字符，使用替换，$x_i \\rightarrow y_j$，然后再从 $X_{i-1}$ 变换到 $Y_{j-1}$ 即可得到 $X_i$ 变换到 $Y_j$ 的过程，两部分的损失分别为 $f(i-1,j-1)$ 和 $c(R)$</li>\n</ul>\n<p>还可以写成如下形式的 DPFE<br>$$f(X_i,Y_j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min_{d \\in {D,I,R}} {f(t(X_i,Y_j,d))+c(d)} &amp; i&gt;0,j&gt;0<br>\\end{cases}$$<br>其中变换定义为<br>$$t(X_i,Y_j,D)=(X_{i-1},Y_j)<br>\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})<br>\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$</p>"},{"title":"Dynamic Programming (1)","date":"2019-08-07T09:29:56.000Z","p":"dp/DP1","mathjax":true,"_content":"我计划开启一系列动态规划的方法介绍，主要参考 《Dynamic Programming · A Computational Tool》这本书。\n\n# 简介\n\n动态规划用于解决一类优化问题，顺序地做出决策，每一次决策使得问题转变为对一个子问题的优化，直到问题解决，这个决策序列就是原始问题的最优解。我们也可以将 问题/子问题 看作状态，决策就是状态间的转移，问题得到解决就对应着`结束状态`。\n<!-- more -->\n\n## 优化原理\n\n> 一个最优决策满足：无论问题初始状态和初始决策是什么，剩余的决策序列构成做出初始决策之后问题状态的最优解。\n\n以最短路径问题为例说明：在一个指定了起点和终点的有权路径图中，无论之前选择了什么路径，我们必须保证从当前节点起，剩余的路径选择必须是最优的。~~这是一种递归的处理方式，要解这个问题，直觉告诉我们似乎可以采用逆推法（隐马尔可夫预测问题的求解），这个直觉很重要，因为下文将会用到它。~~\n\n### 顺序决策过程\n考虑具有如下形式的优化问题：$opt_{d \\in \\Delta} \\{H(d)\\}$，其中 d 为决策，决策空间为 $\\Delta$，H 为目标函数，H(d) 的最优解记为 $d^{\\ast}$：$d^{\\ast}=\\arg opt_d \\{H(d)\\}$。动态规划问题则要求寻找有序决策集 $\\{d_1,...,d_n\\}$，使得目标函数 $h(d_1,...,h_n)$ 取得最优 $H^{\\ast}$。\n\n我们可以枚举 $\\{d_1,...,d_n\\}$ 所有可能的取值，然后代入目标函数进行计算，这就是 “暴力” 解法，但是这只在决策空间较小时有效，在决策空间很大时，这种方法效率非常低不可取，所以此时我们需要按顺序做出决策 $d_1,...,d_n$，使得\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2}\\{...\\{opt_{d_n \\in D_n}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.1)\\end{aligned}$$\n\n其中 $(d_1,...,d_n) \\in \\Delta=D_1 \\times ... \\times D_n$。通常，第 i 个决策空间依赖于前面所有的决策：$d_i \\in D_i(d_1,...,d_{i-1})$，于是式 (1.1) 可改写为\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.2)\\end{aligned}$$\n\n式 (1.2) 的优化操作是一个嵌套结构，可以由内向外解决问题，解最内层的优化问题，此时前序所有决策均看作已知，可得到最优 $d_n$，记作 $d_n^{\\ast}(d_1,...,d_{n-1})$，可以将 $d_n^{\\ast}$ 看作是其前序决策的函数。向外逆推，直到解出最外层优化问题 $opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast},...,d_n^{\\ast} \\}$ 的解 $d_1^{\\ast}$。\n\n如果改变决策顺序目标函数的最优解相同，如\n$$\\begin{aligned} &opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)} \\{... \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})} \\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_n \\in D_n} \\{opt_{d_{n-1} \\in D_{n-1}(d_n)} \\{... \\{opt_{d_1 \\in D_n(d_2,...,d_n)} \\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.3) \\end{aligned}$$\n\n那么决策空间 $D_i$ 可能会跟之前有所不同，因为此时 $D_i$ 依赖于 $(d_{i+1},...,d_n)$，所以问题求解的效率会随着决策顺序的变化而有所改变。\n\n回到前面式 (1.2)，假设我们暂时做出最外层决策 $d_1$，此时 $d_1$ 是否是最优决策还尚未可知，但是根据前面所说的优化原理，无论之前做出什么决策，之后的决策需要保证是最优的，所以有\n$$\\begin{aligned}H^{\\ast}&=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1}\\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1)\\} \\qquad \\qquad(1.4) \\end{aligned}$$\n\n其中 $d_i^{\\ast}(d_1), \\ i>1$ 可以看作是输入参数 $d_1$ 的偏函数（partial function）。最外层决策的最优解则为 $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1))\\}$，可以看出，$d_1$ 与其后序的决策互相耦合，要解这样的问题还是有点棘手。\n\n#### 目标函数可分\n一种方法是将 $d_1$ 决策与 $d_2,...,d_n$ 决策独立开来，也就是说通过解决形如 $opt_{d_1 \\in D_1}\\{H'(d_1)\\}$ 问题的最优解以得到 $d_1$ 的最佳决策，这是一种贪心算法，这种算法在局部最优 $opt_{d_1}\\{H'(d_1)\\}$ 与全局最优 $H^{\\ast}$ 一致的情况下是有效的。我们先分析这种特殊情况，因为它足够简单：假设目标函数 h 强可分，即\n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ ... \\circ C_n(d_n) \\qquad (1.5)$$\n其中 $C_i$ 是决策 $d_i$ 关联的决策损失函数，$\\circ$ 是某种关联二元操作（例如 加 或 乘）并具有属性 \n$$opt_d\\{a \\circ C(d)\\}=a \\circ opt_d\\{C(d)\\}$$\n其中 a 不依赖于决策 d。在序列决策过程中，损失 $C_n$ 不仅依赖于 $d_n$，还依赖于当前问题所处的状态 $(d_1,d_2,...,d_{n-1})$，所以改写上式为\n$$h(d_1,...,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\qquad(1.6)$$\n\n定义如果目标函数 h 满足\n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n) \\qquad(1.7)$$\n那么称其弱可分（强可分是弱可分的一种特殊情况），此时有\n$$\\begin{aligned} & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}\\{C_2(d_1,d_2) \\circ ... \\circ \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_n(d_1,...,d_n)\\}...\\}\\} \\qquad(1.8) \\end{aligned}$$\n最后一个等式根据 $\\circ$ 操作的属性推导。\n\n令 $f(d_1,...,d_n)$ 表示序列决策过程在已经做出决策 $d_1,...,d_{i-1}$ 时的最优解，即\n$$f(d_1,...,d_{i-1})=opt_{d_i}\\{opt_{d_{i+1}}\\{... \\{opt_{d_n} \\{C_i(d_i|d_1,...,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,...,d_i) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\}\\}...\\}\\} \\ (1.9)$$\n其中因为篇幅起见省略了每个决策的取值空间 $D_i$。现在序列决策过程可表示为\n$$\\begin{aligned}f(\\emptyset)&=opt_{d_1}\\{opt_{d_2}\\{...\\{opt_{d_n}\\{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1})\\}\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ opt_{d_2}\\{C(d_2|d_1) \\circ...\\circ opt_{d_n}\\{C_n(d_n|d_1,...,d_{n-1})\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ f(d_1)\\}  \\qquad \\quad (1.10) \\end{aligned}$$\n一般地，我们有\n$$f(d_1,...,d_{i-1})=opt_{d_i \\in D_i(d_1,...,d_{i-1})}\\{C_i(d_i|d_1,...,d_{i-1})\\circ f(d_1,...,d_i)\\} \\qquad(1.11)$$\n\n于是我们得到问题的递归函数形式，这就是优化问题的动态规划函数方程（DPFE）。\n\n### DPFE\n求解 DPFE 中的 $f(d_1,...,d_{i-1})$，定义状态 $S=(d_1,...,d_{i-1})$，那么 $i=|S|+1=|\\{d_1,...,d_{i-1}\\}|+1$，于是可以改写 DPFE 为以下形式，\n$$f(S)=opt_{d_i \\in D_i(S)}\\{C_i(d_i|S) \\circ f(S')\\} \\qquad (1.12)$$\n其中 $S'=(d_1,...,d_i)$ 是下一状态，$\\emptyset$ 是初始状态。记状态空间为 $\\mathcal S$，由于 DPFE 是递归的，要终止递归，则要求具备一些基本情况（或称边界条件），例如 $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$，对于某个基本（终止）态 $S_0$，$f(S_0)$ 不使用 DPFE 计算其值，而是有一个给定的常数值 b，这就表示到达基本态时，递归结束。\n\n值得注意的是决策序列的长度 n 并非固定，当决策使得目标到达基本态时结束决策过程。前面我们定义状态 S 表示已经做过的决策，下一决策 d 则从 D(S) 中进行选择，但实际上为了表示方便，直接定义状态 S 为下一决策 d 的可选决策空间，即 $d \\in S$，于是 DPFE 变为\n$$f(S)=opt_{d \\in S} \\{C(d|S) \\circ f(S')\\} \\qquad (1.13)$$\n\n我们可以使用状态转移系统或者有向图对简单序列决策过程进行建模，状态 S 为节点，决策 d 使得状态从 S 转移到 S'，D(S) 表示处于状态 S 时的决策空间。考虑使用有向图建模，节点表示 DPFE 的状态，边表示状态间的转移，转移对应着决策，边标记为 b(S,S')，表示决策 d 的损失 C(d|S)，其中下一状态 $S'=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$，T 是转移函数，于是 DPFE 转变为\n$$f(S)=opt_S\\{b(S,S') \\circ f(S')\\} \\qquad(1.14) $$\n\nDPFE 的反转形式为\n$$f'(S)=opt_{S'}\\{f'(S') \\circ b(S',S)\\} \\qquad(1.15) $$\n其中 f'(S) 表示从基本态 $S_0$ 到状态 S 的最优解，注意与前面 f(S) 表示从状态 S 到基本态 $S_0$ 的最优解区分开来。式 (1.14) 为后向形式（backward），式 (1.15) 为前向形式（forward）。\n\n### 动态规划的基本要素\n动态规划的基本形式为\n$$f(S)=opt_{d \\in D(S)} \\{R(S,d) \\circ f(T(S,d))\\}  \\quad (1.16)$$\n其中，S 表示状态空间 $\\mathcal S$ 中的某个状态，d 是决策空间 D(S) 中的某个决策，R(S,d) 是收益函数（或称损失函数，记为 C(d|S)，收益对应最大化，损失对应最小化），T(S,d) 是转移函数，$\\circ$ 是二元操作符。为简单起见，我们只考虑离散情况。\n\n### 线性搜索\n现在我们来看一个实际例子。问题是需要排列一个长度为 N 的数组 A 中的元素，元素 x 具有概率 $p_x$，通过最小化排列的损失来优化线性搜索过程，例如 A={a,b,c}，且 $p_a=0.2,p_b=0.5,p_c=0.3$，于是共有 6 中排列方式 abc,acb,bac,bca,cab,cba（注意：每个排列均代表一种决策序列），其中 bca 排列的损失为 1.7，计算如下：\n1. Strong separable，方法 S  \n   $1p_b+2p_c+3p_a$\n2. Weak separable，方法 W  \n   $(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$\n\n最优排列问题可看作是序列决策过程，每个决策用于决定将 A 的元素置于排列后 A' 的哪个位置上。决策的损失可互相独立（强可分），即方法 S 损失定义为：元素 x 的损失为 $ip_x$，其中 i 为 x 在 A' 中位置；对于 W 方法，决策的损失依赖于决策的顺序，或者说依赖于决策空间，如果以从 A' 的开始到最后这样的顺序进行决策，还以 bca 排列为例说明，第一次决策的空间为 {a,b,c}，第一次决策选择 b 置于 A' 第一个位置，然后第二次决策空间为 {a,c}... 依次类推，决策损失与决策空间相关，定义为决策空间中各元素概率之和 $\\sum_{x\\in D_i} p_x$。\n\n假设决策顺序为 i=1,2,3，那么对应的决策空间为 $D_1=A,D_2=A-\\{d_1\\},D_3=A-\\{d_1,d_2\\}$，方法 S 的目标函数为 $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$，于是有\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{1p_{d_1}+2p_{d_2}+3p_{d_3}\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{1p_{d_1}+\\min_{d_2 \\in A-\\{d_1\\}}\\{2p_{d_2}+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{3p_{d_3}\\}\\}\\} \\end{aligned}$$\n\n方法 W 的目标函数为 $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x$，于是有\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-\\{d_1\\}}\\{\\sum_{x\\in A-\\{d_1\\}}p_x+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\} \\end{aligned}$$\n\n有了损失的计算方法之后，__线性搜索__ 就是依次计算并比较所有排列的损失，具有最小损失的就是最优排列。经过计算发现，bca 就是最佳排列。\n\n### 问题的表示和解\n上一小节中的例子可以使用 DP 求解。定义状态 S 为元素的集合，可从中选择决策来确定哪个元素应该放置在 A' 中的哪个位置。DPFE 的形式如下\n$$f(S)=\\min_{x \\in S} \\{C(x|S)+f(S-\\{x\\})\\}     \\quad(1.17)$$\n其中 $S \\in 2^A$，$2^A$ 是 A 的幂集。基本态 $f(\\emptyset)=0$，我们目标是要求出 $f(A)$。\n\n注意这是前向形式的 DPFE。要写成这是后向形式的 DPFE，则根据式 (1.15) 改写如下\n$$f(S)=\\min_{S'} \\{C(x|S')+f(S')\\}     \\quad(1.18)$$\n其中 $S \\in 2^A$，此时我们目标是求 $f(\\emptyset)$，基本态 $f(A)=0$。S' 是 S 的前导状态，从前导状态 S' 经过决策 x 到达状态 S。\n\n基于方法 W，损失函数为\n$$C_W(x|S)=\\sum_{y\\in S}p_y$$\n基于上一小节的讨论可知，当前的损失与当前决策 x 无关，而依赖于临决策之前的状态 S 有关，即 S 中各元素的概率和。\n\n基于方法 S，那么损失函数为\n$$C_S(x|S)=(N+1-|S|)p_x$$\n可见此时损失只与当前决策 x 以及决策的序号有关，其中与序号有关是假定了第一个决策确定某元素置于 A' 的第一个位置，第二个决策确定 A' 第二个位置上的元素...依次类推。如果决策顺序反过来，即第一个决策确定 A' 最后一个位置上的元素，那么损失函数需要修改为 $C_S'(x|S)=|S|p_x$。如果将 S 中元素按概率降序排列，事实表明第一个元素（具有最大概率值）将会最小化 $C(x|S)+f(S-\\{x\\})$，这是一种贪心策略，即只求当前状态下的最优解。事实上确实存在一类 DP 问题用贪心策略也可解决，这个我们暂时不讨论。\n\n现在我们来看下如何解 DP 问题，以式 (1.17) 表示的 DPFE 为例进行说明：\n$$\\begin{aligned} f(\\{a,b,c\\}) &= \\min\\{C(a|\\{a,b,c\\})+f(\\{b,c\\}), C(b|\\{a,b,c\\})+f(\\{a,c\\}), C(c|\\{a,b,c\\})+f(\\{a,b\\})\\}\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(b|\\{b,c\\}+f(\\{c\\}),C(c|\\{b,c\\}+f(\\{b\\})\\}\n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(a|\\{a,c\\}+f(\\{c\\}),C(c|\\{a,c\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(a|\\{a,b\\}+f(\\{b\\}),C(c|\\{a,b\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(c|\\{c\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(b|\\{b\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(a|\\{a\\})+f(\\emptyset)\\} \n\\\\\\\\ f(\\emptyset) &= 0 \\end{aligned}$$\n其中损失函数可以分别使用方法 S 和方法 W 进行代入计算，这里略。\n\n再以式 (1.18) 表示的 DPFE 进行说明：\n$$\\begin{aligned} f(\\{a,b,c\\}) &= 0\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(a|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0 \n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(b|\\{a,a,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(c|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(a|\\{a,c\\})+f(\\{a,c\\}), C(b|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.5+1.0,0.8+1.0\\}=1.5\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(a|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.8+1.0\\}=1.7\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(b|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{a,c\\})+f(\\{a,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.5+1.0\\}=1.5\n\\\\\\\\ f(\\emptyset) &= \\min \\{C(a|a)+f(\\{a\\}), C(b|b)+f(\\{b\\}), C(c|c)+f(\\{c\\})\\}\\stackrel W = \\min \\{0.2+1.5,0.5+1.7,0.3+1.5\\}=1.7 \\end{aligned}$$\n以上式中 $\\stackrel W=$ 之后部分均表示使用方法 W 进行计算，这是为了演示，方法 S 的代入计算略。\n\n### 带阶决策\n第一次决策记为阶 1，第二次决策记为阶 2... 依次类推。将阶号包含进状态 S 的定义中有时候会带来方便甚至是非常有必要的。还以前面的例子进行说明，DPFE 形式 (1.17) 可改写为\n$$f(k,S)=\\min_{x \\in S} \\{C(x|k,S)+f(k+1,S-\\{x\\})\\} \\quad(1.19)$$\n\n### Path-States\n状态 S 可以定义为当前已经做过的决策 $(d_1,...,d_{i-1})$，在有向图中用节点表示状态，状态 S 与初始态 $\\emptyset$ 到状态 S 之间的路径有关联（其实这个路径也可以表示为 $(d_1,...,d_{i-1})$），需要特别注意的是，前面的状态 S 是无序的，比如 S={a,b}，表示当前已经做出了决策 a 和 b（或者说做出决策选择了 a 和 b，读者根据具体语境进行调整理解），至于决策 a 和 b 的顺序则未定义，而这里 Path-States 中的状态是有序的，S=(a,b)，表示当前已经做出两个决策，第一个决策是 a 且第二个决策是 b。于是此时 DPFE 的形式可写为\n$$f(S)=\\min_{x \\notin S} \\{C(x|S)+f(S + (x))\\} \\qquad(1.20)$$\n\n前面线性搜索一节中的例子计算过程为\n$$\\begin{aligned} f(\\emptyset) &= \\min \\{C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)\\}\\stackrel S= \\min\\{0.2+1.9,0.5+1.2,0.3+1.6\\}=1.7\n\\\\\\\\ f(a) &= \\min \\{C(b|a)+f(ab), C(c|a)+f(ac)\\}\\stackrel S= \\min\\{2*0.5+0.9,2*0.3+1.5\\}=1.9\n\\\\\\\\ f(b) &= \\min \\{C(a|b)+f(ba), C(c|b)+f(bc)\\}\\stackrel S= \\min\\{2*0.2+0.9,2*0.3+0.6\\}=1.2\n\\\\\\\\ f(c) &= \\min \\{C(a|c)+f(ca), C(b|c)+f(cb)\\}\\stackrel S= \\min\\{2*0.2+1.5,2*0.5+0.6\\}=1.6\n\\\\\\\\ f(ab) &= \\min \\{C(c|ab)+f(abc)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(ac) &= \\min \\{C(b|ac)+f(acb)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(ba) &= \\min \\{C(c|ba)+f(bac)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(bc) &= \\min \\{C(a|bc)+f(bca)\\}\\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(ca) &= \\min \\{C(b|ca)+f(cab)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(cb) &= \\min \\{C(a|cb)+f(cba)\\} \\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(abc) &= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$\n由于状态是有序的，所以共有 $N!$ 个基本态，损失函数依然可以使用方法 S 和方法 W 计算，例如使用方法 S，损失 $C(c|ab)$ 表示第三次决策使用 c，那么根据方法 S 的定义有 $C(c|ab)=3p_c=3*0.3=0.9$。上面计算中 $\\stackrel S=$ 之后部分均表示使用方法 S 进行计算。方法 W 的计算略。\n\n### 松弛 Relaxation\n松弛在数学中指通过某种迭代方法逐步得到更好的近似解。考虑一个有限集合 $\\{a_1,...,a_N\\}$，其最小值可以通过计算结对元素最小化来求解\n$$x^{\\ast}=\\min\\{\\min\\{...\\{\\min\\{a_1,a_2\\},a_3\\},...\\},a_N\\}$$\n偏最小化序列为 $x_1=a_1, x_2=\\min\\{x_1,a_2\\},...$，为了表示的统一，可以令 $x_1=\\min\\{x_0,a_1\\}, x_0=\\infty$。序列 $x_1,x_2,...$ 逐步逼近并最终达到 $x^{\\ast}$。松弛就是刻画这种逐步逼近的特性。借助松弛的思想，动态规划问题的 DPFE 可表示为\n$$\\begin{aligned} f(S)&=\\min_{x \\in S} \\{C(x|S)+f(S_x')\\}\n\\\\\\\\ &=\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}'),...,C(x_m|S)+f(S_{x_m}')\\} \\end{aligned}$$\n其中 $S=\\{x_1,x_2,...,x_m\\}$，$S_x'$ 是选择 x 之后的下一状态，与其计算所有的 $C(x|S)+f(S_x')$ 值，我们不如逐步逼近最终值\n$$f(S)=\\min\\{\\min\\{...\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}')\\},...\\},C(x_m|S)+f(S_{x_m}')\\} \\quad (1.21)$$\n注意 $C(x_i|S)$ 中状态全部是 S，并且所有的 $f(S_{x_i}')$ 需要提前全部计算出来。\n\n带阶形式的 DPFE 为\n$$f(k,S)=\\min_x \\{C(x|k,S) + f(k-1,S_x')\\} \\qquad(1.22)$$\n其中 k 表示阶段，这里 k 可能不太好理解，我们可以想象从状态 S 经过恰好 k 次变换到达终止态 T，下文最短路径问题中会借鉴式 (1.22)，彼时再回过头来理解可能更加容易些。序列 $f(0,S),f(1,S),...$ 近似于 $f(S)$，序列最小值为 $f(S)$，但这个序列不保证单调性（比如振荡逼近），所以 $f(S)=\\min_k \\{f(k,S)\\}$，注意 $f(k,S)$ 不是 $f(k-1,S)$ 的函数，而是 $f(k-1,S_x')$ 的函数。既然 $f(k,S)$ 序列不一定单调，我们定义一个新的序列 $F(k,S)$ 来保证单调性，\n$$F(k,S)=\\min\\{F(k-1,S), \\min_x \\{C(x|k,S)+F(k-1,S_x')\\}\\} \\quad(1.23)$$\n这里我们可以将 $F(k,S)$ 理解为从状态 S 到终止态 T 最多变换 k 次的损失，与式 (1.22) 中恰好变换 k 次是有区别的。 \n\n### 最短路径问题\n在解决前述的线性搜索问题中，我们使用了状态转换图模型，不难发现解决这种线性搜索问题等价于在图中搜索最短路径。\n\n考虑到有环图的复杂性，我们先讨论无环图。对于一个无环图，从起点 s 到终点 t 的最短路径使用 DPFE 可表示为\n$$f(p)=\\min_q \\{b(p,q)+f(q)\\} \\qquad(1.24)$$\n其中 b(p,q) 表示从 p 到 q 的距离，q 是 p 的直接邻点，f(p) 表示从 p 到 t 的最短路径，基本态条件为 f(t)=0。如果 q 不是 p 的直接邻点，那么可以认为 $b(p,q)=\\infty$。无环图中，要计算 f(p) 则需要先计算 f(q)。采用自底向上的方式计算，具体不展开。\n\n在有环图中，p 和 q 可能互为后继节点，f(p) 和 f(q) 互相依赖。为了方便，我们假定有环图中没有自环，这样假设是有原因的，如果有自环，即一条从 p 到 p 的分支，考虑以下三种情况：\n1. b(p,p) > 0，这条分支会被忽略，因为会增加距离\n2. b(p,p) < 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，从而一直缩短距离\n3. b(p,p) = 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，却不改变距离\n\n所以可以忽略掉自环，以下所讨论的有环图中均没有自环。对于一个有环图，DPFE 为\n$$f(p) = \\min_q \\{b(p,q)+f(q)\\} \\qquad(1.25)$$\n其中 f(q) 可能依赖于 f(p)，做特殊处理：初始时令 $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$。式 (1.25) 与 (1.24) 一样，因为都是解最短路径模型，只是有环图中进行求解的时候需要做特殊处理。\n\n举例说明，如图 1.2，\n![](/images/DP1_fig1.png)\n\n根据式 (1.25) 计算过程如下\n$$\\begin{aligned}f(s)&=\\min \\{b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)\\}=\\min \\{3+f(x),5+f(y),\\infty+f(t)\\}\n\\\\\\\\f(x)&=\\min \\{b(x,y)+f(y),b(x,t)+f(t)\\}=\\min \\{1+f(y),8+f(t)\\}\n\\\\\\\\f(y)&=\\min \\{b(y,x)+f(x),b(y,t)+f(t)\\}=\\min \\{2+f(x),5+f(t)\\}\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n显然 f(x) 与 f(y) 互相依赖。\n\n初始化时假设 $f(s)=f(x)=f(y)=\\infty$，第一次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+\\infty,5+\\infty,\\infty+f(t)\\}=\\infty\n\\\\\\\\f(x)&=\\min \\{1+\\infty,8+0\\}=8\n\\\\\\\\f(y)&=\\min \\{2+\\infty,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n第二次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+8,5+5,\\infty+0\\}=10\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+8,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n第三次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+6,5+5,\\infty+0\\}=9\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+6,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n由于第三次迭代 $f(x),f(y),f(t)$ 均未改变，故第三次迭代后计算到的 f(s) 就是最终 f(s)。\n\n还可以利用带阶 Relaxation 解决有环图问题，仿照式 (1.22)，我们针对最短路径模型改为 DPFE 为\n$$f(k,p)=\\min_q \\{b(p,q)+f(k-1,q)\\} \\qquad(1.26)$$\n其中 f(k,p) 表示从 p 到 t 的最短距离，k 表示 p 到 t 的某个路径需要走恰好 k 步。于是基本条件为： $f(0,t)=0;f(k,t)=\\infty, k>0;f(0,p)=\\infty, \\forall p \\ne t$，这表示 t 到 t 走 0 步，代价为 0，走大于 0 步，代价为 $\\infty$，因为既然到了 t 点，我们不希望再继续走下去。p 到 t 走 0 步，代价为 $\\infty$，这驱使我们从 p 点走出去。还以上面的例子说明，根据式 (1.26) 计算过程为\n$$\\begin{aligned}f(k,s)&=\\min \\{b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)\\}\n\\\\\\\\f(k,x)&=\\min \\{b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)\\}\n\\\\\\\\f(k,y)&=\\min \\{b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)\\}\n\\\\\\\\f(k,t) \\end{aligned}$$\n根据基本条件，也就是 k=0 的初始条件，第一次迭代，\n$$\\begin{aligned}f(1,s)&=\\min \\{3+f(0,x),5+f(0,y),\\infty+f(0,t)\\}=\\infty\n\\\\\\\\f(1,x)&=\\min \\{1+f(0,y),8+f(0,t)\\}=8\n\\\\\\\\f(1,y)&=\\min \\{2+f(0,x),5+f(0,t)\\}=5\n\\\\\\\\f(1,t)&=\\infty \\end{aligned}$$\n第二次迭代，\n$$\\begin{aligned}f(2,s)&=\\min \\{3+f(1,x),5+f(1,y),\\infty+f(1,t)\\}=10\n\\\\\\\\f(2,x)&=\\min \\{1+f(1,y),8+f(1,t)\\}=6\n\\\\\\\\f(2,y)&=\\min \\{2+f(1,x),5+f(1,t)\\}=10\n\\\\\\\\f(2,t)&=\\infty \\end{aligned}$$\n第三次迭代，\n$$\\begin{aligned}f(3,s)&=\\min \\{3+f(2,x),5+f(2,y),\\infty+f(2,t)\\}=9\n\\\\\\\\f(3,x)&=\\min \\{1+f(2,y),8+f(2,t)\\}=11\n\\\\\\\\f(3,y)&=\\min \\{2+f(1,x),5+f(2,t)\\}=8\n\\\\\\\\f(3,t)&=\\infty \\end{aligned}$$\n假设图中总共有 N 个节点，那么从任意一点 p 到 t 可以走恰好 k 步，k 取值为 {0,1,...,N-1}，k 不能大于等于 N，否则就存在某个节点经过两次，从而路径中存在环 circle，在任意两点路径均大于 0 的情况下，显然有环的路径不可能是最短路径。这里的例子中，N=4，所以 k 最大为 3，经过三次迭代后，就没必要再迭代下去了，否则路径中存在环，迭代下去的 f 值只会越来越大。于是根据 $f(p)=\\min_k \\{f(k,p)\\}$ 得\n$$\\begin{aligned}f(s)&=\\min \\{\\infty,\\infty,10,9\\}=9\n\\\\\\\\f(x)&=\\min \\{\\infty,8,6,11\\}=6\n\\\\\\\\f(y)&=\\min \\{\\infty,5,10,8\\}=5\n\\\\\\\\f(t)&=\\min \\{0,\\infty,\\infty,\\infty\\}=0 \\end{aligned}$$\n从以上求解过程不难发现，迭代计算结果序列并不收敛，这也说明了式 (1.22) 中 $f(k,S)$ 序列不单调性。\n\n既然 $f(k,S)$ 序列不单调，我们参考式 (1.23) 为最短路径问题改写合适的 DPFE，如下\n$$F(k,p)=\\min \\{F(k-1,p), \\ \\min_q \\{b(p,q)+F(k-1,q)\\}\\} \\qquad(1.27)$$\n\n其中 $F(k,p)$ 表示从 p 到 t 最多走 k 步的最短路径。显然要获得全局最短路径，我们必须考虑从 p 到 t 最多走 $N-1$ 步的最短路径，即目标是计算 $F(N-1,s)$，k 最大为 $N-1$，超过则路径出现环。基本条件不难得知为 $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$，这个不用过多解释了，相信现在大家都能理解。\n\n\n还有其他形式的 DPFE，由于篇幅有限就不一一介绍，待后面分析具体例子的时候再穿插说明。","source":"_posts/dp/DP1.md","raw":"---\ntitle: Dynamic Programming (1)\ndate: 2019-08-07 17:29:56\np: dp/DP1\ntags:\n    - math\n    - DP\ncategory: math\nmathjax: true\n---\n我计划开启一系列动态规划的方法介绍，主要参考 《Dynamic Programming · A Computational Tool》这本书。\n\n# 简介\n\n动态规划用于解决一类优化问题，顺序地做出决策，每一次决策使得问题转变为对一个子问题的优化，直到问题解决，这个决策序列就是原始问题的最优解。我们也可以将 问题/子问题 看作状态，决策就是状态间的转移，问题得到解决就对应着`结束状态`。\n<!-- more -->\n\n## 优化原理\n\n> 一个最优决策满足：无论问题初始状态和初始决策是什么，剩余的决策序列构成做出初始决策之后问题状态的最优解。\n\n以最短路径问题为例说明：在一个指定了起点和终点的有权路径图中，无论之前选择了什么路径，我们必须保证从当前节点起，剩余的路径选择必须是最优的。~~这是一种递归的处理方式，要解这个问题，直觉告诉我们似乎可以采用逆推法（隐马尔可夫预测问题的求解），这个直觉很重要，因为下文将会用到它。~~\n\n### 顺序决策过程\n考虑具有如下形式的优化问题：$opt_{d \\in \\Delta} \\{H(d)\\}$，其中 d 为决策，决策空间为 $\\Delta$，H 为目标函数，H(d) 的最优解记为 $d^{\\ast}$：$d^{\\ast}=\\arg opt_d \\{H(d)\\}$。动态规划问题则要求寻找有序决策集 $\\{d_1,...,d_n\\}$，使得目标函数 $h(d_1,...,h_n)$ 取得最优 $H^{\\ast}$。\n\n我们可以枚举 $\\{d_1,...,d_n\\}$ 所有可能的取值，然后代入目标函数进行计算，这就是 “暴力” 解法，但是这只在决策空间较小时有效，在决策空间很大时，这种方法效率非常低不可取，所以此时我们需要按顺序做出决策 $d_1,...,d_n$，使得\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2}\\{...\\{opt_{d_n \\in D_n}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.1)\\end{aligned}$$\n\n其中 $(d_1,...,d_n) \\in \\Delta=D_1 \\times ... \\times D_n$。通常，第 i 个决策空间依赖于前面所有的决策：$d_i \\in D_i(d_1,...,d_{i-1})$，于是式 (1.1) 可改写为\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.2)\\end{aligned}$$\n\n式 (1.2) 的优化操作是一个嵌套结构，可以由内向外解决问题，解最内层的优化问题，此时前序所有决策均看作已知，可得到最优 $d_n$，记作 $d_n^{\\ast}(d_1,...,d_{n-1})$，可以将 $d_n^{\\ast}$ 看作是其前序决策的函数。向外逆推，直到解出最外层优化问题 $opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast},...,d_n^{\\ast} \\}$ 的解 $d_1^{\\ast}$。\n\n如果改变决策顺序目标函数的最优解相同，如\n$$\\begin{aligned} &opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)} \\{... \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})} \\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_n \\in D_n} \\{opt_{d_{n-1} \\in D_{n-1}(d_n)} \\{... \\{opt_{d_1 \\in D_n(d_2,...,d_n)} \\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.3) \\end{aligned}$$\n\n那么决策空间 $D_i$ 可能会跟之前有所不同，因为此时 $D_i$ 依赖于 $(d_{i+1},...,d_n)$，所以问题求解的效率会随着决策顺序的变化而有所改变。\n\n回到前面式 (1.2)，假设我们暂时做出最外层决策 $d_1$，此时 $d_1$ 是否是最优决策还尚未可知，但是根据前面所说的优化原理，无论之前做出什么决策，之后的决策需要保证是最优的，所以有\n$$\\begin{aligned}H^{\\ast}&=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1}\\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1)\\} \\qquad \\qquad(1.4) \\end{aligned}$$\n\n其中 $d_i^{\\ast}(d_1), \\ i>1$ 可以看作是输入参数 $d_1$ 的偏函数（partial function）。最外层决策的最优解则为 $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1))\\}$，可以看出，$d_1$ 与其后序的决策互相耦合，要解这样的问题还是有点棘手。\n\n#### 目标函数可分\n一种方法是将 $d_1$ 决策与 $d_2,...,d_n$ 决策独立开来，也就是说通过解决形如 $opt_{d_1 \\in D_1}\\{H'(d_1)\\}$ 问题的最优解以得到 $d_1$ 的最佳决策，这是一种贪心算法，这种算法在局部最优 $opt_{d_1}\\{H'(d_1)\\}$ 与全局最优 $H^{\\ast}$ 一致的情况下是有效的。我们先分析这种特殊情况，因为它足够简单：假设目标函数 h 强可分，即\n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ ... \\circ C_n(d_n) \\qquad (1.5)$$\n其中 $C_i$ 是决策 $d_i$ 关联的决策损失函数，$\\circ$ 是某种关联二元操作（例如 加 或 乘）并具有属性 \n$$opt_d\\{a \\circ C(d)\\}=a \\circ opt_d\\{C(d)\\}$$\n其中 a 不依赖于决策 d。在序列决策过程中，损失 $C_n$ 不仅依赖于 $d_n$，还依赖于当前问题所处的状态 $(d_1,d_2,...,d_{n-1})$，所以改写上式为\n$$h(d_1,...,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\qquad(1.6)$$\n\n定义如果目标函数 h 满足\n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n) \\qquad(1.7)$$\n那么称其弱可分（强可分是弱可分的一种特殊情况），此时有\n$$\\begin{aligned} & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}\\{C_2(d_1,d_2) \\circ ... \\circ \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_n(d_1,...,d_n)\\}...\\}\\} \\qquad(1.8) \\end{aligned}$$\n最后一个等式根据 $\\circ$ 操作的属性推导。\n\n令 $f(d_1,...,d_n)$ 表示序列决策过程在已经做出决策 $d_1,...,d_{i-1}$ 时的最优解，即\n$$f(d_1,...,d_{i-1})=opt_{d_i}\\{opt_{d_{i+1}}\\{... \\{opt_{d_n} \\{C_i(d_i|d_1,...,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,...,d_i) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\}\\}...\\}\\} \\ (1.9)$$\n其中因为篇幅起见省略了每个决策的取值空间 $D_i$。现在序列决策过程可表示为\n$$\\begin{aligned}f(\\emptyset)&=opt_{d_1}\\{opt_{d_2}\\{...\\{opt_{d_n}\\{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1})\\}\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ opt_{d_2}\\{C(d_2|d_1) \\circ...\\circ opt_{d_n}\\{C_n(d_n|d_1,...,d_{n-1})\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ f(d_1)\\}  \\qquad \\quad (1.10) \\end{aligned}$$\n一般地，我们有\n$$f(d_1,...,d_{i-1})=opt_{d_i \\in D_i(d_1,...,d_{i-1})}\\{C_i(d_i|d_1,...,d_{i-1})\\circ f(d_1,...,d_i)\\} \\qquad(1.11)$$\n\n于是我们得到问题的递归函数形式，这就是优化问题的动态规划函数方程（DPFE）。\n\n### DPFE\n求解 DPFE 中的 $f(d_1,...,d_{i-1})$，定义状态 $S=(d_1,...,d_{i-1})$，那么 $i=|S|+1=|\\{d_1,...,d_{i-1}\\}|+1$，于是可以改写 DPFE 为以下形式，\n$$f(S)=opt_{d_i \\in D_i(S)}\\{C_i(d_i|S) \\circ f(S')\\} \\qquad (1.12)$$\n其中 $S'=(d_1,...,d_i)$ 是下一状态，$\\emptyset$ 是初始状态。记状态空间为 $\\mathcal S$，由于 DPFE 是递归的，要终止递归，则要求具备一些基本情况（或称边界条件），例如 $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$，对于某个基本（终止）态 $S_0$，$f(S_0)$ 不使用 DPFE 计算其值，而是有一个给定的常数值 b，这就表示到达基本态时，递归结束。\n\n值得注意的是决策序列的长度 n 并非固定，当决策使得目标到达基本态时结束决策过程。前面我们定义状态 S 表示已经做过的决策，下一决策 d 则从 D(S) 中进行选择，但实际上为了表示方便，直接定义状态 S 为下一决策 d 的可选决策空间，即 $d \\in S$，于是 DPFE 变为\n$$f(S)=opt_{d \\in S} \\{C(d|S) \\circ f(S')\\} \\qquad (1.13)$$\n\n我们可以使用状态转移系统或者有向图对简单序列决策过程进行建模，状态 S 为节点，决策 d 使得状态从 S 转移到 S'，D(S) 表示处于状态 S 时的决策空间。考虑使用有向图建模，节点表示 DPFE 的状态，边表示状态间的转移，转移对应着决策，边标记为 b(S,S')，表示决策 d 的损失 C(d|S)，其中下一状态 $S'=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$，T 是转移函数，于是 DPFE 转变为\n$$f(S)=opt_S\\{b(S,S') \\circ f(S')\\} \\qquad(1.14) $$\n\nDPFE 的反转形式为\n$$f'(S)=opt_{S'}\\{f'(S') \\circ b(S',S)\\} \\qquad(1.15) $$\n其中 f'(S) 表示从基本态 $S_0$ 到状态 S 的最优解，注意与前面 f(S) 表示从状态 S 到基本态 $S_0$ 的最优解区分开来。式 (1.14) 为后向形式（backward），式 (1.15) 为前向形式（forward）。\n\n### 动态规划的基本要素\n动态规划的基本形式为\n$$f(S)=opt_{d \\in D(S)} \\{R(S,d) \\circ f(T(S,d))\\}  \\quad (1.16)$$\n其中，S 表示状态空间 $\\mathcal S$ 中的某个状态，d 是决策空间 D(S) 中的某个决策，R(S,d) 是收益函数（或称损失函数，记为 C(d|S)，收益对应最大化，损失对应最小化），T(S,d) 是转移函数，$\\circ$ 是二元操作符。为简单起见，我们只考虑离散情况。\n\n### 线性搜索\n现在我们来看一个实际例子。问题是需要排列一个长度为 N 的数组 A 中的元素，元素 x 具有概率 $p_x$，通过最小化排列的损失来优化线性搜索过程，例如 A={a,b,c}，且 $p_a=0.2,p_b=0.5,p_c=0.3$，于是共有 6 中排列方式 abc,acb,bac,bca,cab,cba（注意：每个排列均代表一种决策序列），其中 bca 排列的损失为 1.7，计算如下：\n1. Strong separable，方法 S  \n   $1p_b+2p_c+3p_a$\n2. Weak separable，方法 W  \n   $(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$\n\n最优排列问题可看作是序列决策过程，每个决策用于决定将 A 的元素置于排列后 A' 的哪个位置上。决策的损失可互相独立（强可分），即方法 S 损失定义为：元素 x 的损失为 $ip_x$，其中 i 为 x 在 A' 中位置；对于 W 方法，决策的损失依赖于决策的顺序，或者说依赖于决策空间，如果以从 A' 的开始到最后这样的顺序进行决策，还以 bca 排列为例说明，第一次决策的空间为 {a,b,c}，第一次决策选择 b 置于 A' 第一个位置，然后第二次决策空间为 {a,c}... 依次类推，决策损失与决策空间相关，定义为决策空间中各元素概率之和 $\\sum_{x\\in D_i} p_x$。\n\n假设决策顺序为 i=1,2,3，那么对应的决策空间为 $D_1=A,D_2=A-\\{d_1\\},D_3=A-\\{d_1,d_2\\}$，方法 S 的目标函数为 $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$，于是有\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{1p_{d_1}+2p_{d_2}+3p_{d_3}\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{1p_{d_1}+\\min_{d_2 \\in A-\\{d_1\\}}\\{2p_{d_2}+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{3p_{d_3}\\}\\}\\} \\end{aligned}$$\n\n方法 W 的目标函数为 $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x$，于是有\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-\\{d_1\\}}\\{\\sum_{x\\in A-\\{d_1\\}}p_x+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\} \\end{aligned}$$\n\n有了损失的计算方法之后，__线性搜索__ 就是依次计算并比较所有排列的损失，具有最小损失的就是最优排列。经过计算发现，bca 就是最佳排列。\n\n### 问题的表示和解\n上一小节中的例子可以使用 DP 求解。定义状态 S 为元素的集合，可从中选择决策来确定哪个元素应该放置在 A' 中的哪个位置。DPFE 的形式如下\n$$f(S)=\\min_{x \\in S} \\{C(x|S)+f(S-\\{x\\})\\}     \\quad(1.17)$$\n其中 $S \\in 2^A$，$2^A$ 是 A 的幂集。基本态 $f(\\emptyset)=0$，我们目标是要求出 $f(A)$。\n\n注意这是前向形式的 DPFE。要写成这是后向形式的 DPFE，则根据式 (1.15) 改写如下\n$$f(S)=\\min_{S'} \\{C(x|S')+f(S')\\}     \\quad(1.18)$$\n其中 $S \\in 2^A$，此时我们目标是求 $f(\\emptyset)$，基本态 $f(A)=0$。S' 是 S 的前导状态，从前导状态 S' 经过决策 x 到达状态 S。\n\n基于方法 W，损失函数为\n$$C_W(x|S)=\\sum_{y\\in S}p_y$$\n基于上一小节的讨论可知，当前的损失与当前决策 x 无关，而依赖于临决策之前的状态 S 有关，即 S 中各元素的概率和。\n\n基于方法 S，那么损失函数为\n$$C_S(x|S)=(N+1-|S|)p_x$$\n可见此时损失只与当前决策 x 以及决策的序号有关，其中与序号有关是假定了第一个决策确定某元素置于 A' 的第一个位置，第二个决策确定 A' 第二个位置上的元素...依次类推。如果决策顺序反过来，即第一个决策确定 A' 最后一个位置上的元素，那么损失函数需要修改为 $C_S'(x|S)=|S|p_x$。如果将 S 中元素按概率降序排列，事实表明第一个元素（具有最大概率值）将会最小化 $C(x|S)+f(S-\\{x\\})$，这是一种贪心策略，即只求当前状态下的最优解。事实上确实存在一类 DP 问题用贪心策略也可解决，这个我们暂时不讨论。\n\n现在我们来看下如何解 DP 问题，以式 (1.17) 表示的 DPFE 为例进行说明：\n$$\\begin{aligned} f(\\{a,b,c\\}) &= \\min\\{C(a|\\{a,b,c\\})+f(\\{b,c\\}), C(b|\\{a,b,c\\})+f(\\{a,c\\}), C(c|\\{a,b,c\\})+f(\\{a,b\\})\\}\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(b|\\{b,c\\}+f(\\{c\\}),C(c|\\{b,c\\}+f(\\{b\\})\\}\n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(a|\\{a,c\\}+f(\\{c\\}),C(c|\\{a,c\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(a|\\{a,b\\}+f(\\{b\\}),C(c|\\{a,b\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(c|\\{c\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(b|\\{b\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(a|\\{a\\})+f(\\emptyset)\\} \n\\\\\\\\ f(\\emptyset) &= 0 \\end{aligned}$$\n其中损失函数可以分别使用方法 S 和方法 W 进行代入计算，这里略。\n\n再以式 (1.18) 表示的 DPFE 进行说明：\n$$\\begin{aligned} f(\\{a,b,c\\}) &= 0\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(a|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0 \n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(b|\\{a,a,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(c|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(a|\\{a,c\\})+f(\\{a,c\\}), C(b|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.5+1.0,0.8+1.0\\}=1.5\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(a|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.8+1.0\\}=1.7\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(b|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{a,c\\})+f(\\{a,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.5+1.0\\}=1.5\n\\\\\\\\ f(\\emptyset) &= \\min \\{C(a|a)+f(\\{a\\}), C(b|b)+f(\\{b\\}), C(c|c)+f(\\{c\\})\\}\\stackrel W = \\min \\{0.2+1.5,0.5+1.7,0.3+1.5\\}=1.7 \\end{aligned}$$\n以上式中 $\\stackrel W=$ 之后部分均表示使用方法 W 进行计算，这是为了演示，方法 S 的代入计算略。\n\n### 带阶决策\n第一次决策记为阶 1，第二次决策记为阶 2... 依次类推。将阶号包含进状态 S 的定义中有时候会带来方便甚至是非常有必要的。还以前面的例子进行说明，DPFE 形式 (1.17) 可改写为\n$$f(k,S)=\\min_{x \\in S} \\{C(x|k,S)+f(k+1,S-\\{x\\})\\} \\quad(1.19)$$\n\n### Path-States\n状态 S 可以定义为当前已经做过的决策 $(d_1,...,d_{i-1})$，在有向图中用节点表示状态，状态 S 与初始态 $\\emptyset$ 到状态 S 之间的路径有关联（其实这个路径也可以表示为 $(d_1,...,d_{i-1})$），需要特别注意的是，前面的状态 S 是无序的，比如 S={a,b}，表示当前已经做出了决策 a 和 b（或者说做出决策选择了 a 和 b，读者根据具体语境进行调整理解），至于决策 a 和 b 的顺序则未定义，而这里 Path-States 中的状态是有序的，S=(a,b)，表示当前已经做出两个决策，第一个决策是 a 且第二个决策是 b。于是此时 DPFE 的形式可写为\n$$f(S)=\\min_{x \\notin S} \\{C(x|S)+f(S + (x))\\} \\qquad(1.20)$$\n\n前面线性搜索一节中的例子计算过程为\n$$\\begin{aligned} f(\\emptyset) &= \\min \\{C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)\\}\\stackrel S= \\min\\{0.2+1.9,0.5+1.2,0.3+1.6\\}=1.7\n\\\\\\\\ f(a) &= \\min \\{C(b|a)+f(ab), C(c|a)+f(ac)\\}\\stackrel S= \\min\\{2*0.5+0.9,2*0.3+1.5\\}=1.9\n\\\\\\\\ f(b) &= \\min \\{C(a|b)+f(ba), C(c|b)+f(bc)\\}\\stackrel S= \\min\\{2*0.2+0.9,2*0.3+0.6\\}=1.2\n\\\\\\\\ f(c) &= \\min \\{C(a|c)+f(ca), C(b|c)+f(cb)\\}\\stackrel S= \\min\\{2*0.2+1.5,2*0.5+0.6\\}=1.6\n\\\\\\\\ f(ab) &= \\min \\{C(c|ab)+f(abc)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(ac) &= \\min \\{C(b|ac)+f(acb)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(ba) &= \\min \\{C(c|ba)+f(bac)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(bc) &= \\min \\{C(a|bc)+f(bca)\\}\\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(ca) &= \\min \\{C(b|ca)+f(cab)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(cb) &= \\min \\{C(a|cb)+f(cba)\\} \\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(abc) &= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$\n由于状态是有序的，所以共有 $N!$ 个基本态，损失函数依然可以使用方法 S 和方法 W 计算，例如使用方法 S，损失 $C(c|ab)$ 表示第三次决策使用 c，那么根据方法 S 的定义有 $C(c|ab)=3p_c=3*0.3=0.9$。上面计算中 $\\stackrel S=$ 之后部分均表示使用方法 S 进行计算。方法 W 的计算略。\n\n### 松弛 Relaxation\n松弛在数学中指通过某种迭代方法逐步得到更好的近似解。考虑一个有限集合 $\\{a_1,...,a_N\\}$，其最小值可以通过计算结对元素最小化来求解\n$$x^{\\ast}=\\min\\{\\min\\{...\\{\\min\\{a_1,a_2\\},a_3\\},...\\},a_N\\}$$\n偏最小化序列为 $x_1=a_1, x_2=\\min\\{x_1,a_2\\},...$，为了表示的统一，可以令 $x_1=\\min\\{x_0,a_1\\}, x_0=\\infty$。序列 $x_1,x_2,...$ 逐步逼近并最终达到 $x^{\\ast}$。松弛就是刻画这种逐步逼近的特性。借助松弛的思想，动态规划问题的 DPFE 可表示为\n$$\\begin{aligned} f(S)&=\\min_{x \\in S} \\{C(x|S)+f(S_x')\\}\n\\\\\\\\ &=\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}'),...,C(x_m|S)+f(S_{x_m}')\\} \\end{aligned}$$\n其中 $S=\\{x_1,x_2,...,x_m\\}$，$S_x'$ 是选择 x 之后的下一状态，与其计算所有的 $C(x|S)+f(S_x')$ 值，我们不如逐步逼近最终值\n$$f(S)=\\min\\{\\min\\{...\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}')\\},...\\},C(x_m|S)+f(S_{x_m}')\\} \\quad (1.21)$$\n注意 $C(x_i|S)$ 中状态全部是 S，并且所有的 $f(S_{x_i}')$ 需要提前全部计算出来。\n\n带阶形式的 DPFE 为\n$$f(k,S)=\\min_x \\{C(x|k,S) + f(k-1,S_x')\\} \\qquad(1.22)$$\n其中 k 表示阶段，这里 k 可能不太好理解，我们可以想象从状态 S 经过恰好 k 次变换到达终止态 T，下文最短路径问题中会借鉴式 (1.22)，彼时再回过头来理解可能更加容易些。序列 $f(0,S),f(1,S),...$ 近似于 $f(S)$，序列最小值为 $f(S)$，但这个序列不保证单调性（比如振荡逼近），所以 $f(S)=\\min_k \\{f(k,S)\\}$，注意 $f(k,S)$ 不是 $f(k-1,S)$ 的函数，而是 $f(k-1,S_x')$ 的函数。既然 $f(k,S)$ 序列不一定单调，我们定义一个新的序列 $F(k,S)$ 来保证单调性，\n$$F(k,S)=\\min\\{F(k-1,S), \\min_x \\{C(x|k,S)+F(k-1,S_x')\\}\\} \\quad(1.23)$$\n这里我们可以将 $F(k,S)$ 理解为从状态 S 到终止态 T 最多变换 k 次的损失，与式 (1.22) 中恰好变换 k 次是有区别的。 \n\n### 最短路径问题\n在解决前述的线性搜索问题中，我们使用了状态转换图模型，不难发现解决这种线性搜索问题等价于在图中搜索最短路径。\n\n考虑到有环图的复杂性，我们先讨论无环图。对于一个无环图，从起点 s 到终点 t 的最短路径使用 DPFE 可表示为\n$$f(p)=\\min_q \\{b(p,q)+f(q)\\} \\qquad(1.24)$$\n其中 b(p,q) 表示从 p 到 q 的距离，q 是 p 的直接邻点，f(p) 表示从 p 到 t 的最短路径，基本态条件为 f(t)=0。如果 q 不是 p 的直接邻点，那么可以认为 $b(p,q)=\\infty$。无环图中，要计算 f(p) 则需要先计算 f(q)。采用自底向上的方式计算，具体不展开。\n\n在有环图中，p 和 q 可能互为后继节点，f(p) 和 f(q) 互相依赖。为了方便，我们假定有环图中没有自环，这样假设是有原因的，如果有自环，即一条从 p 到 p 的分支，考虑以下三种情况：\n1. b(p,p) > 0，这条分支会被忽略，因为会增加距离\n2. b(p,p) < 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，从而一直缩短距离\n3. b(p,p) = 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，却不改变距离\n\n所以可以忽略掉自环，以下所讨论的有环图中均没有自环。对于一个有环图，DPFE 为\n$$f(p) = \\min_q \\{b(p,q)+f(q)\\} \\qquad(1.25)$$\n其中 f(q) 可能依赖于 f(p)，做特殊处理：初始时令 $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$。式 (1.25) 与 (1.24) 一样，因为都是解最短路径模型，只是有环图中进行求解的时候需要做特殊处理。\n\n举例说明，如图 1.2，\n![](/images/DP1_fig1.png)\n\n根据式 (1.25) 计算过程如下\n$$\\begin{aligned}f(s)&=\\min \\{b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)\\}=\\min \\{3+f(x),5+f(y),\\infty+f(t)\\}\n\\\\\\\\f(x)&=\\min \\{b(x,y)+f(y),b(x,t)+f(t)\\}=\\min \\{1+f(y),8+f(t)\\}\n\\\\\\\\f(y)&=\\min \\{b(y,x)+f(x),b(y,t)+f(t)\\}=\\min \\{2+f(x),5+f(t)\\}\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n显然 f(x) 与 f(y) 互相依赖。\n\n初始化时假设 $f(s)=f(x)=f(y)=\\infty$，第一次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+\\infty,5+\\infty,\\infty+f(t)\\}=\\infty\n\\\\\\\\f(x)&=\\min \\{1+\\infty,8+0\\}=8\n\\\\\\\\f(y)&=\\min \\{2+\\infty,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n第二次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+8,5+5,\\infty+0\\}=10\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+8,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n第三次迭代，\n$$\\begin{aligned}f(s)&=\\min \\{3+6,5+5,\\infty+0\\}=9\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+6,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n由于第三次迭代 $f(x),f(y),f(t)$ 均未改变，故第三次迭代后计算到的 f(s) 就是最终 f(s)。\n\n还可以利用带阶 Relaxation 解决有环图问题，仿照式 (1.22)，我们针对最短路径模型改为 DPFE 为\n$$f(k,p)=\\min_q \\{b(p,q)+f(k-1,q)\\} \\qquad(1.26)$$\n其中 f(k,p) 表示从 p 到 t 的最短距离，k 表示 p 到 t 的某个路径需要走恰好 k 步。于是基本条件为： $f(0,t)=0;f(k,t)=\\infty, k>0;f(0,p)=\\infty, \\forall p \\ne t$，这表示 t 到 t 走 0 步，代价为 0，走大于 0 步，代价为 $\\infty$，因为既然到了 t 点，我们不希望再继续走下去。p 到 t 走 0 步，代价为 $\\infty$，这驱使我们从 p 点走出去。还以上面的例子说明，根据式 (1.26) 计算过程为\n$$\\begin{aligned}f(k,s)&=\\min \\{b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)\\}\n\\\\\\\\f(k,x)&=\\min \\{b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)\\}\n\\\\\\\\f(k,y)&=\\min \\{b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)\\}\n\\\\\\\\f(k,t) \\end{aligned}$$\n根据基本条件，也就是 k=0 的初始条件，第一次迭代，\n$$\\begin{aligned}f(1,s)&=\\min \\{3+f(0,x),5+f(0,y),\\infty+f(0,t)\\}=\\infty\n\\\\\\\\f(1,x)&=\\min \\{1+f(0,y),8+f(0,t)\\}=8\n\\\\\\\\f(1,y)&=\\min \\{2+f(0,x),5+f(0,t)\\}=5\n\\\\\\\\f(1,t)&=\\infty \\end{aligned}$$\n第二次迭代，\n$$\\begin{aligned}f(2,s)&=\\min \\{3+f(1,x),5+f(1,y),\\infty+f(1,t)\\}=10\n\\\\\\\\f(2,x)&=\\min \\{1+f(1,y),8+f(1,t)\\}=6\n\\\\\\\\f(2,y)&=\\min \\{2+f(1,x),5+f(1,t)\\}=10\n\\\\\\\\f(2,t)&=\\infty \\end{aligned}$$\n第三次迭代，\n$$\\begin{aligned}f(3,s)&=\\min \\{3+f(2,x),5+f(2,y),\\infty+f(2,t)\\}=9\n\\\\\\\\f(3,x)&=\\min \\{1+f(2,y),8+f(2,t)\\}=11\n\\\\\\\\f(3,y)&=\\min \\{2+f(1,x),5+f(2,t)\\}=8\n\\\\\\\\f(3,t)&=\\infty \\end{aligned}$$\n假设图中总共有 N 个节点，那么从任意一点 p 到 t 可以走恰好 k 步，k 取值为 {0,1,...,N-1}，k 不能大于等于 N，否则就存在某个节点经过两次，从而路径中存在环 circle，在任意两点路径均大于 0 的情况下，显然有环的路径不可能是最短路径。这里的例子中，N=4，所以 k 最大为 3，经过三次迭代后，就没必要再迭代下去了，否则路径中存在环，迭代下去的 f 值只会越来越大。于是根据 $f(p)=\\min_k \\{f(k,p)\\}$ 得\n$$\\begin{aligned}f(s)&=\\min \\{\\infty,\\infty,10,9\\}=9\n\\\\\\\\f(x)&=\\min \\{\\infty,8,6,11\\}=6\n\\\\\\\\f(y)&=\\min \\{\\infty,5,10,8\\}=5\n\\\\\\\\f(t)&=\\min \\{0,\\infty,\\infty,\\infty\\}=0 \\end{aligned}$$\n从以上求解过程不难发现，迭代计算结果序列并不收敛，这也说明了式 (1.22) 中 $f(k,S)$ 序列不单调性。\n\n既然 $f(k,S)$ 序列不单调，我们参考式 (1.23) 为最短路径问题改写合适的 DPFE，如下\n$$F(k,p)=\\min \\{F(k-1,p), \\ \\min_q \\{b(p,q)+F(k-1,q)\\}\\} \\qquad(1.27)$$\n\n其中 $F(k,p)$ 表示从 p 到 t 最多走 k 步的最短路径。显然要获得全局最短路径，我们必须考虑从 p 到 t 最多走 $N-1$ 步的最短路径，即目标是计算 $F(N-1,s)$，k 最大为 $N-1$，超过则路径出现环。基本条件不难得知为 $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$，这个不用过多解释了，相信现在大家都能理解。\n\n\n还有其他形式的 DPFE，由于篇幅有限就不一一介绍，待后面分析具体例子的时候再穿插说明。","slug":"dp/DP1","published":1,"updated":"2020-04-24T10:31:05.843Z","_id":"ck9dzcjg5002sgga68e1411qa","comments":1,"layout":"post","photos":[],"link":"","content":"<p>我计划开启一系列动态规划的方法介绍，主要参考 《Dynamic Programming · A Computational Tool》这本书。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>动态规划用于解决一类优化问题，顺序地做出决策，每一次决策使得问题转变为对一个子问题的优化，直到问题解决，这个决策序列就是原始问题的最优解。我们也可以将 问题/子问题 看作状态，决策就是状态间的转移，问题得到解决就对应着<code>结束状态</code>。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"优化原理\"><a href=\"#优化原理\" class=\"headerlink\" title=\"优化原理\"></a>优化原理</h2><blockquote>\n<p>一个最优决策满足：无论问题初始状态和初始决策是什么，剩余的决策序列构成做出初始决策之后问题状态的最优解。</p>\n</blockquote>\n<p>以最短路径问题为例说明：在一个指定了起点和终点的有权路径图中，无论之前选择了什么路径，我们必须保证从当前节点起，剩余的路径选择必须是最优的。<del>这是一种递归的处理方式，要解这个问题，直觉告诉我们似乎可以采用逆推法（隐马尔可夫预测问题的求解），这个直觉很重要，因为下文将会用到它。</del></p>\n<h3 id=\"顺序决策过程\"><a href=\"#顺序决策过程\" class=\"headerlink\" title=\"顺序决策过程\"></a>顺序决策过程</h3><p>考虑具有如下形式的优化问题：$opt_{d \\in \\Delta} {H(d)}$，其中 d 为决策，决策空间为 $\\Delta$，H 为目标函数，H(d) 的最优解记为 $d^{\\ast}$：$d^{\\ast}=\\arg opt_d {H(d)}$。动态规划问题则要求寻找有序决策集 ${d_1,…,d_n}$，使得目标函数 $h(d_1,…,h_n)$ 取得最优 $H^{\\ast}$。</p>\n<p>我们可以枚举 ${d_1,…,d_n}$ 所有可能的取值，然后代入目标函数进行计算，这就是 “暴力” 解法，但是这只在决策空间较小时有效，在决策空间很大时，这种方法效率非常低不可取，所以此时我们需要按顺序做出决策 $d_1,…,d_n$，使得<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,…,d_n)\\in \\Delta} {h(d_1,…,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2}{…{opt_{d_n \\in D_n}{h(d_1,…,d_n)}}…}} \\quad(1.1)\\end{aligned}$$</p>\n<p>其中 $(d_1,…,d_n) \\in \\Delta=D_1 \\times … \\times D_n$。通常，第 i 个决策空间依赖于前面所有的决策：$d_i \\in D_i(d_1,…,d_{i-1})$，于是式 (1.1) 可改写为<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,…,d_n)\\in \\Delta} {h(d_1,…,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}} \\quad(1.2)\\end{aligned}$$</p>\n<p>式 (1.2) 的优化操作是一个嵌套结构，可以由内向外解决问题，解最内层的优化问题，此时前序所有决策均看作已知，可得到最优 $d_n$，记作 $d_n^{\\ast}(d_1,…,d_{n-1})$，可以将 $d_n^{\\ast}$ 看作是其前序决策的函数。向外逆推，直到解出最外层优化问题 $opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast},…,d_n^{\\ast} }$ 的解 $d_1^{\\ast}$。</p>\n<p>如果改变决策顺序目标函数的最优解相同，如<br>$$\\begin{aligned} &amp;opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)} {… {opt_{d_n \\in D_n(d_1,…,d_{n-1})} {h(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_n \\in D_n} {opt_{d_{n-1} \\in D_{n-1}(d_n)} {… {opt_{d_1 \\in D_n(d_2,…,d_n)} {h(d_1,…,d_n)}}…}} \\quad(1.3) \\end{aligned}$$</p>\n<p>那么决策空间 $D_i$ 可能会跟之前有所不同，因为此时 $D_i$ 依赖于 $(d_{i+1},…,d_n)$，所以问题求解的效率会随着决策顺序的变化而有所改变。</p>\n<p>回到前面式 (1.2)，假设我们暂时做出最外层决策 $d_1$，此时 $d_1$ 是否是最优决策还尚未可知，但是根据前面所说的优化原理，无论之前做出什么决策，之后的决策需要保证是最优的，所以有<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}}<br>\\\\ &amp;=opt_{d_1 \\in D_1}{h(d_1,d_2^{\\ast}(d_1),…,d_n^{\\ast}(d_1)} \\qquad \\qquad(1.4) \\end{aligned}$$</p>\n<p>其中 $d_i^{\\ast}(d_1), \\ i&gt;1$ 可以看作是输入参数 $d_1$ 的偏函数（partial function）。最外层决策的最优解则为 $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast}(d_1),…,d_n^{\\ast}(d_1))}$，可以看出，$d_1$ 与其后序的决策互相耦合，要解这样的问题还是有点棘手。</p>\n<h4 id=\"目标函数可分\"><a href=\"#目标函数可分\" class=\"headerlink\" title=\"目标函数可分\"></a>目标函数可分</h4><p>一种方法是将 $d_1$ 决策与 $d_2,…,d_n$ 决策独立开来，也就是说通过解决形如 $opt_{d_1 \\in D_1}{H’(d_1)}$ 问题的最优解以得到 $d_1$ 的最佳决策，这是一种贪心算法，这种算法在局部最优 $opt_{d_1}{H’(d_1)}$ 与全局最优 $H^{\\ast}$ 一致的情况下是有效的。我们先分析这种特殊情况，因为它足够简单：假设目标函数 h 强可分，即<br>$$h(d_1,…,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ … \\circ C_n(d_n) \\qquad (1.5)$$<br>其中 $C_i$ 是决策 $d_i$ 关联的决策损失函数，$\\circ$ 是某种关联二元操作（例如 加 或 乘）并具有属性<br>$$opt_d{a \\circ C(d)}=a \\circ opt_d{C(d)}$$<br>其中 a 不依赖于决策 d。在序列决策过程中，损失 $C_n$ 不仅依赖于 $d_n$，还依赖于当前问题所处的状态 $(d_1,d_2,…,d_{n-1})$，所以改写上式为<br>$$h(d_1,…,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1}) \\qquad(1.6)$$</p>\n<p>定义如果目标函数 h 满足<br>$$h(d_1,…,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ … \\circ C_n(d_1,…,d_n) \\qquad(1.7)$$<br>那么称其弱可分（强可分是弱可分的一种特殊情况），此时有<br>$$\\begin{aligned} &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{C_1(d_1) \\circ C_2(d_1,d_2) \\circ … \\circ C_n(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}{C_2(d_1,d_2) \\circ … \\circ {opt_{d_n \\in D_n(d_1,…,d_{n-1})}{C_n(d_1,…,d_n)}…}} \\qquad(1.8) \\end{aligned}$$<br>最后一个等式根据 $\\circ$ 操作的属性推导。</p>\n<p>令 $f(d_1,…,d_n)$ 表示序列决策过程在已经做出决策 $d_1,…,d_{i-1}$ 时的最优解，即<br>$$f(d_1,…,d_{i-1})=opt_{d_i}{opt_{d_{i+1}}{… {opt_{d_n} {C_i(d_i|d_1,…,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,…,d_i) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1}) }}…}} \\ (1.9)$$<br>其中因为篇幅起见省略了每个决策的取值空间 $D_i$。现在序列决策过程可表示为<br>$$\\begin{aligned}f(\\emptyset)&amp;=opt_{d_1}{opt_{d_2}{…{opt_{d_n}{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1})}}…}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ opt_{d_2}{C(d_2|d_1) \\circ…\\circ opt_{d_n}{C_n(d_n|d_1,…,d_{n-1})}…}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ f(d_1)}  \\qquad \\quad (1.10) \\end{aligned}$$<br>一般地，我们有<br>$$f(d_1,…,d_{i-1})=opt_{d_i \\in D_i(d_1,…,d_{i-1})}{C_i(d_i|d_1,…,d_{i-1})\\circ f(d_1,…,d_i)} \\qquad(1.11)$$</p>\n<p>于是我们得到问题的递归函数形式，这就是优化问题的动态规划函数方程（DPFE）。</p>\n<h3 id=\"DPFE\"><a href=\"#DPFE\" class=\"headerlink\" title=\"DPFE\"></a>DPFE</h3><p>求解 DPFE 中的 $f(d_1,…,d_{i-1})$，定义状态 $S=(d_1,…,d_{i-1})$，那么 $i=|S|+1=|{d_1,…,d_{i-1}}|+1$，于是可以改写 DPFE 为以下形式，<br>$$f(S)=opt_{d_i \\in D_i(S)}{C_i(d_i|S) \\circ f(S’)} \\qquad (1.12)$$<br>其中 $S’=(d_1,…,d_i)$ 是下一状态，$\\emptyset$ 是初始状态。记状态空间为 $\\mathcal S$，由于 DPFE 是递归的，要终止递归，则要求具备一些基本情况（或称边界条件），例如 $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$，对于某个基本（终止）态 $S_0$，$f(S_0)$ 不使用 DPFE 计算其值，而是有一个给定的常数值 b，这就表示到达基本态时，递归结束。</p>\n<p>值得注意的是决策序列的长度 n 并非固定，当决策使得目标到达基本态时结束决策过程。前面我们定义状态 S 表示已经做过的决策，下一决策 d 则从 D(S) 中进行选择，但实际上为了表示方便，直接定义状态 S 为下一决策 d 的可选决策空间，即 $d \\in S$，于是 DPFE 变为<br>$$f(S)=opt_{d \\in S} {C(d|S) \\circ f(S’)} \\qquad (1.13)$$</p>\n<p>我们可以使用状态转移系统或者有向图对简单序列决策过程进行建模，状态 S 为节点，决策 d 使得状态从 S 转移到 S’，D(S) 表示处于状态 S 时的决策空间。考虑使用有向图建模，节点表示 DPFE 的状态，边表示状态间的转移，转移对应着决策，边标记为 b(S,S’)，表示决策 d 的损失 C(d|S)，其中下一状态 $S’=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$，T 是转移函数，于是 DPFE 转变为<br>$$f(S)=opt_S{b(S,S’) \\circ f(S’)} \\qquad(1.14) $$</p>\n<p>DPFE 的反转形式为<br>$$f’(S)=opt_{S’}{f’(S’) \\circ b(S’,S)} \\qquad(1.15) $$<br>其中 f’(S) 表示从基本态 $S_0$ 到状态 S 的最优解，注意与前面 f(S) 表示从状态 S 到基本态 $S_0$ 的最优解区分开来。式 (1.14) 为后向形式（backward），式 (1.15) 为前向形式（forward）。</p>\n<h3 id=\"动态规划的基本要素\"><a href=\"#动态规划的基本要素\" class=\"headerlink\" title=\"动态规划的基本要素\"></a>动态规划的基本要素</h3><p>动态规划的基本形式为<br>$$f(S)=opt_{d \\in D(S)} {R(S,d) \\circ f(T(S,d))}  \\quad (1.16)$$<br>其中，S 表示状态空间 $\\mathcal S$ 中的某个状态，d 是决策空间 D(S) 中的某个决策，R(S,d) 是收益函数（或称损失函数，记为 C(d|S)，收益对应最大化，损失对应最小化），T(S,d) 是转移函数，$\\circ$ 是二元操作符。为简单起见，我们只考虑离散情况。</p>\n<h3 id=\"线性搜索\"><a href=\"#线性搜索\" class=\"headerlink\" title=\"线性搜索\"></a>线性搜索</h3><p>现在我们来看一个实际例子。问题是需要排列一个长度为 N 的数组 A 中的元素，元素 x 具有概率 $p_x$，通过最小化排列的损失来优化线性搜索过程，例如 A={a,b,c}，且 $p_a=0.2,p_b=0.5,p_c=0.3$，于是共有 6 中排列方式 abc,acb,bac,bca,cab,cba（注意：每个排列均代表一种决策序列），其中 bca 排列的损失为 1.7，计算如下：</p>\n<ol>\n<li>Strong separable，方法 S<br>$1p_b+2p_c+3p_a$</li>\n<li>Weak separable，方法 W<br>$(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$</li>\n</ol>\n<p>最优排列问题可看作是序列决策过程，每个决策用于决定将 A 的元素置于排列后 A’ 的哪个位置上。决策的损失可互相独立（强可分），即方法 S 损失定义为：元素 x 的损失为 $ip_x$，其中 i 为 x 在 A’ 中位置；对于 W 方法，决策的损失依赖于决策的顺序，或者说依赖于决策空间，如果以从 A’ 的开始到最后这样的顺序进行决策，还以 bca 排列为例说明，第一次决策的空间为 {a,b,c}，第一次决策选择 b 置于 A’ 第一个位置，然后第二次决策空间为 {a,c}… 依次类推，决策损失与决策空间相关，定义为决策空间中各元素概率之和 $\\sum_{x\\in D_i} p_x$。</p>\n<p>假设决策顺序为 i=1,2,3，那么对应的决策空间为 $D_1=A,D_2=A-{d_1},D_3=A-{d_1,d_2}$，方法 S 的目标函数为 $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$，于是有<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{1p_{d_1}+2p_{d_2}+3p_{d_3}}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{1p_{d_1}+\\min_{d_2 \\in A-{d_1}}{2p_{d_2}+\\min_{d_3\\in A-{d_1,d_2}}{3p_{d_3}}}} \\end{aligned}$$</p>\n<p>方法 W 的目标函数为 $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x$，于是有<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-{d_1}}{\\sum_{x\\in A-{d_1}}p_x+\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A-{d_1,d_2}}p_x}}} \\end{aligned}$$</p>\n<p>有了损失的计算方法之后，<strong>线性搜索</strong> 就是依次计算并比较所有排列的损失，具有最小损失的就是最优排列。经过计算发现，bca 就是最佳排列。</p>\n<h3 id=\"问题的表示和解\"><a href=\"#问题的表示和解\" class=\"headerlink\" title=\"问题的表示和解\"></a>问题的表示和解</h3><p>上一小节中的例子可以使用 DP 求解。定义状态 S 为元素的集合，可从中选择决策来确定哪个元素应该放置在 A’ 中的哪个位置。DPFE 的形式如下<br>$$f(S)=\\min_{x \\in S} {C(x|S)+f(S-{x})}     \\quad(1.17)$$<br>其中 $S \\in 2^A$，$2^A$ 是 A 的幂集。基本态 $f(\\emptyset)=0$，我们目标是要求出 $f(A)$。</p>\n<p>注意这是前向形式的 DPFE。要写成这是后向形式的 DPFE，则根据式 (1.15) 改写如下<br>$$f(S)=\\min_{S’} {C(x|S’)+f(S’)}     \\quad(1.18)$$<br>其中 $S \\in 2^A$，此时我们目标是求 $f(\\emptyset)$，基本态 $f(A)=0$。S’ 是 S 的前导状态，从前导状态 S’ 经过决策 x 到达状态 S。</p>\n<p>基于方法 W，损失函数为<br>$$C_W(x|S)=\\sum_{y\\in S}p_y$$<br>基于上一小节的讨论可知，当前的损失与当前决策 x 无关，而依赖于临决策之前的状态 S 有关，即 S 中各元素的概率和。</p>\n<p>基于方法 S，那么损失函数为<br>$$C_S(x|S)=(N+1-|S|)p_x$$<br>可见此时损失只与当前决策 x 以及决策的序号有关，其中与序号有关是假定了第一个决策确定某元素置于 A’ 的第一个位置，第二个决策确定 A’ 第二个位置上的元素…依次类推。如果决策顺序反过来，即第一个决策确定 A’ 最后一个位置上的元素，那么损失函数需要修改为 $C_S’(x|S)=|S|p_x$。如果将 S 中元素按概率降序排列，事实表明第一个元素（具有最大概率值）将会最小化 $C(x|S)+f(S-{x})$，这是一种贪心策略，即只求当前状态下的最优解。事实上确实存在一类 DP 问题用贪心策略也可解决，这个我们暂时不讨论。</p>\n<p>现在我们来看下如何解 DP 问题，以式 (1.17) 表示的 DPFE 为例进行说明：<br>$$\\begin{aligned} f({a,b,c}) &amp;= \\min{C(a|{a,b,c})+f({b,c}), C(b|{a,b,c})+f({a,c}), C(c|{a,b,c})+f({a,b})}<br>\\\\ f({b,c}) &amp;= \\min{C(b|{b,c}+f({c}),C(c|{b,c}+f({b})}<br>\\\\ f({a,c}) &amp;= \\min{C(a|{a,c}+f({c}),C(c|{a,c}+f({a})}<br>\\\\ f({a,b}) &amp;= \\min{C(a|{a,b}+f({b}),C(c|{a,b}+f({a})}<br>\\\\ f({c}) &amp;= \\min {C(c|{c})+f(\\emptyset)}<br>\\\\ f({b}) &amp;= \\min {C(b|{b})+f(\\emptyset)}<br>\\\\ f({a}) &amp;= \\min {C(a|{a})+f(\\emptyset)}<br>\\\\ f(\\emptyset) &amp;= 0 \\end{aligned}$$<br>其中损失函数可以分别使用方法 S 和方法 W 进行代入计算，这里略。</p>\n<p>再以式 (1.18) 表示的 DPFE 进行说明：<br>$$\\begin{aligned} f({a,b,c}) &amp;= 0<br>\\\\ f({b,c}) &amp;= \\min{C(a|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,c}) &amp;= \\min{C(b|{a,a,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,b}) &amp;= \\min{C(c|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({c}) &amp;= \\min {C(a|{a,c})+f({a,c}), C(b|{b,c})+f({b,c})}\\stackrel W = \\min {0.5+1.0,0.8+1.0}=1.5<br>\\\\ f({b}) &amp;= \\min {C(a|{a,b})+f({a,b}), C(c|{b,c})+f({b,c})}\\stackrel W = \\min {0.7+1.0,0.8+1.0}=1.7<br>\\\\ f({a}) &amp;= \\min {C(b|{a,b})+f({a,b}), C(c|{a,c})+f({a,c})}\\stackrel W = \\min {0.7+1.0,0.5+1.0}=1.5<br>\\\\ f(\\emptyset) &amp;= \\min {C(a|a)+f({a}), C(b|b)+f({b}), C(c|c)+f({c})}\\stackrel W = \\min {0.2+1.5,0.5+1.7,0.3+1.5}=1.7 \\end{aligned}$$<br>以上式中 $\\stackrel W=$ 之后部分均表示使用方法 W 进行计算，这是为了演示，方法 S 的代入计算略。</p>\n<h3 id=\"带阶决策\"><a href=\"#带阶决策\" class=\"headerlink\" title=\"带阶决策\"></a>带阶决策</h3><p>第一次决策记为阶 1，第二次决策记为阶 2… 依次类推。将阶号包含进状态 S 的定义中有时候会带来方便甚至是非常有必要的。还以前面的例子进行说明，DPFE 形式 (1.17) 可改写为<br>$$f(k,S)=\\min_{x \\in S} {C(x|k,S)+f(k+1,S-{x})} \\quad(1.19)$$</p>\n<h3 id=\"Path-States\"><a href=\"#Path-States\" class=\"headerlink\" title=\"Path-States\"></a>Path-States</h3><p>状态 S 可以定义为当前已经做过的决策 $(d_1,…,d_{i-1})$，在有向图中用节点表示状态，状态 S 与初始态 $\\emptyset$ 到状态 S 之间的路径有关联（其实这个路径也可以表示为 $(d_1,…,d_{i-1})$），需要特别注意的是，前面的状态 S 是无序的，比如 S={a,b}，表示当前已经做出了决策 a 和 b（或者说做出决策选择了 a 和 b，读者根据具体语境进行调整理解），至于决策 a 和 b 的顺序则未定义，而这里 Path-States 中的状态是有序的，S=(a,b)，表示当前已经做出两个决策，第一个决策是 a 且第二个决策是 b。于是此时 DPFE 的形式可写为<br>$$f(S)=\\min_{x \\notin S} {C(x|S)+f(S + (x))} \\qquad(1.20)$$</p>\n<p>前面线性搜索一节中的例子计算过程为<br>$$\\begin{aligned} f(\\emptyset) &amp;= \\min {C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)}\\stackrel S= \\min{0.2+1.9,0.5+1.2,0.3+1.6}=1.7<br>\\\\ f(a) &amp;= \\min {C(b|a)+f(ab), C(c|a)+f(ac)}\\stackrel S= \\min{2<em>0.5+0.9,2</em>0.3+1.5}=1.9<br>\\\\ f(b) &amp;= \\min {C(a|b)+f(ba), C(c|b)+f(bc)}\\stackrel S= \\min{2<em>0.2+0.9,2</em>0.3+0.6}=1.2<br>\\\\ f(c) &amp;= \\min {C(a|c)+f(ca), C(b|c)+f(cb)}\\stackrel S= \\min{2<em>0.2+1.5,2</em>0.5+0.6}=1.6<br>\\\\ f(ab) &amp;= \\min {C(c|ab)+f(abc)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(ac) &amp;= \\min {C(b|ac)+f(acb)}\\stackrel S= 3</em>0.5=1.5<br>\\\\ f(ba) &amp;= \\min {C(c|ba)+f(bac)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(bc) &amp;= \\min {C(a|bc)+f(bca)}\\stackrel S= 3</em>0.2=0.6<br>\\\\ f(ca) &amp;= \\min {C(b|ca)+f(cab)}\\stackrel S= 3<em>0.5=1.5<br>\\\\ f(cb) &amp;= \\min {C(a|cb)+f(cba)} \\stackrel S= 3</em>0.2=0.6<br>\\\\ f(abc) &amp;= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$<br>由于状态是有序的，所以共有 $N!$ 个基本态，损失函数依然可以使用方法 S 和方法 W 计算，例如使用方法 S，损失 $C(c|ab)$ 表示第三次决策使用 c，那么根据方法 S 的定义有 $C(c|ab)=3p_c=3*0.3=0.9$。上面计算中 $\\stackrel S=$ 之后部分均表示使用方法 S 进行计算。方法 W 的计算略。</p>\n<h3 id=\"松弛-Relaxation\"><a href=\"#松弛-Relaxation\" class=\"headerlink\" title=\"松弛 Relaxation\"></a>松弛 Relaxation</h3><p>松弛在数学中指通过某种迭代方法逐步得到更好的近似解。考虑一个有限集合 ${a_1,…,a_N}$，其最小值可以通过计算结对元素最小化来求解<br>$$x^{\\ast}=\\min{\\min{…{\\min{a_1,a_2},a_3},…},a_N}$$<br>偏最小化序列为 $x_1=a_1, x_2=\\min{x_1,a_2},…$，为了表示的统一，可以令 $x_1=\\min{x_0,a_1}, x_0=\\infty$。序列 $x_1,x_2,…$ 逐步逼近并最终达到 $x^{\\ast}$。松弛就是刻画这种逐步逼近的特性。借助松弛的思想，动态规划问题的 DPFE 可表示为<br>$$\\begin{aligned} f(S)&amp;=\\min_{x \\in S} {C(x|S)+f(S_x’)}<br>\\\\ &amp;=\\min{C(x_1|S)+f(S_{x_1}’), C(x_2|S)+f(S_{x_2}’),…,C(x_m|S)+f(S_{x_m}’)} \\end{aligned}$$<br>其中 $S={x_1,x_2,…,x_m}$，$S_x’$ 是选择 x 之后的下一状态，与其计算所有的 $C(x|S)+f(S_x’)$ 值，我们不如逐步逼近最终值<br>$$f(S)=\\min{\\min{…\\min{C(x_1|S)+f(S_{x_1}’), C(x_2|S)+f(S_{x_2}’)},…},C(x_m|S)+f(S_{x_m}’)} \\quad (1.21)$$<br>注意 $C(x_i|S)$ 中状态全部是 S，并且所有的 $f(S_{x_i}’)$ 需要提前全部计算出来。</p>\n<p>带阶形式的 DPFE 为<br>$$f(k,S)=\\min_x {C(x|k,S) + f(k-1,S_x’)} \\qquad(1.22)$$<br>其中 k 表示阶段，这里 k 可能不太好理解，我们可以想象从状态 S 经过恰好 k 次变换到达终止态 T，下文最短路径问题中会借鉴式 (1.22)，彼时再回过头来理解可能更加容易些。序列 $f(0,S),f(1,S),…$ 近似于 $f(S)$，序列最小值为 $f(S)$，但这个序列不保证单调性（比如振荡逼近），所以 $f(S)=\\min_k {f(k,S)}$，注意 $f(k,S)$ 不是 $f(k-1,S)$ 的函数，而是 $f(k-1,S_x’)$ 的函数。既然 $f(k,S)$ 序列不一定单调，我们定义一个新的序列 $F(k,S)$ 来保证单调性，<br>$$F(k,S)=\\min{F(k-1,S), \\min_x {C(x|k,S)+F(k-1,S_x’)}} \\quad(1.23)$$<br>这里我们可以将 $F(k,S)$ 理解为从状态 S 到终止态 T 最多变换 k 次的损失，与式 (1.22) 中恰好变换 k 次是有区别的。 </p>\n<h3 id=\"最短路径问题\"><a href=\"#最短路径问题\" class=\"headerlink\" title=\"最短路径问题\"></a>最短路径问题</h3><p>在解决前述的线性搜索问题中，我们使用了状态转换图模型，不难发现解决这种线性搜索问题等价于在图中搜索最短路径。</p>\n<p>考虑到有环图的复杂性，我们先讨论无环图。对于一个无环图，从起点 s 到终点 t 的最短路径使用 DPFE 可表示为<br>$$f(p)=\\min_q {b(p,q)+f(q)} \\qquad(1.24)$$<br>其中 b(p,q) 表示从 p 到 q 的距离，q 是 p 的直接邻点，f(p) 表示从 p 到 t 的最短路径，基本态条件为 f(t)=0。如果 q 不是 p 的直接邻点，那么可以认为 $b(p,q)=\\infty$。无环图中，要计算 f(p) 则需要先计算 f(q)。采用自底向上的方式计算，具体不展开。</p>\n<p>在有环图中，p 和 q 可能互为后继节点，f(p) 和 f(q) 互相依赖。为了方便，我们假定有环图中没有自环，这样假设是有原因的，如果有自环，即一条从 p 到 p 的分支，考虑以下三种情况：</p>\n<ol>\n<li>b(p,p) &gt; 0，这条分支会被忽略，因为会增加距离</li>\n<li>b(p,p) &lt; 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，从而一直缩短距离</li>\n<li>b(p,p) = 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，却不改变距离</li>\n</ol>\n<p>所以可以忽略掉自环，以下所讨论的有环图中均没有自环。对于一个有环图，DPFE 为<br>$$f(p) = \\min_q {b(p,q)+f(q)} \\qquad(1.25)$$<br>其中 f(q) 可能依赖于 f(p)，做特殊处理：初始时令 $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$。式 (1.25) 与 (1.24) 一样，因为都是解最短路径模型，只是有环图中进行求解的时候需要做特殊处理。</p>\n<p>举例说明，如图 1.2，<br><img src=\"/images/DP1_fig1.png\" alt=\"\"></p>\n<p>根据式 (1.25) 计算过程如下<br>$$\\begin{aligned}f(s)&amp;=\\min {b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)}=\\min {3+f(x),5+f(y),\\infty+f(t)}<br>\\\\f(x)&amp;=\\min {b(x,y)+f(y),b(x,t)+f(t)}=\\min {1+f(y),8+f(t)}<br>\\\\f(y)&amp;=\\min {b(y,x)+f(x),b(y,t)+f(t)}=\\min {2+f(x),5+f(t)}<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>显然 f(x) 与 f(y) 互相依赖。</p>\n<p>初始化时假设 $f(s)=f(x)=f(y)=\\infty$，第一次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+\\infty,5+\\infty,\\infty+f(t)}=\\infty<br>\\\\f(x)&amp;=\\min {1+\\infty,8+0}=8<br>\\\\f(y)&amp;=\\min {2+\\infty,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$</p>\n<p>第二次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+8,5+5,\\infty+0}=10<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+8,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>第三次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+6,5+5,\\infty+0}=9<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+6,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>由于第三次迭代 $f(x),f(y),f(t)$ 均未改变，故第三次迭代后计算到的 f(s) 就是最终 f(s)。</p>\n<p>还可以利用带阶 Relaxation 解决有环图问题，仿照式 (1.22)，我们针对最短路径模型改为 DPFE 为<br>$$f(k,p)=\\min_q {b(p,q)+f(k-1,q)} \\qquad(1.26)$$<br>其中 f(k,p) 表示从 p 到 t 的最短距离，k 表示 p 到 t 的某个路径需要走恰好 k 步。于是基本条件为： $f(0,t)=0;f(k,t)=\\infty, k&gt;0;f(0,p)=\\infty, \\forall p \\ne t$，这表示 t 到 t 走 0 步，代价为 0，走大于 0 步，代价为 $\\infty$，因为既然到了 t 点，我们不希望再继续走下去。p 到 t 走 0 步，代价为 $\\infty$，这驱使我们从 p 点走出去。还以上面的例子说明，根据式 (1.26) 计算过程为<br>$$\\begin{aligned}f(k,s)&amp;=\\min {b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)}<br>\\\\f(k,x)&amp;=\\min {b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)}<br>\\\\f(k,y)&amp;=\\min {b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)}<br>\\\\f(k,t) \\end{aligned}$$<br>根据基本条件，也就是 k=0 的初始条件，第一次迭代，<br>$$\\begin{aligned}f(1,s)&amp;=\\min {3+f(0,x),5+f(0,y),\\infty+f(0,t)}=\\infty<br>\\\\f(1,x)&amp;=\\min {1+f(0,y),8+f(0,t)}=8<br>\\\\f(1,y)&amp;=\\min {2+f(0,x),5+f(0,t)}=5<br>\\\\f(1,t)&amp;=\\infty \\end{aligned}$$<br>第二次迭代，<br>$$\\begin{aligned}f(2,s)&amp;=\\min {3+f(1,x),5+f(1,y),\\infty+f(1,t)}=10<br>\\\\f(2,x)&amp;=\\min {1+f(1,y),8+f(1,t)}=6<br>\\\\f(2,y)&amp;=\\min {2+f(1,x),5+f(1,t)}=10<br>\\\\f(2,t)&amp;=\\infty \\end{aligned}$$<br>第三次迭代，<br>$$\\begin{aligned}f(3,s)&amp;=\\min {3+f(2,x),5+f(2,y),\\infty+f(2,t)}=9<br>\\\\f(3,x)&amp;=\\min {1+f(2,y),8+f(2,t)}=11<br>\\\\f(3,y)&amp;=\\min {2+f(1,x),5+f(2,t)}=8<br>\\\\f(3,t)&amp;=\\infty \\end{aligned}$$<br>假设图中总共有 N 个节点，那么从任意一点 p 到 t 可以走恰好 k 步，k 取值为 {0,1,…,N-1}，k 不能大于等于 N，否则就存在某个节点经过两次，从而路径中存在环 circle，在任意两点路径均大于 0 的情况下，显然有环的路径不可能是最短路径。这里的例子中，N=4，所以 k 最大为 3，经过三次迭代后，就没必要再迭代下去了，否则路径中存在环，迭代下去的 f 值只会越来越大。于是根据 $f(p)=\\min_k {f(k,p)}$ 得<br>$$\\begin{aligned}f(s)&amp;=\\min {\\infty,\\infty,10,9}=9<br>\\\\f(x)&amp;=\\min {\\infty,8,6,11}=6<br>\\\\f(y)&amp;=\\min {\\infty,5,10,8}=5<br>\\\\f(t)&amp;=\\min {0,\\infty,\\infty,\\infty}=0 \\end{aligned}$$<br>从以上求解过程不难发现，迭代计算结果序列并不收敛，这也说明了式 (1.22) 中 $f(k,S)$ 序列不单调性。</p>\n<p>既然 $f(k,S)$ 序列不单调，我们参考式 (1.23) 为最短路径问题改写合适的 DPFE，如下<br>$$F(k,p)=\\min {F(k-1,p), \\ \\min_q {b(p,q)+F(k-1,q)}} \\qquad(1.27)$$</p>\n<p>其中 $F(k,p)$ 表示从 p 到 t 最多走 k 步的最短路径。显然要获得全局最短路径，我们必须考虑从 p 到 t 最多走 $N-1$ 步的最短路径，即目标是计算 $F(N-1,s)$，k 最大为 $N-1$，超过则路径出现环。基本条件不难得知为 $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$，这个不用过多解释了，相信现在大家都能理解。</p>\n<p>还有其他形式的 DPFE，由于篇幅有限就不一一介绍，待后面分析具体例子的时候再穿插说明。</p>\n","site":{"data":{}},"excerpt":"<p>我计划开启一系列动态规划的方法介绍，主要参考 《Dynamic Programming · A Computational Tool》这本书。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>动态规划用于解决一类优化问题，顺序地做出决策，每一次决策使得问题转变为对一个子问题的优化，直到问题解决，这个决策序列就是原始问题的最优解。我们也可以将 问题/子问题 看作状态，决策就是状态间的转移，问题得到解决就对应着<code>结束状态</code>。</p>","more":"<h2 id=\"优化原理\"><a href=\"#优化原理\" class=\"headerlink\" title=\"优化原理\"></a>优化原理</h2><blockquote>\n<p>一个最优决策满足：无论问题初始状态和初始决策是什么，剩余的决策序列构成做出初始决策之后问题状态的最优解。</p>\n</blockquote>\n<p>以最短路径问题为例说明：在一个指定了起点和终点的有权路径图中，无论之前选择了什么路径，我们必须保证从当前节点起，剩余的路径选择必须是最优的。<del>这是一种递归的处理方式，要解这个问题，直觉告诉我们似乎可以采用逆推法（隐马尔可夫预测问题的求解），这个直觉很重要，因为下文将会用到它。</del></p>\n<h3 id=\"顺序决策过程\"><a href=\"#顺序决策过程\" class=\"headerlink\" title=\"顺序决策过程\"></a>顺序决策过程</h3><p>考虑具有如下形式的优化问题：$opt_{d \\in \\Delta} {H(d)}$，其中 d 为决策，决策空间为 $\\Delta$，H 为目标函数，H(d) 的最优解记为 $d^{\\ast}$：$d^{\\ast}=\\arg opt_d {H(d)}$。动态规划问题则要求寻找有序决策集 ${d_1,…,d_n}$，使得目标函数 $h(d_1,…,h_n)$ 取得最优 $H^{\\ast}$。</p>\n<p>我们可以枚举 ${d_1,…,d_n}$ 所有可能的取值，然后代入目标函数进行计算，这就是 “暴力” 解法，但是这只在决策空间较小时有效，在决策空间很大时，这种方法效率非常低不可取，所以此时我们需要按顺序做出决策 $d_1,…,d_n$，使得<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,…,d_n)\\in \\Delta} {h(d_1,…,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2}{…{opt_{d_n \\in D_n}{h(d_1,…,d_n)}}…}} \\quad(1.1)\\end{aligned}$$</p>\n<p>其中 $(d_1,…,d_n) \\in \\Delta=D_1 \\times … \\times D_n$。通常，第 i 个决策空间依赖于前面所有的决策：$d_i \\in D_i(d_1,…,d_{i-1})$，于是式 (1.1) 可改写为<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,…,d_n)\\in \\Delta} {h(d_1,…,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}} \\quad(1.2)\\end{aligned}$$</p>\n<p>式 (1.2) 的优化操作是一个嵌套结构，可以由内向外解决问题，解最内层的优化问题，此时前序所有决策均看作已知，可得到最优 $d_n$，记作 $d_n^{\\ast}(d_1,…,d_{n-1})$，可以将 $d_n^{\\ast}$ 看作是其前序决策的函数。向外逆推，直到解出最外层优化问题 $opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast},…,d_n^{\\ast} }$ 的解 $d_1^{\\ast}$。</p>\n<p>如果改变决策顺序目标函数的最优解相同，如<br>$$\\begin{aligned} &amp;opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)} {… {opt_{d_n \\in D_n(d_1,…,d_{n-1})} {h(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_n \\in D_n} {opt_{d_{n-1} \\in D_{n-1}(d_n)} {… {opt_{d_1 \\in D_n(d_2,…,d_n)} {h(d_1,…,d_n)}}…}} \\quad(1.3) \\end{aligned}$$</p>\n<p>那么决策空间 $D_i$ 可能会跟之前有所不同，因为此时 $D_i$ 依赖于 $(d_{i+1},…,d_n)$，所以问题求解的效率会随着决策顺序的变化而有所改变。</p>\n<p>回到前面式 (1.2)，假设我们暂时做出最外层决策 $d_1$，此时 $d_1$ 是否是最优决策还尚未可知，但是根据前面所说的优化原理，无论之前做出什么决策，之后的决策需要保证是最优的，所以有<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}}<br>\\\\ &amp;=opt_{d_1 \\in D_1}{h(d_1,d_2^{\\ast}(d_1),…,d_n^{\\ast}(d_1)} \\qquad \\qquad(1.4) \\end{aligned}$$</p>\n<p>其中 $d_i^{\\ast}(d_1), \\ i&gt;1$ 可以看作是输入参数 $d_1$ 的偏函数（partial function）。最外层决策的最优解则为 $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast}(d_1),…,d_n^{\\ast}(d_1))}$，可以看出，$d_1$ 与其后序的决策互相耦合，要解这样的问题还是有点棘手。</p>\n<h4 id=\"目标函数可分\"><a href=\"#目标函数可分\" class=\"headerlink\" title=\"目标函数可分\"></a>目标函数可分</h4><p>一种方法是将 $d_1$ 决策与 $d_2,…,d_n$ 决策独立开来，也就是说通过解决形如 $opt_{d_1 \\in D_1}{H’(d_1)}$ 问题的最优解以得到 $d_1$ 的最佳决策，这是一种贪心算法，这种算法在局部最优 $opt_{d_1}{H’(d_1)}$ 与全局最优 $H^{\\ast}$ 一致的情况下是有效的。我们先分析这种特殊情况，因为它足够简单：假设目标函数 h 强可分，即<br>$$h(d_1,…,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ … \\circ C_n(d_n) \\qquad (1.5)$$<br>其中 $C_i$ 是决策 $d_i$ 关联的决策损失函数，$\\circ$ 是某种关联二元操作（例如 加 或 乘）并具有属性<br>$$opt_d{a \\circ C(d)}=a \\circ opt_d{C(d)}$$<br>其中 a 不依赖于决策 d。在序列决策过程中，损失 $C_n$ 不仅依赖于 $d_n$，还依赖于当前问题所处的状态 $(d_1,d_2,…,d_{n-1})$，所以改写上式为<br>$$h(d_1,…,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1}) \\qquad(1.6)$$</p>\n<p>定义如果目标函数 h 满足<br>$$h(d_1,…,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ … \\circ C_n(d_1,…,d_n) \\qquad(1.7)$$<br>那么称其弱可分（强可分是弱可分的一种特殊情况），此时有<br>$$\\begin{aligned} &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{h(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{…{opt_{d_n \\in D_n(d_1,…,d_{n-1})}{C_1(d_1) \\circ C_2(d_1,d_2) \\circ … \\circ C_n(d_1,…,d_n)}}…}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}{C_2(d_1,d_2) \\circ … \\circ {opt_{d_n \\in D_n(d_1,…,d_{n-1})}{C_n(d_1,…,d_n)}…}} \\qquad(1.8) \\end{aligned}$$<br>最后一个等式根据 $\\circ$ 操作的属性推导。</p>\n<p>令 $f(d_1,…,d_n)$ 表示序列决策过程在已经做出决策 $d_1,…,d_{i-1}$ 时的最优解，即<br>$$f(d_1,…,d_{i-1})=opt_{d_i}{opt_{d_{i+1}}{… {opt_{d_n} {C_i(d_i|d_1,…,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,…,d_i) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1}) }}…}} \\ (1.9)$$<br>其中因为篇幅起见省略了每个决策的取值空间 $D_i$。现在序列决策过程可表示为<br>$$\\begin{aligned}f(\\emptyset)&amp;=opt_{d_1}{opt_{d_2}{…{opt_{d_n}{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ … \\circ C_n(d_n|d_1,…,d_{n-1})}}…}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ opt_{d_2}{C(d_2|d_1) \\circ…\\circ opt_{d_n}{C_n(d_n|d_1,…,d_{n-1})}…}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ f(d_1)}  \\qquad \\quad (1.10) \\end{aligned}$$<br>一般地，我们有<br>$$f(d_1,…,d_{i-1})=opt_{d_i \\in D_i(d_1,…,d_{i-1})}{C_i(d_i|d_1,…,d_{i-1})\\circ f(d_1,…,d_i)} \\qquad(1.11)$$</p>\n<p>于是我们得到问题的递归函数形式，这就是优化问题的动态规划函数方程（DPFE）。</p>\n<h3 id=\"DPFE\"><a href=\"#DPFE\" class=\"headerlink\" title=\"DPFE\"></a>DPFE</h3><p>求解 DPFE 中的 $f(d_1,…,d_{i-1})$，定义状态 $S=(d_1,…,d_{i-1})$，那么 $i=|S|+1=|{d_1,…,d_{i-1}}|+1$，于是可以改写 DPFE 为以下形式，<br>$$f(S)=opt_{d_i \\in D_i(S)}{C_i(d_i|S) \\circ f(S’)} \\qquad (1.12)$$<br>其中 $S’=(d_1,…,d_i)$ 是下一状态，$\\emptyset$ 是初始状态。记状态空间为 $\\mathcal S$，由于 DPFE 是递归的，要终止递归，则要求具备一些基本情况（或称边界条件），例如 $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$，对于某个基本（终止）态 $S_0$，$f(S_0)$ 不使用 DPFE 计算其值，而是有一个给定的常数值 b，这就表示到达基本态时，递归结束。</p>\n<p>值得注意的是决策序列的长度 n 并非固定，当决策使得目标到达基本态时结束决策过程。前面我们定义状态 S 表示已经做过的决策，下一决策 d 则从 D(S) 中进行选择，但实际上为了表示方便，直接定义状态 S 为下一决策 d 的可选决策空间，即 $d \\in S$，于是 DPFE 变为<br>$$f(S)=opt_{d \\in S} {C(d|S) \\circ f(S’)} \\qquad (1.13)$$</p>\n<p>我们可以使用状态转移系统或者有向图对简单序列决策过程进行建模，状态 S 为节点，决策 d 使得状态从 S 转移到 S’，D(S) 表示处于状态 S 时的决策空间。考虑使用有向图建模，节点表示 DPFE 的状态，边表示状态间的转移，转移对应着决策，边标记为 b(S,S’)，表示决策 d 的损失 C(d|S)，其中下一状态 $S’=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$，T 是转移函数，于是 DPFE 转变为<br>$$f(S)=opt_S{b(S,S’) \\circ f(S’)} \\qquad(1.14) $$</p>\n<p>DPFE 的反转形式为<br>$$f’(S)=opt_{S’}{f’(S’) \\circ b(S’,S)} \\qquad(1.15) $$<br>其中 f’(S) 表示从基本态 $S_0$ 到状态 S 的最优解，注意与前面 f(S) 表示从状态 S 到基本态 $S_0$ 的最优解区分开来。式 (1.14) 为后向形式（backward），式 (1.15) 为前向形式（forward）。</p>\n<h3 id=\"动态规划的基本要素\"><a href=\"#动态规划的基本要素\" class=\"headerlink\" title=\"动态规划的基本要素\"></a>动态规划的基本要素</h3><p>动态规划的基本形式为<br>$$f(S)=opt_{d \\in D(S)} {R(S,d) \\circ f(T(S,d))}  \\quad (1.16)$$<br>其中，S 表示状态空间 $\\mathcal S$ 中的某个状态，d 是决策空间 D(S) 中的某个决策，R(S,d) 是收益函数（或称损失函数，记为 C(d|S)，收益对应最大化，损失对应最小化），T(S,d) 是转移函数，$\\circ$ 是二元操作符。为简单起见，我们只考虑离散情况。</p>\n<h3 id=\"线性搜索\"><a href=\"#线性搜索\" class=\"headerlink\" title=\"线性搜索\"></a>线性搜索</h3><p>现在我们来看一个实际例子。问题是需要排列一个长度为 N 的数组 A 中的元素，元素 x 具有概率 $p_x$，通过最小化排列的损失来优化线性搜索过程，例如 A={a,b,c}，且 $p_a=0.2,p_b=0.5,p_c=0.3$，于是共有 6 中排列方式 abc,acb,bac,bca,cab,cba（注意：每个排列均代表一种决策序列），其中 bca 排列的损失为 1.7，计算如下：</p>\n<ol>\n<li>Strong separable，方法 S<br>$1p_b+2p_c+3p_a$</li>\n<li>Weak separable，方法 W<br>$(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$</li>\n</ol>\n<p>最优排列问题可看作是序列决策过程，每个决策用于决定将 A 的元素置于排列后 A’ 的哪个位置上。决策的损失可互相独立（强可分），即方法 S 损失定义为：元素 x 的损失为 $ip_x$，其中 i 为 x 在 A’ 中位置；对于 W 方法，决策的损失依赖于决策的顺序，或者说依赖于决策空间，如果以从 A’ 的开始到最后这样的顺序进行决策，还以 bca 排列为例说明，第一次决策的空间为 {a,b,c}，第一次决策选择 b 置于 A’ 第一个位置，然后第二次决策空间为 {a,c}… 依次类推，决策损失与决策空间相关，定义为决策空间中各元素概率之和 $\\sum_{x\\in D_i} p_x$。</p>\n<p>假设决策顺序为 i=1,2,3，那么对应的决策空间为 $D_1=A,D_2=A-{d_1},D_3=A-{d_1,d_2}$，方法 S 的目标函数为 $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$，于是有<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{1p_{d_1}+2p_{d_2}+3p_{d_3}}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{1p_{d_1}+\\min_{d_2 \\in A-{d_1}}{2p_{d_2}+\\min_{d_3\\in A-{d_1,d_2}}{3p_{d_3}}}} \\end{aligned}$$</p>\n<p>方法 W 的目标函数为 $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x$，于是有<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-{d_1}}{\\sum_{x\\in A-{d_1}}p_x+\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A-{d_1,d_2}}p_x}}} \\end{aligned}$$</p>\n<p>有了损失的计算方法之后，<strong>线性搜索</strong> 就是依次计算并比较所有排列的损失，具有最小损失的就是最优排列。经过计算发现，bca 就是最佳排列。</p>\n<h3 id=\"问题的表示和解\"><a href=\"#问题的表示和解\" class=\"headerlink\" title=\"问题的表示和解\"></a>问题的表示和解</h3><p>上一小节中的例子可以使用 DP 求解。定义状态 S 为元素的集合，可从中选择决策来确定哪个元素应该放置在 A’ 中的哪个位置。DPFE 的形式如下<br>$$f(S)=\\min_{x \\in S} {C(x|S)+f(S-{x})}     \\quad(1.17)$$<br>其中 $S \\in 2^A$，$2^A$ 是 A 的幂集。基本态 $f(\\emptyset)=0$，我们目标是要求出 $f(A)$。</p>\n<p>注意这是前向形式的 DPFE。要写成这是后向形式的 DPFE，则根据式 (1.15) 改写如下<br>$$f(S)=\\min_{S’} {C(x|S’)+f(S’)}     \\quad(1.18)$$<br>其中 $S \\in 2^A$，此时我们目标是求 $f(\\emptyset)$，基本态 $f(A)=0$。S’ 是 S 的前导状态，从前导状态 S’ 经过决策 x 到达状态 S。</p>\n<p>基于方法 W，损失函数为<br>$$C_W(x|S)=\\sum_{y\\in S}p_y$$<br>基于上一小节的讨论可知，当前的损失与当前决策 x 无关，而依赖于临决策之前的状态 S 有关，即 S 中各元素的概率和。</p>\n<p>基于方法 S，那么损失函数为<br>$$C_S(x|S)=(N+1-|S|)p_x$$<br>可见此时损失只与当前决策 x 以及决策的序号有关，其中与序号有关是假定了第一个决策确定某元素置于 A’ 的第一个位置，第二个决策确定 A’ 第二个位置上的元素…依次类推。如果决策顺序反过来，即第一个决策确定 A’ 最后一个位置上的元素，那么损失函数需要修改为 $C_S’(x|S)=|S|p_x$。如果将 S 中元素按概率降序排列，事实表明第一个元素（具有最大概率值）将会最小化 $C(x|S)+f(S-{x})$，这是一种贪心策略，即只求当前状态下的最优解。事实上确实存在一类 DP 问题用贪心策略也可解决，这个我们暂时不讨论。</p>\n<p>现在我们来看下如何解 DP 问题，以式 (1.17) 表示的 DPFE 为例进行说明：<br>$$\\begin{aligned} f({a,b,c}) &amp;= \\min{C(a|{a,b,c})+f({b,c}), C(b|{a,b,c})+f({a,c}), C(c|{a,b,c})+f({a,b})}<br>\\\\ f({b,c}) &amp;= \\min{C(b|{b,c}+f({c}),C(c|{b,c}+f({b})}<br>\\\\ f({a,c}) &amp;= \\min{C(a|{a,c}+f({c}),C(c|{a,c}+f({a})}<br>\\\\ f({a,b}) &amp;= \\min{C(a|{a,b}+f({b}),C(c|{a,b}+f({a})}<br>\\\\ f({c}) &amp;= \\min {C(c|{c})+f(\\emptyset)}<br>\\\\ f({b}) &amp;= \\min {C(b|{b})+f(\\emptyset)}<br>\\\\ f({a}) &amp;= \\min {C(a|{a})+f(\\emptyset)}<br>\\\\ f(\\emptyset) &amp;= 0 \\end{aligned}$$<br>其中损失函数可以分别使用方法 S 和方法 W 进行代入计算，这里略。</p>\n<p>再以式 (1.18) 表示的 DPFE 进行说明：<br>$$\\begin{aligned} f({a,b,c}) &amp;= 0<br>\\\\ f({b,c}) &amp;= \\min{C(a|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,c}) &amp;= \\min{C(b|{a,a,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,b}) &amp;= \\min{C(c|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({c}) &amp;= \\min {C(a|{a,c})+f({a,c}), C(b|{b,c})+f({b,c})}\\stackrel W = \\min {0.5+1.0,0.8+1.0}=1.5<br>\\\\ f({b}) &amp;= \\min {C(a|{a,b})+f({a,b}), C(c|{b,c})+f({b,c})}\\stackrel W = \\min {0.7+1.0,0.8+1.0}=1.7<br>\\\\ f({a}) &amp;= \\min {C(b|{a,b})+f({a,b}), C(c|{a,c})+f({a,c})}\\stackrel W = \\min {0.7+1.0,0.5+1.0}=1.5<br>\\\\ f(\\emptyset) &amp;= \\min {C(a|a)+f({a}), C(b|b)+f({b}), C(c|c)+f({c})}\\stackrel W = \\min {0.2+1.5,0.5+1.7,0.3+1.5}=1.7 \\end{aligned}$$<br>以上式中 $\\stackrel W=$ 之后部分均表示使用方法 W 进行计算，这是为了演示，方法 S 的代入计算略。</p>\n<h3 id=\"带阶决策\"><a href=\"#带阶决策\" class=\"headerlink\" title=\"带阶决策\"></a>带阶决策</h3><p>第一次决策记为阶 1，第二次决策记为阶 2… 依次类推。将阶号包含进状态 S 的定义中有时候会带来方便甚至是非常有必要的。还以前面的例子进行说明，DPFE 形式 (1.17) 可改写为<br>$$f(k,S)=\\min_{x \\in S} {C(x|k,S)+f(k+1,S-{x})} \\quad(1.19)$$</p>\n<h3 id=\"Path-States\"><a href=\"#Path-States\" class=\"headerlink\" title=\"Path-States\"></a>Path-States</h3><p>状态 S 可以定义为当前已经做过的决策 $(d_1,…,d_{i-1})$，在有向图中用节点表示状态，状态 S 与初始态 $\\emptyset$ 到状态 S 之间的路径有关联（其实这个路径也可以表示为 $(d_1,…,d_{i-1})$），需要特别注意的是，前面的状态 S 是无序的，比如 S={a,b}，表示当前已经做出了决策 a 和 b（或者说做出决策选择了 a 和 b，读者根据具体语境进行调整理解），至于决策 a 和 b 的顺序则未定义，而这里 Path-States 中的状态是有序的，S=(a,b)，表示当前已经做出两个决策，第一个决策是 a 且第二个决策是 b。于是此时 DPFE 的形式可写为<br>$$f(S)=\\min_{x \\notin S} {C(x|S)+f(S + (x))} \\qquad(1.20)$$</p>\n<p>前面线性搜索一节中的例子计算过程为<br>$$\\begin{aligned} f(\\emptyset) &amp;= \\min {C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)}\\stackrel S= \\min{0.2+1.9,0.5+1.2,0.3+1.6}=1.7<br>\\\\ f(a) &amp;= \\min {C(b|a)+f(ab), C(c|a)+f(ac)}\\stackrel S= \\min{2<em>0.5+0.9,2</em>0.3+1.5}=1.9<br>\\\\ f(b) &amp;= \\min {C(a|b)+f(ba), C(c|b)+f(bc)}\\stackrel S= \\min{2<em>0.2+0.9,2</em>0.3+0.6}=1.2<br>\\\\ f(c) &amp;= \\min {C(a|c)+f(ca), C(b|c)+f(cb)}\\stackrel S= \\min{2<em>0.2+1.5,2</em>0.5+0.6}=1.6<br>\\\\ f(ab) &amp;= \\min {C(c|ab)+f(abc)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(ac) &amp;= \\min {C(b|ac)+f(acb)}\\stackrel S= 3</em>0.5=1.5<br>\\\\ f(ba) &amp;= \\min {C(c|ba)+f(bac)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(bc) &amp;= \\min {C(a|bc)+f(bca)}\\stackrel S= 3</em>0.2=0.6<br>\\\\ f(ca) &amp;= \\min {C(b|ca)+f(cab)}\\stackrel S= 3<em>0.5=1.5<br>\\\\ f(cb) &amp;= \\min {C(a|cb)+f(cba)} \\stackrel S= 3</em>0.2=0.6<br>\\\\ f(abc) &amp;= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$<br>由于状态是有序的，所以共有 $N!$ 个基本态，损失函数依然可以使用方法 S 和方法 W 计算，例如使用方法 S，损失 $C(c|ab)$ 表示第三次决策使用 c，那么根据方法 S 的定义有 $C(c|ab)=3p_c=3*0.3=0.9$。上面计算中 $\\stackrel S=$ 之后部分均表示使用方法 S 进行计算。方法 W 的计算略。</p>\n<h3 id=\"松弛-Relaxation\"><a href=\"#松弛-Relaxation\" class=\"headerlink\" title=\"松弛 Relaxation\"></a>松弛 Relaxation</h3><p>松弛在数学中指通过某种迭代方法逐步得到更好的近似解。考虑一个有限集合 ${a_1,…,a_N}$，其最小值可以通过计算结对元素最小化来求解<br>$$x^{\\ast}=\\min{\\min{…{\\min{a_1,a_2},a_3},…},a_N}$$<br>偏最小化序列为 $x_1=a_1, x_2=\\min{x_1,a_2},…$，为了表示的统一，可以令 $x_1=\\min{x_0,a_1}, x_0=\\infty$。序列 $x_1,x_2,…$ 逐步逼近并最终达到 $x^{\\ast}$。松弛就是刻画这种逐步逼近的特性。借助松弛的思想，动态规划问题的 DPFE 可表示为<br>$$\\begin{aligned} f(S)&amp;=\\min_{x \\in S} {C(x|S)+f(S_x’)}<br>\\\\ &amp;=\\min{C(x_1|S)+f(S_{x_1}’), C(x_2|S)+f(S_{x_2}’),…,C(x_m|S)+f(S_{x_m}’)} \\end{aligned}$$<br>其中 $S={x_1,x_2,…,x_m}$，$S_x’$ 是选择 x 之后的下一状态，与其计算所有的 $C(x|S)+f(S_x’)$ 值，我们不如逐步逼近最终值<br>$$f(S)=\\min{\\min{…\\min{C(x_1|S)+f(S_{x_1}’), C(x_2|S)+f(S_{x_2}’)},…},C(x_m|S)+f(S_{x_m}’)} \\quad (1.21)$$<br>注意 $C(x_i|S)$ 中状态全部是 S，并且所有的 $f(S_{x_i}’)$ 需要提前全部计算出来。</p>\n<p>带阶形式的 DPFE 为<br>$$f(k,S)=\\min_x {C(x|k,S) + f(k-1,S_x’)} \\qquad(1.22)$$<br>其中 k 表示阶段，这里 k 可能不太好理解，我们可以想象从状态 S 经过恰好 k 次变换到达终止态 T，下文最短路径问题中会借鉴式 (1.22)，彼时再回过头来理解可能更加容易些。序列 $f(0,S),f(1,S),…$ 近似于 $f(S)$，序列最小值为 $f(S)$，但这个序列不保证单调性（比如振荡逼近），所以 $f(S)=\\min_k {f(k,S)}$，注意 $f(k,S)$ 不是 $f(k-1,S)$ 的函数，而是 $f(k-1,S_x’)$ 的函数。既然 $f(k,S)$ 序列不一定单调，我们定义一个新的序列 $F(k,S)$ 来保证单调性，<br>$$F(k,S)=\\min{F(k-1,S), \\min_x {C(x|k,S)+F(k-1,S_x’)}} \\quad(1.23)$$<br>这里我们可以将 $F(k,S)$ 理解为从状态 S 到终止态 T 最多变换 k 次的损失，与式 (1.22) 中恰好变换 k 次是有区别的。 </p>\n<h3 id=\"最短路径问题\"><a href=\"#最短路径问题\" class=\"headerlink\" title=\"最短路径问题\"></a>最短路径问题</h3><p>在解决前述的线性搜索问题中，我们使用了状态转换图模型，不难发现解决这种线性搜索问题等价于在图中搜索最短路径。</p>\n<p>考虑到有环图的复杂性，我们先讨论无环图。对于一个无环图，从起点 s 到终点 t 的最短路径使用 DPFE 可表示为<br>$$f(p)=\\min_q {b(p,q)+f(q)} \\qquad(1.24)$$<br>其中 b(p,q) 表示从 p 到 q 的距离，q 是 p 的直接邻点，f(p) 表示从 p 到 t 的最短路径，基本态条件为 f(t)=0。如果 q 不是 p 的直接邻点，那么可以认为 $b(p,q)=\\infty$。无环图中，要计算 f(p) 则需要先计算 f(q)。采用自底向上的方式计算，具体不展开。</p>\n<p>在有环图中，p 和 q 可能互为后继节点，f(p) 和 f(q) 互相依赖。为了方便，我们假定有环图中没有自环，这样假设是有原因的，如果有自环，即一条从 p 到 p 的分支，考虑以下三种情况：</p>\n<ol>\n<li>b(p,p) &gt; 0，这条分支会被忽略，因为会增加距离</li>\n<li>b(p,p) &lt; 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，从而一直缩短距离</li>\n<li>b(p,p) = 0，问题本身不是定义良好的，可以不断的向路径中添加这条分支，却不改变距离</li>\n</ol>\n<p>所以可以忽略掉自环，以下所讨论的有环图中均没有自环。对于一个有环图，DPFE 为<br>$$f(p) = \\min_q {b(p,q)+f(q)} \\qquad(1.25)$$<br>其中 f(q) 可能依赖于 f(p)，做特殊处理：初始时令 $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$。式 (1.25) 与 (1.24) 一样，因为都是解最短路径模型，只是有环图中进行求解的时候需要做特殊处理。</p>\n<p>举例说明，如图 1.2，<br><img src=\"/images/DP1_fig1.png\" alt=\"\"></p>\n<p>根据式 (1.25) 计算过程如下<br>$$\\begin{aligned}f(s)&amp;=\\min {b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)}=\\min {3+f(x),5+f(y),\\infty+f(t)}<br>\\\\f(x)&amp;=\\min {b(x,y)+f(y),b(x,t)+f(t)}=\\min {1+f(y),8+f(t)}<br>\\\\f(y)&amp;=\\min {b(y,x)+f(x),b(y,t)+f(t)}=\\min {2+f(x),5+f(t)}<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>显然 f(x) 与 f(y) 互相依赖。</p>\n<p>初始化时假设 $f(s)=f(x)=f(y)=\\infty$，第一次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+\\infty,5+\\infty,\\infty+f(t)}=\\infty<br>\\\\f(x)&amp;=\\min {1+\\infty,8+0}=8<br>\\\\f(y)&amp;=\\min {2+\\infty,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$</p>\n<p>第二次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+8,5+5,\\infty+0}=10<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+8,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>第三次迭代，<br>$$\\begin{aligned}f(s)&amp;=\\min {3+6,5+5,\\infty+0}=9<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+6,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br>由于第三次迭代 $f(x),f(y),f(t)$ 均未改变，故第三次迭代后计算到的 f(s) 就是最终 f(s)。</p>\n<p>还可以利用带阶 Relaxation 解决有环图问题，仿照式 (1.22)，我们针对最短路径模型改为 DPFE 为<br>$$f(k,p)=\\min_q {b(p,q)+f(k-1,q)} \\qquad(1.26)$$<br>其中 f(k,p) 表示从 p 到 t 的最短距离，k 表示 p 到 t 的某个路径需要走恰好 k 步。于是基本条件为： $f(0,t)=0;f(k,t)=\\infty, k&gt;0;f(0,p)=\\infty, \\forall p \\ne t$，这表示 t 到 t 走 0 步，代价为 0，走大于 0 步，代价为 $\\infty$，因为既然到了 t 点，我们不希望再继续走下去。p 到 t 走 0 步，代价为 $\\infty$，这驱使我们从 p 点走出去。还以上面的例子说明，根据式 (1.26) 计算过程为<br>$$\\begin{aligned}f(k,s)&amp;=\\min {b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)}<br>\\\\f(k,x)&amp;=\\min {b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)}<br>\\\\f(k,y)&amp;=\\min {b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)}<br>\\\\f(k,t) \\end{aligned}$$<br>根据基本条件，也就是 k=0 的初始条件，第一次迭代，<br>$$\\begin{aligned}f(1,s)&amp;=\\min {3+f(0,x),5+f(0,y),\\infty+f(0,t)}=\\infty<br>\\\\f(1,x)&amp;=\\min {1+f(0,y),8+f(0,t)}=8<br>\\\\f(1,y)&amp;=\\min {2+f(0,x),5+f(0,t)}=5<br>\\\\f(1,t)&amp;=\\infty \\end{aligned}$$<br>第二次迭代，<br>$$\\begin{aligned}f(2,s)&amp;=\\min {3+f(1,x),5+f(1,y),\\infty+f(1,t)}=10<br>\\\\f(2,x)&amp;=\\min {1+f(1,y),8+f(1,t)}=6<br>\\\\f(2,y)&amp;=\\min {2+f(1,x),5+f(1,t)}=10<br>\\\\f(2,t)&amp;=\\infty \\end{aligned}$$<br>第三次迭代，<br>$$\\begin{aligned}f(3,s)&amp;=\\min {3+f(2,x),5+f(2,y),\\infty+f(2,t)}=9<br>\\\\f(3,x)&amp;=\\min {1+f(2,y),8+f(2,t)}=11<br>\\\\f(3,y)&amp;=\\min {2+f(1,x),5+f(2,t)}=8<br>\\\\f(3,t)&amp;=\\infty \\end{aligned}$$<br>假设图中总共有 N 个节点，那么从任意一点 p 到 t 可以走恰好 k 步，k 取值为 {0,1,…,N-1}，k 不能大于等于 N，否则就存在某个节点经过两次，从而路径中存在环 circle，在任意两点路径均大于 0 的情况下，显然有环的路径不可能是最短路径。这里的例子中，N=4，所以 k 最大为 3，经过三次迭代后，就没必要再迭代下去了，否则路径中存在环，迭代下去的 f 值只会越来越大。于是根据 $f(p)=\\min_k {f(k,p)}$ 得<br>$$\\begin{aligned}f(s)&amp;=\\min {\\infty,\\infty,10,9}=9<br>\\\\f(x)&amp;=\\min {\\infty,8,6,11}=6<br>\\\\f(y)&amp;=\\min {\\infty,5,10,8}=5<br>\\\\f(t)&amp;=\\min {0,\\infty,\\infty,\\infty}=0 \\end{aligned}$$<br>从以上求解过程不难发现，迭代计算结果序列并不收敛，这也说明了式 (1.22) 中 $f(k,S)$ 序列不单调性。</p>\n<p>既然 $f(k,S)$ 序列不单调，我们参考式 (1.23) 为最短路径问题改写合适的 DPFE，如下<br>$$F(k,p)=\\min {F(k-1,p), \\ \\min_q {b(p,q)+F(k-1,q)}} \\qquad(1.27)$$</p>\n<p>其中 $F(k,p)$ 表示从 p 到 t 最多走 k 步的最短路径。显然要获得全局最短路径，我们必须考虑从 p 到 t 最多走 $N-1$ 步的最短路径，即目标是计算 $F(N-1,s)$，k 最大为 $N-1$，超过则路径出现环。基本条件不难得知为 $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$，这个不用过多解释了，相信现在大家都能理解。</p>\n<p>还有其他形式的 DPFE，由于篇幅有限就不一一介绍，待后面分析具体例子的时候再穿插说明。</p>"},{"title":"PyTorch-2","p":"pytorch/PyTorch-2","date":"2019-06-13T02:19:52.000Z","_content":"# torch installization\n<!-- more -->\n依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是\n```\nimport torch\n```\n于是阅读 `torch/__init__.py`，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。__\n\n`__init__.py`除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用\n{\n  return initModule();\n}\n```\n根据python3扩展库的规则可知，`import torch._C` ，调用PyInit__C函数（调用名为PyInit_&lt;package>的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。\n\nshm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。\n\ninitModule方法定义在torch/csrc/Module.cpp，\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // 声明并定义模块初始化函数\n  // 向methods中添加方法定义\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // 真正的扩展模块定义\n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // 扩展模块名\n    nullptr,                           \n    -1,\n    methods.data()                       // 模块中的方法定义\n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // 创建模块并确保创建成功\n  // 对模块进行各种初始化\n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // 执行cuda相关的初始化\n#endif\n  ...\n  // 定义模块的属性设置函数，setter\n  // 属性名为name，值为v，incref表示是否对值对象增加引用计数\n  // 设置成功返回1，否则返回0\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // 设置模块属性\n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // 向模块添加方法\n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // 设置模块其他属性\n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  // 增加 _THNN 属性\n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\n从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。\n# methods/members in torch._C\n- 使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括\n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  同上类似\n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- 生成模块torch._C 后再向其添加如下成员：\n\n    - 向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。\n\n        torch._C._TensorBase这个类型具有属性\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        并且具有以下方法\n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        类型torch._C._FunctionBase， 这个类型具有方法和属性为\n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n        类型 torch._C._VariableFunctions 包含方法\n        ```python\n        arange\n        as_tensor\n        ...\n        empty       # 出现我们这里所讨论的 torch.empty\n        empty_like\n        ...\n        ```\n\n        不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。\n\n    - 向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。\n\n    - 向torch._C添加模块， _nn，cpp，_onnx。\n\n    - 向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。\n\n注意到从Module.cpp文件中头文件引用：\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\n可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。\n\n一方面，头文件 TH/TH.h 中引用了#include <TH/THGeneral.h>，在aten/src/TH目录下的CMakeLists.txt中有这么一行\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\n在THGeneral.h中有如下宏定义\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)\n```\n另一方面，torch/csrc/THP.h 中引用了#include <torch/src/Storage.h>，在这个Storage.h中有如下语句\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\n而文件TH/THGenerateAllType.h则包含语句\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有\n```\n// 此时 TH_GENERIC_FILE是已定义的\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义\n```\n以TH/THGenerateFloatType.h为例说明，此文件中有语句\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换\n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n类似地，#include <TH/THGenerateDoubleType.h>，则得到THPDoubleStorage_init，\n\n#include <TH/THGenerateIntTypes.h> 得到\n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n对4组include中的其他三组，则得到\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\n以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init 函数声明\n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\n上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\n在TH/THGeneral.h中存在宏定义\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\n由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为\"FloatStorageBase\"，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。\n\n以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\n即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。\n\n其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。\n\ntorch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。\n\n现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\n可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\n（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）\n\n然后查看类创建 THPStorage_(pynew) 的定义\n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数\n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // 分配内存，让self指向这个内存块\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // 获取参数allocator的值\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      // 转为 c10::Allocator 指针\n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // 提供了cdata值\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // 返回THPStorage指针\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // 未提供cdata值，则需要创建THWStorage类型实例\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     // 使用其他方法设置 self->cdata\n}   \n```\n从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义\n```\n#define THWStorage THStorage\n```\n转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n在 TH/generic/THStorage.h 中找到宏定义\n```\n#define THStorage at::StorageImpl\n```\n在 c10/core/StorageImpl.h 中找到结构定义\n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // 数据类型\n  DataPtr data_ptr_;             // 数据指针\n  int64_t numel_;                // 数据数量\n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // 数据的内存分配器\n};\n}\n```\n所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为\n```\nnew                // 新建THStorage，未指定 size，即size=0，使用默认Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // 新建THStorage，指定 size，使用默认Allocator\nnewWithAllocator   // 新建THStorage，指定 size 和 Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n此外有宏定义\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\n函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。\n\n（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）\n\n以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // 新建scalar_t 类型\n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\n从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\n我们看上上个代码片段中StorageImpl构造函数的实参，\n\n首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）\n```\n#define scalar_t float\n```\n于是，\n```\ncaffe2::TypeMeta::Make<scalar_t>()    // 假设 THQUANTIZED 未定义\n```\ncaffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n经过宏替换，得到 _typeMetaDataInstance的模板函数定义\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n还有一组宏，用于生成模板特例化，\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// 调用\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n对于系统内部变量如 float，得到函数模板特例化的定义\n```\n// 函数声明\nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\n另外，在c10/util/typeid.cpp中有如下调用\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n经过宏替换得到\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n于是函数模板特例化最终形式为，\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，\n```\nstruct TypeMetaData final {\n// 函数类型的别名\nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // 占位new\nusing Copy = void(const void*, void*, size_t);  // 类型数组拷贝\nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //构造函数\n\nsize_t itemsize_;  // 类型占多少字节\nNew* new_;\nPlacementNew* placementNew_;   // 定位放置 new\nCopy* copy_;        // 类型拷贝\nDelete* delete_;    // 类型析构\nTypeIdentifier id_; // 类型全局唯一id\nconst char* name_;  // 类型名称\n};\n```\n我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义\n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // 类型T占多少字节\n          _PickNew<T>(),             // 通过 new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // 获取类型的全局唯一id，\n          typeName};                 // 类型名称，例如float的名称为\"float\"\n```\n构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用\n```\nTypeIdentifier::Get<T>()\n```\n在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。\n\n对于其他类型如 int，double，int64_t等类似处理。\n\nPyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。\n\n至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为\n```\nsize,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）\ngetTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配\ntrue                      // 被构造的StorageImpl可以resize\n```\n创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase\n\n记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。\n\nFloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。\n\n类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\n函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。\n\n`torch._C`模块初始化过程到这里就完成了。回到 `torch/__init__.py`，继续看看 import torch时接下来做了哪些事情：\n\n1. 定义了模块函数 typename，is_tensor，is_storage等\n2. 导入torch下其他子模块\n3. 调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理\n4. 调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：\n    - 初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；\n    - 初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；\n    - 初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；\n    - 初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor\n    - 共享内存管理初始化，设置文件路径；\n    - 执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n        其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。\n\n到这里，import torch 的工作全部完成。\n\n# 后记：\n初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。","source":"_posts/pytorch/PyTorch-2.md","raw":"---\ntitle: PyTorch-2\np: pytorch/PyTorch-2\ndate: 2019-06-13 10:19:52\ntags: PyTorch\ncategories: DL Framework\n---\n# torch installization\n<!-- more -->\n依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是\n```\nimport torch\n```\n于是阅读 `torch/__init__.py`，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。__\n\n`__init__.py`除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用\n{\n  return initModule();\n}\n```\n根据python3扩展库的规则可知，`import torch._C` ，调用PyInit__C函数（调用名为PyInit_&lt;package>的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。\n\nshm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。\n\ninitModule方法定义在torch/csrc/Module.cpp，\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // 声明并定义模块初始化函数\n  // 向methods中添加方法定义\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // 真正的扩展模块定义\n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // 扩展模块名\n    nullptr,                           \n    -1,\n    methods.data()                       // 模块中的方法定义\n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // 创建模块并确保创建成功\n  // 对模块进行各种初始化\n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // 执行cuda相关的初始化\n#endif\n  ...\n  // 定义模块的属性设置函数，setter\n  // 属性名为name，值为v，incref表示是否对值对象增加引用计数\n  // 设置成功返回1，否则返回0\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // 设置模块属性\n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // 向模块添加方法\n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // 设置模块其他属性\n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  // 增加 _THNN 属性\n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\n从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。\n# methods/members in torch._C\n- 使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括\n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  同上类似\n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- 生成模块torch._C 后再向其添加如下成员：\n\n    - 向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。\n\n        torch._C._TensorBase这个类型具有属性\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        并且具有以下方法\n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        类型torch._C._FunctionBase， 这个类型具有方法和属性为\n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n        类型 torch._C._VariableFunctions 包含方法\n        ```python\n        arange\n        as_tensor\n        ...\n        empty       # 出现我们这里所讨论的 torch.empty\n        empty_like\n        ...\n        ```\n\n        不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。\n\n    - 向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。\n\n    - 向torch._C添加模块， _nn，cpp，_onnx。\n\n    - 向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。\n\n注意到从Module.cpp文件中头文件引用：\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\n可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。\n\n一方面，头文件 TH/TH.h 中引用了#include <TH/THGeneral.h>，在aten/src/TH目录下的CMakeLists.txt中有这么一行\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\n在THGeneral.h中有如下宏定义\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)\n```\n另一方面，torch/csrc/THP.h 中引用了#include <torch/src/Storage.h>，在这个Storage.h中有如下语句\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\n而文件TH/THGenerateAllType.h则包含语句\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有\n```\n// 此时 TH_GENERIC_FILE是已定义的\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义\n```\n以TH/THGenerateFloatType.h为例说明，此文件中有语句\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换\n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n类似地，#include <TH/THGenerateDoubleType.h>，则得到THPDoubleStorage_init，\n\n#include <TH/THGenerateIntTypes.h> 得到\n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n对4组include中的其他三组，则得到\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\n以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init 函数声明\n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\n上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\n在TH/THGeneral.h中存在宏定义\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\n由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为\"FloatStorageBase\"，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。\n\n以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\n即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。\n\n其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。\n\ntorch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。\n\n现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\n可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\n（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）\n\n然后查看类创建 THPStorage_(pynew) 的定义\n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数\n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // 分配内存，让self指向这个内存块\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // 获取参数allocator的值\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      // 转为 c10::Allocator 指针\n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // 提供了cdata值\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // 返回THPStorage指针\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // 未提供cdata值，则需要创建THWStorage类型实例\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     // 使用其他方法设置 self->cdata\n}   \n```\n从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义\n```\n#define THWStorage THStorage\n```\n转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n在 TH/generic/THStorage.h 中找到宏定义\n```\n#define THStorage at::StorageImpl\n```\n在 c10/core/StorageImpl.h 中找到结构定义\n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // 数据类型\n  DataPtr data_ptr_;             // 数据指针\n  int64_t numel_;                // 数据数量\n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // 数据的内存分配器\n};\n}\n```\n所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为\n```\nnew                // 新建THStorage，未指定 size，即size=0，使用默认Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // 新建THStorage，指定 size，使用默认Allocator\nnewWithAllocator   // 新建THStorage，指定 size 和 Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n此外有宏定义\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\n函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。\n\n（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）\n\n以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // 新建scalar_t 类型\n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\n从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\n我们看上上个代码片段中StorageImpl构造函数的实参，\n\n首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）\n```\n#define scalar_t float\n```\n于是，\n```\ncaffe2::TypeMeta::Make<scalar_t>()    // 假设 THQUANTIZED 未定义\n```\ncaffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n经过宏替换，得到 _typeMetaDataInstance的模板函数定义\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n还有一组宏，用于生成模板特例化，\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// 调用\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n对于系统内部变量如 float，得到函数模板特例化的定义\n```\n// 函数声明\nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\n另外，在c10/util/typeid.cpp中有如下调用\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n经过宏替换得到\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n于是函数模板特例化最终形式为，\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，\n```\nstruct TypeMetaData final {\n// 函数类型的别名\nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // 占位new\nusing Copy = void(const void*, void*, size_t);  // 类型数组拷贝\nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //构造函数\n\nsize_t itemsize_;  // 类型占多少字节\nNew* new_;\nPlacementNew* placementNew_;   // 定位放置 new\nCopy* copy_;        // 类型拷贝\nDelete* delete_;    // 类型析构\nTypeIdentifier id_; // 类型全局唯一id\nconst char* name_;  // 类型名称\n};\n```\n我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义\n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // 类型T占多少字节\n          _PickNew<T>(),             // 通过 new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // 获取类型的全局唯一id，\n          typeName};                 // 类型名称，例如float的名称为\"float\"\n```\n构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用\n```\nTypeIdentifier::Get<T>()\n```\n在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。\n\n对于其他类型如 int，double，int64_t等类似处理。\n\nPyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。\n\n至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为\n```\nsize,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）\ngetTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配\ntrue                      // 被构造的StorageImpl可以resize\n```\n创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase\n\n记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。\n\nFloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。\n\n类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\n函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。\n\n`torch._C`模块初始化过程到这里就完成了。回到 `torch/__init__.py`，继续看看 import torch时接下来做了哪些事情：\n\n1. 定义了模块函数 typename，is_tensor，is_storage等\n2. 导入torch下其他子模块\n3. 调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理\n4. 调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：\n    - 初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；\n    - 初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；\n    - 初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；\n    - 初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor\n    - 共享内存管理初始化，设置文件路径；\n    - 执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n        其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。\n\n到这里，import torch 的工作全部完成。\n\n# 后记：\n初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。","slug":"pytorch/PyTorch-2","published":1,"updated":"2020-04-24T10:34:28.998Z","_id":"ck9dzcjg9002ugga675qd6yrm","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1><a id=\"more\"></a>\n<p>依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n<p>于是阅读 <code>torch/__init__.py</code>，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags&#x3D;sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ +&#x3D; [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] !&#x3D; &#39;_&#39; and</span><br><span class=\"line\">            not name.endswith(&#39;Base&#39;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n<p><strong>将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。</strong></p>\n<p><code>__init__.py</code>除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   &#x2F;&#x2F; 在python脚本中，import _C 时调用</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>根据python3扩展库的规则可知，<code>import torch._C</code> ，调用PyInit__C函数（调用名为PyInit_&lt;package&gt;的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。</p>\n<p>shm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;&#x2F;lib&#x2F;$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n<p>从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>TORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。</p>\n<p>initModule方法定义在torch/csrc/Module.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       &#x2F;&#x2F; 模块中有关cuda部分的初始化函数声明</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 &#x2F;&#x2F; 声明并定义模块初始化函数</span><br><span class=\"line\">  &#x2F;&#x2F; 向methods中添加方法定义</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &#x2F;&#x2F; 真正的扩展模块定义</span><br><span class=\"line\">  static struct PyModuleDef torchmodule &#x3D; &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          &#x2F;&#x2F; 扩展模块名</span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       &#x2F;&#x2F; 模块中的方法定义</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module &#x3D; PyModule_Create(&amp;torchmodule)); &#x2F;&#x2F; 创建模块并确保创建成功</span><br><span class=\"line\">  &#x2F;&#x2F; 对模块进行各种初始化</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       &#x2F;&#x2F; 执行cuda相关的初始化</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &#x2F;&#x2F; 定义模块的属性设置函数，setter</span><br><span class=\"line\">  &#x2F;&#x2F; 属性名为name，值为v，incref表示是否对值对象增加引用计数</span><br><span class=\"line\">  &#x2F;&#x2F; 设置成功返回1，否则返回0</span><br><span class=\"line\">  auto set_module_attr &#x3D; [&amp;](const char* name, PyObject* v, bool incref &#x3D; true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) &#x3D;&#x3D; 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F; 设置模块属性</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  &#x2F;&#x2F; 向模块添加方法</span><br><span class=\"line\">  auto py_module &#x3D; py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    &#x2F;&#x2F; 设置模块其他属性</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  &#x2F;&#x2F; 增加 _THNN 属性</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p>使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch&#x2F;csrc&#x2F;autograd&#x2F;init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch&#x2F;csrc&#x2F;multiprocessing&#x2F;init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  同上类似</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch&#x2F;csrc&#x2F;cuda&#x2F;Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch&#x2F;csrc&#x2F;distributed&#x2F;Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></li>\n<li><p>生成模块torch._C 后再向其添加如下成员：</p>\n<ul>\n<li><p>向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。</p>\n<p>  torch._C._TensorBase这个类型具有属性</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n<p>  并且具有以下方法</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch&#x2F;csrc&#x2F;autograd&#x2F;generated&#x2F;python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n<p>  类型torch._C._FunctionBase， 这个类型具有方法和属性为</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n<p>  类型 torch._C._VariableFunctions 包含方法</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arange</span><br><span class=\"line\">as_tensor</span><br><span class=\"line\">...</span><br><span class=\"line\">empty       <span class=\"comment\"># 出现我们这里所讨论的 torch.empty</span></span><br><span class=\"line\">empty_like</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>  不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。</p>\n</li>\n<li><p>向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。</p>\n</li>\n<li><p>向torch._C添加模块， _nn，cpp，_onnx。</p>\n</li>\n<li><p>向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。</p>\n<p>注意到从Module.cpp文件中头文件引用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;TH.h&gt;               &#x2F;&#x2F; TH&#x3D;TorcH</span><br><span class=\"line\">#include &lt;c10&#x2F;util&#x2F;Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen&#x2F;ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;THP.h&gt;      &#x2F;&#x2F; THP&#x3D;TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。</p>\n<p>一方面，头文件 TH/TH.h 中引用了#include &lt;TH/THGeneral.h&gt;，在aten/src/TH目录下的CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;&#x2F;THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n<p>在THGeneral.h中有如下宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)</span><br></pre></td></tr></table></figure>\n<p>另一方面，torch/csrc/THP.h 中引用了#include &lt;torch/src/Storage.h&gt;，在这个Storage.h中有如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n<p>上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&quot;         &#x2F;&#x2F; (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      &#x2F;&#x2F; (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n<p>而文件TH/THGenerateAllType.h则包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n<p>4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 此时 TH_GENERIC_FILE是已定义的</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     &#x2F;&#x2F; 这里将TH_GENERIC_FILE 设为未定义</span><br></pre></td></tr></table></figure>\n<p>以TH/THGenerateFloatType.h为例说明，此文件中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         &#x2F;&#x2F; (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n<p>注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n<p>类似地，#include &lt;TH/THGenerateDoubleType.h&gt;，则得到THPDoubleStorage_init，</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; 得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n<p>对4组include中的其他三组，则得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n<p>以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;THP.h&gt;                   &#x2F;&#x2F; include THPxxxStorage_init 函数声明</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n<p>又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&quot;              &#x2F;&#x2F; (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   &#x2F;&#x2F; (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods &#x3D; methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members &#x3D; THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset &#x3D; THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n<p>在TH/THGeneral.h中存在宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n<p>由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为”FloatStorageBase”，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。</p>\n<p>以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n<p>即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。</p>\n<p>其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。</p>\n<p>torch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。</p>\n<p>现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType &#x3D; &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               &#x2F;* tp_name *&#x2F;</span><br><span class=\"line\">  sizeof(THPStorage),                          &#x2F;* tp_basicsize *&#x2F;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          &#x2F;* tp_new *&#x2F;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）</p>\n<p>然后查看类创建 THPStorage_(pynew) 的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args &#x3D; args ? PyTuple_Size(args) : 0;   &#x2F;&#x2F; 可变长度参数的个数</span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); &#x2F;&#x2F; 分配内存，让self指向这个内存块</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator &#x3D; nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs !&#x3D; nullptr) &#123;                               &#x2F;&#x2F; named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr &#x3D; PyDict_GetItemString(kwargs, &quot;allocator&quot;); &#x2F;&#x2F; 获取参数allocator的值</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      &#x2F;&#x2F; 转为 c10::Allocator 指针</span><br><span class=\"line\">      allocator &#x3D; static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs &#x3D; PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr &#x3D; PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs&#x3D;&#x3D;1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   &#x2F;&#x2F; 提供了cdata值</span><br><span class=\"line\">        THWStorage *ptr &#x3D; (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata &#x3D; ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       &#x2F;&#x2F; 返回THPStorage指针</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs &#x3D;&#x3D; 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            &#x2F;&#x2F; 未提供cdata值，则需要创建THWStorage类型实例</span><br><span class=\"line\">      self-&gt;cdata &#x3D; THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata &#x3D; THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     &#x2F;&#x2F; 使用其他方法设置 self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n<p>转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;generic&#x2F;THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10&#x2F;core&#x2F;StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n<p>在 TH/generic/THStorage.h 中找到宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n<p>在 c10/core/StorageImpl.h 中找到结构定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  &#x2F;&#x2F; 数据类型</span><br><span class=\"line\">  DataPtr data_ptr_;             &#x2F;&#x2F; 数据指针</span><br><span class=\"line\">  int64_t numel_;                &#x2F;&#x2F; 数据数量</span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         &#x2F;&#x2F; 数据的内存分配器</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                &#x2F;&#x2F; 新建THStorage，未指定 size，即size&#x3D;0，使用默认Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        &#x2F;&#x2F; 新建THStorage，指定 size，使用默认Allocator</span><br><span class=\"line\">newWithAllocator   &#x2F;&#x2F; 新建THStorage，指定 size 和 Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>此外有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     &#x2F;&#x2F; torch&#x2F;csrc&#x2F;THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   &#x2F;&#x2F; TH&#x2F;THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n<p>函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。</p>\n<p>（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）</p>\n<p>以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage &#x3D; c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        &#x2F;&#x2F; 新建scalar_t 类型</span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们看上上个代码片段中StorageImpl构造函数的实参，</p>\n<p>首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n<p>于是，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    &#x2F;&#x2F; 假设 THQUANTIZED 未定义</span><br></pre></td></tr></table></figure>\n<p>caffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) &#x3D;    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n<p>经过宏替换，得到 _typeMetaDataInstance的模板函数定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>还有一组宏，用于生成模板特例化，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) &#x3D; _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">&#x2F;&#x2F; 调用</span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n<p>对于系统内部变量如 float，得到函数模板特例化的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 函数声明</span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另外，在c10/util/typeid.cpp中有如下调用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n<p>经过宏替换得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    &#x3D; _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>于是函数模板特例化最终形式为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>detail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">&#x2F;&#x2F; 函数类型的别名</span><br><span class=\"line\">using New &#x3D; void*();                            &#x2F;&#x2F; new</span><br><span class=\"line\">using PlacementNew &#x3D; void(void*, size_t);       &#x2F;&#x2F; 占位new</span><br><span class=\"line\">using Copy &#x3D; void(const void*, void*, size_t);  &#x2F;&#x2F; 类型数组拷贝</span><br><span class=\"line\">using PlacementDelete &#x3D; void(void*, size_t);</span><br><span class=\"line\">using Delete &#x3D; void(void*);</span><br><span class=\"line\">... &#x2F;&#x2F;构造函数</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  &#x2F;&#x2F; 类型占多少字节</span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   &#x2F;&#x2F; 定位放置 new</span><br><span class=\"line\">Copy* copy_;        &#x2F;&#x2F; 类型拷贝</span><br><span class=\"line\">Delete* delete_;    &#x2F;&#x2F; 类型析构</span><br><span class=\"line\">TypeIdentifier id_; &#x2F;&#x2F; 类型全局唯一id</span><br><span class=\"line\">const char* name_;  &#x2F;&#x2F; 类型名称</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 &#x2F;&#x2F; 类型T占多少字节</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             &#x2F;&#x2F; 通过 new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  &#x2F;&#x2F; 获取类型的全局唯一id，</span><br><span class=\"line\">          typeName&#125;;                 &#x2F;&#x2F; 类型名称，例如float的名称为&quot;float&quot;</span><br></pre></td></tr></table></figure>\n<p>构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n<p>在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。</p>\n<p>对于其他类型如 int，double，int64_t等类似处理。</p>\n<p>PyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。</p>\n<p>至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             &#x2F;&#x2F; 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）</span><br><span class=\"line\">getTHDefaultAllocator(),  &#x2F;&#x2F; 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配</span><br><span class=\"line\">true                      &#x2F;&#x2F; 被构造的StorageImpl可以resize</span><br></pre></td></tr></table></figure>\n<p>创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase</p>\n<p>记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。</p>\n<p>FloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。</p>\n<p>类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n<p>函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。</p>\n<p><code>torch._C</code>模块初始化过程到这里就完成了。回到 <code>torch/__init__.py</code>，继续看看 import torch时接下来做了哪些事情：</p>\n<ol>\n<li>定义了模块函数 typename，is_tensor，is_storage等</li>\n<li>导入torch下其他子模块</li>\n<li>调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理</li>\n<li>调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：<ul>\n<li>初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；</li>\n<li>初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；</li>\n<li>初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；</li>\n<li>初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor</li>\n<li>共享内存管理初始化，设置文件路径；</li>\n<li>执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n  其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)&#96;</span><br></pre></td></tr></table></figure>\n  这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。</li>\n</ul>\n</li>\n</ol>\n<p>到这里，import torch 的工作全部完成。</p>\n<h1 id=\"后记：\"><a href=\"#后记：\" class=\"headerlink\" title=\"后记：\"></a>后记：</h1><p>初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1>","more":"<p>依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n<p>于是阅读 <code>torch/__init__.py</code>，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags&#x3D;sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ +&#x3D; [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] !&#x3D; &#39;_&#39; and</span><br><span class=\"line\">            not name.endswith(&#39;Base&#39;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n<p><strong>将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。</strong></p>\n<p><code>__init__.py</code>除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   &#x2F;&#x2F; 在python脚本中，import _C 时调用</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>根据python3扩展库的规则可知，<code>import torch._C</code> ，调用PyInit__C函数（调用名为PyInit_&lt;package&gt;的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。</p>\n<p>shm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;&#x2F;lib&#x2F;$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n<p>从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n<p>TORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。</p>\n<p>initModule方法定义在torch/csrc/Module.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       &#x2F;&#x2F; 模块中有关cuda部分的初始化函数声明</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 &#x2F;&#x2F; 声明并定义模块初始化函数</span><br><span class=\"line\">  &#x2F;&#x2F; 向methods中添加方法定义</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &#x2F;&#x2F; 真正的扩展模块定义</span><br><span class=\"line\">  static struct PyModuleDef torchmodule &#x3D; &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          &#x2F;&#x2F; 扩展模块名</span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       &#x2F;&#x2F; 模块中的方法定义</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module &#x3D; PyModule_Create(&amp;torchmodule)); &#x2F;&#x2F; 创建模块并确保创建成功</span><br><span class=\"line\">  &#x2F;&#x2F; 对模块进行各种初始化</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       &#x2F;&#x2F; 执行cuda相关的初始化</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &#x2F;&#x2F; 定义模块的属性设置函数，setter</span><br><span class=\"line\">  &#x2F;&#x2F; 属性名为name，值为v，incref表示是否对值对象增加引用计数</span><br><span class=\"line\">  &#x2F;&#x2F; 设置成功返回1，否则返回0</span><br><span class=\"line\">  auto set_module_attr &#x3D; [&amp;](const char* name, PyObject* v, bool incref &#x3D; true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) &#x3D;&#x3D; 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F; 设置模块属性</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  &#x2F;&#x2F; 向模块添加方法</span><br><span class=\"line\">  auto py_module &#x3D; py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    &#x2F;&#x2F; 设置模块其他属性</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  &#x2F;&#x2F; 增加 _THNN 属性</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p>使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch&#x2F;csrc&#x2F;autograd&#x2F;init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch&#x2F;csrc&#x2F;multiprocessing&#x2F;init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  同上类似</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch&#x2F;csrc&#x2F;cuda&#x2F;Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch&#x2F;csrc&#x2F;distributed&#x2F;Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></li>\n<li><p>生成模块torch._C 后再向其添加如下成员：</p>\n<ul>\n<li><p>向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。</p>\n<p>  torch._C._TensorBase这个类型具有属性</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n<p>  并且具有以下方法</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch&#x2F;csrc&#x2F;autograd&#x2F;generated&#x2F;python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n<p>  类型torch._C._FunctionBase， 这个类型具有方法和属性为</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n<p>  类型 torch._C._VariableFunctions 包含方法</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arange</span><br><span class=\"line\">as_tensor</span><br><span class=\"line\">...</span><br><span class=\"line\">empty       <span class=\"comment\"># 出现我们这里所讨论的 torch.empty</span></span><br><span class=\"line\">empty_like</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>  不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。</p>\n</li>\n<li><p>向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。</p>\n</li>\n<li><p>向torch._C添加模块， _nn，cpp，_onnx。</p>\n</li>\n<li><p>向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。</p>\n<p>注意到从Module.cpp文件中头文件引用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;TH.h&gt;               &#x2F;&#x2F; TH&#x3D;TorcH</span><br><span class=\"line\">#include &lt;c10&#x2F;util&#x2F;Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen&#x2F;ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;THP.h&gt;      &#x2F;&#x2F; THP&#x3D;TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。</p>\n<p>一方面，头文件 TH/TH.h 中引用了#include &lt;TH/THGeneral.h&gt;，在aten/src/TH目录下的CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;&#x2F;THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n<p>在THGeneral.h中有如下宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)</span><br></pre></td></tr></table></figure>\n<p>另一方面，torch/csrc/THP.h 中引用了#include &lt;torch/src/Storage.h&gt;，在这个Storage.h中有如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n<p>上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.h&quot;         &#x2F;&#x2F; (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      &#x2F;&#x2F; (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n<p>而文件TH/THGenerateAllType.h则包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n<p>4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 此时 TH_GENERIC_FILE是已定义的</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     &#x2F;&#x2F; 这里将TH_GENERIC_FILE 设为未定义</span><br></pre></td></tr></table></figure>\n<p>以TH/THGenerateFloatType.h为例说明，此文件中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         &#x2F;&#x2F; (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n<p>注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n<p>类似地，#include &lt;TH/THGenerateDoubleType.h&gt;，则得到THPDoubleStorage_init，</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; 得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n<p>对4组include中的其他三组，则得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n<p>以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH&#x2F;THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;THP.h&gt;                   &#x2F;&#x2F; include THPxxxStorage_init 函数声明</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n<p>又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch&#x2F;csrc&#x2F;generic&#x2F;Storage.cpp&quot;              &#x2F;&#x2F; (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   &#x2F;&#x2F; (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods &#x3D; methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members &#x3D; THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset &#x3D; THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n<p>在TH/THGeneral.h中存在宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n<p>由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为”FloatStorageBase”，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。</p>\n<p>以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n<p>即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。</p>\n<p>其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。</p>\n<p>torch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。</p>\n<p>现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType &#x3D; &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               &#x2F;* tp_name *&#x2F;</span><br><span class=\"line\">  sizeof(THPStorage),                          &#x2F;* tp_basicsize *&#x2F;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          &#x2F;* tp_new *&#x2F;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）</p>\n<p>然后查看类创建 THPStorage_(pynew) 的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args &#x3D; args ? PyTuple_Size(args) : 0;   &#x2F;&#x2F; 可变长度参数的个数</span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); &#x2F;&#x2F; 分配内存，让self指向这个内存块</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator &#x3D; nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs !&#x3D; nullptr) &#123;                               &#x2F;&#x2F; named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr &#x3D; PyDict_GetItemString(kwargs, &quot;allocator&quot;); &#x2F;&#x2F; 获取参数allocator的值</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      &#x2F;&#x2F; 转为 c10::Allocator 指针</span><br><span class=\"line\">      allocator &#x3D; static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs &#x3D; PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr &#x3D; PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs&#x3D;&#x3D;1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   &#x2F;&#x2F; 提供了cdata值</span><br><span class=\"line\">        THWStorage *ptr &#x3D; (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata &#x3D; ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       &#x2F;&#x2F; 返回THPStorage指针</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs &#x3D;&#x3D; 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            &#x2F;&#x2F; 未提供cdata值，则需要创建THWStorage类型实例</span><br><span class=\"line\">      self-&gt;cdata &#x3D; THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata &#x3D; THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     &#x2F;&#x2F; 使用其他方法设置 self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n<p>转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH&#x2F;generic&#x2F;THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10&#x2F;core&#x2F;StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n<p>在 TH/generic/THStorage.h 中找到宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n<p>在 c10/core/StorageImpl.h 中找到结构定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  &#x2F;&#x2F; 数据类型</span><br><span class=\"line\">  DataPtr data_ptr_;             &#x2F;&#x2F; 数据指针</span><br><span class=\"line\">  int64_t numel_;                &#x2F;&#x2F; 数据数量</span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         &#x2F;&#x2F; 数据的内存分配器</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                &#x2F;&#x2F; 新建THStorage，未指定 size，即size&#x3D;0，使用默认Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        &#x2F;&#x2F; 新建THStorage，指定 size，使用默认Allocator</span><br><span class=\"line\">newWithAllocator   &#x2F;&#x2F; 新建THStorage，指定 size 和 Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>此外有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     &#x2F;&#x2F; torch&#x2F;csrc&#x2F;THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   &#x2F;&#x2F; TH&#x2F;THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n<p>函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。</p>\n<p>（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）</p>\n<p>以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage &#x3D; c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        &#x2F;&#x2F; 新建scalar_t 类型</span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们看上上个代码片段中StorageImpl构造函数的实参，</p>\n<p>首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n<p>于是，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    &#x2F;&#x2F; 假设 THQUANTIZED 未定义</span><br></pre></td></tr></table></figure>\n<p>caffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) &#x3D;    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n<p>经过宏替换，得到 _typeMetaDataInstance的模板函数定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>还有一组宏，用于生成模板特例化，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) &#x3D; _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">&#x2F;&#x2F; 调用</span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n<p>对于系统内部变量如 float，得到函数模板特例化的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 函数声明</span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另外，在c10/util/typeid.cpp中有如下调用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n<p>经过宏替换得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    &#x3D; _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>于是函数模板特例化最终形式为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>detail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">&#x2F;&#x2F; 函数类型的别名</span><br><span class=\"line\">using New &#x3D; void*();                            &#x2F;&#x2F; new</span><br><span class=\"line\">using PlacementNew &#x3D; void(void*, size_t);       &#x2F;&#x2F; 占位new</span><br><span class=\"line\">using Copy &#x3D; void(const void*, void*, size_t);  &#x2F;&#x2F; 类型数组拷贝</span><br><span class=\"line\">using PlacementDelete &#x3D; void(void*, size_t);</span><br><span class=\"line\">using Delete &#x3D; void(void*);</span><br><span class=\"line\">... &#x2F;&#x2F;构造函数</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  &#x2F;&#x2F; 类型占多少字节</span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   &#x2F;&#x2F; 定位放置 new</span><br><span class=\"line\">Copy* copy_;        &#x2F;&#x2F; 类型拷贝</span><br><span class=\"line\">Delete* delete_;    &#x2F;&#x2F; 类型析构</span><br><span class=\"line\">TypeIdentifier id_; &#x2F;&#x2F; 类型全局唯一id</span><br><span class=\"line\">const char* name_;  &#x2F;&#x2F; 类型名称</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 &#x2F;&#x2F; 类型T占多少字节</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             &#x2F;&#x2F; 通过 new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  &#x2F;&#x2F; 获取类型的全局唯一id，</span><br><span class=\"line\">          typeName&#125;;                 &#x2F;&#x2F; 类型名称，例如float的名称为&quot;float&quot;</span><br></pre></td></tr></table></figure>\n<p>构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n<p>在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。</p>\n<p>对于其他类型如 int，double，int64_t等类似处理。</p>\n<p>PyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。</p>\n<p>至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             &#x2F;&#x2F; 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）</span><br><span class=\"line\">getTHDefaultAllocator(),  &#x2F;&#x2F; 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配</span><br><span class=\"line\">true                      &#x2F;&#x2F; 被构造的StorageImpl可以resize</span><br></pre></td></tr></table></figure>\n<p>创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase</p>\n<p>记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。</p>\n<p>FloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。</p>\n<p>类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n<p>函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。</p>\n<p><code>torch._C</code>模块初始化过程到这里就完成了。回到 <code>torch/__init__.py</code>，继续看看 import torch时接下来做了哪些事情：</p>\n<ol>\n<li>定义了模块函数 typename，is_tensor，is_storage等</li>\n<li>导入torch下其他子模块</li>\n<li>调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理</li>\n<li>调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：<ul>\n<li>初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；</li>\n<li>初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；</li>\n<li>初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；</li>\n<li>初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor</li>\n<li>共享内存管理初始化，设置文件路径；</li>\n<li>执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n  其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)&#96;</span><br></pre></td></tr></table></figure>\n  这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。</li>\n</ul>\n</li>\n</ol>\n<p>到这里，import torch 的工作全部完成。</p>\n<h1 id=\"后记：\"><a href=\"#后记：\" class=\"headerlink\" title=\"后记：\"></a>后记：</h1><p>初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。</p>"},{"title":"PyTorch-3","p":"pytorch/PyTorch-3","date":"2019-06-18T08:44:44.000Z","_content":"在 [PyTorch-2](PyTorch-2) 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的[官方文档](https://pytorch.org/docs/stable/torch.html)，了解其中各个函数的具体实现。\n<!-- more -->\n# torch 包\n从 `torch/__init__.py` 中可以查看所有的 torch 包的所有字段，包括：\n1. 直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等\n2. 从 torch 包的模块中导入的函数/类，如\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3. 从 torch._C 中导入的字段/函数/类\n4. 从 torch._C._VariableFunctions 导入的字段/函数\n   \nPyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。\n## torch.empty\n这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：\n1. caffe2/CMakeLists.txt 中的文件生成语句为\n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2. 执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，\n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：\n1. 在 caffe2/CMakeLists.txt 中有语句\n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2. 在文件 cmake/Codegen.cmake 中调用 `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n   （在 aten/src/ATen/native/native_functions.yaml 找到 `empty` 的函数签名）\n3. aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件\n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4. 根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件\n   - CMakeLists.txt 中的 add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt 中有，\n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n   事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。\n   \n找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，\n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。\n\n## empty 定义\n我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // 函数签名在vector中的下标\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：\n1. `out` 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor\n2. `out` 参数不为None, 则检查 `out` 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 `out` 的 requires_grad 属性重置为参数 requires_grad\n\n然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，\n```\n// empty 函数调用者提供了 Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty 函数调用者未提供 Tensor 'out'，需要根据参数 options 创建\ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n### 有输出 Tensor\n我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），\n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，\n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n显然，根据输出 Tensor 是否是稀疏的进行不同的处理。\n1. 输出 Tensor 是稀疏的\n   \n   对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n   先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n   其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n   根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。\n2. 输出 Tensor 是密集的\n   \n   Tensor 的 resize_ 方法定义见 TensorMethods.h，为\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n   调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下\n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n   可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，\n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         // 获取 Tensor 的底层实现类对象\n       // 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块\n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，\n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           // 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存\n           // size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等\n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // 未指定步幅，则数据布局是近邻的，连续的，即，stride=1\n           self->set_sizes_contiguous(size);    // 设置当前 size 为新的 size\n           storage_size = self->numel();        // 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize 操作\n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset() 通常返回 0\n           // 只有需要更多的元素数量时，才重新分配内存\n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n   我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，\n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           // 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存\n           // 设置 Tensor 内部存储指向新数据，同时返回旧数据\n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   // 旧数据 size，元素数量\n           storage->set_numel(size);                // 设置新的元素熟路\n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // 内存数据考虑\n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n   从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么\n   1. N1 >= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中\n   2. N1 < N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。\n\n实验验证上述 torch.empty 过程，代码如下，\n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize 到一个较大的 size\nprint(x)\ntorch.empty(1,2,out=x)  # resize 到一个较小的 size\nprint(x)\ntorch.empty(4,4,out=x)  # 再次 resize 到一个较大的 size\n```\n本次输出如下，从以下结果可以看出是符合上述过程的。\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n### 无输出 Tensor\n回到 torch/csrc/autograd/generated/python_torch_functions_dispatch.h 这个文件，无输出 tensor 的 dispatch_empty 函数直接调用 torch::empty，此函数位于 torch/csrc/autograd/generated/variable_factories.h，在此函数定义中，我们暂且忽略 jit 跟踪部分的代码（用于跟踪记录有关 Tensor 的操作），核心的实现代码为\n```c++\ninline at::Tensor empty(at::IntList size, const at::TensorOptions & options={}) {\n    ...     // jit tracing\n    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(false));\n    auto result = autograd::make_variable(tensor, options.requires_grad()); // 将 Tensor 转为 Variable\n    ...     // jit tracing\n    return result\n}\n```\n其中 at::empty 位于安装时动态生成的源文件 Functions.h（见上文分析），这个函数定义为\n```c++\nstatic inline Tensor empty(IntList size, const TensorOptions & options) {\n    return at::getType(options).empty(size, options);\n}\n```\n与有输出 Tensor 的 empty 函数实现逻辑类似，这里 at::getType(options) 根据给定的 options 构造出 TypeExtendedInterface 接口的具体实现类的 instance，具体而言，根据 options.backend(), options.dtype() 和 options.is_variable() 获取具体类型实例，而类型实例是事先注册好的，以 CPU 为 backend 为例说明，在 aten/src/ATen/Context.cpp 中 Context 的构造函数中，执行函数 register_cpu_types(this) 进行注册，而 register_cpu_type(Context* context) 函数位于 build/aten/src/ATen/RegisterCPU.cpp 文件，此文件由 aten/src/ATen/gen.py 中的 generate_outputs 函数生成（关于 gen.py 文件，上文也有介绍），现在我们来看看 register_cpu_types 中注册哪些类型\n```\nCPUByteType\nCPUCharType\nCPUDoubleType\nCPUFloatType\nCPUIntType\nCPULongType\nCPUShortType\n...\n```\n我们随便选择一个类型，比如 CPUByteType，查看其中 empty 函数实现，\n```\nTensor CPUByteType::empty(IntList size, const TensorOptions & options) const {\n    const DeviceGuard device_guard(options.device());   // 准备在指定 device 上构造 Tensor\n    return at::native::empty_cpu(size, options);\n}\n```\n以上 at::native::empty_cpu 函数位于 aten/src/ATen/native/TensorFactories.cpp 中，函数实现体的部分为\n```c++\nauto* allocator = at::getCPUAllocator();\nint64_t nelements = prod_intlist(size); // 连乘（各维度值），得到总元素数量\nauto dtype = options.dtype();\nauto storage_impl = c10::make_intrusive<StorageImpl>(\n    dtype,\n    nelements,\n    allocator->allocate(nelements*dtype.itemsize()),\n    allocator,\n    /*resizeable=*/true\n);\nauto tensor = detail::make_tensor<TensorImpl>(storage_impl, at::CPUTensorId(), false);\n```\n继续查看 c10::make_intrusive<StorageImpl> 函数定义，不难得知先进行 new StorageImpl(...)，然后 wrap 为 intrusive_ptr，在 [PyTorch-2](2019/06/13/PyTorch-2) 中，我们讨论过各种 Tensor 的底层实现都是 StorageImpl，所以 StorageImpl 对象可以通过 detail::make_tensor 转为对应的 Tensor。根据 at::getCPUAllocator 查看其定义得知获得的是 THDefaultAllocator 实例，其 allocate 方法调用 THAlloc 分配内存，THAlloc 内部调用 THAllocInternal 分配内存，而这个函数又使用 malloc（某些情况下也会使用 posix_memalign 申请对齐内存） 申请一块未初始化的内存。\n\n示例：\n```python\nimport torch\ntorch.empty(2,3)\n```\n结果为（每次执行结果可能不同，不固定）\n```\ntensor([[1.6504e-12,3.0637e-41,1.6588e-12],\n        [3.0637e-41,4.4842e-44,0.0000e+00]])\n```\n\n### Tensor 的由来\n这里我们讨论 torch.empty 函数是如何返回得到 torch.Tensor 对象的。一开始，在 `torch/__init__.py` 中 `import autograd`，继而查看 `torch/autograd/__init__.py`，发现如下调用\n```python\nif not torch._C._autograd_init():\n```\n_autograd_init 这个 python 函数在 torch/csrc/Module.cpp 中注册，其底层实现是由 THPAutograd_initExtension 完成，这个 c++ 函数声明位于头文件 torch/csrc/autograd/autograd.h 中，函数实现位于 torch/csrc/autograd/init.cpp 中，看下这个函数的部分定义\n```c++\n// 加载 torch/tensor.py 模块\nauto tensor_module = THPObjectPtr(PyImport_ImportModule(\"torch.tensor\"));\n// 获取 torch/tensor.py 中的 Tensor 类型\nTHPVariableClass = PyObject_GetAttrString(tensor_module, \"Tensor\");\n```\n要知道 `THPVariableClass` 这个类型对象声明位于 torch/csrc/autograd/python_variable.h 中\n```c++\nTHP_API PyObject *THPVariableClass;\n```\n嗯，这是一个 extern 声明，其原本定义位于 torch/csrc/autograd/python_variable.cpp 中。好，现在回到 torch.empty 的底层 c++ 实现部分，即上文 THPVariable_empty 函数定义，在 dispatch_empty 返回一个 Variable 对象后，经过 wrap 包装为 PyObject，来看 wrap 的定义，位于 torch/csrc/autograd/utils/wrap_outputs.h 中，其内部调用 THPVariable_Wrap，这个函数也位于 torch/csrc/autograd/python_variable.cpp，与 THPVariableClass 定义在同一个文件中，前面我们已经知道 THPVariableClass 就是 torch/tensor.py 中的 Tensor 类型，而这里 THPVariable_Wrap 通过调用 THPVariable_NewWithVar 将 Variable 对象包装为 THPVariableClass 对象，即 Tensor 实例。THPVariable_NewWithVar 函数定义的部分代码为\n```c++\nstatic PyObject* THPVariable_NewWithVar(PyTypeObject* type, Variable var) {\nPyObject *obj=type->tp_alloc(type, 0);      // 申请 torch.Tensor 所需要的内存\nif(obj) {\n    auto v = (THPVariable*)obj; // cast 为 THPVariable 类型指针，即 torch.Tensor 的基类 torch._C._TensorBase 的指针\n    new(&v->cdata) Variable(std::move(var));    // 指定内存中，移动构造 Variable（C++ 版本的 Tensor）\n    v->cdata.set_pyobj(obj);\n    ...\n}\nreturn obj;\n}\n```\n\n示例\n```python\n>>> type(torch.empty(2,3))\n<class 'torch.Tensor'>\n```\n\n# PS\n好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。","source":"_posts/pytorch/PyTorch-3.md","raw":"---\ntitle: PyTorch-3\np: pytorch/PyTorch-3\ndate: 2019-06-18 16:44:44\ntags: PyTorch\ncategories: DL Framework\n---\n在 [PyTorch-2](PyTorch-2) 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的[官方文档](https://pytorch.org/docs/stable/torch.html)，了解其中各个函数的具体实现。\n<!-- more -->\n# torch 包\n从 `torch/__init__.py` 中可以查看所有的 torch 包的所有字段，包括：\n1. 直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等\n2. 从 torch 包的模块中导入的函数/类，如\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3. 从 torch._C 中导入的字段/函数/类\n4. 从 torch._C._VariableFunctions 导入的字段/函数\n   \nPyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。\n## torch.empty\n这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：\n1. caffe2/CMakeLists.txt 中的文件生成语句为\n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2. 执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，\n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：\n1. 在 caffe2/CMakeLists.txt 中有语句\n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2. 在文件 cmake/Codegen.cmake 中调用 `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n   （在 aten/src/ATen/native/native_functions.yaml 找到 `empty` 的函数签名）\n3. aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件\n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4. 根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件\n   - CMakeLists.txt 中的 add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt 中有，\n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n   事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。\n   \n找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，\n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。\n\n## empty 定义\n我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // 函数签名在vector中的下标\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：\n1. `out` 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor\n2. `out` 参数不为None, 则检查 `out` 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 `out` 的 requires_grad 属性重置为参数 requires_grad\n\n然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，\n```\n// empty 函数调用者提供了 Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty 函数调用者未提供 Tensor 'out'，需要根据参数 options 创建\ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n### 有输出 Tensor\n我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），\n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，\n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n显然，根据输出 Tensor 是否是稀疏的进行不同的处理。\n1. 输出 Tensor 是稀疏的\n   \n   对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n   先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n   其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n   根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。\n2. 输出 Tensor 是密集的\n   \n   Tensor 的 resize_ 方法定义见 TensorMethods.h，为\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n   调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下\n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n   可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，\n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         // 获取 Tensor 的底层实现类对象\n       // 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块\n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，\n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           // 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存\n           // size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等\n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // 未指定步幅，则数据布局是近邻的，连续的，即，stride=1\n           self->set_sizes_contiguous(size);    // 设置当前 size 为新的 size\n           storage_size = self->numel();        // 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize 操作\n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset() 通常返回 0\n           // 只有需要更多的元素数量时，才重新分配内存\n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n   我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，\n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           // 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存\n           // 设置 Tensor 内部存储指向新数据，同时返回旧数据\n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   // 旧数据 size，元素数量\n           storage->set_numel(size);                // 设置新的元素熟路\n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // 内存数据考虑\n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n   从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么\n   1. N1 >= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中\n   2. N1 < N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。\n\n实验验证上述 torch.empty 过程，代码如下，\n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize 到一个较大的 size\nprint(x)\ntorch.empty(1,2,out=x)  # resize 到一个较小的 size\nprint(x)\ntorch.empty(4,4,out=x)  # 再次 resize 到一个较大的 size\n```\n本次输出如下，从以下结果可以看出是符合上述过程的。\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n### 无输出 Tensor\n回到 torch/csrc/autograd/generated/python_torch_functions_dispatch.h 这个文件，无输出 tensor 的 dispatch_empty 函数直接调用 torch::empty，此函数位于 torch/csrc/autograd/generated/variable_factories.h，在此函数定义中，我们暂且忽略 jit 跟踪部分的代码（用于跟踪记录有关 Tensor 的操作），核心的实现代码为\n```c++\ninline at::Tensor empty(at::IntList size, const at::TensorOptions & options={}) {\n    ...     // jit tracing\n    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(false));\n    auto result = autograd::make_variable(tensor, options.requires_grad()); // 将 Tensor 转为 Variable\n    ...     // jit tracing\n    return result\n}\n```\n其中 at::empty 位于安装时动态生成的源文件 Functions.h（见上文分析），这个函数定义为\n```c++\nstatic inline Tensor empty(IntList size, const TensorOptions & options) {\n    return at::getType(options).empty(size, options);\n}\n```\n与有输出 Tensor 的 empty 函数实现逻辑类似，这里 at::getType(options) 根据给定的 options 构造出 TypeExtendedInterface 接口的具体实现类的 instance，具体而言，根据 options.backend(), options.dtype() 和 options.is_variable() 获取具体类型实例，而类型实例是事先注册好的，以 CPU 为 backend 为例说明，在 aten/src/ATen/Context.cpp 中 Context 的构造函数中，执行函数 register_cpu_types(this) 进行注册，而 register_cpu_type(Context* context) 函数位于 build/aten/src/ATen/RegisterCPU.cpp 文件，此文件由 aten/src/ATen/gen.py 中的 generate_outputs 函数生成（关于 gen.py 文件，上文也有介绍），现在我们来看看 register_cpu_types 中注册哪些类型\n```\nCPUByteType\nCPUCharType\nCPUDoubleType\nCPUFloatType\nCPUIntType\nCPULongType\nCPUShortType\n...\n```\n我们随便选择一个类型，比如 CPUByteType，查看其中 empty 函数实现，\n```\nTensor CPUByteType::empty(IntList size, const TensorOptions & options) const {\n    const DeviceGuard device_guard(options.device());   // 准备在指定 device 上构造 Tensor\n    return at::native::empty_cpu(size, options);\n}\n```\n以上 at::native::empty_cpu 函数位于 aten/src/ATen/native/TensorFactories.cpp 中，函数实现体的部分为\n```c++\nauto* allocator = at::getCPUAllocator();\nint64_t nelements = prod_intlist(size); // 连乘（各维度值），得到总元素数量\nauto dtype = options.dtype();\nauto storage_impl = c10::make_intrusive<StorageImpl>(\n    dtype,\n    nelements,\n    allocator->allocate(nelements*dtype.itemsize()),\n    allocator,\n    /*resizeable=*/true\n);\nauto tensor = detail::make_tensor<TensorImpl>(storage_impl, at::CPUTensorId(), false);\n```\n继续查看 c10::make_intrusive<StorageImpl> 函数定义，不难得知先进行 new StorageImpl(...)，然后 wrap 为 intrusive_ptr，在 [PyTorch-2](2019/06/13/PyTorch-2) 中，我们讨论过各种 Tensor 的底层实现都是 StorageImpl，所以 StorageImpl 对象可以通过 detail::make_tensor 转为对应的 Tensor。根据 at::getCPUAllocator 查看其定义得知获得的是 THDefaultAllocator 实例，其 allocate 方法调用 THAlloc 分配内存，THAlloc 内部调用 THAllocInternal 分配内存，而这个函数又使用 malloc（某些情况下也会使用 posix_memalign 申请对齐内存） 申请一块未初始化的内存。\n\n示例：\n```python\nimport torch\ntorch.empty(2,3)\n```\n结果为（每次执行结果可能不同，不固定）\n```\ntensor([[1.6504e-12,3.0637e-41,1.6588e-12],\n        [3.0637e-41,4.4842e-44,0.0000e+00]])\n```\n\n### Tensor 的由来\n这里我们讨论 torch.empty 函数是如何返回得到 torch.Tensor 对象的。一开始，在 `torch/__init__.py` 中 `import autograd`，继而查看 `torch/autograd/__init__.py`，发现如下调用\n```python\nif not torch._C._autograd_init():\n```\n_autograd_init 这个 python 函数在 torch/csrc/Module.cpp 中注册，其底层实现是由 THPAutograd_initExtension 完成，这个 c++ 函数声明位于头文件 torch/csrc/autograd/autograd.h 中，函数实现位于 torch/csrc/autograd/init.cpp 中，看下这个函数的部分定义\n```c++\n// 加载 torch/tensor.py 模块\nauto tensor_module = THPObjectPtr(PyImport_ImportModule(\"torch.tensor\"));\n// 获取 torch/tensor.py 中的 Tensor 类型\nTHPVariableClass = PyObject_GetAttrString(tensor_module, \"Tensor\");\n```\n要知道 `THPVariableClass` 这个类型对象声明位于 torch/csrc/autograd/python_variable.h 中\n```c++\nTHP_API PyObject *THPVariableClass;\n```\n嗯，这是一个 extern 声明，其原本定义位于 torch/csrc/autograd/python_variable.cpp 中。好，现在回到 torch.empty 的底层 c++ 实现部分，即上文 THPVariable_empty 函数定义，在 dispatch_empty 返回一个 Variable 对象后，经过 wrap 包装为 PyObject，来看 wrap 的定义，位于 torch/csrc/autograd/utils/wrap_outputs.h 中，其内部调用 THPVariable_Wrap，这个函数也位于 torch/csrc/autograd/python_variable.cpp，与 THPVariableClass 定义在同一个文件中，前面我们已经知道 THPVariableClass 就是 torch/tensor.py 中的 Tensor 类型，而这里 THPVariable_Wrap 通过调用 THPVariable_NewWithVar 将 Variable 对象包装为 THPVariableClass 对象，即 Tensor 实例。THPVariable_NewWithVar 函数定义的部分代码为\n```c++\nstatic PyObject* THPVariable_NewWithVar(PyTypeObject* type, Variable var) {\nPyObject *obj=type->tp_alloc(type, 0);      // 申请 torch.Tensor 所需要的内存\nif(obj) {\n    auto v = (THPVariable*)obj; // cast 为 THPVariable 类型指针，即 torch.Tensor 的基类 torch._C._TensorBase 的指针\n    new(&v->cdata) Variable(std::move(var));    // 指定内存中，移动构造 Variable（C++ 版本的 Tensor）\n    v->cdata.set_pyobj(obj);\n    ...\n}\nreturn obj;\n}\n```\n\n示例\n```python\n>>> type(torch.empty(2,3))\n<class 'torch.Tensor'>\n```\n\n# PS\n好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。","slug":"pytorch/PyTorch-3","published":1,"updated":"2020-04-24T10:34:35.902Z","_id":"ck9dzcjgj002xgga62ozddn7i","comments":1,"layout":"post","photos":[],"link":"","content":"<p>在 <a href=\"PyTorch-2\">PyTorch-2</a> 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的<a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>，了解其中各个函数的具体实现。</p>\n<a id=\"more\"></a>\n<h1 id=\"torch-包\"><a href=\"#torch-包\" class=\"headerlink\" title=\"torch 包\"></a>torch 包</h1><p>从 <code>torch/__init__.py</code> 中可以查看所有的 torch 包的所有字段，包括：</p>\n<ol>\n<li>直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等</li>\n<li>从 torch 包的模块中导入的函数/类，如<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></li>\n<li>从 torch._C 中导入的字段/函数/类</li>\n<li>从 torch._C._VariableFunctions 导入的字段/函数</li>\n</ol>\n<p>PyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。</p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p>这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：</p>\n<ol>\n<li>caffe2/CMakeLists.txt 中的文件生成语句为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;&#x2F;csrc&#x2F;autograd&#x2F;generated&#x2F;python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools&#x2F;setup_helpers&#x2F;generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure></li>\n<li>执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：</li>\n<li>在 caffe2/CMakeLists.txt 中有语句<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(..&#x2F;cmake&#x2F;Codegen.cmake)</span><br></pre></td></tr></table></figure></li>\n<li>在文件 cmake/Codegen.cmake 中调用 <code>gen.py</code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n（在 aten/src/ATen/native/native_functions.yaml 找到 <code>empty</code> 的函数签名）</li>\n<li>aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure></li>\n<li>根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件<ul>\n<li>CMakeLists.txt 中的 add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt 中有，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;&#x2F;ATen)</span><br></pre></td></tr></table></figure>\n事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。</li>\n</ul>\n</li>\n</ol>\n<p>找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS &#x3D; CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, &#x2F;*traceable&#x3D;*&#x2F;$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r &#x3D; parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module&#x3D;False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env &#x3D; &#123;</span><br><span class=\"line\">            &#39;name&#39;: name,</span><br><span class=\"line\">            &#39;dispatch_name&#39;: &#39;dispatch_&#123;&#125;&#39;.format(name),</span><br><span class=\"line\">            &#39;pycname&#39;: &#39;THPVariable_&#123;&#125;&#39;.format(name),</span><br><span class=\"line\">            &#39;signature&#39;: [],</span><br><span class=\"line\">            &#39;max_args&#39;: max(len(o[&#39;arguments&#39;])+len(o[&#39;python_binding_arguments&#39;]) for o in declarations),</span><br><span class=\"line\">            &#39;unpack_self&#39;: [],</span><br><span class=\"line\">            &#39;dispatch&#39;: [],</span><br><span class=\"line\">            &#39;declare_namedtuple_return_types&#39;: &#39;&#39;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... &#x2F;&#x2F; 向 env 增加 key-value pair or 更新 env 中已有 key 的 value</span><br><span class=\"line\">        if len(declarations) &#x3D;&#x3D; 1 and len(declarations[0][&#39;args&#39;]) &#x3D;&#x3D; 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl &#x3D; PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&#39;flags&#39;] &#x3D; &#39;METH_VARARGS | METH_KEYWORDS&#39;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&#39;flags&#39;] +&#x3D; &#39; | METH_STATIC&#39;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n<p>通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。</p>\n<h2 id=\"empty-定义\"><a href=\"#empty-定义\" class=\"headerlink\" title=\"empty 定义\"></a>empty 定义</h2><p>我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out&#x3D;None, ScalarType dtype&#x3D;None, Layout layout&#x3D;torch.strided, Device device&#x3D;None, bool requires_grad&#x3D;False)&quot;,</span><br><span class=\"line\">    &#125;, &#x2F;*tracebalbe*&#x2F;true); &#x2F;&#x2F; 大括号初始化器，得到函数签名的vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r &#x3D; parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F; 函数签名在vector中的下标</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  &#x2F;&#x2F; parameter &#39;out&#39; is None</span><br><span class=\"line\">            auto size &#x3D; r.intlist(0);</span><br><span class=\"line\">            auto dtype &#x3D; r.scalartype(2);</span><br><span class=\"line\">            auto device &#x3D; r.device(4);</span><br><span class=\"line\">            const auto options &#x3D; TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：</p>\n<ol>\n<li><code>out</code> 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor</li>\n<li><code>out</code> 参数不为None, 则检查 <code>out</code> 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 <code>out</code> 的 requires_grad 属性重置为参数 requires_grad</li>\n</ol>\n<p>然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; empty 函数调用者提供了 Tensor &#39;out&#39;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#x2F;&#x2F; empty 函数调用者未提供 Tensor &#39;out&#39;，需要根据参数 options 创建</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"有输出-Tensor\"><a href=\"#有输出-Tensor\" class=\"headerlink\" title=\"有输出 Tensor\"></a>有输出 Tensor</h3><p>我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&#39;Functions.h&#39;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n<p>现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(&#x2F;* native_actuals *&#x2F; result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>显然，根据输出 Tensor 是否是稀疏的进行不同的处理。</p>\n<ol>\n<li><p>输出 Tensor 是稀疏的</p>\n<p>对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(&#x2F;* actuals *&#x2F; self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。</p>\n</li>\n<li><p>输出 Tensor 是密集的</p>\n<p>Tensor 的 resize_ 方法定义见 TensorMethods.h，为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor &amp; <span class=\"title\">CPUByteType::resize_</span><span class=\"params\">(Tensor &amp; self, IntList <span class=\"built_in\">size</span>)</span> <span class=\"keyword\">const</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, <span class=\"built_in\">size</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList <span class=\"built_in\">size</span>)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">// 获取 Tensor 的底层实现类对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块</span></span><br><span class=\"line\">    resize_impl_cpu_(self_, <span class=\"built_in\">size</span>, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(<span class=\"built_in\">size</span>.<span class=\"built_in\">size</span>()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList <span class=\"built_in\">size</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == <span class=\"built_in\">size</span> &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存</span></span><br><span class=\"line\">        <span class=\"comment\">// size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// 未指定步幅，则数据布局是近邻的，连续的，即，stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(<span class=\"built_in\">size</span>);    <span class=\"comment\">// 设置当前 size 为新的 size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">// 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize 操作</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset() 通常返回 0</span></span><br><span class=\"line\">        <span class=\"comment\">// 只有需要更多的元素数量时，才重新分配内存</span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> <span class=\"built_in\">size</span>)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">size</span> != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*<span class=\"built_in\">size</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存</span></span><br><span class=\"line\">        <span class=\"comment\">// 设置 Tensor 内部存储指向新数据，同时返回旧数据</span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">// 旧数据 size，元素数量</span></span><br><span class=\"line\">        storage-&gt;set_numel(<span class=\"built_in\">size</span>);                <span class=\"comment\">// 设置新的元素熟路</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// 内存数据考虑</span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.<span class=\"built_in\">get</span>(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么</p>\n<ol>\n<li>N1 &gt;= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中</li>\n<li>N1 &lt; N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。</li>\n</ol>\n</li>\n</ol>\n<p>实验验证上述 torch.empty 过程，代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize 到一个较大的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize 到一个较小的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\"># 再次 resize 到一个较大的 size</span></span><br></pre></td></tr></table></figure>\n<p>本次输出如下，从以下结果可以看出是符合上述过程的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n<h3 id=\"无输出-Tensor\"><a href=\"#无输出-Tensor\" class=\"headerlink\" title=\"无输出 Tensor\"></a>无输出 Tensor</h3><p>回到 torch/csrc/autograd/generated/python_torch_functions_dispatch.h 这个文件，无输出 tensor 的 dispatch_empty 函数直接调用 torch::empty，此函数位于 torch/csrc/autograd/generated/variable_factories.h，在此函数定义中，我们暂且忽略 jit 跟踪部分的代码（用于跟踪记录有关 Tensor 的操作），核心的实现代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> at::Tensor <span class=\"title\">empty</span><span class=\"params\">(at::IntList <span class=\"built_in\">size</span>, <span class=\"keyword\">const</span> at::TensorOptions &amp; options=&#123;&#125;)</span> </span>&#123;</span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    at::Tensor tensor = at::empty(<span class=\"built_in\">size</span>, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad()); <span class=\"comment\">// 将 Tensor 转为 Variable</span></span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 at::empty 位于安装时动态生成的源文件 Functions.h（见上文分析），这个函数定义为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> Tensor <span class=\"title\">empty</span><span class=\"params\">(IntList <span class=\"built_in\">size</span>, <span class=\"keyword\">const</span> TensorOptions &amp; options)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::getType(options).empty(<span class=\"built_in\">size</span>, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>与有输出 Tensor 的 empty 函数实现逻辑类似，这里 at::getType(options) 根据给定的 options 构造出 TypeExtendedInterface 接口的具体实现类的 instance，具体而言，根据 options.backend(), options.dtype() 和 options.is_variable() 获取具体类型实例，而类型实例是事先注册好的，以 CPU 为 backend 为例说明，在 aten/src/ATen/Context.cpp 中 Context 的构造函数中，执行函数 register_cpu_types(this) 进行注册，而 register_cpu_type(Context* context) 函数位于 build/aten/src/ATen/RegisterCPU.cpp 文件，此文件由 aten/src/ATen/gen.py 中的 generate_outputs 函数生成（关于 gen.py 文件，上文也有介绍），现在我们来看看 register_cpu_types 中注册哪些类型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPUByteType</span><br><span class=\"line\">CPUCharType</span><br><span class=\"line\">CPUDoubleType</span><br><span class=\"line\">CPUFloatType</span><br><span class=\"line\">CPUIntType</span><br><span class=\"line\">CPULongType</span><br><span class=\"line\">CPUShortType</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们随便选择一个类型，比如 CPUByteType，查看其中 empty 函数实现，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor CPUByteType::empty(IntList size, const TensorOptions &amp; options) const &#123;</span><br><span class=\"line\">    const DeviceGuard device_guard(options.device());   &#x2F;&#x2F; 准备在指定 device 上构造 Tensor</span><br><span class=\"line\">    return at::native::empty_cpu(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上 at::native::empty_cpu 函数位于 aten/src/ATen/native/TensorFactories.cpp 中，函数实现体的部分为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span>* allocator = at::getCPUAllocator();</span><br><span class=\"line\"><span class=\"keyword\">int64_t</span> nelements = prod_intlist(<span class=\"built_in\">size</span>); <span class=\"comment\">// 连乘（各维度值），得到总元素数量</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = options.dtype();</span><br><span class=\"line\"><span class=\"keyword\">auto</span> storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class=\"line\">    dtype,</span><br><span class=\"line\">    nelements,</span><br><span class=\"line\">    allocator-&gt;allocate(nelements*dtype.itemsize()),</span><br><span class=\"line\">    allocator,</span><br><span class=\"line\">    <span class=\"comment\">/*resizeable=*/</span><span class=\"literal\">true</span></span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor = detail::make_tensor&lt;TensorImpl&gt;(storage_impl, at::CPUTensorId(), <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n<p>继续查看 c10::make_intrusive<StorageImpl> 函数定义，不难得知先进行 new StorageImpl(…)，然后 wrap 为 intrusive_ptr，在 <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> 中，我们讨论过各种 Tensor 的底层实现都是 StorageImpl，所以 StorageImpl 对象可以通过 detail::make_tensor 转为对应的 Tensor。根据 at::getCPUAllocator 查看其定义得知获得的是 THDefaultAllocator 实例，其 allocate 方法调用 THAlloc 分配内存，THAlloc 内部调用 THAllocInternal 分配内存，而这个函数又使用 malloc（某些情况下也会使用 posix_memalign 申请对齐内存） 申请一块未初始化的内存。</p>\n<p>示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<p>结果为（每次执行结果可能不同，不固定）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1.6504e-12,3.0637e-41,1.6588e-12],</span><br><span class=\"line\">        [3.0637e-41,4.4842e-44,0.0000e+00]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tensor-的由来\"><a href=\"#Tensor-的由来\" class=\"headerlink\" title=\"Tensor 的由来\"></a>Tensor 的由来</h3><p>这里我们讨论 torch.empty 函数是如何返回得到 torch.Tensor 对象的。一开始，在 <code>torch/__init__.py</code> 中 <code>import autograd</code>，继而查看 <code>torch/autograd/__init__.py</code>，发现如下调用</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch._C._autograd_init():</span><br></pre></td></tr></table></figure>\n<p>_autograd_init 这个 python 函数在 torch/csrc/Module.cpp 中注册，其底层实现是由 THPAutograd_initExtension 完成，这个 c++ 函数声明位于头文件 torch/csrc/autograd/autograd.h 中，函数实现位于 torch/csrc/autograd/init.cpp 中，看下这个函数的部分定义</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 加载 torch/tensor.py 模块</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor_module = THPObjectPtr(PyImport_ImportModule(<span class=\"string\">\"torch.tensor\"</span>));</span><br><span class=\"line\"><span class=\"comment\">// 获取 torch/tensor.py 中的 Tensor 类型</span></span><br><span class=\"line\">THPVariableClass = PyObject_GetAttrString(tensor_module, <span class=\"string\">\"Tensor\"</span>);</span><br></pre></td></tr></table></figure>\n<p>要知道 <code>THPVariableClass</code> 这个类型对象声明位于 torch/csrc/autograd/python_variable.h 中</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THP_API PyObject *THPVariableClass;</span><br></pre></td></tr></table></figure>\n<p>嗯，这是一个 extern 声明，其原本定义位于 torch/csrc/autograd/python_variable.cpp 中。好，现在回到 torch.empty 的底层 c++ 实现部分，即上文 THPVariable_empty 函数定义，在 dispatch_empty 返回一个 Variable 对象后，经过 wrap 包装为 PyObject，来看 wrap 的定义，位于 torch/csrc/autograd/utils/wrap_outputs.h 中，其内部调用 THPVariable_Wrap，这个函数也位于 torch/csrc/autograd/python_variable.cpp，与 THPVariableClass 定义在同一个文件中，前面我们已经知道 THPVariableClass 就是 torch/tensor.py 中的 Tensor 类型，而这里 THPVariable_Wrap 通过调用 THPVariable_NewWithVar 将 Variable 对象包装为 THPVariableClass 对象，即 Tensor 实例。THPVariable_NewWithVar 函数定义的部分代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject* <span class=\"title\">THPVariable_NewWithVar</span><span class=\"params\">(PyTypeObject* type, Variable var)</span> </span>&#123;</span><br><span class=\"line\">PyObject *obj=type-&gt;tp_alloc(type, <span class=\"number\">0</span>);      <span class=\"comment\">// 申请 torch.Tensor 所需要的内存</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(obj) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> v = (THPVariable*)obj; <span class=\"comment\">// cast 为 THPVariable 类型指针，即 torch.Tensor 的基类 torch._C._TensorBase 的指针</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span>(&amp;v-&gt;cdata) Variable(<span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(var));    <span class=\"comment\">// 指定内存中，移动构造 Variable（C++ 版本的 Tensor）</span></span><br><span class=\"line\">    v-&gt;cdata.set_pyobj(obj);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">return</span> obj;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>示例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">&lt;<span class=\"class\"><span class=\"keyword\">class</span> '<span class=\"title\">torch</span>.<span class=\"title\">Tensor</span>'&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p>好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。</p>\n","site":{"data":{}},"excerpt":"<p>在 <a href=\"PyTorch-2\">PyTorch-2</a> 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的<a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>，了解其中各个函数的具体实现。</p>","more":"<h1 id=\"torch-包\"><a href=\"#torch-包\" class=\"headerlink\" title=\"torch 包\"></a>torch 包</h1><p>从 <code>torch/__init__.py</code> 中可以查看所有的 torch 包的所有字段，包括：</p>\n<ol>\n<li>直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等</li>\n<li>从 torch 包的模块中导入的函数/类，如<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></li>\n<li>从 torch._C 中导入的字段/函数/类</li>\n<li>从 torch._C._VariableFunctions 导入的字段/函数</li>\n</ol>\n<p>PyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。</p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p>这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：</p>\n<ol>\n<li>caffe2/CMakeLists.txt 中的文件生成语句为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;&#x2F;csrc&#x2F;autograd&#x2F;generated&#x2F;python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools&#x2F;setup_helpers&#x2F;generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure></li>\n<li>执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：</li>\n<li>在 caffe2/CMakeLists.txt 中有语句<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(..&#x2F;cmake&#x2F;Codegen.cmake)</span><br></pre></td></tr></table></figure></li>\n<li>在文件 cmake/Codegen.cmake 中调用 <code>gen.py</code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n（在 aten/src/ATen/native/native_functions.yaml 找到 <code>empty</code> 的函数签名）</li>\n<li>aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure></li>\n<li>根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件<ul>\n<li>CMakeLists.txt 中的 add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt 中有，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;&#x2F;ATen)</span><br></pre></td></tr></table></figure>\n事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。</li>\n</ul>\n</li>\n</ol>\n<p>找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS &#x3D; CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, &#x2F;*traceable&#x3D;*&#x2F;$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r &#x3D; parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module&#x3D;False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env &#x3D; &#123;</span><br><span class=\"line\">            &#39;name&#39;: name,</span><br><span class=\"line\">            &#39;dispatch_name&#39;: &#39;dispatch_&#123;&#125;&#39;.format(name),</span><br><span class=\"line\">            &#39;pycname&#39;: &#39;THPVariable_&#123;&#125;&#39;.format(name),</span><br><span class=\"line\">            &#39;signature&#39;: [],</span><br><span class=\"line\">            &#39;max_args&#39;: max(len(o[&#39;arguments&#39;])+len(o[&#39;python_binding_arguments&#39;]) for o in declarations),</span><br><span class=\"line\">            &#39;unpack_self&#39;: [],</span><br><span class=\"line\">            &#39;dispatch&#39;: [],</span><br><span class=\"line\">            &#39;declare_namedtuple_return_types&#39;: &#39;&#39;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... &#x2F;&#x2F; 向 env 增加 key-value pair or 更新 env 中已有 key 的 value</span><br><span class=\"line\">        if len(declarations) &#x3D;&#x3D; 1 and len(declarations[0][&#39;args&#39;]) &#x3D;&#x3D; 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl &#x3D; PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&#39;flags&#39;] &#x3D; &#39;METH_VARARGS | METH_KEYWORDS&#39;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&#39;flags&#39;] +&#x3D; &#39; | METH_STATIC&#39;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n<p>通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。</p>\n<h2 id=\"empty-定义\"><a href=\"#empty-定义\" class=\"headerlink\" title=\"empty 定义\"></a>empty 定义</h2><p>我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out&#x3D;None, ScalarType dtype&#x3D;None, Layout layout&#x3D;torch.strided, Device device&#x3D;None, bool requires_grad&#x3D;False)&quot;,</span><br><span class=\"line\">    &#125;, &#x2F;*tracebalbe*&#x2F;true); &#x2F;&#x2F; 大括号初始化器，得到函数签名的vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r &#x3D; parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F; 函数签名在vector中的下标</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  &#x2F;&#x2F; parameter &#39;out&#39; is None</span><br><span class=\"line\">            auto size &#x3D; r.intlist(0);</span><br><span class=\"line\">            auto dtype &#x3D; r.scalartype(2);</span><br><span class=\"line\">            auto device &#x3D; r.device(4);</span><br><span class=\"line\">            const auto options &#x3D; TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：</p>\n<ol>\n<li><code>out</code> 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor</li>\n<li><code>out</code> 参数不为None, 则检查 <code>out</code> 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 <code>out</code> 的 requires_grad 属性重置为参数 requires_grad</li>\n</ol>\n<p>然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; empty 函数调用者提供了 Tensor &#39;out&#39;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#x2F;&#x2F; empty 函数调用者未提供 Tensor &#39;out&#39;，需要根据参数 options 创建</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"有输出-Tensor\"><a href=\"#有输出-Tensor\" class=\"headerlink\" title=\"有输出 Tensor\"></a>有输出 Tensor</h3><p>我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&#39;Functions.h&#39;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n<p>现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(&#x2F;* native_actuals *&#x2F; result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>显然，根据输出 Tensor 是否是稀疏的进行不同的处理。</p>\n<ol>\n<li><p>输出 Tensor 是稀疏的</p>\n<p>对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(&#x2F;* actuals *&#x2F; self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。</p>\n</li>\n<li><p>输出 Tensor 是密集的</p>\n<p>Tensor 的 resize_ 方法定义见 TensorMethods.h，为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor &amp; <span class=\"title\">CPUByteType::resize_</span><span class=\"params\">(Tensor &amp; self, IntList <span class=\"built_in\">size</span>)</span> <span class=\"keyword\">const</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, <span class=\"built_in\">size</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList <span class=\"built_in\">size</span>)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">// 获取 Tensor 的底层实现类对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块</span></span><br><span class=\"line\">    resize_impl_cpu_(self_, <span class=\"built_in\">size</span>, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(<span class=\"built_in\">size</span>.<span class=\"built_in\">size</span>()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList <span class=\"built_in\">size</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == <span class=\"built_in\">size</span> &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存</span></span><br><span class=\"line\">        <span class=\"comment\">// size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// 未指定步幅，则数据布局是近邻的，连续的，即，stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(<span class=\"built_in\">size</span>);    <span class=\"comment\">// 设置当前 size 为新的 size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">// 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize 操作</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset() 通常返回 0</span></span><br><span class=\"line\">        <span class=\"comment\">// 只有需要更多的元素数量时，才重新分配内存</span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> <span class=\"built_in\">size</span>)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">size</span> != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*<span class=\"built_in\">size</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存</span></span><br><span class=\"line\">        <span class=\"comment\">// 设置 Tensor 内部存储指向新数据，同时返回旧数据</span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">// 旧数据 size，元素数量</span></span><br><span class=\"line\">        storage-&gt;set_numel(<span class=\"built_in\">size</span>);                <span class=\"comment\">// 设置新的元素熟路</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// 内存数据考虑</span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.<span class=\"built_in\">get</span>(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么</p>\n<ol>\n<li>N1 &gt;= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中</li>\n<li>N1 &lt; N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。</li>\n</ol>\n</li>\n</ol>\n<p>实验验证上述 torch.empty 过程，代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize 到一个较大的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize 到一个较小的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\"># 再次 resize 到一个较大的 size</span></span><br></pre></td></tr></table></figure>\n<p>本次输出如下，从以下结果可以看出是符合上述过程的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n<h3 id=\"无输出-Tensor\"><a href=\"#无输出-Tensor\" class=\"headerlink\" title=\"无输出 Tensor\"></a>无输出 Tensor</h3><p>回到 torch/csrc/autograd/generated/python_torch_functions_dispatch.h 这个文件，无输出 tensor 的 dispatch_empty 函数直接调用 torch::empty，此函数位于 torch/csrc/autograd/generated/variable_factories.h，在此函数定义中，我们暂且忽略 jit 跟踪部分的代码（用于跟踪记录有关 Tensor 的操作），核心的实现代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> at::Tensor <span class=\"title\">empty</span><span class=\"params\">(at::IntList <span class=\"built_in\">size</span>, <span class=\"keyword\">const</span> at::TensorOptions &amp; options=&#123;&#125;)</span> </span>&#123;</span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    at::Tensor tensor = at::empty(<span class=\"built_in\">size</span>, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad()); <span class=\"comment\">// 将 Tensor 转为 Variable</span></span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 at::empty 位于安装时动态生成的源文件 Functions.h（见上文分析），这个函数定义为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> Tensor <span class=\"title\">empty</span><span class=\"params\">(IntList <span class=\"built_in\">size</span>, <span class=\"keyword\">const</span> TensorOptions &amp; options)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::getType(options).empty(<span class=\"built_in\">size</span>, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>与有输出 Tensor 的 empty 函数实现逻辑类似，这里 at::getType(options) 根据给定的 options 构造出 TypeExtendedInterface 接口的具体实现类的 instance，具体而言，根据 options.backend(), options.dtype() 和 options.is_variable() 获取具体类型实例，而类型实例是事先注册好的，以 CPU 为 backend 为例说明，在 aten/src/ATen/Context.cpp 中 Context 的构造函数中，执行函数 register_cpu_types(this) 进行注册，而 register_cpu_type(Context* context) 函数位于 build/aten/src/ATen/RegisterCPU.cpp 文件，此文件由 aten/src/ATen/gen.py 中的 generate_outputs 函数生成（关于 gen.py 文件，上文也有介绍），现在我们来看看 register_cpu_types 中注册哪些类型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPUByteType</span><br><span class=\"line\">CPUCharType</span><br><span class=\"line\">CPUDoubleType</span><br><span class=\"line\">CPUFloatType</span><br><span class=\"line\">CPUIntType</span><br><span class=\"line\">CPULongType</span><br><span class=\"line\">CPUShortType</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>我们随便选择一个类型，比如 CPUByteType，查看其中 empty 函数实现，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor CPUByteType::empty(IntList size, const TensorOptions &amp; options) const &#123;</span><br><span class=\"line\">    const DeviceGuard device_guard(options.device());   &#x2F;&#x2F; 准备在指定 device 上构造 Tensor</span><br><span class=\"line\">    return at::native::empty_cpu(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上 at::native::empty_cpu 函数位于 aten/src/ATen/native/TensorFactories.cpp 中，函数实现体的部分为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span>* allocator = at::getCPUAllocator();</span><br><span class=\"line\"><span class=\"keyword\">int64_t</span> nelements = prod_intlist(<span class=\"built_in\">size</span>); <span class=\"comment\">// 连乘（各维度值），得到总元素数量</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = options.dtype();</span><br><span class=\"line\"><span class=\"keyword\">auto</span> storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class=\"line\">    dtype,</span><br><span class=\"line\">    nelements,</span><br><span class=\"line\">    allocator-&gt;allocate(nelements*dtype.itemsize()),</span><br><span class=\"line\">    allocator,</span><br><span class=\"line\">    <span class=\"comment\">/*resizeable=*/</span><span class=\"literal\">true</span></span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor = detail::make_tensor&lt;TensorImpl&gt;(storage_impl, at::CPUTensorId(), <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n<p>继续查看 c10::make_intrusive<StorageImpl> 函数定义，不难得知先进行 new StorageImpl(…)，然后 wrap 为 intrusive_ptr，在 <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> 中，我们讨论过各种 Tensor 的底层实现都是 StorageImpl，所以 StorageImpl 对象可以通过 detail::make_tensor 转为对应的 Tensor。根据 at::getCPUAllocator 查看其定义得知获得的是 THDefaultAllocator 实例，其 allocate 方法调用 THAlloc 分配内存，THAlloc 内部调用 THAllocInternal 分配内存，而这个函数又使用 malloc（某些情况下也会使用 posix_memalign 申请对齐内存） 申请一块未初始化的内存。</p>\n<p>示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<p>结果为（每次执行结果可能不同，不固定）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1.6504e-12,3.0637e-41,1.6588e-12],</span><br><span class=\"line\">        [3.0637e-41,4.4842e-44,0.0000e+00]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tensor-的由来\"><a href=\"#Tensor-的由来\" class=\"headerlink\" title=\"Tensor 的由来\"></a>Tensor 的由来</h3><p>这里我们讨论 torch.empty 函数是如何返回得到 torch.Tensor 对象的。一开始，在 <code>torch/__init__.py</code> 中 <code>import autograd</code>，继而查看 <code>torch/autograd/__init__.py</code>，发现如下调用</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch._C._autograd_init():</span><br></pre></td></tr></table></figure>\n<p>_autograd_init 这个 python 函数在 torch/csrc/Module.cpp 中注册，其底层实现是由 THPAutograd_initExtension 完成，这个 c++ 函数声明位于头文件 torch/csrc/autograd/autograd.h 中，函数实现位于 torch/csrc/autograd/init.cpp 中，看下这个函数的部分定义</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 加载 torch/tensor.py 模块</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor_module = THPObjectPtr(PyImport_ImportModule(<span class=\"string\">\"torch.tensor\"</span>));</span><br><span class=\"line\"><span class=\"comment\">// 获取 torch/tensor.py 中的 Tensor 类型</span></span><br><span class=\"line\">THPVariableClass = PyObject_GetAttrString(tensor_module, <span class=\"string\">\"Tensor\"</span>);</span><br></pre></td></tr></table></figure>\n<p>要知道 <code>THPVariableClass</code> 这个类型对象声明位于 torch/csrc/autograd/python_variable.h 中</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THP_API PyObject *THPVariableClass;</span><br></pre></td></tr></table></figure>\n<p>嗯，这是一个 extern 声明，其原本定义位于 torch/csrc/autograd/python_variable.cpp 中。好，现在回到 torch.empty 的底层 c++ 实现部分，即上文 THPVariable_empty 函数定义，在 dispatch_empty 返回一个 Variable 对象后，经过 wrap 包装为 PyObject，来看 wrap 的定义，位于 torch/csrc/autograd/utils/wrap_outputs.h 中，其内部调用 THPVariable_Wrap，这个函数也位于 torch/csrc/autograd/python_variable.cpp，与 THPVariableClass 定义在同一个文件中，前面我们已经知道 THPVariableClass 就是 torch/tensor.py 中的 Tensor 类型，而这里 THPVariable_Wrap 通过调用 THPVariable_NewWithVar 将 Variable 对象包装为 THPVariableClass 对象，即 Tensor 实例。THPVariable_NewWithVar 函数定义的部分代码为</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject* <span class=\"title\">THPVariable_NewWithVar</span><span class=\"params\">(PyTypeObject* type, Variable var)</span> </span>&#123;</span><br><span class=\"line\">PyObject *obj=type-&gt;tp_alloc(type, <span class=\"number\">0</span>);      <span class=\"comment\">// 申请 torch.Tensor 所需要的内存</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(obj) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> v = (THPVariable*)obj; <span class=\"comment\">// cast 为 THPVariable 类型指针，即 torch.Tensor 的基类 torch._C._TensorBase 的指针</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span>(&amp;v-&gt;cdata) Variable(<span class=\"built_in\">std</span>::<span class=\"built_in\">move</span>(var));    <span class=\"comment\">// 指定内存中，移动构造 Variable（C++ 版本的 Tensor）</span></span><br><span class=\"line\">    v-&gt;cdata.set_pyobj(obj);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">return</span> obj;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>示例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">&lt;<span class=\"class\"><span class=\"keyword\">class</span> '<span class=\"title\">torch</span>.<span class=\"title\">Tensor</span>'&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p>好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。</p>"},{"title":"cv.methods","p":"cv/methods1","date":"2020-05-27T03:30:03.000Z","mathjax":true,"_content":"\n来总结一下 CV 中的常见方法或技巧。      \n\n<!-- more -->\n\n# 原始图像处理\n## crop resize\n\n由于\n\n## resize\n图像resize 后，可能会变大，也可能变小。\n","source":"_posts/cv/methods.md","raw":"---\ntitle: cv.methods\np: cv/methods1\ndate: 2020-05-27 11:30:03\ntags: CV\nmathjax: true\n---\n\n来总结一下 CV 中的常见方法或技巧。      \n\n<!-- more -->\n\n# 原始图像处理\n## crop resize\n\n由于\n\n## resize\n图像resize 后，可能会变大，也可能变小。\n","slug":"cv/methods","published":1,"updated":"2021-01-18T07:04:04.578Z","_id":"ckjmjft0b00006gdj4qg2d2e7","comments":1,"layout":"post","photos":[],"link":"","content":"<p>来总结一下 CV 中的常见方法或技巧。      </p>\n<a id=\"more\"></a>\n\n<h1 id=\"原始图像处理\"><a href=\"#原始图像处理\" class=\"headerlink\" title=\"原始图像处理\"></a>原始图像处理</h1><h2 id=\"crop-resize\"><a href=\"#crop-resize\" class=\"headerlink\" title=\"crop resize\"></a>crop resize</h2><p>由于</p>\n<h2 id=\"resize\"><a href=\"#resize\" class=\"headerlink\" title=\"resize\"></a>resize</h2><p>图像resize 后，可能会变大，也可能变小。</p>\n","site":{"data":{}},"excerpt":"<p>来总结一下 CV 中的常见方法或技巧。      </p>","more":"<h1 id=\"原始图像处理\"><a href=\"#原始图像处理\" class=\"headerlink\" title=\"原始图像处理\"></a>原始图像处理</h1><h2 id=\"crop-resize\"><a href=\"#crop-resize\" class=\"headerlink\" title=\"crop resize\"></a>crop resize</h2><p>由于</p>\n<h2 id=\"resize\"><a href=\"#resize\" class=\"headerlink\" title=\"resize\"></a>resize</h2><p>图像resize 后，可能会变大，也可能变小。</p>"},{"title":"Histogram Equalization","date":"2020-06-23T10:07:00.000Z","_content":"","source":"_posts/dip/hist_equal.md","raw":"---\ntitle: Histogram Equalization\ndate: 2020-06-23 18:07:00\ntags:\n---\n","slug":"dip/hist_equal","published":1,"updated":"2020-06-23T10:07:00.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjmjft0d00016gdj7lpq8lbj","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"深度学习环境搭建","p":"pytorch/DL-env","date":"2019-09-09T08:38:11.000Z","_content":"本文仅针对 ubuntu 系统进行讨论。\n\n搭建深度学习环境 tensorflow，pytorch 等，如需要 GPU 加速，一般选择安装 \n<!-- more -->\nNVIDIA cuda 工具包，以前通常需要预先安装：\n1. NVIDIA driver\n2. cuda\n3. cudnn\n\n# NVIDIA driver\n曾经安装 NVIDIA 驱动采取的比较复杂的方法，先是 close nouveau，让系统进入命令行，然后安装事先下载好的驱动安装文件 `NVIDIA-Linux-x86_64-xxx.xxx.run`，这里使用比较简单的安装方法，打开 ubuntu 的 Software & Updates，点击 Additional Drivers，选择 `Using NVIDIA driver metapackage from nvidia-driver-xxx` 然后点击 `Apply Changes` 进行驱动安装。\n\n# cuda & cudnn\n直接使用 conda 安装 pytorch，安装过程比较简单，执行以下命令即可，\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n```\n\n如果下载较慢，可使用清华源，执行命令，\n```\nconda config\nconda config --set show_channel_urls yes\n\ncd ~\nvi .condarc\n```\n打开 `.condarc` 文件并添加\n```\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n\n然后执行\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n```\n\n# 安装 tensorflow\n```\nconda install tensorflow-gpu\n```\n这条命令会自动安装合适的 cuda 和 cudnn\n\n# PyTorch\n下载源码\n```\ngit clone https://github.com/pytorch/pytorch.git\n```\n更新源码\n```\ngit reset --hard\ngit pull origin master\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n安装\n```\npython setup.py install\n```\n如果不想安装，仅仅编译生成，那么执行\n```\npython setup.py build\n```\n由于我这里安装了 Clang 和 llvm，设置了 `CPLUS_INCLUDE_PATH`，导致生成的过程中 include 到 llvm 的头文件，所以可以临时屏蔽 llvm 的头文件路径，\n```\nexport CPLUS_INCLUDE_PATH='' && python setup.py build\n```\n\n现在增加了 develop 模式，conda 环境下执行\n```\ncd [pytorch github project root path]\npython setup.py develop\n```\n这样只会生成一个位于 site-packages 中的 torch 的 egg-link，可以随时修改 pytorch 源码，而不用重装 pytorch。\n\n容器运行 pytorch-gpu\n```\ndocker pull pytorch/pytorch:1.7.1-cuda11.0-cudnn8-level\ndocker run -p 9527:22 --gpus all -rm -itd --ipc=host -v /home/xx/xx:/home/xx/xx --name pytorch pytorch/pytorch:1.7.1-cuda11.0-cudnn8-level\n```\n\n# 安装 mmdetection\n以 conda 虚拟环境名称 `base` 为例，其中已经安装了 PyTorch，cudatoolkit 等包，还有一些包如`matplotlib, pillow, opencv` 等图像处理相关的包也需要安装，可以使用\n```\nconda list\n```\n查看。现在要安装 mmdetection，\n\n1. 安装 mmcv，这是 open-mmlab 一众库的基础，\n```\ngit clone https://github.com/open-mmlab/mmcv.git\n```\n\n进入根目录\n```\ncd mmcv\n```\n以开发模式安装，\n```\nMMCV_WITH_OPS=1 pip install -e .\n```\n其中，MMCV_WITH_OPS 默认为 0，表示 cpu 模式下运行 mmcv（轻量级模式），为 1 时 启用 cuda 加速。`pip install -e .` 表示可编辑模型安装当前目录的库，等同于 `python setup.py develop`。\n\n下载 mmdetection 源码，\n```\ngit clone https://github.com/open-mmlab/mmdetection.git\n```\n同样地，以开发模式安装，\n```\ncd mmdetection\npip install -r requirements/build.txt\npython setup.py develop\n```\n","source":"_posts/pytorch/DL-env.md","raw":"---\ntitle: 深度学习环境搭建\np: pytorch/DL-env\ndate: 2019-09-09 16:38:11\ntags: DL\n---\n本文仅针对 ubuntu 系统进行讨论。\n\n搭建深度学习环境 tensorflow，pytorch 等，如需要 GPU 加速，一般选择安装 \n<!-- more -->\nNVIDIA cuda 工具包，以前通常需要预先安装：\n1. NVIDIA driver\n2. cuda\n3. cudnn\n\n# NVIDIA driver\n曾经安装 NVIDIA 驱动采取的比较复杂的方法，先是 close nouveau，让系统进入命令行，然后安装事先下载好的驱动安装文件 `NVIDIA-Linux-x86_64-xxx.xxx.run`，这里使用比较简单的安装方法，打开 ubuntu 的 Software & Updates，点击 Additional Drivers，选择 `Using NVIDIA driver metapackage from nvidia-driver-xxx` 然后点击 `Apply Changes` 进行驱动安装。\n\n# cuda & cudnn\n直接使用 conda 安装 pytorch，安装过程比较简单，执行以下命令即可，\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n```\n\n如果下载较慢，可使用清华源，执行命令，\n```\nconda config\nconda config --set show_channel_urls yes\n\ncd ~\nvi .condarc\n```\n打开 `.condarc` 文件并添加\n```\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n\n然后执行\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n```\n\n# 安装 tensorflow\n```\nconda install tensorflow-gpu\n```\n这条命令会自动安装合适的 cuda 和 cudnn\n\n# PyTorch\n下载源码\n```\ngit clone https://github.com/pytorch/pytorch.git\n```\n更新源码\n```\ngit reset --hard\ngit pull origin master\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n安装\n```\npython setup.py install\n```\n如果不想安装，仅仅编译生成，那么执行\n```\npython setup.py build\n```\n由于我这里安装了 Clang 和 llvm，设置了 `CPLUS_INCLUDE_PATH`，导致生成的过程中 include 到 llvm 的头文件，所以可以临时屏蔽 llvm 的头文件路径，\n```\nexport CPLUS_INCLUDE_PATH='' && python setup.py build\n```\n\n现在增加了 develop 模式，conda 环境下执行\n```\ncd [pytorch github project root path]\npython setup.py develop\n```\n这样只会生成一个位于 site-packages 中的 torch 的 egg-link，可以随时修改 pytorch 源码，而不用重装 pytorch。\n\n容器运行 pytorch-gpu\n```\ndocker pull pytorch/pytorch:1.7.1-cuda11.0-cudnn8-level\ndocker run -p 9527:22 --gpus all -rm -itd --ipc=host -v /home/xx/xx:/home/xx/xx --name pytorch pytorch/pytorch:1.7.1-cuda11.0-cudnn8-level\n```\n\n# 安装 mmdetection\n以 conda 虚拟环境名称 `base` 为例，其中已经安装了 PyTorch，cudatoolkit 等包，还有一些包如`matplotlib, pillow, opencv` 等图像处理相关的包也需要安装，可以使用\n```\nconda list\n```\n查看。现在要安装 mmdetection，\n\n1. 安装 mmcv，这是 open-mmlab 一众库的基础，\n```\ngit clone https://github.com/open-mmlab/mmcv.git\n```\n\n进入根目录\n```\ncd mmcv\n```\n以开发模式安装，\n```\nMMCV_WITH_OPS=1 pip install -e .\n```\n其中，MMCV_WITH_OPS 默认为 0，表示 cpu 模式下运行 mmcv（轻量级模式），为 1 时 启用 cuda 加速。`pip install -e .` 表示可编辑模型安装当前目录的库，等同于 `python setup.py develop`。\n\n下载 mmdetection 源码，\n```\ngit clone https://github.com/open-mmlab/mmdetection.git\n```\n同样地，以开发模式安装，\n```\ncd mmdetection\npip install -r requirements/build.txt\npython setup.py develop\n```\n","slug":"pytorch/DL-env","published":1,"updated":"2021-02-09T01:22:35.061Z","_id":"ckjmjft0e00036gdj6he8h57y","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文仅针对 ubuntu 系统进行讨论。</p>\n<p>搭建深度学习环境 tensorflow，pytorch 等，如需要 GPU 加速，一般选择安装 </p>\n<a id=\"more\"></a>\n<p>NVIDIA cuda 工具包，以前通常需要预先安装：</p>\n<ol>\n<li>NVIDIA driver</li>\n<li>cuda</li>\n<li>cudnn</li>\n</ol>\n<h1 id=\"NVIDIA-driver\"><a href=\"#NVIDIA-driver\" class=\"headerlink\" title=\"NVIDIA driver\"></a>NVIDIA driver</h1><p>曾经安装 NVIDIA 驱动采取的比较复杂的方法，先是 close nouveau，让系统进入命令行，然后安装事先下载好的驱动安装文件 <code>NVIDIA-Linux-x86_64-xxx.xxx.run</code>，这里使用比较简单的安装方法，打开 ubuntu 的 Software &amp; Updates，点击 Additional Drivers，选择 <code>Using NVIDIA driver metapackage from nvidia-driver-xxx</code> 然后点击 <code>Apply Changes</code> 进行驱动安装。</p>\n<h1 id=\"cuda-amp-cudnn\"><a href=\"#cuda-amp-cudnn\" class=\"headerlink\" title=\"cuda &amp; cudnn\"></a>cuda &amp; cudnn</h1><p>直接使用 conda 安装 pytorch，安装过程比较简单，执行以下命令即可，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure>\n\n<p>如果下载较慢，可使用清华源，执行命令，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br><span class=\"line\"></span><br><span class=\"line\">cd ~</span><br><span class=\"line\">vi .condarc</span><br></pre></td></tr></table></figure>\n<p>打开 <code>.condarc</code> 文件并添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">show_channel_urls: true</span><br><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main</span><br><span class=\"line\">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br></pre></td></tr></table></figure>\n\n<p>然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装-tensorflow\"><a href=\"#安装-tensorflow\" class=\"headerlink\" title=\"安装 tensorflow\"></a>安装 tensorflow</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install tensorflow-gpu</span><br></pre></td></tr></table></figure>\n<p>这条命令会自动安装合适的 cuda 和 cudnn</p>\n<h1 id=\"PyTorch\"><a href=\"#PyTorch\" class=\"headerlink\" title=\"PyTorch\"></a>PyTorch</h1><p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch.git</span><br></pre></td></tr></table></figure>\n<p>更新源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard</span><br><span class=\"line\">git pull origin master</span><br><span class=\"line\">git submodule sync</span><br><span class=\"line\">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>\n\n<p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<p>如果不想安装，仅仅编译生成，那么执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br></pre></td></tr></table></figure>\n<p>由于我这里安装了 Clang 和 llvm，设置了 <code>CPLUS_INCLUDE_PATH</code>，导致生成的过程中 include 到 llvm 的头文件，所以可以临时屏蔽 llvm 的头文件路径，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export CPLUS_INCLUDE_PATH&#x3D;&#39;&#39; &amp;&amp; python setup.py build</span><br></pre></td></tr></table></figure>\n\n<p>现在增加了 develop 模式，conda 环境下执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd [pytorch github project root path]</span><br><span class=\"line\">python setup.py develop</span><br></pre></td></tr></table></figure>\n<p>这样只会生成一个位于 site-packages 中的 torch 的 egg-link，可以随时修改 pytorch 源码，而不用重装 pytorch。</p>\n<p>容器运行 pytorch-gpu</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull pytorch&#x2F;pytorch:1.7.1-cuda11.0-cudnn8-level</span><br><span class=\"line\">docker run -p 9527:22 --gpus all -rm -itd --ipc&#x3D;host -v &#x2F;home&#x2F;xx&#x2F;xx:&#x2F;home&#x2F;xx&#x2F;xx --name pytorch pytorch&#x2F;pytorch:1.7.1-cuda11.0-cudnn8-level</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装-mmdetection\"><a href=\"#安装-mmdetection\" class=\"headerlink\" title=\"安装 mmdetection\"></a>安装 mmdetection</h1><p>以 conda 虚拟环境名称 <code>base</code> 为例，其中已经安装了 PyTorch，cudatoolkit 等包，还有一些包如<code>matplotlib, pillow, opencv</code> 等图像处理相关的包也需要安装，可以使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n<p>查看。现在要安装 mmdetection，</p>\n<ol>\n<li>安装 mmcv，这是 open-mmlab 一众库的基础，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmcv.git</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>进入根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd mmcv</span><br></pre></td></tr></table></figure>\n<p>以开发模式安装，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MMCV_WITH_OPS&#x3D;1 pip install -e .</span><br></pre></td></tr></table></figure>\n<p>其中，MMCV_WITH_OPS 默认为 0，表示 cpu 模式下运行 mmcv（轻量级模式），为 1 时 启用 cuda 加速。<code>pip install -e .</code> 表示可编辑模型安装当前目录的库，等同于 <code>python setup.py develop</code>。</p>\n<p>下载 mmdetection 源码，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmdetection.git</span><br></pre></td></tr></table></figure>\n<p>同样地，以开发模式安装，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd mmdetection</span><br><span class=\"line\">pip install -r requirements&#x2F;build.txt</span><br><span class=\"line\">python setup.py develop</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>本文仅针对 ubuntu 系统进行讨论。</p>\n<p>搭建深度学习环境 tensorflow，pytorch 等，如需要 GPU 加速，一般选择安装 </p>","more":"<p>NVIDIA cuda 工具包，以前通常需要预先安装：</p>\n<ol>\n<li>NVIDIA driver</li>\n<li>cuda</li>\n<li>cudnn</li>\n</ol>\n<h1 id=\"NVIDIA-driver\"><a href=\"#NVIDIA-driver\" class=\"headerlink\" title=\"NVIDIA driver\"></a>NVIDIA driver</h1><p>曾经安装 NVIDIA 驱动采取的比较复杂的方法，先是 close nouveau，让系统进入命令行，然后安装事先下载好的驱动安装文件 <code>NVIDIA-Linux-x86_64-xxx.xxx.run</code>，这里使用比较简单的安装方法，打开 ubuntu 的 Software &amp; Updates，点击 Additional Drivers，选择 <code>Using NVIDIA driver metapackage from nvidia-driver-xxx</code> 然后点击 <code>Apply Changes</code> 进行驱动安装。</p>\n<h1 id=\"cuda-amp-cudnn\"><a href=\"#cuda-amp-cudnn\" class=\"headerlink\" title=\"cuda &amp; cudnn\"></a>cuda &amp; cudnn</h1><p>直接使用 conda 安装 pytorch，安装过程比较简单，执行以下命令即可，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure>\n\n<p>如果下载较慢，可使用清华源，执行命令，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br><span class=\"line\"></span><br><span class=\"line\">cd ~</span><br><span class=\"line\">vi .condarc</span><br></pre></td></tr></table></figure>\n<p>打开 <code>.condarc</code> 文件并添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">show_channel_urls: true</span><br><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main</span><br><span class=\"line\">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class=\"line\">  pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br></pre></td></tr></table></figure>\n\n<p>然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装-tensorflow\"><a href=\"#安装-tensorflow\" class=\"headerlink\" title=\"安装 tensorflow\"></a>安装 tensorflow</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install tensorflow-gpu</span><br></pre></td></tr></table></figure>\n<p>这条命令会自动安装合适的 cuda 和 cudnn</p>\n<h1 id=\"PyTorch\"><a href=\"#PyTorch\" class=\"headerlink\" title=\"PyTorch\"></a>PyTorch</h1><p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch.git</span><br></pre></td></tr></table></figure>\n<p>更新源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard</span><br><span class=\"line\">git pull origin master</span><br><span class=\"line\">git submodule sync</span><br><span class=\"line\">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>\n\n<p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<p>如果不想安装，仅仅编译生成，那么执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br></pre></td></tr></table></figure>\n<p>由于我这里安装了 Clang 和 llvm，设置了 <code>CPLUS_INCLUDE_PATH</code>，导致生成的过程中 include 到 llvm 的头文件，所以可以临时屏蔽 llvm 的头文件路径，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export CPLUS_INCLUDE_PATH&#x3D;&#39;&#39; &amp;&amp; python setup.py build</span><br></pre></td></tr></table></figure>\n\n<p>现在增加了 develop 模式，conda 环境下执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd [pytorch github project root path]</span><br><span class=\"line\">python setup.py develop</span><br></pre></td></tr></table></figure>\n<p>这样只会生成一个位于 site-packages 中的 torch 的 egg-link，可以随时修改 pytorch 源码，而不用重装 pytorch。</p>\n<p>容器运行 pytorch-gpu</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull pytorch&#x2F;pytorch:1.7.1-cuda11.0-cudnn8-level</span><br><span class=\"line\">docker run -p 9527:22 --gpus all -rm -itd --ipc&#x3D;host -v &#x2F;home&#x2F;xx&#x2F;xx:&#x2F;home&#x2F;xx&#x2F;xx --name pytorch pytorch&#x2F;pytorch:1.7.1-cuda11.0-cudnn8-level</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装-mmdetection\"><a href=\"#安装-mmdetection\" class=\"headerlink\" title=\"安装 mmdetection\"></a>安装 mmdetection</h1><p>以 conda 虚拟环境名称 <code>base</code> 为例，其中已经安装了 PyTorch，cudatoolkit 等包，还有一些包如<code>matplotlib, pillow, opencv</code> 等图像处理相关的包也需要安装，可以使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n<p>查看。现在要安装 mmdetection，</p>\n<ol>\n<li>安装 mmcv，这是 open-mmlab 一众库的基础，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmcv.git</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>进入根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd mmcv</span><br></pre></td></tr></table></figure>\n<p>以开发模式安装，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MMCV_WITH_OPS&#x3D;1 pip install -e .</span><br></pre></td></tr></table></figure>\n<p>其中，MMCV_WITH_OPS 默认为 0，表示 cpu 模式下运行 mmcv（轻量级模式），为 1 时 启用 cuda 加速。<code>pip install -e .</code> 表示可编辑模型安装当前目录的库，等同于 <code>python setup.py develop</code>。</p>\n<p>下载 mmdetection 源码，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmdetection.git</span><br></pre></td></tr></table></figure>\n<p>同样地，以开发模式安装，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd mmdetection</span><br><span class=\"line\">pip install -r requirements&#x2F;build.txt</span><br><span class=\"line\">python setup.py develop</span><br></pre></td></tr></table></figure>"},{"title":"Cross Entropy Loss","p":"dl/x_ent_loss","date":"2021-01-12T02:35:40.000Z","mathjax":true,"_content":"\n深度学习中有很多损失计算方式，不同的损失适合不同的问题，所以有必要对损失进行归类总结一下。\n<!-- more -->\n# 分类问题\n考虑以下两种分类问题。\n## 多分类\n多分类（二分类可以看作是其特殊的一种情况）指每个样本属于`C` 个分类中的一个，预测值通常是一个长度为 `C` 的向量，ground truth 为 one-hot 向量。\n## 多标签分类\n共 `C` 种分类，每个样本可以属于其中一种或多种分类，网络输出依然是 `C` 长度的向量，ground truth 向量元素值为 `0` 或 `1`，且可以有多个 `1`。\n\n# 激活函数\n对于分类问题，网络最后一层的输出为长度为`C` 的向量（通常使用全连接），其元素值的范围为实数域，所以需要一个激活层，从而方便损失计算（以及梯度反向传播计算）。激活层有以下几种：\n## Sigmoid\n对向量的每个元素（神经元）使用 Sigmoid 函数，使得输出向量位于 `(0,1)` 之间，此时向量中最大元素对应的就是样本的预测分类。\n\nSigmoid 常用于二分类问题，此时标记为正类/负类，ground truth 为 `1/0`，网络最后一层可以使用单个神经元，神经元的输出经过 Sigmoid 后处于范围 `(0,1)`，如果值大于等于 0.5，那么就属于正类，否则属于负类。\n## Softmax\nSoftmax 除了可以将神经元输出压缩到 `(0,1)` 之间，还使各元素值和为 1。Softmax 通常用于多分类。\n\n# 分类损失\n# Cross-Entropy Loss\n单个样本的交叉熵损失计算公式为，\n$$CE=-\\sum_{i=1}^C y_i \\log (x_i)$$\n其中 $y_i$ 为 ground truth 向量中的第`i`个元素值，$x_i$ 为预测向量中第 `i` 个值，从这里也可以看出，正是有了上述激活层，才使得 $\\log(x_i)$ 这一项有意义。多分类问题中，由于 GT 是 one-hot 向量，所以只有 $i=c$ 这项保留下来，其他项均为零。\n\n二分类问题中，GT: $y=1$ 表示正，$y=0$ 表示负，最后一层输出为单个值（经过 Sigmoid 激活）`x`，所以单个样本的交叉熵损失为 \n$$CE=-y \\log (x) - (1-y) \\log(1-x)$$\n\n这跟负对数似然是一样的。\n\n多标签分类问题中，假设一共有 `C` 种分类，对于单个样本而言，需要做 `C` 次二分类预测，交叉熵损失为\n$$CE=-\\sum_{i=1}^C y_i \\log(x_i) - (1-y_i) \\log(1-x_i)$$\n\n\n## Balanced Cross Entropy\n一种常见的解决分类不均衡的方法是引入一个权重因子 $\\alpha \\in [0,1]$，定义 \n$$\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\ 1-\\alpha & y=0 \\end{cases}$$\n\n考虑二分类问题，为了表示简便，定义真实分类对应的预测值为\n$$x_t=\\begin{cases} x & y=1 \\\\ 1-x & y=0 \\end{cases}$$\n\n$\\alpha$ 均衡交叉熵损失为\n$$CE=-\\alpha_t \\log(x_t)$$\n通常，取 $\\alpha$ 为类别的频率的倒数，这样就增加了低频类别的贡献，降低了高频类别的贡献。也可以将 $\\alpha$ 看作超参，并使用 cross validation 获取一个较好的值。\n\n## Focal Loss\n单个样本的 Focal loss 为，\n$$FL=-(1-x_t) ^{\\gamma} \\log(x_t)$$\n展开则为\n$$FL=-y(1-x)^{\\gamma} \\log(x) -(1-y)x^{\\gamma} \\log(1-x)$$\n\n其中，$\\gamma \\ge 0$。\n\n\n相比于交叉熵损失，Focal loss 增加了一个 scale 因子 $(1-x_t)^{\\gamma}$，当 $x_t$ 越大，表明分类越是正确，越是应该降低其对损失的贡献，所以这个 scale 因子动态降低了那些 easy 样本对损失的贡献，从而使模型更专注于处理 hard 样本。\n\n类似地，可以对 Focal loss 进行 $\\alpha$ 均衡以处理类别不均衡的问题，此时 Focal loss 变体为\n$$FL=-\\alpha_t (1-x_t)^{\\gamma} \\log (x_t)$$\n","source":"_posts/dl/x_ent_loss.md","raw":"---\ntitle: Cross Entropy Loss\np: dl/x_ent_loss\ndate: 2021-01-12 10:35:40\ntags: Deep Learning\nmathjax: true\n---\n\n深度学习中有很多损失计算方式，不同的损失适合不同的问题，所以有必要对损失进行归类总结一下。\n<!-- more -->\n# 分类问题\n考虑以下两种分类问题。\n## 多分类\n多分类（二分类可以看作是其特殊的一种情况）指每个样本属于`C` 个分类中的一个，预测值通常是一个长度为 `C` 的向量，ground truth 为 one-hot 向量。\n## 多标签分类\n共 `C` 种分类，每个样本可以属于其中一种或多种分类，网络输出依然是 `C` 长度的向量，ground truth 向量元素值为 `0` 或 `1`，且可以有多个 `1`。\n\n# 激活函数\n对于分类问题，网络最后一层的输出为长度为`C` 的向量（通常使用全连接），其元素值的范围为实数域，所以需要一个激活层，从而方便损失计算（以及梯度反向传播计算）。激活层有以下几种：\n## Sigmoid\n对向量的每个元素（神经元）使用 Sigmoid 函数，使得输出向量位于 `(0,1)` 之间，此时向量中最大元素对应的就是样本的预测分类。\n\nSigmoid 常用于二分类问题，此时标记为正类/负类，ground truth 为 `1/0`，网络最后一层可以使用单个神经元，神经元的输出经过 Sigmoid 后处于范围 `(0,1)`，如果值大于等于 0.5，那么就属于正类，否则属于负类。\n## Softmax\nSoftmax 除了可以将神经元输出压缩到 `(0,1)` 之间，还使各元素值和为 1。Softmax 通常用于多分类。\n\n# 分类损失\n# Cross-Entropy Loss\n单个样本的交叉熵损失计算公式为，\n$$CE=-\\sum_{i=1}^C y_i \\log (x_i)$$\n其中 $y_i$ 为 ground truth 向量中的第`i`个元素值，$x_i$ 为预测向量中第 `i` 个值，从这里也可以看出，正是有了上述激活层，才使得 $\\log(x_i)$ 这一项有意义。多分类问题中，由于 GT 是 one-hot 向量，所以只有 $i=c$ 这项保留下来，其他项均为零。\n\n二分类问题中，GT: $y=1$ 表示正，$y=0$ 表示负，最后一层输出为单个值（经过 Sigmoid 激活）`x`，所以单个样本的交叉熵损失为 \n$$CE=-y \\log (x) - (1-y) \\log(1-x)$$\n\n这跟负对数似然是一样的。\n\n多标签分类问题中，假设一共有 `C` 种分类，对于单个样本而言，需要做 `C` 次二分类预测，交叉熵损失为\n$$CE=-\\sum_{i=1}^C y_i \\log(x_i) - (1-y_i) \\log(1-x_i)$$\n\n\n## Balanced Cross Entropy\n一种常见的解决分类不均衡的方法是引入一个权重因子 $\\alpha \\in [0,1]$，定义 \n$$\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\ 1-\\alpha & y=0 \\end{cases}$$\n\n考虑二分类问题，为了表示简便，定义真实分类对应的预测值为\n$$x_t=\\begin{cases} x & y=1 \\\\ 1-x & y=0 \\end{cases}$$\n\n$\\alpha$ 均衡交叉熵损失为\n$$CE=-\\alpha_t \\log(x_t)$$\n通常，取 $\\alpha$ 为类别的频率的倒数，这样就增加了低频类别的贡献，降低了高频类别的贡献。也可以将 $\\alpha$ 看作超参，并使用 cross validation 获取一个较好的值。\n\n## Focal Loss\n单个样本的 Focal loss 为，\n$$FL=-(1-x_t) ^{\\gamma} \\log(x_t)$$\n展开则为\n$$FL=-y(1-x)^{\\gamma} \\log(x) -(1-y)x^{\\gamma} \\log(1-x)$$\n\n其中，$\\gamma \\ge 0$。\n\n\n相比于交叉熵损失，Focal loss 增加了一个 scale 因子 $(1-x_t)^{\\gamma}$，当 $x_t$ 越大，表明分类越是正确，越是应该降低其对损失的贡献，所以这个 scale 因子动态降低了那些 easy 样本对损失的贡献，从而使模型更专注于处理 hard 样本。\n\n类似地，可以对 Focal loss 进行 $\\alpha$ 均衡以处理类别不均衡的问题，此时 Focal loss 变体为\n$$FL=-\\alpha_t (1-x_t)^{\\gamma} \\log (x_t)$$\n","slug":"dl/x_ent_loss","published":1,"updated":"2021-01-12T09:33:12.331Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxr7wgn0000y4dj0j506tzu","content":"<p>深度学习中有很多损失计算方式，不同的损失适合不同的问题，所以有必要对损失进行归类总结一下。</p>\n<a id=\"more\"></a>\n<h1 id=\"分类问题\"><a href=\"#分类问题\" class=\"headerlink\" title=\"分类问题\"></a>分类问题</h1><p>考虑以下两种分类问题。</p>\n<h2 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h2><p>多分类（二分类可以看作是其特殊的一种情况）指每个样本属于<code>C</code> 个分类中的一个，预测值通常是一个长度为 <code>C</code> 的向量，ground truth 为 one-hot 向量。</p>\n<h2 id=\"多标签分类\"><a href=\"#多标签分类\" class=\"headerlink\" title=\"多标签分类\"></a>多标签分类</h2><p>共 <code>C</code> 种分类，每个样本可以属于其中一种或多种分类，网络输出依然是 <code>C</code> 长度的向量，ground truth 向量元素值为 <code>0</code> 或 <code>1</code>，且可以有多个 <code>1</code>。</p>\n<h1 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h1><p>对于分类问题，网络最后一层的输出为长度为<code>C</code> 的向量（通常使用全连接），其元素值的范围为实数域，所以需要一个激活层，从而方便损失计算（以及梯度反向传播计算）。激活层有以下几种：</p>\n<h2 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h2><p>对向量的每个元素（神经元）使用 Sigmoid 函数，使得输出向量位于 <code>(0,1)</code> 之间，此时向量中最大元素对应的就是样本的预测分类。</p>\n<p>Sigmoid 常用于二分类问题，此时标记为正类/负类，ground truth 为 <code>1/0</code>，网络最后一层可以使用单个神经元，神经元的输出经过 Sigmoid 后处于范围 <code>(0,1)</code>，如果值大于等于 0.5，那么就属于正类，否则属于负类。</p>\n<h2 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h2><p>Softmax 除了可以将神经元输出压缩到 <code>(0,1)</code> 之间，还使各元素值和为 1。Softmax 通常用于多分类。</p>\n<h1 id=\"分类损失\"><a href=\"#分类损失\" class=\"headerlink\" title=\"分类损失\"></a>分类损失</h1><h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>单个样本的交叉熵损失计算公式为，<br>$$CE=-\\sum_{i=1}^C y_i \\log (x_i)$$<br>其中 $y_i$ 为 ground truth 向量中的第<code>i</code>个元素值，$x_i$ 为预测向量中第 <code>i</code> 个值，从这里也可以看出，正是有了上述激活层，才使得 $\\log(x_i)$ 这一项有意义。多分类问题中，由于 GT 是 one-hot 向量，所以只有 $i=c$ 这项保留下来，其他项均为零。</p>\n<p>二分类问题中，GT: $y=1$ 表示正，$y=0$ 表示负，最后一层输出为单个值（经过 Sigmoid 激活）<code>x</code>，所以单个样本的交叉熵损失为<br>$$CE=-y \\log (x) - (1-y) \\log(1-x)$$</p>\n<p>这跟负对数似然是一样的。</p>\n<p>多标签分类问题中，假设一共有 <code>C</code> 种分类，对于单个样本而言，需要做 <code>C</code> 次二分类预测，交叉熵损失为<br>$$CE=-\\sum_{i=1}^C y_i \\log(x_i) - (1-y_i) \\log(1-x_i)$$</p>\n<h2 id=\"Balanced-Cross-Entropy\"><a href=\"#Balanced-Cross-Entropy\" class=\"headerlink\" title=\"Balanced Cross Entropy\"></a>Balanced Cross Entropy</h2><p>一种常见的解决分类不均衡的方法是引入一个权重因子 $\\alpha \\in [0,1]$，定义<br>$$\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\ 1-\\alpha &amp; y=0 \\end{cases}$$</p>\n<p>考虑二分类问题，为了表示简便，定义真实分类对应的预测值为<br>$$x_t=\\begin{cases} x &amp; y=1 \\ 1-x &amp; y=0 \\end{cases}$$</p>\n<p>$\\alpha$ 均衡交叉熵损失为<br>$$CE=-\\alpha_t \\log(x_t)$$<br>通常，取 $\\alpha$ 为类别的频率的倒数，这样就增加了低频类别的贡献，降低了高频类别的贡献。也可以将 $\\alpha$ 看作超参，并使用 cross validation 获取一个较好的值。</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>单个样本的 Focal loss 为，<br>$$FL=-(1-x_t) ^{\\gamma} \\log(x_t)$$<br>展开则为<br>$$FL=-y(1-x)^{\\gamma} \\log(x) -(1-y)x^{\\gamma} \\log(1-x)$$</p>\n<p>其中，$\\gamma \\ge 0$。</p>\n<p>相比于交叉熵损失，Focal loss 增加了一个 scale 因子 $(1-x_t)^{\\gamma}$，当 $x_t$ 越大，表明分类越是正确，越是应该降低其对损失的贡献，所以这个 scale 因子动态降低了那些 easy 样本对损失的贡献，从而使模型更专注于处理 hard 样本。</p>\n<p>类似地，可以对 Focal loss 进行 $\\alpha$ 均衡以处理类别不均衡的问题，此时 Focal loss 变体为<br>$$FL=-\\alpha_t (1-x_t)^{\\gamma} \\log (x_t)$$</p>\n","site":{"data":{}},"excerpt":"<p>深度学习中有很多损失计算方式，不同的损失适合不同的问题，所以有必要对损失进行归类总结一下。</p>","more":"<h1 id=\"分类问题\"><a href=\"#分类问题\" class=\"headerlink\" title=\"分类问题\"></a>分类问题</h1><p>考虑以下两种分类问题。</p>\n<h2 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h2><p>多分类（二分类可以看作是其特殊的一种情况）指每个样本属于<code>C</code> 个分类中的一个，预测值通常是一个长度为 <code>C</code> 的向量，ground truth 为 one-hot 向量。</p>\n<h2 id=\"多标签分类\"><a href=\"#多标签分类\" class=\"headerlink\" title=\"多标签分类\"></a>多标签分类</h2><p>共 <code>C</code> 种分类，每个样本可以属于其中一种或多种分类，网络输出依然是 <code>C</code> 长度的向量，ground truth 向量元素值为 <code>0</code> 或 <code>1</code>，且可以有多个 <code>1</code>。</p>\n<h1 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h1><p>对于分类问题，网络最后一层的输出为长度为<code>C</code> 的向量（通常使用全连接），其元素值的范围为实数域，所以需要一个激活层，从而方便损失计算（以及梯度反向传播计算）。激活层有以下几种：</p>\n<h2 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h2><p>对向量的每个元素（神经元）使用 Sigmoid 函数，使得输出向量位于 <code>(0,1)</code> 之间，此时向量中最大元素对应的就是样本的预测分类。</p>\n<p>Sigmoid 常用于二分类问题，此时标记为正类/负类，ground truth 为 <code>1/0</code>，网络最后一层可以使用单个神经元，神经元的输出经过 Sigmoid 后处于范围 <code>(0,1)</code>，如果值大于等于 0.5，那么就属于正类，否则属于负类。</p>\n<h2 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h2><p>Softmax 除了可以将神经元输出压缩到 <code>(0,1)</code> 之间，还使各元素值和为 1。Softmax 通常用于多分类。</p>\n<h1 id=\"分类损失\"><a href=\"#分类损失\" class=\"headerlink\" title=\"分类损失\"></a>分类损失</h1><h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>单个样本的交叉熵损失计算公式为，<br>$$CE=-\\sum_{i=1}^C y_i \\log (x_i)$$<br>其中 $y_i$ 为 ground truth 向量中的第<code>i</code>个元素值，$x_i$ 为预测向量中第 <code>i</code> 个值，从这里也可以看出，正是有了上述激活层，才使得 $\\log(x_i)$ 这一项有意义。多分类问题中，由于 GT 是 one-hot 向量，所以只有 $i=c$ 这项保留下来，其他项均为零。</p>\n<p>二分类问题中，GT: $y=1$ 表示正，$y=0$ 表示负，最后一层输出为单个值（经过 Sigmoid 激活）<code>x</code>，所以单个样本的交叉熵损失为<br>$$CE=-y \\log (x) - (1-y) \\log(1-x)$$</p>\n<p>这跟负对数似然是一样的。</p>\n<p>多标签分类问题中，假设一共有 <code>C</code> 种分类，对于单个样本而言，需要做 <code>C</code> 次二分类预测，交叉熵损失为<br>$$CE=-\\sum_{i=1}^C y_i \\log(x_i) - (1-y_i) \\log(1-x_i)$$</p>\n<h2 id=\"Balanced-Cross-Entropy\"><a href=\"#Balanced-Cross-Entropy\" class=\"headerlink\" title=\"Balanced Cross Entropy\"></a>Balanced Cross Entropy</h2><p>一种常见的解决分类不均衡的方法是引入一个权重因子 $\\alpha \\in [0,1]$，定义<br>$$\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\ 1-\\alpha &amp; y=0 \\end{cases}$$</p>\n<p>考虑二分类问题，为了表示简便，定义真实分类对应的预测值为<br>$$x_t=\\begin{cases} x &amp; y=1 \\ 1-x &amp; y=0 \\end{cases}$$</p>\n<p>$\\alpha$ 均衡交叉熵损失为<br>$$CE=-\\alpha_t \\log(x_t)$$<br>通常，取 $\\alpha$ 为类别的频率的倒数，这样就增加了低频类别的贡献，降低了高频类别的贡献。也可以将 $\\alpha$ 看作超参，并使用 cross validation 获取一个较好的值。</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>单个样本的 Focal loss 为，<br>$$FL=-(1-x_t) ^{\\gamma} \\log(x_t)$$<br>展开则为<br>$$FL=-y(1-x)^{\\gamma} \\log(x) -(1-y)x^{\\gamma} \\log(1-x)$$</p>\n<p>其中，$\\gamma \\ge 0$。</p>\n<p>相比于交叉熵损失，Focal loss 增加了一个 scale 因子 $(1-x_t)^{\\gamma}$，当 $x_t$ 越大，表明分类越是正确，越是应该降低其对损失的贡献，所以这个 scale 因子动态降低了那些 easy 样本对损失的贡献，从而使模型更专注于处理 hard 样本。</p>\n<p>类似地，可以对 Focal loss 进行 $\\alpha$ 均衡以处理类别不均衡的问题，此时 Focal loss 变体为<br>$$FL=-\\alpha_t (1-x_t)^{\\gamma} \\log (x_t)$$</p>"},{"title":"Loss 2","p":"pytorch/loss_2","date":"2021-01-13T08:54:38.000Z","mathjax":true,"_content":"继上一篇 [loss 1](2021/1/12/pytorch/loss_1)，本篇介绍 PyTorch 的其他损失。\n<!-- more -->\n\n\n# MarginRankingLoss\n给定两个输入 $x_1, \\ x_2$，以及一个 label 值 $y \\in \\{1,-1\\}$。当 $y=1$，认为 $x_1$ 应该比 $x_2$ 大；当 $y=-1$，认为 $x_1$ 应该比 $x_2$ 小，所以损失为\n$$l=\\max(0, -y(x_1-x_2) + \\text{margin})$$\n上式中增加了一个 `margin` 项，根据\n$$-y(x_1-x_2)+\\text{margin} \\le 0$$\n\n当 $y=1$ 时，需要满足 $x_1\\ge x_2+\\text{margin}$ 损失才降为 0。\n\n当 $y=-1$ 时，需要满足 $x_1+\\text{margin} \\le x_2$ 损失才降为 0。\n\n适用于（二）<b>分类</b>问题。\n# MultiLabelMarginLoss\n适用于多标签多分类问题。每个类别独立进行二分类（为正 or 为负）预测，预测值 x 是一个 2D tensor，shape 为 $(N,C)$，其中 $N$ 表示批大小，$C$ 表示类别数。target 与 x 同 shape。暂且考虑单个样本，此时 x 和 target 均为长度 `C` 的向量，x 表示各分类的预测概率，target （用 y 表示）表示样本所属分类索引，例如 $y=(3,0,-1,1)$，表示样本属于 `0` 分类和 `3` 分类，从第一个负值开始，之后的全部忽略。借鉴 `MarginRankingLoss` 思想，对于预测值 x，认为其中<b>样本所属分类的元素值比样本不属分类的元素值大</b>，这个例子中，样本所属分类为 $\\{0,3\\}$，所以认为应该是 $x_0,\\ x_3 > x_1,\\ x_2$，据此不难理解单个样本的损失为\n$$l=\\sum_{i,j} \\frac {\\max[0, 1-(x_{y_j} - x_i)]} C$$\n其中，$j \\in \\mathcal J=\\{0,1,...,k-1\\}$，且 $y_k<0$，$i \\in \\{0,1,...,C-1\\}-\\{y_j|j \\in \\mathcal J\\}$，即， $j$ 为 target 向量中开始的连续非负元素索引，$y_j$ 表示样本所属分类索引，i 为样本不属分类索引。\n\n当分类正确时，损失为0，此时需要满足条件 $1-(x_{y_j}-x_i)\\le 0 \\Rightarrow x_{y_j}\\ge 1+x_i$，这说明降低损失会使得样本所属分类的预测概率 $x_{y_j} \\rightarrow 0$，样本不属分类的预测概率 $x_i \\rightarrow 0$。在 test 阶段，对预测值 x 设置一个低阈值即可。\n\n# SoftMarginLoss\n适用于二分类问题。上面两种 MarginLoss 均采用了 `max(0,x)` 函数，这个函数在 `x=0` 处不可导。`SoftMarginLoss` 借助 logistic 函数解决了这个问题。Logistic 函数\n$$\\sigma(x)=\\frac 1 {1+\\exp (-x)}$$\n预测值 x，分类 $y\\in \\{1,-1\\}$，似然函数为\n$$\\mathcal L =\\mathbb I(y=1)f(x)+\\mathbb I(y=-1)(1-f(x))=[1+\\exp(-yx)]^{-1}$$\n 负对数似然函数（损失）为\n$$l= \\log(1+\\exp(-yx))$$\n所以 `SoftMarginLoss` <b>就是 logistic 回归的负对数似然损失</b>。预测输入 input tensor 的 shape 为 $(*)$，其中 $*$ 表示任意维度，target 与 input 的 shape 相同。损失按像素计算，输出与 input 同 shape，如果按求和或平均归约，那么输出为一标量。\n\n# MultiLabelSoftMarginLoss\n适用于多标签多分类问题。每个类别各自独立做二分类（为正或负）。input 和 target 有相同的 shape：$(N,C)$，target 值为 0 或 1（这与 SoftMarginLoss 的 1 或 -1 竟然不统一）。于是，单个样本的损失为，\n$$l=-\\frac 1 C \\sum_{i=1}^C y_i \\log \\left(\\frac 1 {1+\\exp(-x_i)}\\right )+(1-y_i)\\log \\left(\\frac {\\exp(-x_i)} {1+\\exp(-x_i)}\\right)$$\n由于这里考虑单个样本，所以上式 $x, \\ y$ 均为长度 $C$ 的向量，由于 y 值取值范围不同，所以上式与 `SoftMarginLoss` 的损失表达式略有不同，但是本质上都是 logistic 负对数似然损失。\n\n输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出 tensor 为一标量。\n\n类签名：\n```python\nMultiLabelSoftMarginLoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str='mean')\n```\n`weight` 如果给定，那么是一个 shape 为 $(C,)$  的 tensor，用于给每个 channel/类别 一个权重。\n\n# MultiMarginLoss\n适用于多分类（单标签）问题。input 为一个 2D tensor $(N,C)$，target shape 为 $(N,)$，表示样本的分类索引，故 $y_i \\in \\{0,1,...,C-1\\}$，对于单个样本而言，此时输入为一个长度 C 的向量 x，target 为标量，也记为 y，表示样本分类索引，显然我们要 $x_y > x_i$，其中 $i \\neq y$，margin-based 损失为\n$$l=\\frac {\\sum_{i \\neq y} \\max(0, d-(x_y-x_i))^p} C$$\n其中 $d$ 为 margin，也就是说需要 $x_y \\ge x_i+d$，样本所属分类的预测概率比其他分类的预测概率大 $d$，损失才为 0。\n\np 值可为 1 或 2，用于控制损失变化速度。还可以给每个类型增加一个权重，此时损失为，\n$$l=\\frac {\\sum_{i \\neq y} \\max[0, w_y(d-(x_y-x_i))^p]} C$$\n注意，权重 $w_y$ 不参与幂运算，且只有样本所属分类对于的权重因子起作用。\n\n类签名：\n```python\nMultiMarginLoss(p: int=1, margin: float=1.0, weight: Optional[torch.Tensor]=None, size_average=None, reduce=None, reduction: str='mean')\n```\ninput shape 为 $(N,C)$，target shape 为 $(N,)$，output 的 shape 为 $(N,)$，可以按求和或平均归约，此时 output 为一标量。\n\n# TripletMarginLoss\n三个tensor：$a, \\ p, \\ n$，分别表示 anchor，正例和负例，shape 均为 $(N,D)$，其中 $N$ 为批大小，$D$ 为特征数。p 表示与 a 同类的另一个样本的特征，n 表示与 a 不同类的样本特征，显然，需要 p 与 a 的特征尽量相近，n 与 a 的特征尽量远离。\n传统上是以 pair 的形式来度量损失，即 $(p,a)$ 为正例对，$(n,a)$ 为负例对，一般表示为 $(x_1, x_2， l)$，当 $l=1$ 表示是正例对，$l=-1$ 表示是负例对，此时损失定义为\n$$l=\\begin{cases} \\Vert \\mathbf x_1-\\mathbf x_2 \\Vert_2 & l=1 \\\\ \\max(0, d-\\Vert \\mathbf x_1- \\mathbf x_2\\Vert_2) & l=-1 \\end{cases}$$\n$l=1$ 是正例对，所以 $\\mathbf x_1$ 应该要尽量接近 $\\mathbf x_2$；$l=-1$ 是负例对，$\\mathbf x_1$ 尽量要远离 $\\mathbf x_2$，且要相距 $d$ 以上。\n\n这里 `TripletMarginLoss` 将 `(a,p,n)` 三者当成一个整体，margin ranking-based 损失定义如下，\n$$l=\\max[d(a,p) - d(a,n)+d_0, 0]$$\n$$d(\\mathbf x_1, \\mathbf x_2)=\\Vert \\mathbf x_1 - \\mathbf x_2 \\Vert_p$$\n其中，$d_0$ 为 margin，计算特征空间中的距离时，使用的是 p 范数，这个 p 与前面正例 p 不一样，根据上下文不难区分。\n\n类签名：\n```python\nTripletMarginLoss(margin: float=1.0, p: float=2.0, eps: float=1e-06, swap: bool=False, size_average=None, reduce=None, reduction: str='mean')\n```\n`swap` 指示是否交换 anchor 和 positive，这用于 hard negative mining。若 `swap=True`，那么 $d(a,n) = d(p,n)$，也就是说，使用 `(p,n)` 的距离作为 negative 与 anchor 的距离。\n\nforward 方法的参数为 anchor, positive 和 negative 三个特征 tensor，shape 均为 $(N,D)$，输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出为一标量。\n\n更多细节可以参考 [`Learning local feature descriptors with triplets and shallow convolutional neural networks`](www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf)\n\n# TripletMarginWithDistanceLoss\n`TripletMarginLoss` 中距离使用的是 p 范数，这里是通过参数提供自定义的距离参数。anchor，positive 和 negative 三个 tensor 的 shape 为 $(N,*)$ ，其中 $*$ 为任意维度，输出 tensor 的未归约 shape 为 $(N,)$，否则为一标量。\n\n# HingeEmbeddingLoss\n$x$ 表示距离（例如 L1 范数），$y \\in \\{1,-1\\}$ 标识是相似还是相反，损失为，\n$$l = \\begin{cases} x & y=1 \\\\ \\max(0, d-x) & y=-1 \\end{cases}$$\n其中 $d$ 为 margin。\n\n输入 x 和 y 的 shape 均为任意维度 $(*)$，输出未归约的 shape 也是 $(*)$，否则为一标量。\n\n# CosineEmbeddingLoss\n`y=1` 表示两个（归一化）向量应该相近，`y=-1` 表示应该相差很远。\n损失如下，\n$$l=\\begin{cases} 1- \\cos(x_1,x_2) & y=1 \\\\ \\max[0, \\cos(x_1,x_2) - d] & y=-1 \\end{cases}$$\n其中 $d$ 表示 margin，默认为 0。\n\n# CTCLoss\n参考文献 [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/icml_2006.pdf)\nRNN 相关的应用领域暂未涉及。略，以后填坑。","source":"_posts/pytorch/loss_2.md","raw":"---\ntitle: Loss 2\np: pytorch/loss_2\ndate: 2021-01-13 16:54:38\ntags: PyTorch\nmathjax: true\n---\n继上一篇 [loss 1](2021/1/12/pytorch/loss_1)，本篇介绍 PyTorch 的其他损失。\n<!-- more -->\n\n\n# MarginRankingLoss\n给定两个输入 $x_1, \\ x_2$，以及一个 label 值 $y \\in \\{1,-1\\}$。当 $y=1$，认为 $x_1$ 应该比 $x_2$ 大；当 $y=-1$，认为 $x_1$ 应该比 $x_2$ 小，所以损失为\n$$l=\\max(0, -y(x_1-x_2) + \\text{margin})$$\n上式中增加了一个 `margin` 项，根据\n$$-y(x_1-x_2)+\\text{margin} \\le 0$$\n\n当 $y=1$ 时，需要满足 $x_1\\ge x_2+\\text{margin}$ 损失才降为 0。\n\n当 $y=-1$ 时，需要满足 $x_1+\\text{margin} \\le x_2$ 损失才降为 0。\n\n适用于（二）<b>分类</b>问题。\n# MultiLabelMarginLoss\n适用于多标签多分类问题。每个类别独立进行二分类（为正 or 为负）预测，预测值 x 是一个 2D tensor，shape 为 $(N,C)$，其中 $N$ 表示批大小，$C$ 表示类别数。target 与 x 同 shape。暂且考虑单个样本，此时 x 和 target 均为长度 `C` 的向量，x 表示各分类的预测概率，target （用 y 表示）表示样本所属分类索引，例如 $y=(3,0,-1,1)$，表示样本属于 `0` 分类和 `3` 分类，从第一个负值开始，之后的全部忽略。借鉴 `MarginRankingLoss` 思想，对于预测值 x，认为其中<b>样本所属分类的元素值比样本不属分类的元素值大</b>，这个例子中，样本所属分类为 $\\{0,3\\}$，所以认为应该是 $x_0,\\ x_3 > x_1,\\ x_2$，据此不难理解单个样本的损失为\n$$l=\\sum_{i,j} \\frac {\\max[0, 1-(x_{y_j} - x_i)]} C$$\n其中，$j \\in \\mathcal J=\\{0,1,...,k-1\\}$，且 $y_k<0$，$i \\in \\{0,1,...,C-1\\}-\\{y_j|j \\in \\mathcal J\\}$，即， $j$ 为 target 向量中开始的连续非负元素索引，$y_j$ 表示样本所属分类索引，i 为样本不属分类索引。\n\n当分类正确时，损失为0，此时需要满足条件 $1-(x_{y_j}-x_i)\\le 0 \\Rightarrow x_{y_j}\\ge 1+x_i$，这说明降低损失会使得样本所属分类的预测概率 $x_{y_j} \\rightarrow 0$，样本不属分类的预测概率 $x_i \\rightarrow 0$。在 test 阶段，对预测值 x 设置一个低阈值即可。\n\n# SoftMarginLoss\n适用于二分类问题。上面两种 MarginLoss 均采用了 `max(0,x)` 函数，这个函数在 `x=0` 处不可导。`SoftMarginLoss` 借助 logistic 函数解决了这个问题。Logistic 函数\n$$\\sigma(x)=\\frac 1 {1+\\exp (-x)}$$\n预测值 x，分类 $y\\in \\{1,-1\\}$，似然函数为\n$$\\mathcal L =\\mathbb I(y=1)f(x)+\\mathbb I(y=-1)(1-f(x))=[1+\\exp(-yx)]^{-1}$$\n 负对数似然函数（损失）为\n$$l= \\log(1+\\exp(-yx))$$\n所以 `SoftMarginLoss` <b>就是 logistic 回归的负对数似然损失</b>。预测输入 input tensor 的 shape 为 $(*)$，其中 $*$ 表示任意维度，target 与 input 的 shape 相同。损失按像素计算，输出与 input 同 shape，如果按求和或平均归约，那么输出为一标量。\n\n# MultiLabelSoftMarginLoss\n适用于多标签多分类问题。每个类别各自独立做二分类（为正或负）。input 和 target 有相同的 shape：$(N,C)$，target 值为 0 或 1（这与 SoftMarginLoss 的 1 或 -1 竟然不统一）。于是，单个样本的损失为，\n$$l=-\\frac 1 C \\sum_{i=1}^C y_i \\log \\left(\\frac 1 {1+\\exp(-x_i)}\\right )+(1-y_i)\\log \\left(\\frac {\\exp(-x_i)} {1+\\exp(-x_i)}\\right)$$\n由于这里考虑单个样本，所以上式 $x, \\ y$ 均为长度 $C$ 的向量，由于 y 值取值范围不同，所以上式与 `SoftMarginLoss` 的损失表达式略有不同，但是本质上都是 logistic 负对数似然损失。\n\n输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出 tensor 为一标量。\n\n类签名：\n```python\nMultiLabelSoftMarginLoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str='mean')\n```\n`weight` 如果给定，那么是一个 shape 为 $(C,)$  的 tensor，用于给每个 channel/类别 一个权重。\n\n# MultiMarginLoss\n适用于多分类（单标签）问题。input 为一个 2D tensor $(N,C)$，target shape 为 $(N,)$，表示样本的分类索引，故 $y_i \\in \\{0,1,...,C-1\\}$，对于单个样本而言，此时输入为一个长度 C 的向量 x，target 为标量，也记为 y，表示样本分类索引，显然我们要 $x_y > x_i$，其中 $i \\neq y$，margin-based 损失为\n$$l=\\frac {\\sum_{i \\neq y} \\max(0, d-(x_y-x_i))^p} C$$\n其中 $d$ 为 margin，也就是说需要 $x_y \\ge x_i+d$，样本所属分类的预测概率比其他分类的预测概率大 $d$，损失才为 0。\n\np 值可为 1 或 2，用于控制损失变化速度。还可以给每个类型增加一个权重，此时损失为，\n$$l=\\frac {\\sum_{i \\neq y} \\max[0, w_y(d-(x_y-x_i))^p]} C$$\n注意，权重 $w_y$ 不参与幂运算，且只有样本所属分类对于的权重因子起作用。\n\n类签名：\n```python\nMultiMarginLoss(p: int=1, margin: float=1.0, weight: Optional[torch.Tensor]=None, size_average=None, reduce=None, reduction: str='mean')\n```\ninput shape 为 $(N,C)$，target shape 为 $(N,)$，output 的 shape 为 $(N,)$，可以按求和或平均归约，此时 output 为一标量。\n\n# TripletMarginLoss\n三个tensor：$a, \\ p, \\ n$，分别表示 anchor，正例和负例，shape 均为 $(N,D)$，其中 $N$ 为批大小，$D$ 为特征数。p 表示与 a 同类的另一个样本的特征，n 表示与 a 不同类的样本特征，显然，需要 p 与 a 的特征尽量相近，n 与 a 的特征尽量远离。\n传统上是以 pair 的形式来度量损失，即 $(p,a)$ 为正例对，$(n,a)$ 为负例对，一般表示为 $(x_1, x_2， l)$，当 $l=1$ 表示是正例对，$l=-1$ 表示是负例对，此时损失定义为\n$$l=\\begin{cases} \\Vert \\mathbf x_1-\\mathbf x_2 \\Vert_2 & l=1 \\\\ \\max(0, d-\\Vert \\mathbf x_1- \\mathbf x_2\\Vert_2) & l=-1 \\end{cases}$$\n$l=1$ 是正例对，所以 $\\mathbf x_1$ 应该要尽量接近 $\\mathbf x_2$；$l=-1$ 是负例对，$\\mathbf x_1$ 尽量要远离 $\\mathbf x_2$，且要相距 $d$ 以上。\n\n这里 `TripletMarginLoss` 将 `(a,p,n)` 三者当成一个整体，margin ranking-based 损失定义如下，\n$$l=\\max[d(a,p) - d(a,n)+d_0, 0]$$\n$$d(\\mathbf x_1, \\mathbf x_2)=\\Vert \\mathbf x_1 - \\mathbf x_2 \\Vert_p$$\n其中，$d_0$ 为 margin，计算特征空间中的距离时，使用的是 p 范数，这个 p 与前面正例 p 不一样，根据上下文不难区分。\n\n类签名：\n```python\nTripletMarginLoss(margin: float=1.0, p: float=2.0, eps: float=1e-06, swap: bool=False, size_average=None, reduce=None, reduction: str='mean')\n```\n`swap` 指示是否交换 anchor 和 positive，这用于 hard negative mining。若 `swap=True`，那么 $d(a,n) = d(p,n)$，也就是说，使用 `(p,n)` 的距离作为 negative 与 anchor 的距离。\n\nforward 方法的参数为 anchor, positive 和 negative 三个特征 tensor，shape 均为 $(N,D)$，输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出为一标量。\n\n更多细节可以参考 [`Learning local feature descriptors with triplets and shallow convolutional neural networks`](www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf)\n\n# TripletMarginWithDistanceLoss\n`TripletMarginLoss` 中距离使用的是 p 范数，这里是通过参数提供自定义的距离参数。anchor，positive 和 negative 三个 tensor 的 shape 为 $(N,*)$ ，其中 $*$ 为任意维度，输出 tensor 的未归约 shape 为 $(N,)$，否则为一标量。\n\n# HingeEmbeddingLoss\n$x$ 表示距离（例如 L1 范数），$y \\in \\{1,-1\\}$ 标识是相似还是相反，损失为，\n$$l = \\begin{cases} x & y=1 \\\\ \\max(0, d-x) & y=-1 \\end{cases}$$\n其中 $d$ 为 margin。\n\n输入 x 和 y 的 shape 均为任意维度 $(*)$，输出未归约的 shape 也是 $(*)$，否则为一标量。\n\n# CosineEmbeddingLoss\n`y=1` 表示两个（归一化）向量应该相近，`y=-1` 表示应该相差很远。\n损失如下，\n$$l=\\begin{cases} 1- \\cos(x_1,x_2) & y=1 \\\\ \\max[0, \\cos(x_1,x_2) - d] & y=-1 \\end{cases}$$\n其中 $d$ 表示 margin，默认为 0。\n\n# CTCLoss\n参考文献 [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/icml_2006.pdf)\nRNN 相关的应用领域暂未涉及。略，以后填坑。","slug":"pytorch/loss_2","published":1,"updated":"2021-01-15T07:12:11.771Z","_id":"ckjxr7whr0001y4dj8k6ubzq2","comments":1,"layout":"post","photos":[],"link":"","content":"<p>继上一篇 <a href=\"2021/1/12/pytorch/loss_1\">loss 1</a>，本篇介绍 PyTorch 的其他损失。</p>\n<a id=\"more\"></a>\n\n\n<h1 id=\"MarginRankingLoss\"><a href=\"#MarginRankingLoss\" class=\"headerlink\" title=\"MarginRankingLoss\"></a>MarginRankingLoss</h1><p>给定两个输入 $x_1, \\ x_2$，以及一个 label 值 $y \\in {1,-1}$。当 $y=1$，认为 $x_1$ 应该比 $x_2$ 大；当 $y=-1$，认为 $x_1$ 应该比 $x_2$ 小，所以损失为<br>$$l=\\max(0, -y(x_1-x_2) + \\text{margin})$$<br>上式中增加了一个 <code>margin</code> 项，根据<br>$$-y(x_1-x_2)+\\text{margin} \\le 0$$</p>\n<p>当 $y=1$ 时，需要满足 $x_1\\ge x_2+\\text{margin}$ 损失才降为 0。</p>\n<p>当 $y=-1$ 时，需要满足 $x_1+\\text{margin} \\le x_2$ 损失才降为 0。</p>\n<p>适用于（二）<b>分类</b>问题。</p>\n<h1 id=\"MultiLabelMarginLoss\"><a href=\"#MultiLabelMarginLoss\" class=\"headerlink\" title=\"MultiLabelMarginLoss\"></a>MultiLabelMarginLoss</h1><p>适用于多标签多分类问题。每个类别独立进行二分类（为正 or 为负）预测，预测值 x 是一个 2D tensor，shape 为 $(N,C)$，其中 $N$ 表示批大小，$C$ 表示类别数。target 与 x 同 shape。暂且考虑单个样本，此时 x 和 target 均为长度 <code>C</code> 的向量，x 表示各分类的预测概率，target （用 y 表示）表示样本所属分类索引，例如 $y=(3,0,-1,1)$，表示样本属于 <code>0</code> 分类和 <code>3</code> 分类，从第一个负值开始，之后的全部忽略。借鉴 <code>MarginRankingLoss</code> 思想，对于预测值 x，认为其中<b>样本所属分类的元素值比样本不属分类的元素值大</b>，这个例子中，样本所属分类为 ${0,3}$，所以认为应该是 $x_0,\\ x_3 &gt; x_1,\\ x_2$，据此不难理解单个样本的损失为<br>$$l=\\sum_{i,j} \\frac {\\max[0, 1-(x_{y_j} - x_i)]} C$$<br>其中，$j \\in \\mathcal J={0,1,…,k-1}$，且 $y_k&lt;0$，$i \\in {0,1,…,C-1}-{y_j|j \\in \\mathcal J}$，即， $j$ 为 target 向量中开始的连续非负元素索引，$y_j$ 表示样本所属分类索引，i 为样本不属分类索引。</p>\n<p>当分类正确时，损失为0，此时需要满足条件 $1-(x_{y_j}-x_i)\\le 0 \\Rightarrow x_{y_j}\\ge 1+x_i$，这说明降低损失会使得样本所属分类的预测概率 $x_{y_j} \\rightarrow 0$，样本不属分类的预测概率 $x_i \\rightarrow 0$。在 test 阶段，对预测值 x 设置一个低阈值即可。</p>\n<h1 id=\"SoftMarginLoss\"><a href=\"#SoftMarginLoss\" class=\"headerlink\" title=\"SoftMarginLoss\"></a>SoftMarginLoss</h1><p>适用于二分类问题。上面两种 MarginLoss 均采用了 <code>max(0,x)</code> 函数，这个函数在 <code>x=0</code> 处不可导。<code>SoftMarginLoss</code> 借助 logistic 函数解决了这个问题。Logistic 函数<br>$$\\sigma(x)=\\frac 1 {1+\\exp (-x)}$$<br>预测值 x，分类 $y\\in {1,-1}$，似然函数为<br>$$\\mathcal L =\\mathbb I(y=1)f(x)+\\mathbb I(y=-1)(1-f(x))=[1+\\exp(-yx)]^{-1}$$<br> 负对数似然函数（损失）为<br>$$l= \\log(1+\\exp(-yx))$$<br>所以 <code>SoftMarginLoss</code> <b>就是 logistic 回归的负对数似然损失</b>。预测输入 input tensor 的 shape 为 $(<em>)$，其中 $</em>$ 表示任意维度，target 与 input 的 shape 相同。损失按像素计算，输出与 input 同 shape，如果按求和或平均归约，那么输出为一标量。</p>\n<h1 id=\"MultiLabelSoftMarginLoss\"><a href=\"#MultiLabelSoftMarginLoss\" class=\"headerlink\" title=\"MultiLabelSoftMarginLoss\"></a>MultiLabelSoftMarginLoss</h1><p>适用于多标签多分类问题。每个类别各自独立做二分类（为正或负）。input 和 target 有相同的 shape：$(N,C)$，target 值为 0 或 1（这与 SoftMarginLoss 的 1 或 -1 竟然不统一）。于是，单个样本的损失为，<br>$$l=-\\frac 1 C \\sum_{i=1}^C y_i \\log \\left(\\frac 1 {1+\\exp(-x_i)}\\right )+(1-y_i)\\log \\left(\\frac {\\exp(-x_i)} {1+\\exp(-x_i)}\\right)$$<br>由于这里考虑单个样本，所以上式 $x, \\ y$ 均为长度 $C$ 的向量，由于 y 值取值范围不同，所以上式与 <code>SoftMarginLoss</code> 的损失表达式略有不同，但是本质上都是 logistic 负对数似然损失。</p>\n<p>输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出 tensor 为一标量。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultiLabelSoftMarginLoss(weight: Optional[torch.Tensor] = <span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>weight</code> 如果给定，那么是一个 shape 为 $(C,)$  的 tensor，用于给每个 channel/类别 一个权重。</p>\n<h1 id=\"MultiMarginLoss\"><a href=\"#MultiMarginLoss\" class=\"headerlink\" title=\"MultiMarginLoss\"></a>MultiMarginLoss</h1><p>适用于多分类（单标签）问题。input 为一个 2D tensor $(N,C)$，target shape 为 $(N,)$，表示样本的分类索引，故 $y_i \\in {0,1,…,C-1}$，对于单个样本而言，此时输入为一个长度 C 的向量 x，target 为标量，也记为 y，表示样本分类索引，显然我们要 $x_y &gt; x_i$，其中 $i \\neq y$，margin-based 损失为<br>$$l=\\frac {\\sum_{i \\neq y} \\max(0, d-(x_y-x_i))^p} C$$<br>其中 $d$ 为 margin，也就是说需要 $x_y \\ge x_i+d$，样本所属分类的预测概率比其他分类的预测概率大 $d$，损失才为 0。</p>\n<p>p 值可为 1 或 2，用于控制损失变化速度。还可以给每个类型增加一个权重，此时损失为，<br>$$l=\\frac {\\sum_{i \\neq y} \\max[0, w_y(d-(x_y-x_i))^p]} C$$<br>注意，权重 $w_y$ 不参与幂运算，且只有样本所属分类对于的权重因子起作用。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultiMarginLoss(p: int=<span class=\"number\">1</span>, margin: float=<span class=\"number\">1.0</span>, weight: Optional[torch.Tensor]=<span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p>input shape 为 $(N,C)$，target shape 为 $(N,)$，output 的 shape 为 $(N,)$，可以按求和或平均归约，此时 output 为一标量。</p>\n<h1 id=\"TripletMarginLoss\"><a href=\"#TripletMarginLoss\" class=\"headerlink\" title=\"TripletMarginLoss\"></a>TripletMarginLoss</h1><p>三个tensor：$a, \\ p, \\ n$，分别表示 anchor，正例和负例，shape 均为 $(N,D)$，其中 $N$ 为批大小，$D$ 为特征数。p 表示与 a 同类的另一个样本的特征，n 表示与 a 不同类的样本特征，显然，需要 p 与 a 的特征尽量相近，n 与 a 的特征尽量远离。<br>传统上是以 pair 的形式来度量损失，即 $(p,a)$ 为正例对，$(n,a)$ 为负例对，一般表示为 $(x_1, x_2， l)$，当 $l=1$ 表示是正例对，$l=-1$ 表示是负例对，此时损失定义为<br>$$l=\\begin{cases} \\Vert \\mathbf x_1-\\mathbf x_2 \\Vert_2 &amp; l=1 \\ \\max(0, d-\\Vert \\mathbf x_1- \\mathbf x_2\\Vert_2) &amp; l=-1 \\end{cases}$$<br>$l=1$ 是正例对，所以 $\\mathbf x_1$ 应该要尽量接近 $\\mathbf x_2$；$l=-1$ 是负例对，$\\mathbf x_1$ 尽量要远离 $\\mathbf x_2$，且要相距 $d$ 以上。</p>\n<p>这里 <code>TripletMarginLoss</code> 将 <code>(a,p,n)</code> 三者当成一个整体，margin ranking-based 损失定义如下，<br>$$l=\\max[d(a,p) - d(a,n)+d_0, 0]$$<br>$$d(\\mathbf x_1, \\mathbf x_2)=\\Vert \\mathbf x_1 - \\mathbf x_2 \\Vert_p$$<br>其中，$d_0$ 为 margin，计算特征空间中的距离时，使用的是 p 范数，这个 p 与前面正例 p 不一样，根据上下文不难区分。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TripletMarginLoss(margin: float=<span class=\"number\">1.0</span>, p: float=<span class=\"number\">2.0</span>, eps: float=<span class=\"number\">1e-06</span>, swap: bool=<span class=\"literal\">False</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>swap</code> 指示是否交换 anchor 和 positive，这用于 hard negative mining。若 <code>swap=True</code>，那么 $d(a,n) = d(p,n)$，也就是说，使用 <code>(p,n)</code> 的距离作为 negative 与 anchor 的距离。</p>\n<p>forward 方法的参数为 anchor, positive 和 negative 三个特征 tensor，shape 均为 $(N,D)$，输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出为一标量。</p>\n<p>更多细节可以参考 <a href=\"www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf\"><code>Learning local feature descriptors with triplets and shallow convolutional neural networks</code></a></p>\n<h1 id=\"TripletMarginWithDistanceLoss\"><a href=\"#TripletMarginWithDistanceLoss\" class=\"headerlink\" title=\"TripletMarginWithDistanceLoss\"></a>TripletMarginWithDistanceLoss</h1><p><code>TripletMarginLoss</code> 中距离使用的是 p 范数，这里是通过参数提供自定义的距离参数。anchor，positive 和 negative 三个 tensor 的 shape 为 $(N,<em>)$ ，其中 $</em>$ 为任意维度，输出 tensor 的未归约 shape 为 $(N,)$，否则为一标量。</p>\n<h1 id=\"HingeEmbeddingLoss\"><a href=\"#HingeEmbeddingLoss\" class=\"headerlink\" title=\"HingeEmbeddingLoss\"></a>HingeEmbeddingLoss</h1><p>$x$ 表示距离（例如 L1 范数），$y \\in {1,-1}$ 标识是相似还是相反，损失为，<br>$$l = \\begin{cases} x &amp; y=1 \\ \\max(0, d-x) &amp; y=-1 \\end{cases}$$<br>其中 $d$ 为 margin。</p>\n<p>输入 x 和 y 的 shape 均为任意维度 $(<em>)$，输出未归约的 shape 也是 $(</em>)$，否则为一标量。</p>\n<h1 id=\"CosineEmbeddingLoss\"><a href=\"#CosineEmbeddingLoss\" class=\"headerlink\" title=\"CosineEmbeddingLoss\"></a>CosineEmbeddingLoss</h1><p><code>y=1</code> 表示两个（归一化）向量应该相近，<code>y=-1</code> 表示应该相差很远。<br>损失如下，<br>$$l=\\begin{cases} 1- \\cos(x_1,x_2) &amp; y=1 \\ \\max[0, \\cos(x_1,x_2) - d] &amp; y=-1 \\end{cases}$$<br>其中 $d$ 表示 margin，默认为 0。</p>\n<h1 id=\"CTCLoss\"><a href=\"#CTCLoss\" class=\"headerlink\" title=\"CTCLoss\"></a>CTCLoss</h1><p>参考文献 <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\" target=\"_blank\" rel=\"noopener\">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a><br>RNN 相关的应用领域暂未涉及。略，以后填坑。</p>\n","site":{"data":{}},"excerpt":"<p>继上一篇 <a href=\"2021/1/12/pytorch/loss_1\">loss 1</a>，本篇介绍 PyTorch 的其他损失。</p>","more":"<h1 id=\"MarginRankingLoss\"><a href=\"#MarginRankingLoss\" class=\"headerlink\" title=\"MarginRankingLoss\"></a>MarginRankingLoss</h1><p>给定两个输入 $x_1, \\ x_2$，以及一个 label 值 $y \\in {1,-1}$。当 $y=1$，认为 $x_1$ 应该比 $x_2$ 大；当 $y=-1$，认为 $x_1$ 应该比 $x_2$ 小，所以损失为<br>$$l=\\max(0, -y(x_1-x_2) + \\text{margin})$$<br>上式中增加了一个 <code>margin</code> 项，根据<br>$$-y(x_1-x_2)+\\text{margin} \\le 0$$</p>\n<p>当 $y=1$ 时，需要满足 $x_1\\ge x_2+\\text{margin}$ 损失才降为 0。</p>\n<p>当 $y=-1$ 时，需要满足 $x_1+\\text{margin} \\le x_2$ 损失才降为 0。</p>\n<p>适用于（二）<b>分类</b>问题。</p>\n<h1 id=\"MultiLabelMarginLoss\"><a href=\"#MultiLabelMarginLoss\" class=\"headerlink\" title=\"MultiLabelMarginLoss\"></a>MultiLabelMarginLoss</h1><p>适用于多标签多分类问题。每个类别独立进行二分类（为正 or 为负）预测，预测值 x 是一个 2D tensor，shape 为 $(N,C)$，其中 $N$ 表示批大小，$C$ 表示类别数。target 与 x 同 shape。暂且考虑单个样本，此时 x 和 target 均为长度 <code>C</code> 的向量，x 表示各分类的预测概率，target （用 y 表示）表示样本所属分类索引，例如 $y=(3,0,-1,1)$，表示样本属于 <code>0</code> 分类和 <code>3</code> 分类，从第一个负值开始，之后的全部忽略。借鉴 <code>MarginRankingLoss</code> 思想，对于预测值 x，认为其中<b>样本所属分类的元素值比样本不属分类的元素值大</b>，这个例子中，样本所属分类为 ${0,3}$，所以认为应该是 $x_0,\\ x_3 &gt; x_1,\\ x_2$，据此不难理解单个样本的损失为<br>$$l=\\sum_{i,j} \\frac {\\max[0, 1-(x_{y_j} - x_i)]} C$$<br>其中，$j \\in \\mathcal J={0,1,…,k-1}$，且 $y_k&lt;0$，$i \\in {0,1,…,C-1}-{y_j|j \\in \\mathcal J}$，即， $j$ 为 target 向量中开始的连续非负元素索引，$y_j$ 表示样本所属分类索引，i 为样本不属分类索引。</p>\n<p>当分类正确时，损失为0，此时需要满足条件 $1-(x_{y_j}-x_i)\\le 0 \\Rightarrow x_{y_j}\\ge 1+x_i$，这说明降低损失会使得样本所属分类的预测概率 $x_{y_j} \\rightarrow 0$，样本不属分类的预测概率 $x_i \\rightarrow 0$。在 test 阶段，对预测值 x 设置一个低阈值即可。</p>\n<h1 id=\"SoftMarginLoss\"><a href=\"#SoftMarginLoss\" class=\"headerlink\" title=\"SoftMarginLoss\"></a>SoftMarginLoss</h1><p>适用于二分类问题。上面两种 MarginLoss 均采用了 <code>max(0,x)</code> 函数，这个函数在 <code>x=0</code> 处不可导。<code>SoftMarginLoss</code> 借助 logistic 函数解决了这个问题。Logistic 函数<br>$$\\sigma(x)=\\frac 1 {1+\\exp (-x)}$$<br>预测值 x，分类 $y\\in {1,-1}$，似然函数为<br>$$\\mathcal L =\\mathbb I(y=1)f(x)+\\mathbb I(y=-1)(1-f(x))=[1+\\exp(-yx)]^{-1}$$<br> 负对数似然函数（损失）为<br>$$l= \\log(1+\\exp(-yx))$$<br>所以 <code>SoftMarginLoss</code> <b>就是 logistic 回归的负对数似然损失</b>。预测输入 input tensor 的 shape 为 $(<em>)$，其中 $</em>$ 表示任意维度，target 与 input 的 shape 相同。损失按像素计算，输出与 input 同 shape，如果按求和或平均归约，那么输出为一标量。</p>\n<h1 id=\"MultiLabelSoftMarginLoss\"><a href=\"#MultiLabelSoftMarginLoss\" class=\"headerlink\" title=\"MultiLabelSoftMarginLoss\"></a>MultiLabelSoftMarginLoss</h1><p>适用于多标签多分类问题。每个类别各自独立做二分类（为正或负）。input 和 target 有相同的 shape：$(N,C)$，target 值为 0 或 1（这与 SoftMarginLoss 的 1 或 -1 竟然不统一）。于是，单个样本的损失为，<br>$$l=-\\frac 1 C \\sum_{i=1}^C y_i \\log \\left(\\frac 1 {1+\\exp(-x_i)}\\right )+(1-y_i)\\log \\left(\\frac {\\exp(-x_i)} {1+\\exp(-x_i)}\\right)$$<br>由于这里考虑单个样本，所以上式 $x, \\ y$ 均为长度 $C$ 的向量，由于 y 值取值范围不同，所以上式与 <code>SoftMarginLoss</code> 的损失表达式略有不同，但是本质上都是 logistic 负对数似然损失。</p>\n<p>输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出 tensor 为一标量。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultiLabelSoftMarginLoss(weight: Optional[torch.Tensor] = <span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>weight</code> 如果给定，那么是一个 shape 为 $(C,)$  的 tensor，用于给每个 channel/类别 一个权重。</p>\n<h1 id=\"MultiMarginLoss\"><a href=\"#MultiMarginLoss\" class=\"headerlink\" title=\"MultiMarginLoss\"></a>MultiMarginLoss</h1><p>适用于多分类（单标签）问题。input 为一个 2D tensor $(N,C)$，target shape 为 $(N,)$，表示样本的分类索引，故 $y_i \\in {0,1,…,C-1}$，对于单个样本而言，此时输入为一个长度 C 的向量 x，target 为标量，也记为 y，表示样本分类索引，显然我们要 $x_y &gt; x_i$，其中 $i \\neq y$，margin-based 损失为<br>$$l=\\frac {\\sum_{i \\neq y} \\max(0, d-(x_y-x_i))^p} C$$<br>其中 $d$ 为 margin，也就是说需要 $x_y \\ge x_i+d$，样本所属分类的预测概率比其他分类的预测概率大 $d$，损失才为 0。</p>\n<p>p 值可为 1 或 2，用于控制损失变化速度。还可以给每个类型增加一个权重，此时损失为，<br>$$l=\\frac {\\sum_{i \\neq y} \\max[0, w_y(d-(x_y-x_i))^p]} C$$<br>注意，权重 $w_y$ 不参与幂运算，且只有样本所属分类对于的权重因子起作用。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MultiMarginLoss(p: int=<span class=\"number\">1</span>, margin: float=<span class=\"number\">1.0</span>, weight: Optional[torch.Tensor]=<span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p>input shape 为 $(N,C)$，target shape 为 $(N,)$，output 的 shape 为 $(N,)$，可以按求和或平均归约，此时 output 为一标量。</p>\n<h1 id=\"TripletMarginLoss\"><a href=\"#TripletMarginLoss\" class=\"headerlink\" title=\"TripletMarginLoss\"></a>TripletMarginLoss</h1><p>三个tensor：$a, \\ p, \\ n$，分别表示 anchor，正例和负例，shape 均为 $(N,D)$，其中 $N$ 为批大小，$D$ 为特征数。p 表示与 a 同类的另一个样本的特征，n 表示与 a 不同类的样本特征，显然，需要 p 与 a 的特征尽量相近，n 与 a 的特征尽量远离。<br>传统上是以 pair 的形式来度量损失，即 $(p,a)$ 为正例对，$(n,a)$ 为负例对，一般表示为 $(x_1, x_2， l)$，当 $l=1$ 表示是正例对，$l=-1$ 表示是负例对，此时损失定义为<br>$$l=\\begin{cases} \\Vert \\mathbf x_1-\\mathbf x_2 \\Vert_2 &amp; l=1 \\ \\max(0, d-\\Vert \\mathbf x_1- \\mathbf x_2\\Vert_2) &amp; l=-1 \\end{cases}$$<br>$l=1$ 是正例对，所以 $\\mathbf x_1$ 应该要尽量接近 $\\mathbf x_2$；$l=-1$ 是负例对，$\\mathbf x_1$ 尽量要远离 $\\mathbf x_2$，且要相距 $d$ 以上。</p>\n<p>这里 <code>TripletMarginLoss</code> 将 <code>(a,p,n)</code> 三者当成一个整体，margin ranking-based 损失定义如下，<br>$$l=\\max[d(a,p) - d(a,n)+d_0, 0]$$<br>$$d(\\mathbf x_1, \\mathbf x_2)=\\Vert \\mathbf x_1 - \\mathbf x_2 \\Vert_p$$<br>其中，$d_0$ 为 margin，计算特征空间中的距离时，使用的是 p 范数，这个 p 与前面正例 p 不一样，根据上下文不难区分。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TripletMarginLoss(margin: float=<span class=\"number\">1.0</span>, p: float=<span class=\"number\">2.0</span>, eps: float=<span class=\"number\">1e-06</span>, swap: bool=<span class=\"literal\">False</span>, size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>swap</code> 指示是否交换 anchor 和 positive，这用于 hard negative mining。若 <code>swap=True</code>，那么 $d(a,n) = d(p,n)$，也就是说，使用 <code>(p,n)</code> 的距离作为 negative 与 anchor 的距离。</p>\n<p>forward 方法的参数为 anchor, positive 和 negative 三个特征 tensor，shape 均为 $(N,D)$，输出 tensor 的 shape 为 $(N,)$，如果按求和或平均归约，那么输出为一标量。</p>\n<p>更多细节可以参考 <a href=\"www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf\"><code>Learning local feature descriptors with triplets and shallow convolutional neural networks</code></a></p>\n<h1 id=\"TripletMarginWithDistanceLoss\"><a href=\"#TripletMarginWithDistanceLoss\" class=\"headerlink\" title=\"TripletMarginWithDistanceLoss\"></a>TripletMarginWithDistanceLoss</h1><p><code>TripletMarginLoss</code> 中距离使用的是 p 范数，这里是通过参数提供自定义的距离参数。anchor，positive 和 negative 三个 tensor 的 shape 为 $(N,<em>)$ ，其中 $</em>$ 为任意维度，输出 tensor 的未归约 shape 为 $(N,)$，否则为一标量。</p>\n<h1 id=\"HingeEmbeddingLoss\"><a href=\"#HingeEmbeddingLoss\" class=\"headerlink\" title=\"HingeEmbeddingLoss\"></a>HingeEmbeddingLoss</h1><p>$x$ 表示距离（例如 L1 范数），$y \\in {1,-1}$ 标识是相似还是相反，损失为，<br>$$l = \\begin{cases} x &amp; y=1 \\ \\max(0, d-x) &amp; y=-1 \\end{cases}$$<br>其中 $d$ 为 margin。</p>\n<p>输入 x 和 y 的 shape 均为任意维度 $(<em>)$，输出未归约的 shape 也是 $(</em>)$，否则为一标量。</p>\n<h1 id=\"CosineEmbeddingLoss\"><a href=\"#CosineEmbeddingLoss\" class=\"headerlink\" title=\"CosineEmbeddingLoss\"></a>CosineEmbeddingLoss</h1><p><code>y=1</code> 表示两个（归一化）向量应该相近，<code>y=-1</code> 表示应该相差很远。<br>损失如下，<br>$$l=\\begin{cases} 1- \\cos(x_1,x_2) &amp; y=1 \\ \\max[0, \\cos(x_1,x_2) - d] &amp; y=-1 \\end{cases}$$<br>其中 $d$ 表示 margin，默认为 0。</p>\n<h1 id=\"CTCLoss\"><a href=\"#CTCLoss\" class=\"headerlink\" title=\"CTCLoss\"></a>CTCLoss</h1><p>参考文献 <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\" target=\"_blank\" rel=\"noopener\">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a><br>RNN 相关的应用领域暂未涉及。略，以后填坑。</p>"},{"title":"pytorch 技巧1","p":"pytorch/tricks_1","date":"2021-01-08T01:45:15.000Z","_content":"假设最后一层的 input shape 为 (N,H,W)，输出为 loss 为一标量，那么最后一层 input 的梯度 shape 为 (N,H,W)，与 input Tensor 自身 shape 相同，然后继续反向传播，倒数第二层的 input shape 为 (N',H',W')，\n假设某层输入input 的 shape 为 (N,H,W)，输出 output 的 shape 为 (N',H',W')，这里为了叙述简单，不考虑 batch 的维度，反向传播时，output 变量的梯度的 shape 应该与 output 变量自身相同，output 对 input 的梯度 shape 应该为\n(N',H',W',N,H,W)，与 ","source":"_posts/pytorch/tricks_1.md","raw":"---\ntitle: pytorch 技巧1\np: pytorch/tricks_1\ndate: 2021-01-08 09:45:15\ntags: PyTorch\n---\n假设最后一层的 input shape 为 (N,H,W)，输出为 loss 为一标量，那么最后一层 input 的梯度 shape 为 (N,H,W)，与 input Tensor 自身 shape 相同，然后继续反向传播，倒数第二层的 input shape 为 (N',H',W')，\n假设某层输入input 的 shape 为 (N,H,W)，输出 output 的 shape 为 (N',H',W')，这里为了叙述简单，不考虑 batch 的维度，反向传播时，output 变量的梯度的 shape 应该与 output 变量自身相同，output 对 input 的梯度 shape 应该为\n(N',H',W',N,H,W)，与 ","slug":"pytorch/tricks_1","published":1,"updated":"2021-01-08T07:22:19.103Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxr7whz0003y4djbsp0bnrp","content":"<p>假设最后一层的 input shape 为 (N,H,W)，输出为 loss 为一标量，那么最后一层 input 的梯度 shape 为 (N,H,W)，与 input Tensor 自身 shape 相同，然后继续反向传播，倒数第二层的 input shape 为 (N’,H’,W’)，<br>假设某层输入input 的 shape 为 (N,H,W)，输出 output 的 shape 为 (N’,H’,W’)，这里为了叙述简单，不考虑 batch 的维度，反向传播时，output 变量的梯度的 shape 应该与 output 变量自身相同，output 对 input 的梯度 shape 应该为<br>(N’,H’,W’,N,H,W)，与 </p>\n","site":{"data":{}},"excerpt":"","more":"<p>假设最后一层的 input shape 为 (N,H,W)，输出为 loss 为一标量，那么最后一层 input 的梯度 shape 为 (N,H,W)，与 input Tensor 自身 shape 相同，然后继续反向传播，倒数第二层的 input shape 为 (N’,H’,W’)，<br>假设某层输入input 的 shape 为 (N,H,W)，输出 output 的 shape 为 (N’,H’,W’)，这里为了叙述简单，不考虑 batch 的维度，反向传播时，output 变量的梯度的 shape 应该与 output 变量自身相同，output 对 input 的梯度 shape 应该为<br>(N’,H’,W’,N,H,W)，与 </p>\n"},{"title":"Loss 1","p":"pytorch/loss_1","date":"2021-01-12T09:35:17.000Z","mathjax":true,"_content":"\n前面介绍了 [交叉熵损失](2021/1/12/dl/x_ent_loss)，本篇就 PyTorch 中的各种 [Loss](https://pytorch.org/docs/stable/nn.html#loss-functions) 进行分解并掌握其用法。\n<!-- more -->\n\n# L1Loss\n基于L1 范数的损失，单个样本的L1损失为 $l_n=|x_n-y_n|$，其中 `n` 为批样本中的样本索引，$x_n$ 为预测值，$y_n$ 为 GT，L1 损失适用于<b>回归</b>问题。\n\n# MSELoss\n均方差（L2范数平方）损失，单个样本损失的计算公式为 $l_n=(x_n-y_n)^2$。适用于<b>回归</b>问题。\n\n# NLLLoss\n负对数似然损失，适用于<b>分类</b>问题。对于单个样本，似然函数为\n$$\\mathcal L=\\prod_{i=1}^C x_i^{y_i}$$\n其中输出向量 $\\mathbf x = (x_1,...,x_C)$ 表示每个分类的预测概率，GT 向量为 $\\mathbf y=(y_1,...,y_C)$，如果是单标签分类，$\\mathbf y$ 为 one-hot，如果是多标签分类，$\\mathbf y$ 中可能有多个元素值为 1。负对数似然则为，\n$$l=-\\sum_{i=1}^C y_i \\log x_i$$\n\n实际在 PyTorch 中，NLLLoss 层的输入 Tensor 的 shape 以及 GT target 的 shape 与上面有所不同，以单标签多分类为例，网络输出 Tensor 的 shape 可以是 $(N,C)$，其中 N 表示批大小，C 表示通道也是类别数。GT target 的 shape 为 `N`，其中每个元素值的范围 `[0,C-1]`，表示某个样本的类别索引，NLLoss 层的输入已经表示样本各分类的概率对数（由`LogSoftmax`得到），负对数似然为\n$$L=(l_1,...,l_N), \\quad l_n=- x_{n,y_n}$$\n\n如果给定参数`weight`，那么其必须是 1-D tensor，长度与类别数`C` 相等，用于给每个类别增加一个权重，参考 [交叉熵损失](2021/1/12/dl/x_ent_loss) 中的 [$\\alpha$ 均衡交叉熵](2021/1/12/dl/x_ent_loss#Balanced-Cross-Entropy)，这在非均衡数据集上较为有效。此时有\n$$l_n=- w_{y_n}  x_{n,y_n}$$\n\n类签名\n```python\ntorch.nn.NLLLoss(weight: Optional[torch.Tensor]=None, size_average=None, ignore_index: int=-100, reduce=None, reduction: str='mean')\n```\n\n`size_average` 和 `reduce` 这两个参数已经过时。`reduction `用于指定批样本的损失向量是否归约（均值或求和）。\n\n`ignore_index` 如果指定，那么当 GT target 值等于 `ignore_index` 时，将会忽略对应的损失贡献。\n\ninput 通过 forward 方法指定，input 表示每个分类的概率对数，这可以通过 `LogSoftmax` 得到，input 的 shape 可以是 $(N,C)$，或者是 $(N,C,d_1,...,d_K)$，对于后者，其 target 的 shape 则为 $(N,d_1,...,d_K)$，此时的（未归约）损失 shape 也是 $(N,d_1,...,d_K)$，相比较于前者，后者就是扩展了维度而已，对于 $(d_1,...d_K)$ 中按像素级地计算负对数似然损失。\n\n\n# CrossEntropyLoss\n交叉熵损失，适用于分类问题。PyTorch 中，这个类（layer）合并了 `LogSoftmax` 和 `NLLLoss`，所以这个 layer 的 input 为为归一化的各分类的原始得分，input 的 shape 可以是 $(N,C)$ 或 $(N,C,d_1,...,d_K)$。target 的 shape 则为 $(N,)$ 或 $(N,d_1,...,d_K)$。\n以 input 的 shape 为 $(N,C)$ 为例，此 layer 的损失计算可表示为（单个样本）\n$$l_n=-\\log \\left(\\frac {\\exp x_{n,y_n}}{\\sum_j \\exp x_{n,j}}\\right)$$\n其中 $y_n \\in [0,C-1]$ 为第 n 个样本的类别索引，$\\sum_j$ 为某个样本对 C 个类别的求和。\n\n除了增加了一个 `LogSoftmax` 的计算，其他均与 NLLoss 层类似，故类签名中的参数介绍略。\n\n# PoissonNLLLoss\nPoisson 损失一般用于服从 poisson 分布的计数数据回归的问题，例如下周教堂人数预测。Poisson 分布如下\n$$P(X=k)=\\frac {\\lambda^k e^{-\\lambda}} {k!}$$\n随机变量 X 的期望 $E[X]=\\lambda$。我们的预测值 $x$ 就是对期望 $\\lambda$ 的预测，target 值就是真实的计数值（例如事件发生的次数，教堂的人数等），target 值用 $y$ 表示，也就是上式中的 $k$，于是单个样本的负对数似然可表示如下：\n$$l= -\\log P(y|x) =-\\log \\frac {x^{y} e^{-x}} {y!}=x-y \\log x+ \\log(y!)$$\n最后一项可以忽略或者适应 Stirling 公式近似求解。因为是一个常数，所以即使忽略掉，也不影响反向传播的计算。\n\n类签名：\n```python\nPoissonNLLLoss(log_input: bool=True, full: bool=False, size_average=None, eps: float=1e-8, reduce=None, reduction: str='mean')\n```\n`log_input` 指明 forward 的输入 input 是否经过了 log 处理，如是 True，那么上式损失计算应改为 $l=e^x - yx$，否则损失计算式为 $l=x-y \\log(x+eps)$。在 Poisson 回归中，假定期望的对数符合线性模型，所以很多时候是对期望的 log 值进行预测，即 `log_input=True`，此时 target 值也要经过 log 处理。\n\n> 程序中为了防止计算数值上下溢，往往会采用 log 处理\n\n`full` 指示是否添加最后一项 $\\log(y!)$。如需要添加，那么使用 Stirling 公式近似，Stirling 公式为\n$$n! \\sim \\sqrt{2 \\pi} n^{n+1/2} e^{-n}$$\n于是有\n$$\\log(n!)=\\frac 1 2 \\log(2 \\pi n)+ n \\log n - n$$\n\nforward 方法的 input 的 shape 是 $(N, *)$，其中 $*$ 表示对维度的扩展，且损失计算都是在 $*$ 维度上按像素级进行计算，故 target 的 shape 也是 $(N, *)$。如果 `reduction` 参数为 `none`，那么输出 shape 也是 $(N, *)$，否则将输出 Tensor 中所有值按 求和或平均 进行归约，最终得到一个标量值。\n\n# KLDivLoss\nKL 散度用于度量两个分布之间的差异。KL 散度损失适用于<b>回归</b>问题。\n\n根据 KL 散度计算损失，KL 散度计算如下，\n$$D(P||Q)=\\sum P(x) \\cdot \\log \\frac {P(x)}{Q(x)}$$\n$$D(P||Q) = \\int_x p(x) \\log \\frac {p(x)}{q(x)} dx$$\n\n预测分布越接近真实分布，那么两者之间的 KL 散度应该越小，所以 KL 散度可以作为一种损失。\nPyTorch 中的类签名：\n```python\nKLDivLoss(size_average=None, reduce=None, reduction: str='mean', log_target: bool=False)\n```\n`log_target` 指示 target 是否经过 log 处理。\n\nforward 方法中，参数 input 表示预测概率，且经过 log 处理，input 的 shape 为 $(N,*)$，其中 $*$ 表示单个样本的所有维度。KL 散度损失按像素级计算（可看作是连续分布的离散采样），\n$$l=y \\cdot (\\log y - x)$$\n其中 $x$ 表示随机变量某个值对应的预测概率，且经过 log 处理，$y$ 表示这个随机变量在这个值处的真实概率。\n\nforward 方法的输出结果的 shape 与 input 相同，为 $(N,*)$，如果 `reduction` 不为 `none`，那么输出结果将按 求和或平均 归约为一个标量值。\n\n# BCEWithLogitsLoss\nPyTorch 中这个 layer 合并了 Sigmoid 层和 BCELoss 层，由于 BCELoss 层计算单个样本的 BCE 损失为，\n$$l=y \\log x + (1-y) \\log (1-x)$$\n其中 $y \\in \\{0,1\\}$ 表示样本的真实分类，$x\\in [0,1]$ 表示样本的预测概率，通常使用 Sigmoid 层来将处于实数域的前一个 layer 输出值压缩到 $[0,1]$ 之间，故为了少写一个 Sigmoid 层，将这两者合并为单个 layer： `BCEWithLogitsLoss`。所以这个 layer 的输入是原始的未归一化的各类别的得分，单个样本的损失为，\n$$l_n=-w_n [y_n \\log \\sigma(x_n) +(1-y_n) \\log (1-\\sigma(x_n))]$$\n这里，批样本中每个样本有各自的一个权重因子 $w_n$。\n\n如果是多标签多分类问题，那么对于每个类别，均独立进行二分类（正或负），记类别索引为 $c$，那么单个样本的损失为\n$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$\n其中 $y_{n,c} \\in \\{0,1\\}$，$x_{n,c} \\in \\mathbb R$。\n\n还可以对正类样本增加一个权重因子 $p_c$，用于权衡最终的召回率和精度，于是上式变为\n$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [p_c y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$\n当 $p_c >1$ 时召回率增大，$p_c<1$ 时 精度增大。$p_c$ 可以取类别 $c$ 下 负样本与正样本数量比，如此可认为正负例相等。\n\nforward 方法中 input 的 shape 为 $(N,*)$，其中 $N$ 为批大小，$*$ 表示单个样本的维度大小，损失按像素计算，故 target 和未归约的 output 的 shape 均为 $(N,*)$，如果对 output 按求和或平均归约，则 output 为一个标量值。\n\n> 这个 layer 比起 Sigmoid 和 BCELoss 两个 layer，在数值计算上更加稳定（能避免数值上下溢），因为使用了 `log-sum-exp` 技巧。\n\n适用于<b>分类</b>问题。\n# SmoothL1Loss\n对 L1Loss 的改进，当 L1 范数低于一定值时，使用差的平方项来代替误差，这是因为当预测值越接近真实值时，损失的梯度应该越小，从而减缓参数的更新幅度。SmoothL1Loss 按像素计算，计算式为，\n$$l_i=\\begin{cases} \\frac 1 {2 \\beta} (x_i-y_i)^2 & |x_i - y_i| < \\beta \\\\ |x_i-y_i|-\\frac 1 {2 \\beta} & \\text{otherwise}  \\end{cases}$$\n适用于<b>回归</b>问题。","source":"_posts/pytorch/loss_1.md","raw":"---\ntitle: Loss 1\np: pytorch/loss_1\ndate: 2021-01-12 17:35:17\ntags: PyTorch\nmathjax: true\n---\n\n前面介绍了 [交叉熵损失](2021/1/12/dl/x_ent_loss)，本篇就 PyTorch 中的各种 [Loss](https://pytorch.org/docs/stable/nn.html#loss-functions) 进行分解并掌握其用法。\n<!-- more -->\n\n# L1Loss\n基于L1 范数的损失，单个样本的L1损失为 $l_n=|x_n-y_n|$，其中 `n` 为批样本中的样本索引，$x_n$ 为预测值，$y_n$ 为 GT，L1 损失适用于<b>回归</b>问题。\n\n# MSELoss\n均方差（L2范数平方）损失，单个样本损失的计算公式为 $l_n=(x_n-y_n)^2$。适用于<b>回归</b>问题。\n\n# NLLLoss\n负对数似然损失，适用于<b>分类</b>问题。对于单个样本，似然函数为\n$$\\mathcal L=\\prod_{i=1}^C x_i^{y_i}$$\n其中输出向量 $\\mathbf x = (x_1,...,x_C)$ 表示每个分类的预测概率，GT 向量为 $\\mathbf y=(y_1,...,y_C)$，如果是单标签分类，$\\mathbf y$ 为 one-hot，如果是多标签分类，$\\mathbf y$ 中可能有多个元素值为 1。负对数似然则为，\n$$l=-\\sum_{i=1}^C y_i \\log x_i$$\n\n实际在 PyTorch 中，NLLLoss 层的输入 Tensor 的 shape 以及 GT target 的 shape 与上面有所不同，以单标签多分类为例，网络输出 Tensor 的 shape 可以是 $(N,C)$，其中 N 表示批大小，C 表示通道也是类别数。GT target 的 shape 为 `N`，其中每个元素值的范围 `[0,C-1]`，表示某个样本的类别索引，NLLoss 层的输入已经表示样本各分类的概率对数（由`LogSoftmax`得到），负对数似然为\n$$L=(l_1,...,l_N), \\quad l_n=- x_{n,y_n}$$\n\n如果给定参数`weight`，那么其必须是 1-D tensor，长度与类别数`C` 相等，用于给每个类别增加一个权重，参考 [交叉熵损失](2021/1/12/dl/x_ent_loss) 中的 [$\\alpha$ 均衡交叉熵](2021/1/12/dl/x_ent_loss#Balanced-Cross-Entropy)，这在非均衡数据集上较为有效。此时有\n$$l_n=- w_{y_n}  x_{n,y_n}$$\n\n类签名\n```python\ntorch.nn.NLLLoss(weight: Optional[torch.Tensor]=None, size_average=None, ignore_index: int=-100, reduce=None, reduction: str='mean')\n```\n\n`size_average` 和 `reduce` 这两个参数已经过时。`reduction `用于指定批样本的损失向量是否归约（均值或求和）。\n\n`ignore_index` 如果指定，那么当 GT target 值等于 `ignore_index` 时，将会忽略对应的损失贡献。\n\ninput 通过 forward 方法指定，input 表示每个分类的概率对数，这可以通过 `LogSoftmax` 得到，input 的 shape 可以是 $(N,C)$，或者是 $(N,C,d_1,...,d_K)$，对于后者，其 target 的 shape 则为 $(N,d_1,...,d_K)$，此时的（未归约）损失 shape 也是 $(N,d_1,...,d_K)$，相比较于前者，后者就是扩展了维度而已，对于 $(d_1,...d_K)$ 中按像素级地计算负对数似然损失。\n\n\n# CrossEntropyLoss\n交叉熵损失，适用于分类问题。PyTorch 中，这个类（layer）合并了 `LogSoftmax` 和 `NLLLoss`，所以这个 layer 的 input 为为归一化的各分类的原始得分，input 的 shape 可以是 $(N,C)$ 或 $(N,C,d_1,...,d_K)$。target 的 shape 则为 $(N,)$ 或 $(N,d_1,...,d_K)$。\n以 input 的 shape 为 $(N,C)$ 为例，此 layer 的损失计算可表示为（单个样本）\n$$l_n=-\\log \\left(\\frac {\\exp x_{n,y_n}}{\\sum_j \\exp x_{n,j}}\\right)$$\n其中 $y_n \\in [0,C-1]$ 为第 n 个样本的类别索引，$\\sum_j$ 为某个样本对 C 个类别的求和。\n\n除了增加了一个 `LogSoftmax` 的计算，其他均与 NLLoss 层类似，故类签名中的参数介绍略。\n\n# PoissonNLLLoss\nPoisson 损失一般用于服从 poisson 分布的计数数据回归的问题，例如下周教堂人数预测。Poisson 分布如下\n$$P(X=k)=\\frac {\\lambda^k e^{-\\lambda}} {k!}$$\n随机变量 X 的期望 $E[X]=\\lambda$。我们的预测值 $x$ 就是对期望 $\\lambda$ 的预测，target 值就是真实的计数值（例如事件发生的次数，教堂的人数等），target 值用 $y$ 表示，也就是上式中的 $k$，于是单个样本的负对数似然可表示如下：\n$$l= -\\log P(y|x) =-\\log \\frac {x^{y} e^{-x}} {y!}=x-y \\log x+ \\log(y!)$$\n最后一项可以忽略或者适应 Stirling 公式近似求解。因为是一个常数，所以即使忽略掉，也不影响反向传播的计算。\n\n类签名：\n```python\nPoissonNLLLoss(log_input: bool=True, full: bool=False, size_average=None, eps: float=1e-8, reduce=None, reduction: str='mean')\n```\n`log_input` 指明 forward 的输入 input 是否经过了 log 处理，如是 True，那么上式损失计算应改为 $l=e^x - yx$，否则损失计算式为 $l=x-y \\log(x+eps)$。在 Poisson 回归中，假定期望的对数符合线性模型，所以很多时候是对期望的 log 值进行预测，即 `log_input=True`，此时 target 值也要经过 log 处理。\n\n> 程序中为了防止计算数值上下溢，往往会采用 log 处理\n\n`full` 指示是否添加最后一项 $\\log(y!)$。如需要添加，那么使用 Stirling 公式近似，Stirling 公式为\n$$n! \\sim \\sqrt{2 \\pi} n^{n+1/2} e^{-n}$$\n于是有\n$$\\log(n!)=\\frac 1 2 \\log(2 \\pi n)+ n \\log n - n$$\n\nforward 方法的 input 的 shape 是 $(N, *)$，其中 $*$ 表示对维度的扩展，且损失计算都是在 $*$ 维度上按像素级进行计算，故 target 的 shape 也是 $(N, *)$。如果 `reduction` 参数为 `none`，那么输出 shape 也是 $(N, *)$，否则将输出 Tensor 中所有值按 求和或平均 进行归约，最终得到一个标量值。\n\n# KLDivLoss\nKL 散度用于度量两个分布之间的差异。KL 散度损失适用于<b>回归</b>问题。\n\n根据 KL 散度计算损失，KL 散度计算如下，\n$$D(P||Q)=\\sum P(x) \\cdot \\log \\frac {P(x)}{Q(x)}$$\n$$D(P||Q) = \\int_x p(x) \\log \\frac {p(x)}{q(x)} dx$$\n\n预测分布越接近真实分布，那么两者之间的 KL 散度应该越小，所以 KL 散度可以作为一种损失。\nPyTorch 中的类签名：\n```python\nKLDivLoss(size_average=None, reduce=None, reduction: str='mean', log_target: bool=False)\n```\n`log_target` 指示 target 是否经过 log 处理。\n\nforward 方法中，参数 input 表示预测概率，且经过 log 处理，input 的 shape 为 $(N,*)$，其中 $*$ 表示单个样本的所有维度。KL 散度损失按像素级计算（可看作是连续分布的离散采样），\n$$l=y \\cdot (\\log y - x)$$\n其中 $x$ 表示随机变量某个值对应的预测概率，且经过 log 处理，$y$ 表示这个随机变量在这个值处的真实概率。\n\nforward 方法的输出结果的 shape 与 input 相同，为 $(N,*)$，如果 `reduction` 不为 `none`，那么输出结果将按 求和或平均 归约为一个标量值。\n\n# BCEWithLogitsLoss\nPyTorch 中这个 layer 合并了 Sigmoid 层和 BCELoss 层，由于 BCELoss 层计算单个样本的 BCE 损失为，\n$$l=y \\log x + (1-y) \\log (1-x)$$\n其中 $y \\in \\{0,1\\}$ 表示样本的真实分类，$x\\in [0,1]$ 表示样本的预测概率，通常使用 Sigmoid 层来将处于实数域的前一个 layer 输出值压缩到 $[0,1]$ 之间，故为了少写一个 Sigmoid 层，将这两者合并为单个 layer： `BCEWithLogitsLoss`。所以这个 layer 的输入是原始的未归一化的各类别的得分，单个样本的损失为，\n$$l_n=-w_n [y_n \\log \\sigma(x_n) +(1-y_n) \\log (1-\\sigma(x_n))]$$\n这里，批样本中每个样本有各自的一个权重因子 $w_n$。\n\n如果是多标签多分类问题，那么对于每个类别，均独立进行二分类（正或负），记类别索引为 $c$，那么单个样本的损失为\n$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$\n其中 $y_{n,c} \\in \\{0,1\\}$，$x_{n,c} \\in \\mathbb R$。\n\n还可以对正类样本增加一个权重因子 $p_c$，用于权衡最终的召回率和精度，于是上式变为\n$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [p_c y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$\n当 $p_c >1$ 时召回率增大，$p_c<1$ 时 精度增大。$p_c$ 可以取类别 $c$ 下 负样本与正样本数量比，如此可认为正负例相等。\n\nforward 方法中 input 的 shape 为 $(N,*)$，其中 $N$ 为批大小，$*$ 表示单个样本的维度大小，损失按像素计算，故 target 和未归约的 output 的 shape 均为 $(N,*)$，如果对 output 按求和或平均归约，则 output 为一个标量值。\n\n> 这个 layer 比起 Sigmoid 和 BCELoss 两个 layer，在数值计算上更加稳定（能避免数值上下溢），因为使用了 `log-sum-exp` 技巧。\n\n适用于<b>分类</b>问题。\n# SmoothL1Loss\n对 L1Loss 的改进，当 L1 范数低于一定值时，使用差的平方项来代替误差，这是因为当预测值越接近真实值时，损失的梯度应该越小，从而减缓参数的更新幅度。SmoothL1Loss 按像素计算，计算式为，\n$$l_i=\\begin{cases} \\frac 1 {2 \\beta} (x_i-y_i)^2 & |x_i - y_i| < \\beta \\\\ |x_i-y_i|-\\frac 1 {2 \\beta} & \\text{otherwise}  \\end{cases}$$\n适用于<b>回归</b>问题。","slug":"pytorch/loss_1","published":1,"updated":"2021-01-15T03:59:35.833Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxr7wi30007y4djcfj42smn","content":"<p>前面介绍了 <a href=\"2021/1/12/dl/x_ent_loss\">交叉熵损失</a>，本篇就 PyTorch 中的各种 <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\" target=\"_blank\" rel=\"noopener\">Loss</a> 进行分解并掌握其用法。</p>\n<a id=\"more\"></a>\n\n<h1 id=\"L1Loss\"><a href=\"#L1Loss\" class=\"headerlink\" title=\"L1Loss\"></a>L1Loss</h1><p>基于L1 范数的损失，单个样本的L1损失为 $l_n=|x_n-y_n|$，其中 <code>n</code> 为批样本中的样本索引，$x_n$ 为预测值，$y_n$ 为 GT，L1 损失适用于<b>回归</b>问题。</p>\n<h1 id=\"MSELoss\"><a href=\"#MSELoss\" class=\"headerlink\" title=\"MSELoss\"></a>MSELoss</h1><p>均方差（L2范数平方）损失，单个样本损失的计算公式为 $l_n=(x_n-y_n)^2$。适用于<b>回归</b>问题。</p>\n<h1 id=\"NLLLoss\"><a href=\"#NLLLoss\" class=\"headerlink\" title=\"NLLLoss\"></a>NLLLoss</h1><p>负对数似然损失，适用于<b>分类</b>问题。对于单个样本，似然函数为<br>$$\\mathcal L=\\prod_{i=1}^C x_i^{y_i}$$<br>其中输出向量 $\\mathbf x = (x_1,…,x_C)$ 表示每个分类的预测概率，GT 向量为 $\\mathbf y=(y_1,…,y_C)$，如果是单标签分类，$\\mathbf y$ 为 one-hot，如果是多标签分类，$\\mathbf y$ 中可能有多个元素值为 1。负对数似然则为，<br>$$l=-\\sum_{i=1}^C y_i \\log x_i$$</p>\n<p>实际在 PyTorch 中，NLLLoss 层的输入 Tensor 的 shape 以及 GT target 的 shape 与上面有所不同，以单标签多分类为例，网络输出 Tensor 的 shape 可以是 $(N,C)$，其中 N 表示批大小，C 表示通道也是类别数。GT target 的 shape 为 <code>N</code>，其中每个元素值的范围 <code>[0,C-1]</code>，表示某个样本的类别索引，NLLoss 层的输入已经表示样本各分类的概率对数（由<code>LogSoftmax</code>得到），负对数似然为<br>$$L=(l_1,…,l_N), \\quad l_n=- x_{n,y_n}$$</p>\n<p>如果给定参数<code>weight</code>，那么其必须是 1-D tensor，长度与类别数<code>C</code> 相等，用于给每个类别增加一个权重，参考 <a href=\"2021/1/12/dl/x_ent_loss\">交叉熵损失</a> 中的 <a href=\"2021/1/12/dl/x_ent_loss#Balanced-Cross-Entropy\">$\\alpha$ 均衡交叉熵</a>，这在非均衡数据集上较为有效。此时有<br>$$l_n=- w_{y_n}  x_{n,y_n}$$</p>\n<p>类签名</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.nn.NLLLoss(weight: Optional[torch.Tensor]=<span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, ignore_index: int=<span class=\"number\">-100</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n\n<p><code>size_average</code> 和 <code>reduce</code> 这两个参数已经过时。<code>reduction</code>用于指定批样本的损失向量是否归约（均值或求和）。</p>\n<p><code>ignore_index</code> 如果指定，那么当 GT target 值等于 <code>ignore_index</code> 时，将会忽略对应的损失贡献。</p>\n<p>input 通过 forward 方法指定，input 表示每个分类的概率对数，这可以通过 <code>LogSoftmax</code> 得到，input 的 shape 可以是 $(N,C)$，或者是 $(N,C,d_1,…,d_K)$，对于后者，其 target 的 shape 则为 $(N,d_1,…,d_K)$，此时的（未归约）损失 shape 也是 $(N,d_1,…,d_K)$，相比较于前者，后者就是扩展了维度而已，对于 $(d_1,…d_K)$ 中按像素级地计算负对数似然损失。</p>\n<h1 id=\"CrossEntropyLoss\"><a href=\"#CrossEntropyLoss\" class=\"headerlink\" title=\"CrossEntropyLoss\"></a>CrossEntropyLoss</h1><p>交叉熵损失，适用于分类问题。PyTorch 中，这个类（layer）合并了 <code>LogSoftmax</code> 和 <code>NLLLoss</code>，所以这个 layer 的 input 为为归一化的各分类的原始得分，input 的 shape 可以是 $(N,C)$ 或 $(N,C,d_1,…,d_K)$。target 的 shape 则为 $(N,)$ 或 $(N,d_1,…,d_K)$。<br>以 input 的 shape 为 $(N,C)$ 为例，此 layer 的损失计算可表示为（单个样本）<br>$$l_n=-\\log \\left(\\frac {\\exp x_{n,y_n}}{\\sum_j \\exp x_{n,j}}\\right)$$<br>其中 $y_n \\in [0,C-1]$ 为第 n 个样本的类别索引，$\\sum_j$ 为某个样本对 C 个类别的求和。</p>\n<p>除了增加了一个 <code>LogSoftmax</code> 的计算，其他均与 NLLoss 层类似，故类签名中的参数介绍略。</p>\n<h1 id=\"PoissonNLLLoss\"><a href=\"#PoissonNLLLoss\" class=\"headerlink\" title=\"PoissonNLLLoss\"></a>PoissonNLLLoss</h1><p>Poisson 损失一般用于服从 poisson 分布的计数数据回归的问题，例如下周教堂人数预测。Poisson 分布如下<br>$$P(X=k)=\\frac {\\lambda^k e^{-\\lambda}} {k!}$$<br>随机变量 X 的期望 $E[X]=\\lambda$。我们的预测值 $x$ 就是对期望 $\\lambda$ 的预测，target 值就是真实的计数值（例如事件发生的次数，教堂的人数等），target 值用 $y$ 表示，也就是上式中的 $k$，于是单个样本的负对数似然可表示如下：<br>$$l= -\\log P(y|x) =-\\log \\frac {x^{y} e^{-x}} {y!}=x-y \\log x+ \\log(y!)$$<br>最后一项可以忽略或者适应 Stirling 公式近似求解。因为是一个常数，所以即使忽略掉，也不影响反向传播的计算。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PoissonNLLLoss(log_input: bool=<span class=\"literal\">True</span>, full: bool=<span class=\"literal\">False</span>, size_average=<span class=\"literal\">None</span>, eps: float=<span class=\"number\">1e-8</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>log_input</code> 指明 forward 的输入 input 是否经过了 log 处理，如是 True，那么上式损失计算应改为 $l=e^x - yx$，否则损失计算式为 $l=x-y \\log(x+eps)$。在 Poisson 回归中，假定期望的对数符合线性模型，所以很多时候是对期望的 log 值进行预测，即 <code>log_input=True</code>，此时 target 值也要经过 log 处理。</p>\n<blockquote>\n<p>程序中为了防止计算数值上下溢，往往会采用 log 处理</p>\n</blockquote>\n<p><code>full</code> 指示是否添加最后一项 $\\log(y!)$。如需要添加，那么使用 Stirling 公式近似，Stirling 公式为<br>$$n! \\sim \\sqrt{2 \\pi} n^{n+1/2} e^{-n}$$<br>于是有<br>$$\\log(n!)=\\frac 1 2 \\log(2 \\pi n)+ n \\log n - n$$</p>\n<p>forward 方法的 input 的 shape 是 $(N, <em>)$，其中 $</em>$ 表示对维度的扩展，且损失计算都是在 $*$ 维度上按像素级进行计算，故 target 的 shape 也是 $(N, *)$。如果 <code>reduction</code> 参数为 <code>none</code>，那么输出 shape 也是 $(N, *)$，否则将输出 Tensor 中所有值按 求和或平均 进行归约，最终得到一个标量值。</p>\n<h1 id=\"KLDivLoss\"><a href=\"#KLDivLoss\" class=\"headerlink\" title=\"KLDivLoss\"></a>KLDivLoss</h1><p>KL 散度用于度量两个分布之间的差异。KL 散度损失适用于<b>回归</b>问题。</p>\n<p>根据 KL 散度计算损失，KL 散度计算如下，<br>$$D(P||Q)=\\sum P(x) \\cdot \\log \\frac {P(x)}{Q(x)}$$<br>$$D(P||Q) = \\int_x p(x) \\log \\frac {p(x)}{q(x)} dx$$</p>\n<p>预测分布越接近真实分布，那么两者之间的 KL 散度应该越小，所以 KL 散度可以作为一种损失。<br>PyTorch 中的类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KLDivLoss(size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>, log_target: bool=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p><code>log_target</code> 指示 target 是否经过 log 处理。</p>\n<p>forward 方法中，参数 input 表示预测概率，且经过 log 处理，input 的 shape 为 $(N,<em>)$，其中 $</em>$ 表示单个样本的所有维度。KL 散度损失按像素级计算（可看作是连续分布的离散采样），<br>$$l=y \\cdot (\\log y - x)$$<br>其中 $x$ 表示随机变量某个值对应的预测概率，且经过 log 处理，$y$ 表示这个随机变量在这个值处的真实概率。</p>\n<p>forward 方法的输出结果的 shape 与 input 相同，为 $(N,*)$，如果 <code>reduction</code> 不为 <code>none</code>，那么输出结果将按 求和或平均 归约为一个标量值。</p>\n<h1 id=\"BCEWithLogitsLoss\"><a href=\"#BCEWithLogitsLoss\" class=\"headerlink\" title=\"BCEWithLogitsLoss\"></a>BCEWithLogitsLoss</h1><p>PyTorch 中这个 layer 合并了 Sigmoid 层和 BCELoss 层，由于 BCELoss 层计算单个样本的 BCE 损失为，<br>$$l=y \\log x + (1-y) \\log (1-x)$$<br>其中 $y \\in {0,1}$ 表示样本的真实分类，$x\\in [0,1]$ 表示样本的预测概率，通常使用 Sigmoid 层来将处于实数域的前一个 layer 输出值压缩到 $[0,1]$ 之间，故为了少写一个 Sigmoid 层，将这两者合并为单个 layer： <code>BCEWithLogitsLoss</code>。所以这个 layer 的输入是原始的未归一化的各类别的得分，单个样本的损失为，<br>$$l_n=-w_n [y_n \\log \\sigma(x_n) +(1-y_n) \\log (1-\\sigma(x_n))]$$<br>这里，批样本中每个样本有各自的一个权重因子 $w_n$。</p>\n<p>如果是多标签多分类问题，那么对于每个类别，均独立进行二分类（正或负），记类别索引为 $c$，那么单个样本的损失为<br>$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$<br>其中 $y_{n,c} \\in {0,1}$，$x_{n,c} \\in \\mathbb R$。</p>\n<p>还可以对正类样本增加一个权重因子 $p_c$，用于权衡最终的召回率和精度，于是上式变为<br>$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [p_c y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$<br>当 $p_c &gt;1$ 时召回率增大，$p_c&lt;1$ 时 精度增大。$p_c$ 可以取类别 $c$ 下 负样本与正样本数量比，如此可认为正负例相等。</p>\n<p>forward 方法中 input 的 shape 为 $(N,<em>)$，其中 $N$ 为批大小，$</em>$ 表示单个样本的维度大小，损失按像素计算，故 target 和未归约的 output 的 shape 均为 $(N,*)$，如果对 output 按求和或平均归约，则 output 为一个标量值。</p>\n<blockquote>\n<p>这个 layer 比起 Sigmoid 和 BCELoss 两个 layer，在数值计算上更加稳定（能避免数值上下溢），因为使用了 <code>log-sum-exp</code> 技巧。</p>\n</blockquote>\n<p>适用于<b>分类</b>问题。</p>\n<h1 id=\"SmoothL1Loss\"><a href=\"#SmoothL1Loss\" class=\"headerlink\" title=\"SmoothL1Loss\"></a>SmoothL1Loss</h1><p>对 L1Loss 的改进，当 L1 范数低于一定值时，使用差的平方项来代替误差，这是因为当预测值越接近真实值时，损失的梯度应该越小，从而减缓参数的更新幅度。SmoothL1Loss 按像素计算，计算式为，<br>$$l_i=\\begin{cases} \\frac 1 {2 \\beta} (x_i-y_i)^2 &amp; |x_i - y_i| &lt; \\beta \\ |x_i-y_i|-\\frac 1 {2 \\beta} &amp; \\text{otherwise}  \\end{cases}$$<br>适用于<b>回归</b>问题。</p>\n","site":{"data":{}},"excerpt":"<p>前面介绍了 <a href=\"2021/1/12/dl/x_ent_loss\">交叉熵损失</a>，本篇就 PyTorch 中的各种 <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\" target=\"_blank\" rel=\"noopener\">Loss</a> 进行分解并掌握其用法。</p>","more":"<h1 id=\"L1Loss\"><a href=\"#L1Loss\" class=\"headerlink\" title=\"L1Loss\"></a>L1Loss</h1><p>基于L1 范数的损失，单个样本的L1损失为 $l_n=|x_n-y_n|$，其中 <code>n</code> 为批样本中的样本索引，$x_n$ 为预测值，$y_n$ 为 GT，L1 损失适用于<b>回归</b>问题。</p>\n<h1 id=\"MSELoss\"><a href=\"#MSELoss\" class=\"headerlink\" title=\"MSELoss\"></a>MSELoss</h1><p>均方差（L2范数平方）损失，单个样本损失的计算公式为 $l_n=(x_n-y_n)^2$。适用于<b>回归</b>问题。</p>\n<h1 id=\"NLLLoss\"><a href=\"#NLLLoss\" class=\"headerlink\" title=\"NLLLoss\"></a>NLLLoss</h1><p>负对数似然损失，适用于<b>分类</b>问题。对于单个样本，似然函数为<br>$$\\mathcal L=\\prod_{i=1}^C x_i^{y_i}$$<br>其中输出向量 $\\mathbf x = (x_1,…,x_C)$ 表示每个分类的预测概率，GT 向量为 $\\mathbf y=(y_1,…,y_C)$，如果是单标签分类，$\\mathbf y$ 为 one-hot，如果是多标签分类，$\\mathbf y$ 中可能有多个元素值为 1。负对数似然则为，<br>$$l=-\\sum_{i=1}^C y_i \\log x_i$$</p>\n<p>实际在 PyTorch 中，NLLLoss 层的输入 Tensor 的 shape 以及 GT target 的 shape 与上面有所不同，以单标签多分类为例，网络输出 Tensor 的 shape 可以是 $(N,C)$，其中 N 表示批大小，C 表示通道也是类别数。GT target 的 shape 为 <code>N</code>，其中每个元素值的范围 <code>[0,C-1]</code>，表示某个样本的类别索引，NLLoss 层的输入已经表示样本各分类的概率对数（由<code>LogSoftmax</code>得到），负对数似然为<br>$$L=(l_1,…,l_N), \\quad l_n=- x_{n,y_n}$$</p>\n<p>如果给定参数<code>weight</code>，那么其必须是 1-D tensor，长度与类别数<code>C</code> 相等，用于给每个类别增加一个权重，参考 <a href=\"2021/1/12/dl/x_ent_loss\">交叉熵损失</a> 中的 <a href=\"2021/1/12/dl/x_ent_loss#Balanced-Cross-Entropy\">$\\alpha$ 均衡交叉熵</a>，这在非均衡数据集上较为有效。此时有<br>$$l_n=- w_{y_n}  x_{n,y_n}$$</p>\n<p>类签名</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.nn.NLLLoss(weight: Optional[torch.Tensor]=<span class=\"literal\">None</span>, size_average=<span class=\"literal\">None</span>, ignore_index: int=<span class=\"number\">-100</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n\n<p><code>size_average</code> 和 <code>reduce</code> 这两个参数已经过时。<code>reduction</code>用于指定批样本的损失向量是否归约（均值或求和）。</p>\n<p><code>ignore_index</code> 如果指定，那么当 GT target 值等于 <code>ignore_index</code> 时，将会忽略对应的损失贡献。</p>\n<p>input 通过 forward 方法指定，input 表示每个分类的概率对数，这可以通过 <code>LogSoftmax</code> 得到，input 的 shape 可以是 $(N,C)$，或者是 $(N,C,d_1,…,d_K)$，对于后者，其 target 的 shape 则为 $(N,d_1,…,d_K)$，此时的（未归约）损失 shape 也是 $(N,d_1,…,d_K)$，相比较于前者，后者就是扩展了维度而已，对于 $(d_1,…d_K)$ 中按像素级地计算负对数似然损失。</p>\n<h1 id=\"CrossEntropyLoss\"><a href=\"#CrossEntropyLoss\" class=\"headerlink\" title=\"CrossEntropyLoss\"></a>CrossEntropyLoss</h1><p>交叉熵损失，适用于分类问题。PyTorch 中，这个类（layer）合并了 <code>LogSoftmax</code> 和 <code>NLLLoss</code>，所以这个 layer 的 input 为为归一化的各分类的原始得分，input 的 shape 可以是 $(N,C)$ 或 $(N,C,d_1,…,d_K)$。target 的 shape 则为 $(N,)$ 或 $(N,d_1,…,d_K)$。<br>以 input 的 shape 为 $(N,C)$ 为例，此 layer 的损失计算可表示为（单个样本）<br>$$l_n=-\\log \\left(\\frac {\\exp x_{n,y_n}}{\\sum_j \\exp x_{n,j}}\\right)$$<br>其中 $y_n \\in [0,C-1]$ 为第 n 个样本的类别索引，$\\sum_j$ 为某个样本对 C 个类别的求和。</p>\n<p>除了增加了一个 <code>LogSoftmax</code> 的计算，其他均与 NLLoss 层类似，故类签名中的参数介绍略。</p>\n<h1 id=\"PoissonNLLLoss\"><a href=\"#PoissonNLLLoss\" class=\"headerlink\" title=\"PoissonNLLLoss\"></a>PoissonNLLLoss</h1><p>Poisson 损失一般用于服从 poisson 分布的计数数据回归的问题，例如下周教堂人数预测。Poisson 分布如下<br>$$P(X=k)=\\frac {\\lambda^k e^{-\\lambda}} {k!}$$<br>随机变量 X 的期望 $E[X]=\\lambda$。我们的预测值 $x$ 就是对期望 $\\lambda$ 的预测，target 值就是真实的计数值（例如事件发生的次数，教堂的人数等），target 值用 $y$ 表示，也就是上式中的 $k$，于是单个样本的负对数似然可表示如下：<br>$$l= -\\log P(y|x) =-\\log \\frac {x^{y} e^{-x}} {y!}=x-y \\log x+ \\log(y!)$$<br>最后一项可以忽略或者适应 Stirling 公式近似求解。因为是一个常数，所以即使忽略掉，也不影响反向传播的计算。</p>\n<p>类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PoissonNLLLoss(log_input: bool=<span class=\"literal\">True</span>, full: bool=<span class=\"literal\">False</span>, size_average=<span class=\"literal\">None</span>, eps: float=<span class=\"number\">1e-8</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>)</span><br></pre></td></tr></table></figure>\n<p><code>log_input</code> 指明 forward 的输入 input 是否经过了 log 处理，如是 True，那么上式损失计算应改为 $l=e^x - yx$，否则损失计算式为 $l=x-y \\log(x+eps)$。在 Poisson 回归中，假定期望的对数符合线性模型，所以很多时候是对期望的 log 值进行预测，即 <code>log_input=True</code>，此时 target 值也要经过 log 处理。</p>\n<blockquote>\n<p>程序中为了防止计算数值上下溢，往往会采用 log 处理</p>\n</blockquote>\n<p><code>full</code> 指示是否添加最后一项 $\\log(y!)$。如需要添加，那么使用 Stirling 公式近似，Stirling 公式为<br>$$n! \\sim \\sqrt{2 \\pi} n^{n+1/2} e^{-n}$$<br>于是有<br>$$\\log(n!)=\\frac 1 2 \\log(2 \\pi n)+ n \\log n - n$$</p>\n<p>forward 方法的 input 的 shape 是 $(N, <em>)$，其中 $</em>$ 表示对维度的扩展，且损失计算都是在 $*$ 维度上按像素级进行计算，故 target 的 shape 也是 $(N, *)$。如果 <code>reduction</code> 参数为 <code>none</code>，那么输出 shape 也是 $(N, *)$，否则将输出 Tensor 中所有值按 求和或平均 进行归约，最终得到一个标量值。</p>\n<h1 id=\"KLDivLoss\"><a href=\"#KLDivLoss\" class=\"headerlink\" title=\"KLDivLoss\"></a>KLDivLoss</h1><p>KL 散度用于度量两个分布之间的差异。KL 散度损失适用于<b>回归</b>问题。</p>\n<p>根据 KL 散度计算损失，KL 散度计算如下，<br>$$D(P||Q)=\\sum P(x) \\cdot \\log \\frac {P(x)}{Q(x)}$$<br>$$D(P||Q) = \\int_x p(x) \\log \\frac {p(x)}{q(x)} dx$$</p>\n<p>预测分布越接近真实分布，那么两者之间的 KL 散度应该越小，所以 KL 散度可以作为一种损失。<br>PyTorch 中的类签名：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KLDivLoss(size_average=<span class=\"literal\">None</span>, reduce=<span class=\"literal\">None</span>, reduction: str=<span class=\"string\">'mean'</span>, log_target: bool=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p><code>log_target</code> 指示 target 是否经过 log 处理。</p>\n<p>forward 方法中，参数 input 表示预测概率，且经过 log 处理，input 的 shape 为 $(N,<em>)$，其中 $</em>$ 表示单个样本的所有维度。KL 散度损失按像素级计算（可看作是连续分布的离散采样），<br>$$l=y \\cdot (\\log y - x)$$<br>其中 $x$ 表示随机变量某个值对应的预测概率，且经过 log 处理，$y$ 表示这个随机变量在这个值处的真实概率。</p>\n<p>forward 方法的输出结果的 shape 与 input 相同，为 $(N,*)$，如果 <code>reduction</code> 不为 <code>none</code>，那么输出结果将按 求和或平均 归约为一个标量值。</p>\n<h1 id=\"BCEWithLogitsLoss\"><a href=\"#BCEWithLogitsLoss\" class=\"headerlink\" title=\"BCEWithLogitsLoss\"></a>BCEWithLogitsLoss</h1><p>PyTorch 中这个 layer 合并了 Sigmoid 层和 BCELoss 层，由于 BCELoss 层计算单个样本的 BCE 损失为，<br>$$l=y \\log x + (1-y) \\log (1-x)$$<br>其中 $y \\in {0,1}$ 表示样本的真实分类，$x\\in [0,1]$ 表示样本的预测概率，通常使用 Sigmoid 层来将处于实数域的前一个 layer 输出值压缩到 $[0,1]$ 之间，故为了少写一个 Sigmoid 层，将这两者合并为单个 layer： <code>BCEWithLogitsLoss</code>。所以这个 layer 的输入是原始的未归一化的各类别的得分，单个样本的损失为，<br>$$l_n=-w_n [y_n \\log \\sigma(x_n) +(1-y_n) \\log (1-\\sigma(x_n))]$$<br>这里，批样本中每个样本有各自的一个权重因子 $w_n$。</p>\n<p>如果是多标签多分类问题，那么对于每个类别，均独立进行二分类（正或负），记类别索引为 $c$，那么单个样本的损失为<br>$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$<br>其中 $y_{n,c} \\in {0,1}$，$x_{n,c} \\in \\mathbb R$。</p>\n<p>还可以对正类样本增加一个权重因子 $p_c$，用于权衡最终的召回率和精度，于是上式变为<br>$$l_n=\\sum_{c=1}^C l_{n,c}=-\\sum_{c=1}^C w_n [p_c y_{n,c} \\log \\sigma(x_{n,c}) +(1-y_{n,c}) \\log (1-\\sigma(x_{n,c}))]$$<br>当 $p_c &gt;1$ 时召回率增大，$p_c&lt;1$ 时 精度增大。$p_c$ 可以取类别 $c$ 下 负样本与正样本数量比，如此可认为正负例相等。</p>\n<p>forward 方法中 input 的 shape 为 $(N,<em>)$，其中 $N$ 为批大小，$</em>$ 表示单个样本的维度大小，损失按像素计算，故 target 和未归约的 output 的 shape 均为 $(N,*)$，如果对 output 按求和或平均归约，则 output 为一个标量值。</p>\n<blockquote>\n<p>这个 layer 比起 Sigmoid 和 BCELoss 两个 layer，在数值计算上更加稳定（能避免数值上下溢），因为使用了 <code>log-sum-exp</code> 技巧。</p>\n</blockquote>\n<p>适用于<b>分类</b>问题。</p>\n<h1 id=\"SmoothL1Loss\"><a href=\"#SmoothL1Loss\" class=\"headerlink\" title=\"SmoothL1Loss\"></a>SmoothL1Loss</h1><p>对 L1Loss 的改进，当 L1 范数低于一定值时，使用差的平方项来代替误差，这是因为当预测值越接近真实值时，损失的梯度应该越小，从而减缓参数的更新幅度。SmoothL1Loss 按像素计算，计算式为，<br>$$l_i=\\begin{cases} \\frac 1 {2 \\beta} (x_i-y_i)^2 &amp; |x_i - y_i| &lt; \\beta \\ |x_i-y_i|-\\frac 1 {2 \\beta} &amp; \\text{otherwise}  \\end{cases}$$<br>适用于<b>回归</b>问题。</p>"},{"title":"Deep Learning Tricks1","p":"dl/tricks1","date":"2021-01-08T06:32:52.000Z","mathjax":true,"_content":"深度学习中有很多训练技巧，这里做一些总结。\n<!-- more -->\n# BatchNorm\n## BatchNorm 的作用：\n1. 防止过拟合。每个样本均经过批归一化后，防止出现离群点，从而导致过拟合。\n2. 加快收敛。梯度下降过程中，每一层参数不断变化，导致输出结果的分布也在不断变化，后层网络就需要不停的适应这种变化。通常称这种变化为 `Internal Covariate Shift`，当数据流经深度网络的某层时，这层的计算结果可能会使得数据变的更大，或这更小，使得后面的网络层学习变得困难。使用 BN 后，BN 应用在每层计算函数之后，激活函数之前，使得数据分布集中在中间位置，这样再应用激活函数才有意义，并且 BN 之后的数据集中在激活函数的中间位置，此位置的梯度较大，从而有效防止了梯度弥散，并加快收敛速度。\n3. 防止梯度弥散。\n\n## BatchNorm 的计算公式\n假设批大小为 `m`，足够小的数 $\\epsilon$，\n\n$\\mu=\\frac 1 m \\sum_{i=1}^m x_i$\n\n$\\sigma^2=\\frac 1 m \\sum_{i=1}^m (x_i - \\mu)^2$\n\n$\\hat x_i=\\frac {x_i -\\mu} {\\sqrt{\\sigma^2+\\epsilon}}$\n\n$y_i=\\gamma \\hat x_i+\\beta$\n\n上面最后一步为 `scale and shift`操作，其中参数 $\\gamma, \\beta$ 是 BN 层需要学习的参数，因为对数据做了归一化处理，使得难以学习到输入数据的特征，所以引入可训练参数 $\\gamma, \\ \\beta$，这样既实现了归一化，又增加参数以可以学习输入数据的特征。\n\n## BatchNorm 适用范围\nBatchNorm 降低了数据之间的绝对差异，考虑相对差异（归一化带来的），所以如果某任务中图像的绝对差异很重要，那么其实不适合使用 BatchNorm。","source":"_posts/dl/tricks_1.md","raw":"---\ntitle: Deep Learning Tricks1\np: dl/tricks1\ndate: 2021-01-08 14:32:52\ntags: Deep Learning\nmathjax: true\n---\n深度学习中有很多训练技巧，这里做一些总结。\n<!-- more -->\n# BatchNorm\n## BatchNorm 的作用：\n1. 防止过拟合。每个样本均经过批归一化后，防止出现离群点，从而导致过拟合。\n2. 加快收敛。梯度下降过程中，每一层参数不断变化，导致输出结果的分布也在不断变化，后层网络就需要不停的适应这种变化。通常称这种变化为 `Internal Covariate Shift`，当数据流经深度网络的某层时，这层的计算结果可能会使得数据变的更大，或这更小，使得后面的网络层学习变得困难。使用 BN 后，BN 应用在每层计算函数之后，激活函数之前，使得数据分布集中在中间位置，这样再应用激活函数才有意义，并且 BN 之后的数据集中在激活函数的中间位置，此位置的梯度较大，从而有效防止了梯度弥散，并加快收敛速度。\n3. 防止梯度弥散。\n\n## BatchNorm 的计算公式\n假设批大小为 `m`，足够小的数 $\\epsilon$，\n\n$\\mu=\\frac 1 m \\sum_{i=1}^m x_i$\n\n$\\sigma^2=\\frac 1 m \\sum_{i=1}^m (x_i - \\mu)^2$\n\n$\\hat x_i=\\frac {x_i -\\mu} {\\sqrt{\\sigma^2+\\epsilon}}$\n\n$y_i=\\gamma \\hat x_i+\\beta$\n\n上面最后一步为 `scale and shift`操作，其中参数 $\\gamma, \\beta$ 是 BN 层需要学习的参数，因为对数据做了归一化处理，使得难以学习到输入数据的特征，所以引入可训练参数 $\\gamma, \\ \\beta$，这样既实现了归一化，又增加参数以可以学习输入数据的特征。\n\n## BatchNorm 适用范围\nBatchNorm 降低了数据之间的绝对差异，考虑相对差异（归一化带来的），所以如果某任务中图像的绝对差异很重要，那么其实不适合使用 BatchNorm。","slug":"dl/tricks_1","published":1,"updated":"2021-01-08T11:11:04.076Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxr7wi40008y4dj40bc9t56","content":"<p>深度学习中有很多训练技巧，这里做一些总结。</p>\n<a id=\"more\"></a>\n<h1 id=\"BatchNorm\"><a href=\"#BatchNorm\" class=\"headerlink\" title=\"BatchNorm\"></a>BatchNorm</h1><h2 id=\"BatchNorm-的作用：\"><a href=\"#BatchNorm-的作用：\" class=\"headerlink\" title=\"BatchNorm 的作用：\"></a>BatchNorm 的作用：</h2><ol>\n<li>防止过拟合。每个样本均经过批归一化后，防止出现离群点，从而导致过拟合。</li>\n<li>加快收敛。梯度下降过程中，每一层参数不断变化，导致输出结果的分布也在不断变化，后层网络就需要不停的适应这种变化。通常称这种变化为 <code>Internal Covariate Shift</code>，当数据流经深度网络的某层时，这层的计算结果可能会使得数据变的更大，或这更小，使得后面的网络层学习变得困难。使用 BN 后，BN 应用在每层计算函数之后，激活函数之前，使得数据分布集中在中间位置，这样再应用激活函数才有意义，并且 BN 之后的数据集中在激活函数的中间位置，此位置的梯度较大，从而有效防止了梯度弥散，并加快收敛速度。</li>\n<li>防止梯度弥散。</li>\n</ol>\n<h2 id=\"BatchNorm-的计算公式\"><a href=\"#BatchNorm-的计算公式\" class=\"headerlink\" title=\"BatchNorm 的计算公式\"></a>BatchNorm 的计算公式</h2><p>假设批大小为 <code>m</code>，足够小的数 $\\epsilon$，</p>\n<p>$\\mu=\\frac 1 m \\sum_{i=1}^m x_i$</p>\n<p>$\\sigma^2=\\frac 1 m \\sum_{i=1}^m (x_i - \\mu)^2$</p>\n<p>$\\hat x_i=\\frac {x_i -\\mu} {\\sqrt{\\sigma^2+\\epsilon}}$</p>\n<p>$y_i=\\gamma \\hat x_i+\\beta$</p>\n<p>上面最后一步为 <code>scale and shift</code>操作，其中参数 $\\gamma, \\beta$ 是 BN 层需要学习的参数，因为对数据做了归一化处理，使得难以学习到输入数据的特征，所以引入可训练参数 $\\gamma, \\ \\beta$，这样既实现了归一化，又增加参数以可以学习输入数据的特征。</p>\n<h2 id=\"BatchNorm-适用范围\"><a href=\"#BatchNorm-适用范围\" class=\"headerlink\" title=\"BatchNorm 适用范围\"></a>BatchNorm 适用范围</h2><p>BatchNorm 降低了数据之间的绝对差异，考虑相对差异（归一化带来的），所以如果某任务中图像的绝对差异很重要，那么其实不适合使用 BatchNorm。</p>\n","site":{"data":{}},"excerpt":"<p>深度学习中有很多训练技巧，这里做一些总结。</p>","more":"<h1 id=\"BatchNorm\"><a href=\"#BatchNorm\" class=\"headerlink\" title=\"BatchNorm\"></a>BatchNorm</h1><h2 id=\"BatchNorm-的作用：\"><a href=\"#BatchNorm-的作用：\" class=\"headerlink\" title=\"BatchNorm 的作用：\"></a>BatchNorm 的作用：</h2><ol>\n<li>防止过拟合。每个样本均经过批归一化后，防止出现离群点，从而导致过拟合。</li>\n<li>加快收敛。梯度下降过程中，每一层参数不断变化，导致输出结果的分布也在不断变化，后层网络就需要不停的适应这种变化。通常称这种变化为 <code>Internal Covariate Shift</code>，当数据流经深度网络的某层时，这层的计算结果可能会使得数据变的更大，或这更小，使得后面的网络层学习变得困难。使用 BN 后，BN 应用在每层计算函数之后，激活函数之前，使得数据分布集中在中间位置，这样再应用激活函数才有意义，并且 BN 之后的数据集中在激活函数的中间位置，此位置的梯度较大，从而有效防止了梯度弥散，并加快收敛速度。</li>\n<li>防止梯度弥散。</li>\n</ol>\n<h2 id=\"BatchNorm-的计算公式\"><a href=\"#BatchNorm-的计算公式\" class=\"headerlink\" title=\"BatchNorm 的计算公式\"></a>BatchNorm 的计算公式</h2><p>假设批大小为 <code>m</code>，足够小的数 $\\epsilon$，</p>\n<p>$\\mu=\\frac 1 m \\sum_{i=1}^m x_i$</p>\n<p>$\\sigma^2=\\frac 1 m \\sum_{i=1}^m (x_i - \\mu)^2$</p>\n<p>$\\hat x_i=\\frac {x_i -\\mu} {\\sqrt{\\sigma^2+\\epsilon}}$</p>\n<p>$y_i=\\gamma \\hat x_i+\\beta$</p>\n<p>上面最后一步为 <code>scale and shift</code>操作，其中参数 $\\gamma, \\beta$ 是 BN 层需要学习的参数，因为对数据做了归一化处理，使得难以学习到输入数据的特征，所以引入可训练参数 $\\gamma, \\ \\beta$，这样既实现了归一化，又增加参数以可以学习输入数据的特征。</p>\n<h2 id=\"BatchNorm-适用范围\"><a href=\"#BatchNorm-适用范围\" class=\"headerlink\" title=\"BatchNorm 适用范围\"></a>BatchNorm 适用范围</h2><p>BatchNorm 降低了数据之间的绝对差异，考虑相对差异（归一化带来的），所以如果某任务中图像的绝对差异很重要，那么其实不适合使用 BatchNorm。</p>"},{"title":"TODO","date":"2021-02-02T03:51:13.000Z","_content":"\n牛顿法/拟牛顿法\n\nhttps://blog.csdn.net/songbinxu/article/details/79677948\n\nhttps://blog.csdn.net/itplus/article/details/21896453\n\nhttps://blog.csdn.net/itplus/article/details/21896619\n","source":"_posts/TODO.md","raw":"---\ntitle: TODO\ndate: 2021-02-02 11:51:13\ntags:\n---\n\n牛顿法/拟牛顿法\n\nhttps://blog.csdn.net/songbinxu/article/details/79677948\n\nhttps://blog.csdn.net/itplus/article/details/21896453\n\nhttps://blog.csdn.net/itplus/article/details/21896619\n","slug":"TODO","published":1,"updated":"2021-02-02T06:02:42.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bub0000tsdjfr1jd1df","content":"<p>牛顿法/拟牛顿法</p>\n<p><a href=\"https://blog.csdn.net/songbinxu/article/details/79677948\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/songbinxu/article/details/79677948</a></p>\n<p><a href=\"https://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/itplus/article/details/21896453</a></p>\n<p><a href=\"https://blog.csdn.net/itplus/article/details/21896619\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/itplus/article/details/21896619</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>牛顿法/拟牛顿法</p>\n<p><a href=\"https://blog.csdn.net/songbinxu/article/details/79677948\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/songbinxu/article/details/79677948</a></p>\n<p><a href=\"https://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/itplus/article/details/21896453</a></p>\n<p><a href=\"https://blog.csdn.net/itplus/article/details/21896619\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/itplus/article/details/21896619</a></p>\n"},{"title":"Receptive Field","date":"2021-02-18T02:51:00.000Z","p":"dl/receptive_field","mathjax":true,"_content":"\n# Size\n\n对于一个 fully CNN 的网络，第 `k` layer 的 receptive field 大小为，\n$$l_k=l_{k-1} + ((f_k-1)*\\prod_{i=1}^{k-1} s_i)$$\n其中，$l_{k-1}$ 表示第 `k-1` layer 上的 receptive field 大小，$f_k$ 是第 `k` layer 的 filter 大小，$s_i$ 是第 `i` layer 上的 stride 大小。这是自底向上计算，从 $l_1$ 开始，$l_1=f_1$。\n\n\n\n还有一种自顶向下的计算方法。假设总共有 `L` 个 layer，每个 layer 的输出 feature map 记为 $f_l, \\ l=1,...,L$，每个 layer 的 filter 大小为 $k_l$，stride 大小记为 $s_l$，记 $r_l$ 为最后一个 layer 关于 feature map $f_l$ 的 receptive field 大小，也就是说，$r_l$ 表示 $f_l$ 上多少个像素点对 $f_L$ 的一个像素点有贡献（这里仅考虑一维 feature map，如果是多维，那么分别独立考虑即可）。那么易知，$r_L=1$，\n\n$r_{L-1}=k_L$，这个也很好理解，上一层 feature map 中，$k_L$ 个像素点对应本层（最后一层）一个像素点。考虑一般情况，已知 $r_l$，求 $r_{l-1}$。\n\n首先假设 $k_l=1$，这样情况就简单些，若 $s_l=1$，那么 $r_{l-1}=r_l$，若 $s_l>1$，那么 $r_{l-1}=s_l \\cdot r_l -(s_l-1)$，因为 $r_l$ 中每两个像素点之间对应到 $f_{l-1}$ 上有 $s_l-1$ 个点，所以 $r_{l-1}=(s_l-1)\\cdot(r_l-1)+ r_l=s_l \\cdot r_l-s_l+1$。\n\n然后当 $k_l>1$，那么需要在 $f_{l-1}$ 上增加 $k_l-1$ 个像素点，于是\n$$r_{l-1}=s_l \\cdot r_l + (k_l-s_l)$$\n其中，$r_L=1, \\ r_{L-1}=k_L$。求解上式过程如下：\n$$r_{L-2}=s_{L-1} r_{L-1}+(k_{L-1}-s_{L-1})=s_{L-1}(k_L-1)+k_{L-1}$$\n$$r_{L-3}=s_{L-2} r_{L-2}+(k_{L-2}-s_{L-2})=s_{L-2}s_{L-1}(k_L-1)+s_{L-2}(k_{L-1}-1)+k_{L-2}$$\n$$\\cdots$$\n$$r_{l}=s_{l+1}\\cdots s_{L-1}(k_L-1)+s_{l+1}\\cdots s_{L-2}(k_{L-1}-1)+ \\cdots s_{l+1}(k_{l+2}-1)+k_{l+1}=1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right]$$\n其中令 $$\\prod_{l+1}^{l}s_i=1$$\n\n于是，\n$$\\begin{aligned} r_{l-1}&=1+\\sum_{j=l}^{L} \\left[(k_{j}-1) \\prod_{i=l}^{j-1}s_i \\right] \\\\ &=1+(k_l-1)+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\cdot s_l \\right] \\\\&=k_l-s_l+s_l \\left(1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right] \\right) \\\\&=s_l \\cdot r_l +k_l-s_l\\end{aligned}$$\n与前面递推式一致，说明通项式计算正确。\n\noutput feature size 的计算为，\n$$w_l=\\frac {w_{l-1}+2p_l-k_l} {s_l}+1$$\n其中 $w$ 表示宽，高 $h$ 的计算类似（以 2D image 数据为例）。\n\n# Region\n对输出 feature map 上一点有贡献的 region （Receptive Field）大小计算如上，还有一个参数也很重要：定位这个 region 的位置。例如输出 feature map 上一点 $f_L(i,j)$，产生这个特征的输入图像上的 region 位置如何求得。\n\n记在特征平面 $f_l$ 上这个 region 的左端和右端的坐标分别为 $u_l, \\ v_l$，这里的<b>坐标从 0 开始</b>，即，第一个像素点的坐标为 `0`，在输出特征平面 $f_L$ 上有 $u_L=v_L=i$，同样地，仅考虑一维情况，对于二维情况，另一维度独立地进行类似计算可得。\n\n同样使用递推的思想，已知 $u_l, \\ v_l$，求 $u_{l-1}, v_{l-1}$。\n\n首先从一个简单的情况开始，假设 $u_l=0$，这表示 $f_l$ 中的 region 左侧位于第一个像素点，此时 $u_{l-1}=-p_l$，即$f_{l-1}$ 左侧填充 $p_l$ 个像素；如果 $u_l=1$，那么 $u_{l-1}=s_l-p_l$，这也很好理解，从 $f_{l-1}$ 最左侧第一个像素点（填充之后为 $-p_l$）向右移动 $s_l$；如果 $u_l=2$，那么继续向右移动 $s_l$，即 $u_{l-1}=2s_l-p_l$，于是一般地，\n$$u_{l-1}=u_l \\cdot s_l -p_l$$\n$$v_{l-1}=v_l \\cdot s_l - p_l + k_l-1$$\n完全式的计算过程如下：\n$$u_{L-1}=u_L \\cdot s_L - p_L$$\n$$u_{L-2}=u_{L-1} \\cdot s_{L-1}-p_{L-1}=s_{L-1}s_L u_L-s_{L-1}p_L-p_{L-1}$$\n$$u_{L-3}=u_{L-2} \\cdot s_{L-2}-p_{L-2}=s_{L-2}s_{L-1}s_L u_L-s_{L-2}s_{L-1}p_L-s_{L-2}p_{L-1}-p_{L-2}$$\n$$\\cdots$$\n$$u_l=s_{l+1}\\cdots s_L u_L-s_{l+1}\\cdots s_{L-1} p_{L}-\\cdots-s_{l+1} p_{l+2}-p_{l+1}=u_L\\prod_{i=l+1}^L s_i-\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1} s_i$$\n\n其中，$\\prod_{i=l+1}^l s_i=1$, 类似地，\n$$v_l=v_L \\prod_{i=l+1}^L s_i - \\sum_{j=l+1}^L(1+p_j-k_j)\\prod_{i=l+1}^{j-1} s_i$$\n\n# Relation\nReceptive Field size 与 region 之间的联系，\n$$r_l=v_l-u_l+1$$\n\n# Stride & Padding\n定义两个变量，有效 stride 和 有效 padding，这两者分别定义如下：\n\n$$S_l=\\prod_{i=l+1}^L s_i$$\n\n$$P_l=\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1}s_i$$\n\n他们的递推公式为，\n$$S_{l-1}=s_l \\cdot S_l$$\n$$P_{l-1}=p_l+s_l \\cdot P_l$$\n\n有着这两个定义变量，region 位置公式可表示为，\n$$u_l=u_L \\cdot S_l - P_l$$\n\n# Center\nreceptive field 的中心可由 region 位置计算得到，在第 `l` layer 上为，\n$$c_l=\\frac {u_l+v_l} 2$$\n\n","source":"_posts/dl/receptive_field.md","raw":"---\ntitle: Receptive Field\ndate: 2021-02-18 10:51:00\np: dl/receptive_field\ntags: Deep Learning, CNN\nmathjax: true\n---\n\n# Size\n\n对于一个 fully CNN 的网络，第 `k` layer 的 receptive field 大小为，\n$$l_k=l_{k-1} + ((f_k-1)*\\prod_{i=1}^{k-1} s_i)$$\n其中，$l_{k-1}$ 表示第 `k-1` layer 上的 receptive field 大小，$f_k$ 是第 `k` layer 的 filter 大小，$s_i$ 是第 `i` layer 上的 stride 大小。这是自底向上计算，从 $l_1$ 开始，$l_1=f_1$。\n\n\n\n还有一种自顶向下的计算方法。假设总共有 `L` 个 layer，每个 layer 的输出 feature map 记为 $f_l, \\ l=1,...,L$，每个 layer 的 filter 大小为 $k_l$，stride 大小记为 $s_l$，记 $r_l$ 为最后一个 layer 关于 feature map $f_l$ 的 receptive field 大小，也就是说，$r_l$ 表示 $f_l$ 上多少个像素点对 $f_L$ 的一个像素点有贡献（这里仅考虑一维 feature map，如果是多维，那么分别独立考虑即可）。那么易知，$r_L=1$，\n\n$r_{L-1}=k_L$，这个也很好理解，上一层 feature map 中，$k_L$ 个像素点对应本层（最后一层）一个像素点。考虑一般情况，已知 $r_l$，求 $r_{l-1}$。\n\n首先假设 $k_l=1$，这样情况就简单些，若 $s_l=1$，那么 $r_{l-1}=r_l$，若 $s_l>1$，那么 $r_{l-1}=s_l \\cdot r_l -(s_l-1)$，因为 $r_l$ 中每两个像素点之间对应到 $f_{l-1}$ 上有 $s_l-1$ 个点，所以 $r_{l-1}=(s_l-1)\\cdot(r_l-1)+ r_l=s_l \\cdot r_l-s_l+1$。\n\n然后当 $k_l>1$，那么需要在 $f_{l-1}$ 上增加 $k_l-1$ 个像素点，于是\n$$r_{l-1}=s_l \\cdot r_l + (k_l-s_l)$$\n其中，$r_L=1, \\ r_{L-1}=k_L$。求解上式过程如下：\n$$r_{L-2}=s_{L-1} r_{L-1}+(k_{L-1}-s_{L-1})=s_{L-1}(k_L-1)+k_{L-1}$$\n$$r_{L-3}=s_{L-2} r_{L-2}+(k_{L-2}-s_{L-2})=s_{L-2}s_{L-1}(k_L-1)+s_{L-2}(k_{L-1}-1)+k_{L-2}$$\n$$\\cdots$$\n$$r_{l}=s_{l+1}\\cdots s_{L-1}(k_L-1)+s_{l+1}\\cdots s_{L-2}(k_{L-1}-1)+ \\cdots s_{l+1}(k_{l+2}-1)+k_{l+1}=1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right]$$\n其中令 $$\\prod_{l+1}^{l}s_i=1$$\n\n于是，\n$$\\begin{aligned} r_{l-1}&=1+\\sum_{j=l}^{L} \\left[(k_{j}-1) \\prod_{i=l}^{j-1}s_i \\right] \\\\ &=1+(k_l-1)+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\cdot s_l \\right] \\\\&=k_l-s_l+s_l \\left(1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right] \\right) \\\\&=s_l \\cdot r_l +k_l-s_l\\end{aligned}$$\n与前面递推式一致，说明通项式计算正确。\n\noutput feature size 的计算为，\n$$w_l=\\frac {w_{l-1}+2p_l-k_l} {s_l}+1$$\n其中 $w$ 表示宽，高 $h$ 的计算类似（以 2D image 数据为例）。\n\n# Region\n对输出 feature map 上一点有贡献的 region （Receptive Field）大小计算如上，还有一个参数也很重要：定位这个 region 的位置。例如输出 feature map 上一点 $f_L(i,j)$，产生这个特征的输入图像上的 region 位置如何求得。\n\n记在特征平面 $f_l$ 上这个 region 的左端和右端的坐标分别为 $u_l, \\ v_l$，这里的<b>坐标从 0 开始</b>，即，第一个像素点的坐标为 `0`，在输出特征平面 $f_L$ 上有 $u_L=v_L=i$，同样地，仅考虑一维情况，对于二维情况，另一维度独立地进行类似计算可得。\n\n同样使用递推的思想，已知 $u_l, \\ v_l$，求 $u_{l-1}, v_{l-1}$。\n\n首先从一个简单的情况开始，假设 $u_l=0$，这表示 $f_l$ 中的 region 左侧位于第一个像素点，此时 $u_{l-1}=-p_l$，即$f_{l-1}$ 左侧填充 $p_l$ 个像素；如果 $u_l=1$，那么 $u_{l-1}=s_l-p_l$，这也很好理解，从 $f_{l-1}$ 最左侧第一个像素点（填充之后为 $-p_l$）向右移动 $s_l$；如果 $u_l=2$，那么继续向右移动 $s_l$，即 $u_{l-1}=2s_l-p_l$，于是一般地，\n$$u_{l-1}=u_l \\cdot s_l -p_l$$\n$$v_{l-1}=v_l \\cdot s_l - p_l + k_l-1$$\n完全式的计算过程如下：\n$$u_{L-1}=u_L \\cdot s_L - p_L$$\n$$u_{L-2}=u_{L-1} \\cdot s_{L-1}-p_{L-1}=s_{L-1}s_L u_L-s_{L-1}p_L-p_{L-1}$$\n$$u_{L-3}=u_{L-2} \\cdot s_{L-2}-p_{L-2}=s_{L-2}s_{L-1}s_L u_L-s_{L-2}s_{L-1}p_L-s_{L-2}p_{L-1}-p_{L-2}$$\n$$\\cdots$$\n$$u_l=s_{l+1}\\cdots s_L u_L-s_{l+1}\\cdots s_{L-1} p_{L}-\\cdots-s_{l+1} p_{l+2}-p_{l+1}=u_L\\prod_{i=l+1}^L s_i-\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1} s_i$$\n\n其中，$\\prod_{i=l+1}^l s_i=1$, 类似地，\n$$v_l=v_L \\prod_{i=l+1}^L s_i - \\sum_{j=l+1}^L(1+p_j-k_j)\\prod_{i=l+1}^{j-1} s_i$$\n\n# Relation\nReceptive Field size 与 region 之间的联系，\n$$r_l=v_l-u_l+1$$\n\n# Stride & Padding\n定义两个变量，有效 stride 和 有效 padding，这两者分别定义如下：\n\n$$S_l=\\prod_{i=l+1}^L s_i$$\n\n$$P_l=\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1}s_i$$\n\n他们的递推公式为，\n$$S_{l-1}=s_l \\cdot S_l$$\n$$P_{l-1}=p_l+s_l \\cdot P_l$$\n\n有着这两个定义变量，region 位置公式可表示为，\n$$u_l=u_L \\cdot S_l - P_l$$\n\n# Center\nreceptive field 的中心可由 region 位置计算得到，在第 `l` layer 上为，\n$$c_l=\\frac {u_l+v_l} 2$$\n\n","slug":"dl/receptive_field","published":1,"updated":"2021-02-19T02:18:58.909Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwe0001tsdjdtp55pab","content":"<h1 id=\"Size\"><a href=\"#Size\" class=\"headerlink\" title=\"Size\"></a>Size</h1><p>对于一个 fully CNN 的网络，第 <code>k</code> layer 的 receptive field 大小为，<br>$$l_k=l_{k-1} + ((f_k-1)*\\prod_{i=1}^{k-1} s_i)$$<br>其中，$l_{k-1}$ 表示第 <code>k-1</code> layer 上的 receptive field 大小，$f_k$ 是第 <code>k</code> layer 的 filter 大小，$s_i$ 是第 <code>i</code> layer 上的 stride 大小。这是自底向上计算，从 $l_1$ 开始，$l_1=f_1$。</p>\n<p>还有一种自顶向下的计算方法。假设总共有 <code>L</code> 个 layer，每个 layer 的输出 feature map 记为 $f_l, \\ l=1,…,L$，每个 layer 的 filter 大小为 $k_l$，stride 大小记为 $s_l$，记 $r_l$ 为最后一个 layer 关于 feature map $f_l$ 的 receptive field 大小，也就是说，$r_l$ 表示 $f_l$ 上多少个像素点对 $f_L$ 的一个像素点有贡献（这里仅考虑一维 feature map，如果是多维，那么分别独立考虑即可）。那么易知，$r_L=1$，</p>\n<p>$r_{L-1}=k_L$，这个也很好理解，上一层 feature map 中，$k_L$ 个像素点对应本层（最后一层）一个像素点。考虑一般情况，已知 $r_l$，求 $r_{l-1}$。</p>\n<p>首先假设 $k_l=1$，这样情况就简单些，若 $s_l=1$，那么 $r_{l-1}=r_l$，若 $s_l&gt;1$，那么 $r_{l-1}=s_l \\cdot r_l -(s_l-1)$，因为 $r_l$ 中每两个像素点之间对应到 $f_{l-1}$ 上有 $s_l-1$ 个点，所以 $r_{l-1}=(s_l-1)\\cdot(r_l-1)+ r_l=s_l \\cdot r_l-s_l+1$。</p>\n<p>然后当 $k_l&gt;1$，那么需要在 $f_{l-1}$ 上增加 $k_l-1$ 个像素点，于是<br>$$r_{l-1}=s_l \\cdot r_l + (k_l-s_l)$$<br>其中，$r_L=1, \\ r_{L-1}=k_L$。求解上式过程如下：<br>$$r_{L-2}=s_{L-1} r_{L-1}+(k_{L-1}-s_{L-1})=s_{L-1}(k_L-1)+k_{L-1}$$<br>$$r_{L-3}=s_{L-2} r_{L-2}+(k_{L-2}-s_{L-2})=s_{L-2}s_{L-1}(k_L-1)+s_{L-2}(k_{L-1}-1)+k_{L-2}$$<br>$$\\cdots$$<br>$$r_{l}=s_{l+1}\\cdots s_{L-1}(k_L-1)+s_{l+1}\\cdots s_{L-2}(k_{L-1}-1)+ \\cdots s_{l+1}(k_{l+2}-1)+k_{l+1}=1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right]$$<br>其中令 $$\\prod_{l+1}^{l}s_i=1$$</p>\n<p>于是，<br>$$\\begin{aligned} r_{l-1}&amp;=1+\\sum_{j=l}^{L} \\left[(k_{j}-1) \\prod_{i=l}^{j-1}s_i \\right] \\ &amp;=1+(k_l-1)+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\cdot s_l \\right] \\&amp;=k_l-s_l+s_l \\left(1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right] \\right) \\&amp;=s_l \\cdot r_l +k_l-s_l\\end{aligned}$$<br>与前面递推式一致，说明通项式计算正确。</p>\n<p>output feature size 的计算为，<br>$$w_l=\\frac {w_{l-1}+2p_l-k_l} {s_l}+1$$<br>其中 $w$ 表示宽，高 $h$ 的计算类似（以 2D image 数据为例）。</p>\n<h1 id=\"Region\"><a href=\"#Region\" class=\"headerlink\" title=\"Region\"></a>Region</h1><p>对输出 feature map 上一点有贡献的 region （Receptive Field）大小计算如上，还有一个参数也很重要：定位这个 region 的位置。例如输出 feature map 上一点 $f_L(i,j)$，产生这个特征的输入图像上的 region 位置如何求得。</p>\n<p>记在特征平面 $f_l$ 上这个 region 的左端和右端的坐标分别为 $u_l, \\ v_l$，这里的<b>坐标从 0 开始</b>，即，第一个像素点的坐标为 <code>0</code>，在输出特征平面 $f_L$ 上有 $u_L=v_L=i$，同样地，仅考虑一维情况，对于二维情况，另一维度独立地进行类似计算可得。</p>\n<p>同样使用递推的思想，已知 $u_l, \\ v_l$，求 $u_{l-1}, v_{l-1}$。</p>\n<p>首先从一个简单的情况开始，假设 $u_l=0$，这表示 $f_l$ 中的 region 左侧位于第一个像素点，此时 $u_{l-1}=-p_l$，即$f_{l-1}$ 左侧填充 $p_l$ 个像素；如果 $u_l=1$，那么 $u_{l-1}=s_l-p_l$，这也很好理解，从 $f_{l-1}$ 最左侧第一个像素点（填充之后为 $-p_l$）向右移动 $s_l$；如果 $u_l=2$，那么继续向右移动 $s_l$，即 $u_{l-1}=2s_l-p_l$，于是一般地，<br>$$u_{l-1}=u_l \\cdot s_l -p_l$$<br>$$v_{l-1}=v_l \\cdot s_l - p_l + k_l-1$$<br>完全式的计算过程如下：<br>$$u_{L-1}=u_L \\cdot s_L - p_L$$<br>$$u_{L-2}=u_{L-1} \\cdot s_{L-1}-p_{L-1}=s_{L-1}s_L u_L-s_{L-1}p_L-p_{L-1}$$<br>$$u_{L-3}=u_{L-2} \\cdot s_{L-2}-p_{L-2}=s_{L-2}s_{L-1}s_L u_L-s_{L-2}s_{L-1}p_L-s_{L-2}p_{L-1}-p_{L-2}$$<br>$$\\cdots$$<br>$$u_l=s_{l+1}\\cdots s_L u_L-s_{l+1}\\cdots s_{L-1} p_{L}-\\cdots-s_{l+1} p_{l+2}-p_{l+1}=u_L\\prod_{i=l+1}^L s_i-\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1} s_i$$</p>\n<p>其中，$\\prod_{i=l+1}^l s_i=1$, 类似地，<br>$$v_l=v_L \\prod_{i=l+1}^L s_i - \\sum_{j=l+1}^L(1+p_j-k_j)\\prod_{i=l+1}^{j-1} s_i$$</p>\n<h1 id=\"Relation\"><a href=\"#Relation\" class=\"headerlink\" title=\"Relation\"></a>Relation</h1><p>Receptive Field size 与 region 之间的联系，<br>$$r_l=v_l-u_l+1$$</p>\n<h1 id=\"Stride-amp-Padding\"><a href=\"#Stride-amp-Padding\" class=\"headerlink\" title=\"Stride &amp; Padding\"></a>Stride &amp; Padding</h1><p>定义两个变量，有效 stride 和 有效 padding，这两者分别定义如下：</p>\n<p>$$S_l=\\prod_{i=l+1}^L s_i$$</p>\n<p>$$P_l=\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1}s_i$$</p>\n<p>他们的递推公式为，<br>$$S_{l-1}=s_l \\cdot S_l$$<br>$$P_{l-1}=p_l+s_l \\cdot P_l$$</p>\n<p>有着这两个定义变量，region 位置公式可表示为，<br>$$u_l=u_L \\cdot S_l - P_l$$</p>\n<h1 id=\"Center\"><a href=\"#Center\" class=\"headerlink\" title=\"Center\"></a>Center</h1><p>receptive field 的中心可由 region 位置计算得到，在第 <code>l</code> layer 上为，<br>$$c_l=\\frac {u_l+v_l} 2$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Size\"><a href=\"#Size\" class=\"headerlink\" title=\"Size\"></a>Size</h1><p>对于一个 fully CNN 的网络，第 <code>k</code> layer 的 receptive field 大小为，<br>$$l_k=l_{k-1} + ((f_k-1)*\\prod_{i=1}^{k-1} s_i)$$<br>其中，$l_{k-1}$ 表示第 <code>k-1</code> layer 上的 receptive field 大小，$f_k$ 是第 <code>k</code> layer 的 filter 大小，$s_i$ 是第 <code>i</code> layer 上的 stride 大小。这是自底向上计算，从 $l_1$ 开始，$l_1=f_1$。</p>\n<p>还有一种自顶向下的计算方法。假设总共有 <code>L</code> 个 layer，每个 layer 的输出 feature map 记为 $f_l, \\ l=1,…,L$，每个 layer 的 filter 大小为 $k_l$，stride 大小记为 $s_l$，记 $r_l$ 为最后一个 layer 关于 feature map $f_l$ 的 receptive field 大小，也就是说，$r_l$ 表示 $f_l$ 上多少个像素点对 $f_L$ 的一个像素点有贡献（这里仅考虑一维 feature map，如果是多维，那么分别独立考虑即可）。那么易知，$r_L=1$，</p>\n<p>$r_{L-1}=k_L$，这个也很好理解，上一层 feature map 中，$k_L$ 个像素点对应本层（最后一层）一个像素点。考虑一般情况，已知 $r_l$，求 $r_{l-1}$。</p>\n<p>首先假设 $k_l=1$，这样情况就简单些，若 $s_l=1$，那么 $r_{l-1}=r_l$，若 $s_l&gt;1$，那么 $r_{l-1}=s_l \\cdot r_l -(s_l-1)$，因为 $r_l$ 中每两个像素点之间对应到 $f_{l-1}$ 上有 $s_l-1$ 个点，所以 $r_{l-1}=(s_l-1)\\cdot(r_l-1)+ r_l=s_l \\cdot r_l-s_l+1$。</p>\n<p>然后当 $k_l&gt;1$，那么需要在 $f_{l-1}$ 上增加 $k_l-1$ 个像素点，于是<br>$$r_{l-1}=s_l \\cdot r_l + (k_l-s_l)$$<br>其中，$r_L=1, \\ r_{L-1}=k_L$。求解上式过程如下：<br>$$r_{L-2}=s_{L-1} r_{L-1}+(k_{L-1}-s_{L-1})=s_{L-1}(k_L-1)+k_{L-1}$$<br>$$r_{L-3}=s_{L-2} r_{L-2}+(k_{L-2}-s_{L-2})=s_{L-2}s_{L-1}(k_L-1)+s_{L-2}(k_{L-1}-1)+k_{L-2}$$<br>$$\\cdots$$<br>$$r_{l}=s_{l+1}\\cdots s_{L-1}(k_L-1)+s_{l+1}\\cdots s_{L-2}(k_{L-1}-1)+ \\cdots s_{l+1}(k_{l+2}-1)+k_{l+1}=1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right]$$<br>其中令 $$\\prod_{l+1}^{l}s_i=1$$</p>\n<p>于是，<br>$$\\begin{aligned} r_{l-1}&amp;=1+\\sum_{j=l}^{L} \\left[(k_{j}-1) \\prod_{i=l}^{j-1}s_i \\right] \\ &amp;=1+(k_l-1)+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\cdot s_l \\right] \\&amp;=k_l-s_l+s_l \\left(1+\\sum_{j=l+1}^{L} \\left[(k_{j}-1) \\prod_{i=l+1}^{j-1}s_i \\right] \\right) \\&amp;=s_l \\cdot r_l +k_l-s_l\\end{aligned}$$<br>与前面递推式一致，说明通项式计算正确。</p>\n<p>output feature size 的计算为，<br>$$w_l=\\frac {w_{l-1}+2p_l-k_l} {s_l}+1$$<br>其中 $w$ 表示宽，高 $h$ 的计算类似（以 2D image 数据为例）。</p>\n<h1 id=\"Region\"><a href=\"#Region\" class=\"headerlink\" title=\"Region\"></a>Region</h1><p>对输出 feature map 上一点有贡献的 region （Receptive Field）大小计算如上，还有一个参数也很重要：定位这个 region 的位置。例如输出 feature map 上一点 $f_L(i,j)$，产生这个特征的输入图像上的 region 位置如何求得。</p>\n<p>记在特征平面 $f_l$ 上这个 region 的左端和右端的坐标分别为 $u_l, \\ v_l$，这里的<b>坐标从 0 开始</b>，即，第一个像素点的坐标为 <code>0</code>，在输出特征平面 $f_L$ 上有 $u_L=v_L=i$，同样地，仅考虑一维情况，对于二维情况，另一维度独立地进行类似计算可得。</p>\n<p>同样使用递推的思想，已知 $u_l, \\ v_l$，求 $u_{l-1}, v_{l-1}$。</p>\n<p>首先从一个简单的情况开始，假设 $u_l=0$，这表示 $f_l$ 中的 region 左侧位于第一个像素点，此时 $u_{l-1}=-p_l$，即$f_{l-1}$ 左侧填充 $p_l$ 个像素；如果 $u_l=1$，那么 $u_{l-1}=s_l-p_l$，这也很好理解，从 $f_{l-1}$ 最左侧第一个像素点（填充之后为 $-p_l$）向右移动 $s_l$；如果 $u_l=2$，那么继续向右移动 $s_l$，即 $u_{l-1}=2s_l-p_l$，于是一般地，<br>$$u_{l-1}=u_l \\cdot s_l -p_l$$<br>$$v_{l-1}=v_l \\cdot s_l - p_l + k_l-1$$<br>完全式的计算过程如下：<br>$$u_{L-1}=u_L \\cdot s_L - p_L$$<br>$$u_{L-2}=u_{L-1} \\cdot s_{L-1}-p_{L-1}=s_{L-1}s_L u_L-s_{L-1}p_L-p_{L-1}$$<br>$$u_{L-3}=u_{L-2} \\cdot s_{L-2}-p_{L-2}=s_{L-2}s_{L-1}s_L u_L-s_{L-2}s_{L-1}p_L-s_{L-2}p_{L-1}-p_{L-2}$$<br>$$\\cdots$$<br>$$u_l=s_{l+1}\\cdots s_L u_L-s_{l+1}\\cdots s_{L-1} p_{L}-\\cdots-s_{l+1} p_{l+2}-p_{l+1}=u_L\\prod_{i=l+1}^L s_i-\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1} s_i$$</p>\n<p>其中，$\\prod_{i=l+1}^l s_i=1$, 类似地，<br>$$v_l=v_L \\prod_{i=l+1}^L s_i - \\sum_{j=l+1}^L(1+p_j-k_j)\\prod_{i=l+1}^{j-1} s_i$$</p>\n<h1 id=\"Relation\"><a href=\"#Relation\" class=\"headerlink\" title=\"Relation\"></a>Relation</h1><p>Receptive Field size 与 region 之间的联系，<br>$$r_l=v_l-u_l+1$$</p>\n<h1 id=\"Stride-amp-Padding\"><a href=\"#Stride-amp-Padding\" class=\"headerlink\" title=\"Stride &amp; Padding\"></a>Stride &amp; Padding</h1><p>定义两个变量，有效 stride 和 有效 padding，这两者分别定义如下：</p>\n<p>$$S_l=\\prod_{i=l+1}^L s_i$$</p>\n<p>$$P_l=\\sum_{j=l+1}^L p_j \\prod_{i=l+1}^{j-1}s_i$$</p>\n<p>他们的递推公式为，<br>$$S_{l-1}=s_l \\cdot S_l$$<br>$$P_{l-1}=p_l+s_l \\cdot P_l$$</p>\n<p>有着这两个定义变量，region 位置公式可表示为，<br>$$u_l=u_L \\cdot S_l - P_l$$</p>\n<h1 id=\"Center\"><a href=\"#Center\" class=\"headerlink\" title=\"Center\"></a>Center</h1><p>receptive field 的中心可由 region 位置计算得到，在第 <code>l</code> layer 上为，<br>$$c_l=\\frac {u_l+v_l} 2$$</p>\n"},{"title":"Faster RCNN","date":"2021-02-18T02:19:31.000Z","p":"obj_det/faster_rcnn","_content":"\n","source":"_posts/obj_det/faster_rcnn.md","raw":"---\ntitle: Faster RCNN\ndate: 2021-02-18 10:19:31\np: obj_det/faster_rcnn\ntags: object detection\n---\n\n","slug":"obj_det/faster_rcnn","published":1,"updated":"2021-02-18T02:20:34.871Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwf0002tsdj2flzhxje","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Hexo Sync","date":"2019-06-13T01:57:11.000Z","p":"tools/Hexo-Sync","_content":"\n场景：\n```\n在A, B两台电脑上同步Hexo博客\n```\n<!-- more -->\n假设在 computer B 上已经初次建立hexo博客 https://shajian.github.io， computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\n在github仓库 shajian.github.io 上新建branch，比如\"hexo\"，这样，\"mater\"主分支用于维护hexo生成的静态博客文件/目录，\"hexo\"分支用于维护hexo部署环境下的所有文件/目录。\n\n在 computer A 上 clone 这个仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n然后可在使用\n```\nhexo new \"<title>\"\n```\n写新文章或直接去source/_posts下修改已有文章，\n部署\n```\nhexo g -d\n```\n然后提交到仓库的hexo分支，进行备份\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n然后就可以去 https://shajian.github.io 浏览本地新增/修改文章内容了。\n\n在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行\n```\n$ npm install\n```\n此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。\n\ncomputer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新\n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n切换到 hexo 分支后，可以进行修改和新增文章了。\n\n由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行\n```\nhexo g -d\n```\n由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。\n\n## hexo 新建自定义路径的文章\n有时候为了方便管理文章，需要将文章归入不同的子目录下。例如，\n```\nhexo new -p pytorch/optim_Adadelta \"PyTorch.optim.Adadelta\"\n```\n表示新建文章，标题为 `PyTorch.optim.Adadelta`，文章所在文件位于 `source/_posts/pytorch/optim_Adadelta.md`。","source":"_posts/tools/Hexo-Sync.md","raw":"---\ntitle: Hexo Sync\ndate: 2019-06-13 9:57:11\np: tools/Hexo-Sync\ntags: tool\n---\n\n场景：\n```\n在A, B两台电脑上同步Hexo博客\n```\n<!-- more -->\n假设在 computer B 上已经初次建立hexo博客 https://shajian.github.io， computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\n在github仓库 shajian.github.io 上新建branch，比如\"hexo\"，这样，\"mater\"主分支用于维护hexo生成的静态博客文件/目录，\"hexo\"分支用于维护hexo部署环境下的所有文件/目录。\n\n在 computer A 上 clone 这个仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n然后可在使用\n```\nhexo new \"<title>\"\n```\n写新文章或直接去source/_posts下修改已有文章，\n部署\n```\nhexo g -d\n```\n然后提交到仓库的hexo分支，进行备份\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n然后就可以去 https://shajian.github.io 浏览本地新增/修改文章内容了。\n\n在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行\n```\n$ npm install\n```\n此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。\n\ncomputer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新\n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n切换到 hexo 分支后，可以进行修改和新增文章了。\n\n由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行\n```\nhexo g -d\n```\n由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。\n\n## hexo 新建自定义路径的文章\n有时候为了方便管理文章，需要将文章归入不同的子目录下。例如，\n```\nhexo new -p pytorch/optim_Adadelta \"PyTorch.optim.Adadelta\"\n```\n表示新建文章，标题为 `PyTorch.optim.Adadelta`，文章所在文件位于 `source/_posts/pytorch/optim_Adadelta.md`。","slug":"tools/Hexo-Sync","published":1,"updated":"2021-02-18T02:20:05.795Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwn0004tsdjbux43r55","content":"<p>场景：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在A, B两台电脑上同步Hexo博客</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<p>假设在 computer B 上已经初次建立hexo博客 <a href=\"https://shajian.github.io，\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io，</a> computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n<p>在github仓库 shajian.github.io 上新建branch，比如”hexo”，这样，”mater”主分支用于维护hexo生成的静态博客文件/目录，”hexo”分支用于维护hexo部署环境下的所有文件/目录。</p>\n<p>在 computer A 上 clone 这个仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https:&#x2F;&#x2F;github&#x2F;shajian&#x2F;shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n<p>将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path&#x2F;to&#x2F;myblog&#x2F;* .&#x2F;&quot; which ignores hidden files&#x2F;directories</span><br><span class=\"line\">$ cp -R path&#x2F;to&#x2F;myblog&#x2F;. .&#x2F;</span><br></pre></td></tr></table></figure>\n<p>然后可在使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n<p>写新文章或直接去source/_posts下修改已有文章，<br>部署</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<p>然后提交到仓库的hexo分支，进行备份</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &#39;title&#39;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>然后就可以去 <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> 浏览本地新增/修改文章内容了。</p>\n<p>在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https:&#x2F;&#x2F;github&#x2F;shajian&#x2F;shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n<p>如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n<p>此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。</p>\n<p>computer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n<p>切换到 hexo 分支后，可以进行修改和新增文章了。</p>\n<p>由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<p>由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。</p>\n<h2 id=\"hexo-新建自定义路径的文章\"><a href=\"#hexo-新建自定义路径的文章\" class=\"headerlink\" title=\"hexo 新建自定义路径的文章\"></a>hexo 新建自定义路径的文章</h2><p>有时候为了方便管理文章，需要将文章归入不同的子目录下。例如，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new -p pytorch&#x2F;optim_Adadelta &quot;PyTorch.optim.Adadelta&quot;</span><br></pre></td></tr></table></figure>\n<p>表示新建文章，标题为 <code>PyTorch.optim.Adadelta</code>，文章所在文件位于 <code>source/_posts/pytorch/optim_Adadelta.md</code>。</p>\n","site":{"data":{}},"excerpt":"<p>场景：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在A, B两台电脑上同步Hexo博客</span><br></pre></td></tr></table></figure>","more":"<p>假设在 computer B 上已经初次建立hexo博客 <a href=\"https://shajian.github.io，\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io，</a> computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n<p>在github仓库 shajian.github.io 上新建branch，比如”hexo”，这样，”mater”主分支用于维护hexo生成的静态博客文件/目录，”hexo”分支用于维护hexo部署环境下的所有文件/目录。</p>\n<p>在 computer A 上 clone 这个仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https:&#x2F;&#x2F;github&#x2F;shajian&#x2F;shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n<p>将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path&#x2F;to&#x2F;myblog&#x2F;* .&#x2F;&quot; which ignores hidden files&#x2F;directories</span><br><span class=\"line\">$ cp -R path&#x2F;to&#x2F;myblog&#x2F;. .&#x2F;</span><br></pre></td></tr></table></figure>\n<p>然后可在使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n<p>写新文章或直接去source/_posts下修改已有文章，<br>部署</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<p>然后提交到仓库的hexo分支，进行备份</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &#39;title&#39;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>然后就可以去 <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> 浏览本地新增/修改文章内容了。</p>\n<p>在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https:&#x2F;&#x2F;github&#x2F;shajian&#x2F;shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n<p>如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n<p>此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。</p>\n<p>computer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n<p>切换到 hexo 分支后，可以进行修改和新增文章了。</p>\n<p>由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<p>由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。</p>\n<h2 id=\"hexo-新建自定义路径的文章\"><a href=\"#hexo-新建自定义路径的文章\" class=\"headerlink\" title=\"hexo 新建自定义路径的文章\"></a>hexo 新建自定义路径的文章</h2><p>有时候为了方便管理文章，需要将文章归入不同的子目录下。例如，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new -p pytorch&#x2F;optim_Adadelta &quot;PyTorch.optim.Adadelta&quot;</span><br></pre></td></tr></table></figure>\n<p>表示新建文章，标题为 <code>PyTorch.optim.Adadelta</code>，文章所在文件位于 <code>source/_posts/pytorch/optim_Adadelta.md</code>。</p>"},{"title":"ubuntu zsh 使用","date":"2021-01-23T10:27:51.000Z","_content":"安装 zsh\n```\nsudo apt-get install zsh\n```\n查看版本，验证是否安装成功\n```\nzsh --version\n```\n更改默认 shell 为 zsh\n```\nchsh -s /bin/zsh\n```\n查看 shell\n```\necho $SHELL\n```\n如果失败，那么重启 shell 即可。\n\n\n安装 oh-my-zsh，这是一个 zsh 配置框架。\n```\nwget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh\n```\n修改文件权限，\n```\nchmod +x install.sh\n```\n 执行安装，\n ```\n ./install.sh\n ```\n\n安装 powerlevel10k 这个主题，\n```\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n```\n\n设置主题，将以下内容添加到 `~/.zshrc`，\n```\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\n```","source":"_posts/tools/shell.md","raw":"---\ntitle: ubuntu zsh 使用\ndate: 2021-01-23 18:27:51\ntags:\n---\n安装 zsh\n```\nsudo apt-get install zsh\n```\n查看版本，验证是否安装成功\n```\nzsh --version\n```\n更改默认 shell 为 zsh\n```\nchsh -s /bin/zsh\n```\n查看 shell\n```\necho $SHELL\n```\n如果失败，那么重启 shell 即可。\n\n\n安装 oh-my-zsh，这是一个 zsh 配置框架。\n```\nwget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh\n```\n修改文件权限，\n```\nchmod +x install.sh\n```\n 执行安装，\n ```\n ./install.sh\n ```\n\n安装 powerlevel10k 这个主题，\n```\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n```\n\n设置主题，将以下内容添加到 `~/.zshrc`，\n```\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\n```","slug":"tools/shell","published":1,"updated":"2021-01-23T10:40:35.878Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwp0006tsdj1xpm6qex","content":"<p>安装 zsh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zsh</span><br></pre></td></tr></table></figure>\n<p>查看版本，验证是否安装成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zsh --version</span><br></pre></td></tr></table></figure>\n<p>更改默认 shell 为 zsh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chsh -s &#x2F;bin&#x2F;zsh</span><br></pre></td></tr></table></figure>\n<p>查看 shell</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo $SHELL</span><br></pre></td></tr></table></figure>\n<p>如果失败，那么重启 shell 即可。</p>\n<p>安装 oh-my-zsh，这是一个 zsh 配置框架。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https:&#x2F;&#x2F;raw.github.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;master&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure>\n<p>修改文件权限，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chmod +x install.sh</span><br></pre></td></tr></table></figure>\n<p> 执行安装，<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;install.sh</span><br></pre></td></tr></table></figure></p>\n<p>安装 powerlevel10k 这个主题，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure>\n\n<p>设置主题，将以下内容添加到 <code>~/.zshrc</code>，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZSH_THEME&#x3D;&quot;powerlevel10k&#x2F;powerlevel10k&quot;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>安装 zsh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zsh</span><br></pre></td></tr></table></figure>\n<p>查看版本，验证是否安装成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zsh --version</span><br></pre></td></tr></table></figure>\n<p>更改默认 shell 为 zsh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chsh -s &#x2F;bin&#x2F;zsh</span><br></pre></td></tr></table></figure>\n<p>查看 shell</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo $SHELL</span><br></pre></td></tr></table></figure>\n<p>如果失败，那么重启 shell 即可。</p>\n<p>安装 oh-my-zsh，这是一个 zsh 配置框架。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https:&#x2F;&#x2F;raw.github.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;master&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure>\n<p>修改文件权限，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chmod +x install.sh</span><br></pre></td></tr></table></figure>\n<p> 执行安装，<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;install.sh</span><br></pre></td></tr></table></figure></p>\n<p>安装 powerlevel10k 这个主题，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure>\n\n<p>设置主题，将以下内容添加到 <code>~/.zshrc</code>，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZSH_THEME&#x3D;&quot;powerlevel10k&#x2F;powerlevel10k&quot;</span><br></pre></td></tr></table></figure>"},{"title":"Dilated Convolution","date":"2021-02-19T01:27:01.000Z","mathjax":true,"_content":"膨胀卷积在卷积核中引入 空洞（holes），将卷积核变大，记膨胀率为 $\\alpha$，卷积核大小为 $k$，那么膨胀后卷积核大小变为 $\\alpha(k-1)+1$，使用膨胀后的卷积核来做卷积计算。\n\n膨胀卷积在图像（实例）分割中应用较多，为了扩大感知区域，同时减少计算量，膨胀卷积效果较好。\n\nDilated Convolution 的设计是为了获取 long-range information，故对大物体比较适用，对小物体则不太适用。Dilated Convolution 一个明显的缺点是 kernel 不连续，产生栅格效应，所以又提出了 Hybrid Dilated Convolution（HDC）混合膨胀卷积。\n\nHDC 的一般设计原则：\n1. 各膨胀卷积的膨胀率不能有大于 1 的公约数（例如 [2,4,6] 公约数为 2），否则会有栅格效应\n2. 膨胀率设计为锯齿状结构，例如 [1,2,5,1,2,5] 这样的循环结构\n3. 膨胀率满足如下关系\n$$M_i=\\max[M_{i+1}-2r_i, 2r_i-M_{i+1}, r_i]$$\n其中 $r_i$ 为第 `i` 层的膨胀率，$M_i$ 为第 `i` 层的最大 dilated rate，网络总共 `L` 层，$M_L=r_L$。\n\n\n","source":"_posts/dl/dilated_conv.md","raw":"---\ntitle: Dilated Convolution\ndate: 2021-02-19 09:27:01\ntags: CNN, Deep Learning\nmathjax: true\n---\n膨胀卷积在卷积核中引入 空洞（holes），将卷积核变大，记膨胀率为 $\\alpha$，卷积核大小为 $k$，那么膨胀后卷积核大小变为 $\\alpha(k-1)+1$，使用膨胀后的卷积核来做卷积计算。\n\n膨胀卷积在图像（实例）分割中应用较多，为了扩大感知区域，同时减少计算量，膨胀卷积效果较好。\n\nDilated Convolution 的设计是为了获取 long-range information，故对大物体比较适用，对小物体则不太适用。Dilated Convolution 一个明显的缺点是 kernel 不连续，产生栅格效应，所以又提出了 Hybrid Dilated Convolution（HDC）混合膨胀卷积。\n\nHDC 的一般设计原则：\n1. 各膨胀卷积的膨胀率不能有大于 1 的公约数（例如 [2,4,6] 公约数为 2），否则会有栅格效应\n2. 膨胀率设计为锯齿状结构，例如 [1,2,5,1,2,5] 这样的循环结构\n3. 膨胀率满足如下关系\n$$M_i=\\max[M_{i+1}-2r_i, 2r_i-M_{i+1}, r_i]$$\n其中 $r_i$ 为第 `i` 层的膨胀率，$M_i$ 为第 `i` 层的最大 dilated rate，网络总共 `L` 层，$M_L=r_L$。\n\n\n","slug":"dl/dilated_conv","published":1,"updated":"2021-02-19T02:18:49.892Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwp0008tsdja5m2cahk","content":"<p>膨胀卷积在卷积核中引入 空洞（holes），将卷积核变大，记膨胀率为 $\\alpha$，卷积核大小为 $k$，那么膨胀后卷积核大小变为 $\\alpha(k-1)+1$，使用膨胀后的卷积核来做卷积计算。</p>\n<p>膨胀卷积在图像（实例）分割中应用较多，为了扩大感知区域，同时减少计算量，膨胀卷积效果较好。</p>\n<p>Dilated Convolution 的设计是为了获取 long-range information，故对大物体比较适用，对小物体则不太适用。Dilated Convolution 一个明显的缺点是 kernel 不连续，产生栅格效应，所以又提出了 Hybrid Dilated Convolution（HDC）混合膨胀卷积。</p>\n<p>HDC 的一般设计原则：</p>\n<ol>\n<li>各膨胀卷积的膨胀率不能有大于 1 的公约数（例如 [2,4,6] 公约数为 2），否则会有栅格效应</li>\n<li>膨胀率设计为锯齿状结构，例如 [1,2,5,1,2,5] 这样的循环结构</li>\n<li>膨胀率满足如下关系<br>$$M_i=\\max[M_{i+1}-2r_i, 2r_i-M_{i+1}, r_i]$$<br>其中 $r_i$ 为第 <code>i</code> 层的膨胀率，$M_i$ 为第 <code>i</code> 层的最大 dilated rate，网络总共 <code>L</code> 层，$M_L=r_L$。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>膨胀卷积在卷积核中引入 空洞（holes），将卷积核变大，记膨胀率为 $\\alpha$，卷积核大小为 $k$，那么膨胀后卷积核大小变为 $\\alpha(k-1)+1$，使用膨胀后的卷积核来做卷积计算。</p>\n<p>膨胀卷积在图像（实例）分割中应用较多，为了扩大感知区域，同时减少计算量，膨胀卷积效果较好。</p>\n<p>Dilated Convolution 的设计是为了获取 long-range information，故对大物体比较适用，对小物体则不太适用。Dilated Convolution 一个明显的缺点是 kernel 不连续，产生栅格效应，所以又提出了 Hybrid Dilated Convolution（HDC）混合膨胀卷积。</p>\n<p>HDC 的一般设计原则：</p>\n<ol>\n<li>各膨胀卷积的膨胀率不能有大于 1 的公约数（例如 [2,4,6] 公约数为 2），否则会有栅格效应</li>\n<li>膨胀率设计为锯齿状结构，例如 [1,2,5,1,2,5] 这样的循环结构</li>\n<li>膨胀率满足如下关系<br>$$M_i=\\max[M_{i+1}-2r_i, 2r_i-M_{i+1}, r_i]$$<br>其中 $r_i$ 为第 <code>i</code> 层的膨胀率，$M_i$ 为第 <code>i</code> 层的最大 dilated rate，网络总共 <code>L</code> 层，$M_L=r_L$。</li>\n</ol>\n"},{"title":"img_cls/resnet.md","p":"img_cls/resnet","date":"2021-01-19T09:25:25.000Z","mathjax":true,"_content":"本篇罗列一些 Resnet 网络的细节。\n<!-- more -->\n\n# ImageNet\n## 数据增强\n1. scale 增强：输入图像的短边（w,h 较小者）被 resize 到 `[256,418]` 中的一个随机数。\n2. 在 resized 的图像或其水平翻转图像上随机位置（随机 center） crop 出一个 `224x224` 大小的部分。\n3. 10-crop testing：测试阶段，对每个测试图像，crop 出上下左右四个角以及center处共 5 个 patch，以及水平翻转后同样的 5 个 patch，这 10 个 patch 经过网络之后的预测结果（softmax 层的输出）再进行平均，即得到这个图像的最终预测，预测为长度等于类别数量的向量。\n\n## 网络结构\n1. baseline 使用 VGG，记为 plain，每个 conv 层与 activation 层之间均插入一个 BatchNorm 层。\n2. 网络结构如下图，其中 conv3_1,  conv4_1, conv5_1 进行了下采样（stride=2）\n![](/images/img_cls/resnet_1.png)<center>图 1. 网络结构</center>\n3. 每个 conv block 均额外增加一个 shortcut，两个 output 在通道维度上进行 concatenate。由于 conv3_1,  conv4_1, conv5_1 进行了下采样，output 的 (H,W) 分别减小到一半，而通道 (C) 增大一倍，故需要将对应的 shortcut 作调整，有两种方法：a. identity mapping（注意 stride=2，以降低 H 和 W），额外维度上 0 填充；b. `1x1` conv，stride=2 的下采样，且输出 channel 增大一倍。\n4. 在 ResNet-18 和 ResNet-34 上，使用 identity mapping（以及 zero-padding 维度填充）。\n5. conv3_1,  conv4_1, conv5_1 进行了下采样，具体而言，对 ResNet-18 和 ResNet-34，conv block 是由两个 `3x3` conv layer 组成，所以是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的第一个 conv layer 执行了下采样；对于 ResNet-50，ResNet-101 以及 ResNet-152，conv block 是由三个 conv layer 组成的（也叫 bottleneck），这三个conv 按顺序分别是 `1x1`, `3x3`, `1x1`， 所以其实是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的 `3x3` 执行了下采样操作。\n6. 整个网络的下采样率为 `2x2x2x2x2=32`，输入为 `224x224`，那么最后的 conv 输出 feature 大小为 `7x7`。对每个 feature map 进行全局均值池化 GAP，那么每个 channel 得到一个值，然后经 full connection 层，得到 1000 个分类得分，最后由 softmax 将其归一化。\n7. ResNet-x，后缀 `x` 表示有参数的 layer 数量。\n8. shortcut 与 residual function 按 channel 维度连接之后再经过 relu 层。\n9. 如果 shortcut 需要下采样，那么令 stride=2 即可。\n## 实验说明\n### plain vs. resnet\n在 18 layers 和 34 layers 两个网络上，对比 plain 和 ResNet，top-1 错误率如下表\n\n| | plain | ResNet|\n|:--------:       | :------:    |   :-------:          |\n|18 layers| 27.94 | 27.88|\n| 34 layers| 28.54| <b> 25.03 </b>|\n <center>表 1. plain 与 Resnet 结果比较。ResNet 采用 identity shortcut。ImageNet 验证集上的错误率（10-crop testing）</center>\n说明：\n\n- plain，随着网络加深，准确率下降。\n- ResNet 采用 identity mapping （zero padding 增加channel）作为 shortcut。ResNet-34 比 ResNet-18 好，这表明加深网络导致恶化的问题可通过 ResNet 得到解决，且能获得更高的准确率。\n- ResNet-18 比 plain-18 收敛更快。\n\n### Projection Shortcut\n使用 `1x1` conv 代替 identity mapping，输出如下，其中第一项为 conv block 的输出，第二项为 shortcut 的输出，\n\n$$\\mathbf y = \\mathcal F(\\mathbf x, \\{W_i\\}) + W_s \\mathbf x $$\n以 34 layers 网络为例，对比如下三种情况：\n\n* A：所有 shortcut 均没有参数，即 identity mapping，以及 zero-padding 以增加维度。与上述 ResNet-34 一样。\n* B：除了需要增加维度的时候（conv3_1, conv4_1, conv5_1）使用 projection shortcut，其他 shortcut 采用 identity mapping。此时无需 zero-padding。\n* C：所有的 shortcuts 均采用 projection shortcut。此时无需 zero-padding。\n\n实验结果如下：\n|model|top-1 err | top-5 err|\n|:--|:--|:--|\n|plain-34| 28.54| 10.02|\n|ResNet-34 A|25.03|7.76|\n|ResNet-34 B|24.52|7.46|\n|ResNet-34 C|24.19|7.4|\n|ResNet-50|22.85|6.71|\n|ResNet-101|21.75|6.05|\n|ResNet-50|<b>21.43</b>|<b>5.71</b>|\n<center>表 2. projection shortcut 方案比较，以及 ResNet 不同深度的比较。<font color='clan'>ResNet-50/101/152 使用方案 B</font>。ImageNet 验证集上的错误率（10-crop testing）</center>\n\n结果分析：\n- 错误率：三种情况均比 plain 有所降低。B 比 A 有所降低，C 比 B 轻微降低。由于 A 中没有 projection shortcut，这表明对于解决准确率恶化的问题，projection shortcut 不是本质性地重要；另一方面，C 中由于引入过多的参数，内存占用和计算时间均增大，故综合起来，使用 B 方案。\n\n### Bottleneck 结构\n图 1 中，每个 conv block 是由两层 conv 组成，把它改成 bottleneck 结构，即三层 conv：`1x1`, `3x3`, `1x1`，第一个 `1x1` 用于降低 channel（维度）（降为 1/4），(H,W) 不变，`3x3` 执行了下采样，(H,W) 均降为一半，channel 不变，最后一个 `1x1` 用于增加 channel，增大到 4 倍，这三个 conv layer 形成一个 bottleneck 结构，如下图右侧，\n![](/images/img_cls/resnet_2.png)<center>图 2. ImageNet 上使用的深度残差函数。左：与图 1 中相同的conv 块结构；右：bottleneck 结构，用在 ResNet-50/101/152 中</center>\n\n使用 Bottleneck 时，identity shortcut 显得尤其重要，如果使用 projection，model 大小和时间复杂度都会大大增加，因为此时 shortcut 连接的两端都是高维（即，输入输出的 channel 都增大到原来 4 倍），这使得 shortcut 上的参数量大大增加。\n\n### ResNet-50/101/152\n将 ResNet-34 中的所有 conv block 替换为 bottleneck 结构，就得到 ResNet-50，如图 1，ResNet-34 有 `3+4+6+3=16` 个 conv block，增加的 layer 数量也就是 16。ResNet-50 使用方案 B。\n\n在 ResNet-50 基础上增加更多的 bottleneck 块，得到 ResNet-101 和 ResNet152。\n\nResNet-50/101/152 比 ResNet-34 改善较多。实验结果如表 2，可见，使用 ResNet 结构可享受深度带来的益处。\n\n除了单模型，作者还对比了各种模型的集成结果，这个集成结果我也不知道是怎么计算的，也许只是几种不同深度的 ResNet 网络的结果平均，这个<font color=\"red\">有待考证</font>。\n\n再在其他数据集上研究 ResNet 的表现。\n\n## 训练\n\n1. 使用 SGD 方法学习\n2. mini-batch=256\n3. 学习率初始为 `0.1`，当错误率进入平稳期时，以  `1/10` 的倍率降低学习率。\n4. 权重衰减因子为 `0.0001`，momentum=0.9\n5. 由于使用了 BatchNorm，所以不使用 Dropout。\n\n# CIFAR-10\n作者这里的目的是为了研究非常深的网络，而非为了 SOTA 的性能，所以仅使用了简单的框架，以 ResNet-34 和对应的 plain 网络为主体框架进行改造，这是因为输入 size 为 `32x32`，这比 ImageNet 数据集的输入 size `224x224` 小很多。\n## 网络结构\n\n第一个 layer 的 kernel 需要调小（ImageNet 数据集上使用的是 `7x7`，太大），使用的是 `3x3` 的 conv，stride=1，无下采样（输出 channel 增大到 16），接着，使用 6n（6 的整数倍）个 conv layer，每 2n 个 conv layer 划分为一组，共 3 组，每组的输出 feature size 分别为 `(32, 16, 8)`，channel （也就是卷积 filter 的数量）分别为 `(16, 32, 64)`。最后使用全局均值池化 GAP + full connection + softmax。有参数的 layer 一共 `6n+2` 个。\n> 第二组和第三组各做了一次 stride=2 的下采样。论文里面没有明说，但我认为可以使用第二、三组中各自第一个 conv layer 来做下采样，与图 1 中一致。\n\n网络结构说明如下：\n\n|输出 map size|32x32|16x16|8x8|\n|:---|:---|:---|:---|\n|#layers|2n+1|2n|2n|\n|#filters|16|32|64|\n\n\n从这个表格可见，如果采用 identity shortcut，如果不进行下采样，那么这个 identity shortcut layer 的输出与输入完全一样，如果进行 rate=2 的下采样，那么 (H,W) 各变为一半大小，而 channel 增加一倍，变为原来 channel 的两倍，residual function 分支的 channel 也是原来输入 channel 的两倍，两个分支的输出 channel 和 (H,W) 均分别保持相同，才能进行 element-wise 的 add 操作。\n\n6n 个 conv layer，每两个 conv layer 使用一个 shortcut，共 3n 个 shortcut，作者使用 identity shortcut。\n\n## 训练说明\n\n- 与 ImageNet 上的训练类似，区别是 mini-batch=128，学习率初始为 `0.1`，在第 32k 次和第 48k 次迭代时，学习率降为 `1/10`，训练共迭代 64k 次。\n- CIFAR-10 训练集大小为 50k，测试集大小为 10k，类别数量为 10。将训练集按 45k/5k 分割为 train/val 集。\n- 原图像采用 4 pixel 的填充，然后以 0.5 的概率水平翻转，再随机 crop 一个 32x32 的 patch，作为网络输入。\n\n## 实验结果\n当 `n={3,5,7,9}` 时，分别得到 `20, 32, 44, 56` 个有参数的 layer 的网络。实验结果如图 3，\n![](/images/img_cls/resnet_3.png)<center>图 3. CIFAR-10 训练结果。细线表示训练错误率，粗线表示测试错误率。左图中 plain-110 的错误率大于 60%，没有在图中显示。</center>\n\n从图 3 中可见，ResNet 可以解决网络深度增加带来的准确率恶化问题。n=18 得到 ResNet-110，此时初始学习率 0.1 过大，导致开始训练时一直难以收敛，故初始化学习率为 0.01，当训练错误率降低到 80% 以下时（大约 400 次迭代），将学习率重置为 0.1 以加快收敛速度继续训练，之后就与训练 ImageNet 一样（阶梯降低学习率）。\n\n训练结果：\n\nResNet-110 表现最好，比其他 deep & thin 的网络有更少的参数和更好的性能，对比如下表，\n||#layers|#params(M)|error(%)|\n|:---|:---|:---|:---|\n|FitNet|19|2.5|8.39|\n|Highway|19|2.3|7.54(7.72 ± 0.16)|\n|Highway|32|1.25|8.8|\n|ResNet|20|0.27|8.75|\n|ResNet|32|0.46|7.51|\n|ResNet|44|0.66|7.17|\n|ResNet|56|0.85|6.97|\n|ResNet|110|1.7|<b>6.43</b>(6.61±0.16)|\n|ResNet|1202|19.4|7.93|\n<center>CIFAR-10 测试集上的分类错误率。其中，与 Highway 中类似，对 ResNet-110 试验了 5 次，得到最佳结果 6.43% 的错误率，均值为 6.61%，标准差为0.16</center>\n\n## Layer 响应分析\n图 4 是 CIFAR-10 上网络（训练好之后）中各个 `3x3` layer 响应的标准差，响应指 BN 之后，activate 之前的 layer 响应。\n![](/images/img_cls/resnet_4.png)<center>图 4. 网络（3x3）各层的响应标准差。上：网络原始各层先后顺序；下：按标准差倒序排列。</center>\n\n从图 4中可见，ResNet 有着比 plain 更小的响应标准差，所以 ResNet 中各层的输出更加集中在 0 附近，避免了梯度消失（现在都使用 relu 而非 sigmoid 激活，避免梯度消失这一说还有意义吗？）。同时注意到，更深的 ResNet 的响应幅度更小，这表明当 layer 数量越多时，均摊到单个 layer 上，其对信号的改变越小。\n\n## 超深网络 layers>1000\nn=200，此时网络层数 6n+2=1202。训练结果（错误率）如上表 和图 3 中所示，训练集错误率 `<0.1%`，测试集错误率 `7.93%`，差强人意，但是比起 ResNet-110，虽然训练错误率差不多，但是测试错误率已经上升，作者认为这是过拟合导致，此时网络太大，而数据集太小，可以使用 `maxout`，`dropout` 等手段来改善过拟合问题，但是作者自己并没有这么做，而只是在设计网络架构时，遵循 deep & thin 的原则，简单地将网络正则化，这是因为本论文的重点在于解决（deep 网络的）优化困难，而非网络正则\n> deep: 网络层数多；thin: 每层的操作少，例如常见的一层为 conv-bn-relu；wide: 每层的 feature size 较大。\n\n\n# 目标检测\n检测采用 Faster R-CNN，将 baseline 从 VGG-16 替换为 ResNet-101。\n\n## 训练方法\n1. 在 ImageNet 上训练，然后再在目标检测数据上对网络进行 fine-tune。","source":"_posts/img_cls/resnet.md","raw":"---\ntitle: img_cls/resnet.md\np: img_cls/resnet\ndate: 2021-01-19 17:25:25\ntags: img cls\nmathjax: true\n---\n本篇罗列一些 Resnet 网络的细节。\n<!-- more -->\n\n# ImageNet\n## 数据增强\n1. scale 增强：输入图像的短边（w,h 较小者）被 resize 到 `[256,418]` 中的一个随机数。\n2. 在 resized 的图像或其水平翻转图像上随机位置（随机 center） crop 出一个 `224x224` 大小的部分。\n3. 10-crop testing：测试阶段，对每个测试图像，crop 出上下左右四个角以及center处共 5 个 patch，以及水平翻转后同样的 5 个 patch，这 10 个 patch 经过网络之后的预测结果（softmax 层的输出）再进行平均，即得到这个图像的最终预测，预测为长度等于类别数量的向量。\n\n## 网络结构\n1. baseline 使用 VGG，记为 plain，每个 conv 层与 activation 层之间均插入一个 BatchNorm 层。\n2. 网络结构如下图，其中 conv3_1,  conv4_1, conv5_1 进行了下采样（stride=2）\n![](/images/img_cls/resnet_1.png)<center>图 1. 网络结构</center>\n3. 每个 conv block 均额外增加一个 shortcut，两个 output 在通道维度上进行 concatenate。由于 conv3_1,  conv4_1, conv5_1 进行了下采样，output 的 (H,W) 分别减小到一半，而通道 (C) 增大一倍，故需要将对应的 shortcut 作调整，有两种方法：a. identity mapping（注意 stride=2，以降低 H 和 W），额外维度上 0 填充；b. `1x1` conv，stride=2 的下采样，且输出 channel 增大一倍。\n4. 在 ResNet-18 和 ResNet-34 上，使用 identity mapping（以及 zero-padding 维度填充）。\n5. conv3_1,  conv4_1, conv5_1 进行了下采样，具体而言，对 ResNet-18 和 ResNet-34，conv block 是由两个 `3x3` conv layer 组成，所以是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的第一个 conv layer 执行了下采样；对于 ResNet-50，ResNet-101 以及 ResNet-152，conv block 是由三个 conv layer 组成的（也叫 bottleneck），这三个conv 按顺序分别是 `1x1`, `3x3`, `1x1`， 所以其实是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的 `3x3` 执行了下采样操作。\n6. 整个网络的下采样率为 `2x2x2x2x2=32`，输入为 `224x224`，那么最后的 conv 输出 feature 大小为 `7x7`。对每个 feature map 进行全局均值池化 GAP，那么每个 channel 得到一个值，然后经 full connection 层，得到 1000 个分类得分，最后由 softmax 将其归一化。\n7. ResNet-x，后缀 `x` 表示有参数的 layer 数量。\n8. shortcut 与 residual function 按 channel 维度连接之后再经过 relu 层。\n9. 如果 shortcut 需要下采样，那么令 stride=2 即可。\n## 实验说明\n### plain vs. resnet\n在 18 layers 和 34 layers 两个网络上，对比 plain 和 ResNet，top-1 错误率如下表\n\n| | plain | ResNet|\n|:--------:       | :------:    |   :-------:          |\n|18 layers| 27.94 | 27.88|\n| 34 layers| 28.54| <b> 25.03 </b>|\n <center>表 1. plain 与 Resnet 结果比较。ResNet 采用 identity shortcut。ImageNet 验证集上的错误率（10-crop testing）</center>\n说明：\n\n- plain，随着网络加深，准确率下降。\n- ResNet 采用 identity mapping （zero padding 增加channel）作为 shortcut。ResNet-34 比 ResNet-18 好，这表明加深网络导致恶化的问题可通过 ResNet 得到解决，且能获得更高的准确率。\n- ResNet-18 比 plain-18 收敛更快。\n\n### Projection Shortcut\n使用 `1x1` conv 代替 identity mapping，输出如下，其中第一项为 conv block 的输出，第二项为 shortcut 的输出，\n\n$$\\mathbf y = \\mathcal F(\\mathbf x, \\{W_i\\}) + W_s \\mathbf x $$\n以 34 layers 网络为例，对比如下三种情况：\n\n* A：所有 shortcut 均没有参数，即 identity mapping，以及 zero-padding 以增加维度。与上述 ResNet-34 一样。\n* B：除了需要增加维度的时候（conv3_1, conv4_1, conv5_1）使用 projection shortcut，其他 shortcut 采用 identity mapping。此时无需 zero-padding。\n* C：所有的 shortcuts 均采用 projection shortcut。此时无需 zero-padding。\n\n实验结果如下：\n|model|top-1 err | top-5 err|\n|:--|:--|:--|\n|plain-34| 28.54| 10.02|\n|ResNet-34 A|25.03|7.76|\n|ResNet-34 B|24.52|7.46|\n|ResNet-34 C|24.19|7.4|\n|ResNet-50|22.85|6.71|\n|ResNet-101|21.75|6.05|\n|ResNet-50|<b>21.43</b>|<b>5.71</b>|\n<center>表 2. projection shortcut 方案比较，以及 ResNet 不同深度的比较。<font color='clan'>ResNet-50/101/152 使用方案 B</font>。ImageNet 验证集上的错误率（10-crop testing）</center>\n\n结果分析：\n- 错误率：三种情况均比 plain 有所降低。B 比 A 有所降低，C 比 B 轻微降低。由于 A 中没有 projection shortcut，这表明对于解决准确率恶化的问题，projection shortcut 不是本质性地重要；另一方面，C 中由于引入过多的参数，内存占用和计算时间均增大，故综合起来，使用 B 方案。\n\n### Bottleneck 结构\n图 1 中，每个 conv block 是由两层 conv 组成，把它改成 bottleneck 结构，即三层 conv：`1x1`, `3x3`, `1x1`，第一个 `1x1` 用于降低 channel（维度）（降为 1/4），(H,W) 不变，`3x3` 执行了下采样，(H,W) 均降为一半，channel 不变，最后一个 `1x1` 用于增加 channel，增大到 4 倍，这三个 conv layer 形成一个 bottleneck 结构，如下图右侧，\n![](/images/img_cls/resnet_2.png)<center>图 2. ImageNet 上使用的深度残差函数。左：与图 1 中相同的conv 块结构；右：bottleneck 结构，用在 ResNet-50/101/152 中</center>\n\n使用 Bottleneck 时，identity shortcut 显得尤其重要，如果使用 projection，model 大小和时间复杂度都会大大增加，因为此时 shortcut 连接的两端都是高维（即，输入输出的 channel 都增大到原来 4 倍），这使得 shortcut 上的参数量大大增加。\n\n### ResNet-50/101/152\n将 ResNet-34 中的所有 conv block 替换为 bottleneck 结构，就得到 ResNet-50，如图 1，ResNet-34 有 `3+4+6+3=16` 个 conv block，增加的 layer 数量也就是 16。ResNet-50 使用方案 B。\n\n在 ResNet-50 基础上增加更多的 bottleneck 块，得到 ResNet-101 和 ResNet152。\n\nResNet-50/101/152 比 ResNet-34 改善较多。实验结果如表 2，可见，使用 ResNet 结构可享受深度带来的益处。\n\n除了单模型，作者还对比了各种模型的集成结果，这个集成结果我也不知道是怎么计算的，也许只是几种不同深度的 ResNet 网络的结果平均，这个<font color=\"red\">有待考证</font>。\n\n再在其他数据集上研究 ResNet 的表现。\n\n## 训练\n\n1. 使用 SGD 方法学习\n2. mini-batch=256\n3. 学习率初始为 `0.1`，当错误率进入平稳期时，以  `1/10` 的倍率降低学习率。\n4. 权重衰减因子为 `0.0001`，momentum=0.9\n5. 由于使用了 BatchNorm，所以不使用 Dropout。\n\n# CIFAR-10\n作者这里的目的是为了研究非常深的网络，而非为了 SOTA 的性能，所以仅使用了简单的框架，以 ResNet-34 和对应的 plain 网络为主体框架进行改造，这是因为输入 size 为 `32x32`，这比 ImageNet 数据集的输入 size `224x224` 小很多。\n## 网络结构\n\n第一个 layer 的 kernel 需要调小（ImageNet 数据集上使用的是 `7x7`，太大），使用的是 `3x3` 的 conv，stride=1，无下采样（输出 channel 增大到 16），接着，使用 6n（6 的整数倍）个 conv layer，每 2n 个 conv layer 划分为一组，共 3 组，每组的输出 feature size 分别为 `(32, 16, 8)`，channel （也就是卷积 filter 的数量）分别为 `(16, 32, 64)`。最后使用全局均值池化 GAP + full connection + softmax。有参数的 layer 一共 `6n+2` 个。\n> 第二组和第三组各做了一次 stride=2 的下采样。论文里面没有明说，但我认为可以使用第二、三组中各自第一个 conv layer 来做下采样，与图 1 中一致。\n\n网络结构说明如下：\n\n|输出 map size|32x32|16x16|8x8|\n|:---|:---|:---|:---|\n|#layers|2n+1|2n|2n|\n|#filters|16|32|64|\n\n\n从这个表格可见，如果采用 identity shortcut，如果不进行下采样，那么这个 identity shortcut layer 的输出与输入完全一样，如果进行 rate=2 的下采样，那么 (H,W) 各变为一半大小，而 channel 增加一倍，变为原来 channel 的两倍，residual function 分支的 channel 也是原来输入 channel 的两倍，两个分支的输出 channel 和 (H,W) 均分别保持相同，才能进行 element-wise 的 add 操作。\n\n6n 个 conv layer，每两个 conv layer 使用一个 shortcut，共 3n 个 shortcut，作者使用 identity shortcut。\n\n## 训练说明\n\n- 与 ImageNet 上的训练类似，区别是 mini-batch=128，学习率初始为 `0.1`，在第 32k 次和第 48k 次迭代时，学习率降为 `1/10`，训练共迭代 64k 次。\n- CIFAR-10 训练集大小为 50k，测试集大小为 10k，类别数量为 10。将训练集按 45k/5k 分割为 train/val 集。\n- 原图像采用 4 pixel 的填充，然后以 0.5 的概率水平翻转，再随机 crop 一个 32x32 的 patch，作为网络输入。\n\n## 实验结果\n当 `n={3,5,7,9}` 时，分别得到 `20, 32, 44, 56` 个有参数的 layer 的网络。实验结果如图 3，\n![](/images/img_cls/resnet_3.png)<center>图 3. CIFAR-10 训练结果。细线表示训练错误率，粗线表示测试错误率。左图中 plain-110 的错误率大于 60%，没有在图中显示。</center>\n\n从图 3 中可见，ResNet 可以解决网络深度增加带来的准确率恶化问题。n=18 得到 ResNet-110，此时初始学习率 0.1 过大，导致开始训练时一直难以收敛，故初始化学习率为 0.01，当训练错误率降低到 80% 以下时（大约 400 次迭代），将学习率重置为 0.1 以加快收敛速度继续训练，之后就与训练 ImageNet 一样（阶梯降低学习率）。\n\n训练结果：\n\nResNet-110 表现最好，比其他 deep & thin 的网络有更少的参数和更好的性能，对比如下表，\n||#layers|#params(M)|error(%)|\n|:---|:---|:---|:---|\n|FitNet|19|2.5|8.39|\n|Highway|19|2.3|7.54(7.72 ± 0.16)|\n|Highway|32|1.25|8.8|\n|ResNet|20|0.27|8.75|\n|ResNet|32|0.46|7.51|\n|ResNet|44|0.66|7.17|\n|ResNet|56|0.85|6.97|\n|ResNet|110|1.7|<b>6.43</b>(6.61±0.16)|\n|ResNet|1202|19.4|7.93|\n<center>CIFAR-10 测试集上的分类错误率。其中，与 Highway 中类似，对 ResNet-110 试验了 5 次，得到最佳结果 6.43% 的错误率，均值为 6.61%，标准差为0.16</center>\n\n## Layer 响应分析\n图 4 是 CIFAR-10 上网络（训练好之后）中各个 `3x3` layer 响应的标准差，响应指 BN 之后，activate 之前的 layer 响应。\n![](/images/img_cls/resnet_4.png)<center>图 4. 网络（3x3）各层的响应标准差。上：网络原始各层先后顺序；下：按标准差倒序排列。</center>\n\n从图 4中可见，ResNet 有着比 plain 更小的响应标准差，所以 ResNet 中各层的输出更加集中在 0 附近，避免了梯度消失（现在都使用 relu 而非 sigmoid 激活，避免梯度消失这一说还有意义吗？）。同时注意到，更深的 ResNet 的响应幅度更小，这表明当 layer 数量越多时，均摊到单个 layer 上，其对信号的改变越小。\n\n## 超深网络 layers>1000\nn=200，此时网络层数 6n+2=1202。训练结果（错误率）如上表 和图 3 中所示，训练集错误率 `<0.1%`，测试集错误率 `7.93%`，差强人意，但是比起 ResNet-110，虽然训练错误率差不多，但是测试错误率已经上升，作者认为这是过拟合导致，此时网络太大，而数据集太小，可以使用 `maxout`，`dropout` 等手段来改善过拟合问题，但是作者自己并没有这么做，而只是在设计网络架构时，遵循 deep & thin 的原则，简单地将网络正则化，这是因为本论文的重点在于解决（deep 网络的）优化困难，而非网络正则\n> deep: 网络层数多；thin: 每层的操作少，例如常见的一层为 conv-bn-relu；wide: 每层的 feature size 较大。\n\n\n# 目标检测\n检测采用 Faster R-CNN，将 baseline 从 VGG-16 替换为 ResNet-101。\n\n## 训练方法\n1. 在 ImageNet 上训练，然后再在目标检测数据上对网络进行 fine-tune。","slug":"img_cls/resnet","published":1,"updated":"2021-01-28T08:12:36.889Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cklbo2bwz000ctsdjbebcd63o","content":"<p>本篇罗列一些 Resnet 网络的细节。</p>\n<a id=\"more\"></a>\n\n<h1 id=\"ImageNet\"><a href=\"#ImageNet\" class=\"headerlink\" title=\"ImageNet\"></a>ImageNet</h1><h2 id=\"数据增强\"><a href=\"#数据增强\" class=\"headerlink\" title=\"数据增强\"></a>数据增强</h2><ol>\n<li>scale 增强：输入图像的短边（w,h 较小者）被 resize 到 <code>[256,418]</code> 中的一个随机数。</li>\n<li>在 resized 的图像或其水平翻转图像上随机位置（随机 center） crop 出一个 <code>224x224</code> 大小的部分。</li>\n<li>10-crop testing：测试阶段，对每个测试图像，crop 出上下左右四个角以及center处共 5 个 patch，以及水平翻转后同样的 5 个 patch，这 10 个 patch 经过网络之后的预测结果（softmax 层的输出）再进行平均，即得到这个图像的最终预测，预测为长度等于类别数量的向量。</li>\n</ol>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><ol>\n<li>baseline 使用 VGG，记为 plain，每个 conv 层与 activation 层之间均插入一个 BatchNorm 层。</li>\n<li>网络结构如下图，其中 conv3_1,  conv4_1, conv5_1 进行了下采样（stride=2）<br><img src=\"/images/img_cls/resnet_1.png\" alt=\"\"><center>图 1. 网络结构</center></li>\n<li>每个 conv block 均额外增加一个 shortcut，两个 output 在通道维度上进行 concatenate。由于 conv3_1,  conv4_1, conv5_1 进行了下采样，output 的 (H,W) 分别减小到一半，而通道 (C) 增大一倍，故需要将对应的 shortcut 作调整，有两种方法：a. identity mapping（注意 stride=2，以降低 H 和 W），额外维度上 0 填充；b. <code>1x1</code> conv，stride=2 的下采样，且输出 channel 增大一倍。</li>\n<li>在 ResNet-18 和 ResNet-34 上，使用 identity mapping（以及 zero-padding 维度填充）。</li>\n<li>conv3_1,  conv4_1, conv5_1 进行了下采样，具体而言，对 ResNet-18 和 ResNet-34，conv block 是由两个 <code>3x3</code> conv layer 组成，所以是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的第一个 conv layer 执行了下采样；对于 ResNet-50，ResNet-101 以及 ResNet-152，conv block 是由三个 conv layer 组成的（也叫 bottleneck），这三个conv 按顺序分别是 <code>1x1</code>, <code>3x3</code>, <code>1x1</code>， 所以其实是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的 <code>3x3</code> 执行了下采样操作。</li>\n<li>整个网络的下采样率为 <code>2x2x2x2x2=32</code>，输入为 <code>224x224</code>，那么最后的 conv 输出 feature 大小为 <code>7x7</code>。对每个 feature map 进行全局均值池化 GAP，那么每个 channel 得到一个值，然后经 full connection 层，得到 1000 个分类得分，最后由 softmax 将其归一化。</li>\n<li>ResNet-x，后缀 <code>x</code> 表示有参数的 layer 数量。</li>\n<li>shortcut 与 residual function 按 channel 维度连接之后再经过 relu 层。</li>\n<li>如果 shortcut 需要下采样，那么令 stride=2 即可。<h2 id=\"实验说明\"><a href=\"#实验说明\" class=\"headerlink\" title=\"实验说明\"></a>实验说明</h2><h3 id=\"plain-vs-resnet\"><a href=\"#plain-vs-resnet\" class=\"headerlink\" title=\"plain vs. resnet\"></a>plain vs. resnet</h3>在 18 layers 和 34 layers 两个网络上，对比 plain 和 ResNet，top-1 错误率如下表</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">plain</th>\n<th align=\"center\">ResNet</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">18 layers</td>\n<td align=\"center\">27.94</td>\n<td align=\"center\">27.88</td>\n</tr>\n<tr>\n<td align=\"center\">34 layers</td>\n<td align=\"center\">28.54</td>\n<td align=\"center\"><b> 25.03 </b></td>\n</tr>\n<tr>\n<td align=\"center\"><center>表 1. plain 与 Resnet 结果比较。ResNet 采用 identity shortcut。ImageNet 验证集上的错误率（10-crop testing）</center></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">说明：</td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<ul>\n<li>plain，随着网络加深，准确率下降。</li>\n<li>ResNet 采用 identity mapping （zero padding 增加channel）作为 shortcut。ResNet-34 比 ResNet-18 好，这表明加深网络导致恶化的问题可通过 ResNet 得到解决，且能获得更高的准确率。</li>\n<li>ResNet-18 比 plain-18 收敛更快。</li>\n</ul>\n<h3 id=\"Projection-Shortcut\"><a href=\"#Projection-Shortcut\" class=\"headerlink\" title=\"Projection Shortcut\"></a>Projection Shortcut</h3><p>使用 <code>1x1</code> conv 代替 identity mapping，输出如下，其中第一项为 conv block 的输出，第二项为 shortcut 的输出，</p>\n<p>$$\\mathbf y = \\mathcal F(\\mathbf x, {W_i}) + W_s \\mathbf x $$<br>以 34 layers 网络为例，对比如下三种情况：</p>\n<ul>\n<li>A：所有 shortcut 均没有参数，即 identity mapping，以及 zero-padding 以增加维度。与上述 ResNet-34 一样。</li>\n<li>B：除了需要增加维度的时候（conv3_1, conv4_1, conv5_1）使用 projection shortcut，其他 shortcut 采用 identity mapping。此时无需 zero-padding。</li>\n<li>C：所有的 shortcuts 均采用 projection shortcut。此时无需 zero-padding。</li>\n</ul>\n<p>实验结果如下：<br>|model|top-1 err | top-5 err|<br>|:–|:–|:–|<br>|plain-34| 28.54| 10.02|<br>|ResNet-34 A|25.03|7.76|<br>|ResNet-34 B|24.52|7.46|<br>|ResNet-34 C|24.19|7.4|<br>|ResNet-50|22.85|6.71|<br>|ResNet-101|21.75|6.05|<br>|ResNet-50|<b>21.43</b>|<b>5.71</b>|</p>\n<center>表 2. projection shortcut 方案比较，以及 ResNet 不同深度的比较。<font color='clan'>ResNet-50/101/152 使用方案 B</font>。ImageNet 验证集上的错误率（10-crop testing）</center>\n\n<p>结果分析：</p>\n<ul>\n<li>错误率：三种情况均比 plain 有所降低。B 比 A 有所降低，C 比 B 轻微降低。由于 A 中没有 projection shortcut，这表明对于解决准确率恶化的问题，projection shortcut 不是本质性地重要；另一方面，C 中由于引入过多的参数，内存占用和计算时间均增大，故综合起来，使用 B 方案。</li>\n</ul>\n<h3 id=\"Bottleneck-结构\"><a href=\"#Bottleneck-结构\" class=\"headerlink\" title=\"Bottleneck 结构\"></a>Bottleneck 结构</h3><p>图 1 中，每个 conv block 是由两层 conv 组成，把它改成 bottleneck 结构，即三层 conv：<code>1x1</code>, <code>3x3</code>, <code>1x1</code>，第一个 <code>1x1</code> 用于降低 channel（维度）（降为 1/4），(H,W) 不变，<code>3x3</code> 执行了下采样，(H,W) 均降为一半，channel 不变，最后一个 <code>1x1</code> 用于增加 channel，增大到 4 倍，这三个 conv layer 形成一个 bottleneck 结构，如下图右侧，<br><img src=\"/images/img_cls/resnet_2.png\" alt=\"\"><center>图 2. ImageNet 上使用的深度残差函数。左：与图 1 中相同的conv 块结构；右：bottleneck 结构，用在 ResNet-50/101/152 中</center></p>\n<p>使用 Bottleneck 时，identity shortcut 显得尤其重要，如果使用 projection，model 大小和时间复杂度都会大大增加，因为此时 shortcut 连接的两端都是高维（即，输入输出的 channel 都增大到原来 4 倍），这使得 shortcut 上的参数量大大增加。</p>\n<h3 id=\"ResNet-50-101-152\"><a href=\"#ResNet-50-101-152\" class=\"headerlink\" title=\"ResNet-50/101/152\"></a>ResNet-50/101/152</h3><p>将 ResNet-34 中的所有 conv block 替换为 bottleneck 结构，就得到 ResNet-50，如图 1，ResNet-34 有 <code>3+4+6+3=16</code> 个 conv block，增加的 layer 数量也就是 16。ResNet-50 使用方案 B。</p>\n<p>在 ResNet-50 基础上增加更多的 bottleneck 块，得到 ResNet-101 和 ResNet152。</p>\n<p>ResNet-50/101/152 比 ResNet-34 改善较多。实验结果如表 2，可见，使用 ResNet 结构可享受深度带来的益处。</p>\n<p>除了单模型，作者还对比了各种模型的集成结果，这个集成结果我也不知道是怎么计算的，也许只是几种不同深度的 ResNet 网络的结果平均，这个<font color=\"red\">有待考证</font>。</p>\n<p>再在其他数据集上研究 ResNet 的表现。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><ol>\n<li>使用 SGD 方法学习</li>\n<li>mini-batch=256</li>\n<li>学习率初始为 <code>0.1</code>，当错误率进入平稳期时，以  <code>1/10</code> 的倍率降低学习率。</li>\n<li>权重衰减因子为 <code>0.0001</code>，momentum=0.9</li>\n<li>由于使用了 BatchNorm，所以不使用 Dropout。</li>\n</ol>\n<h1 id=\"CIFAR-10\"><a href=\"#CIFAR-10\" class=\"headerlink\" title=\"CIFAR-10\"></a>CIFAR-10</h1><p>作者这里的目的是为了研究非常深的网络，而非为了 SOTA 的性能，所以仅使用了简单的框架，以 ResNet-34 和对应的 plain 网络为主体框架进行改造，这是因为输入 size 为 <code>32x32</code>，这比 ImageNet 数据集的输入 size <code>224x224</code> 小很多。</p>\n<h2 id=\"网络结构-1\"><a href=\"#网络结构-1\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>第一个 layer 的 kernel 需要调小（ImageNet 数据集上使用的是 <code>7x7</code>，太大），使用的是 <code>3x3</code> 的 conv，stride=1，无下采样（输出 channel 增大到 16），接着，使用 6n（6 的整数倍）个 conv layer，每 2n 个 conv layer 划分为一组，共 3 组，每组的输出 feature size 分别为 <code>(32, 16, 8)</code>，channel （也就是卷积 filter 的数量）分别为 <code>(16, 32, 64)</code>。最后使用全局均值池化 GAP + full connection + softmax。有参数的 layer 一共 <code>6n+2</code> 个。</p>\n<blockquote>\n<p>第二组和第三组各做了一次 stride=2 的下采样。论文里面没有明说，但我认为可以使用第二、三组中各自第一个 conv layer 来做下采样，与图 1 中一致。</p>\n</blockquote>\n<p>网络结构说明如下：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">输出 map size</th>\n<th align=\"left\">32x32</th>\n<th align=\"left\">16x16</th>\n<th align=\"left\">8x8</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">#layers</td>\n<td align=\"left\">2n+1</td>\n<td align=\"left\">2n</td>\n<td align=\"left\">2n</td>\n</tr>\n<tr>\n<td align=\"left\">#filters</td>\n<td align=\"left\">16</td>\n<td align=\"left\">32</td>\n<td align=\"left\">64</td>\n</tr>\n</tbody></table>\n<p>从这个表格可见，如果采用 identity shortcut，如果不进行下采样，那么这个 identity shortcut layer 的输出与输入完全一样，如果进行 rate=2 的下采样，那么 (H,W) 各变为一半大小，而 channel 增加一倍，变为原来 channel 的两倍，residual function 分支的 channel 也是原来输入 channel 的两倍，两个分支的输出 channel 和 (H,W) 均分别保持相同，才能进行 element-wise 的 add 操作。</p>\n<p>6n 个 conv layer，每两个 conv layer 使用一个 shortcut，共 3n 个 shortcut，作者使用 identity shortcut。</p>\n<h2 id=\"训练说明\"><a href=\"#训练说明\" class=\"headerlink\" title=\"训练说明\"></a>训练说明</h2><ul>\n<li>与 ImageNet 上的训练类似，区别是 mini-batch=128，学习率初始为 <code>0.1</code>，在第 32k 次和第 48k 次迭代时，学习率降为 <code>1/10</code>，训练共迭代 64k 次。</li>\n<li>CIFAR-10 训练集大小为 50k，测试集大小为 10k，类别数量为 10。将训练集按 45k/5k 分割为 train/val 集。</li>\n<li>原图像采用 4 pixel 的填充，然后以 0.5 的概率水平翻转，再随机 crop 一个 32x32 的 patch，作为网络输入。</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>当 <code>n={3,5,7,9}</code> 时，分别得到 <code>20, 32, 44, 56</code> 个有参数的 layer 的网络。实验结果如图 3，<br><img src=\"/images/img_cls/resnet_3.png\" alt=\"\"><center>图 3. CIFAR-10 训练结果。细线表示训练错误率，粗线表示测试错误率。左图中 plain-110 的错误率大于 60%，没有在图中显示。</center></p>\n<p>从图 3 中可见，ResNet 可以解决网络深度增加带来的准确率恶化问题。n=18 得到 ResNet-110，此时初始学习率 0.1 过大，导致开始训练时一直难以收敛，故初始化学习率为 0.01，当训练错误率降低到 80% 以下时（大约 400 次迭代），将学习率重置为 0.1 以加快收敛速度继续训练，之后就与训练 ImageNet 一样（阶梯降低学习率）。</p>\n<p>训练结果：</p>\n<p>ResNet-110 表现最好，比其他 deep &amp; thin 的网络有更少的参数和更好的性能，对比如下表，<br>||#layers|#params(M)|error(%)|<br>|:—|:—|:—|:—|<br>|FitNet|19|2.5|8.39|<br>|Highway|19|2.3|7.54(7.72 ± 0.16)|<br>|Highway|32|1.25|8.8|<br>|ResNet|20|0.27|8.75|<br>|ResNet|32|0.46|7.51|<br>|ResNet|44|0.66|7.17|<br>|ResNet|56|0.85|6.97|<br>|ResNet|110|1.7|<b>6.43</b>(6.61±0.16)|<br>|ResNet|1202|19.4|7.93|</p>\n<center>CIFAR-10 测试集上的分类错误率。其中，与 Highway 中类似，对 ResNet-110 试验了 5 次，得到最佳结果 6.43% 的错误率，均值为 6.61%，标准差为0.16</center>\n\n<h2 id=\"Layer-响应分析\"><a href=\"#Layer-响应分析\" class=\"headerlink\" title=\"Layer 响应分析\"></a>Layer 响应分析</h2><p>图 4 是 CIFAR-10 上网络（训练好之后）中各个 <code>3x3</code> layer 响应的标准差，响应指 BN 之后，activate 之前的 layer 响应。<br><img src=\"/images/img_cls/resnet_4.png\" alt=\"\"><center>图 4. 网络（3x3）各层的响应标准差。上：网络原始各层先后顺序；下：按标准差倒序排列。</center></p>\n<p>从图 4中可见，ResNet 有着比 plain 更小的响应标准差，所以 ResNet 中各层的输出更加集中在 0 附近，避免了梯度消失（现在都使用 relu 而非 sigmoid 激活，避免梯度消失这一说还有意义吗？）。同时注意到，更深的 ResNet 的响应幅度更小，这表明当 layer 数量越多时，均摊到单个 layer 上，其对信号的改变越小。</p>\n<h2 id=\"超深网络-layers-gt-1000\"><a href=\"#超深网络-layers-gt-1000\" class=\"headerlink\" title=\"超深网络 layers&gt;1000\"></a>超深网络 layers&gt;1000</h2><p>n=200，此时网络层数 6n+2=1202。训练结果（错误率）如上表 和图 3 中所示，训练集错误率 <code>&lt;0.1%</code>，测试集错误率 <code>7.93%</code>，差强人意，但是比起 ResNet-110，虽然训练错误率差不多，但是测试错误率已经上升，作者认为这是过拟合导致，此时网络太大，而数据集太小，可以使用 <code>maxout</code>，<code>dropout</code> 等手段来改善过拟合问题，但是作者自己并没有这么做，而只是在设计网络架构时，遵循 deep &amp; thin 的原则，简单地将网络正则化，这是因为本论文的重点在于解决（deep 网络的）优化困难，而非网络正则</p>\n<blockquote>\n<p>deep: 网络层数多；thin: 每层的操作少，例如常见的一层为 conv-bn-relu；wide: 每层的 feature size 较大。</p>\n</blockquote>\n<h1 id=\"目标检测\"><a href=\"#目标检测\" class=\"headerlink\" title=\"目标检测\"></a>目标检测</h1><p>检测采用 Faster R-CNN，将 baseline 从 VGG-16 替换为 ResNet-101。</p>\n<h2 id=\"训练方法\"><a href=\"#训练方法\" class=\"headerlink\" title=\"训练方法\"></a>训练方法</h2><ol>\n<li>在 ImageNet 上训练，然后再在目标检测数据上对网络进行 fine-tune。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>本篇罗列一些 Resnet 网络的细节。</p>","more":"<h1 id=\"ImageNet\"><a href=\"#ImageNet\" class=\"headerlink\" title=\"ImageNet\"></a>ImageNet</h1><h2 id=\"数据增强\"><a href=\"#数据增强\" class=\"headerlink\" title=\"数据增强\"></a>数据增强</h2><ol>\n<li>scale 增强：输入图像的短边（w,h 较小者）被 resize 到 <code>[256,418]</code> 中的一个随机数。</li>\n<li>在 resized 的图像或其水平翻转图像上随机位置（随机 center） crop 出一个 <code>224x224</code> 大小的部分。</li>\n<li>10-crop testing：测试阶段，对每个测试图像，crop 出上下左右四个角以及center处共 5 个 patch，以及水平翻转后同样的 5 个 patch，这 10 个 patch 经过网络之后的预测结果（softmax 层的输出）再进行平均，即得到这个图像的最终预测，预测为长度等于类别数量的向量。</li>\n</ol>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><ol>\n<li>baseline 使用 VGG，记为 plain，每个 conv 层与 activation 层之间均插入一个 BatchNorm 层。</li>\n<li>网络结构如下图，其中 conv3_1,  conv4_1, conv5_1 进行了下采样（stride=2）<br><img src=\"/images/img_cls/resnet_1.png\" alt=\"\"><center>图 1. 网络结构</center></li>\n<li>每个 conv block 均额外增加一个 shortcut，两个 output 在通道维度上进行 concatenate。由于 conv3_1,  conv4_1, conv5_1 进行了下采样，output 的 (H,W) 分别减小到一半，而通道 (C) 增大一倍，故需要将对应的 shortcut 作调整，有两种方法：a. identity mapping（注意 stride=2，以降低 H 和 W），额外维度上 0 填充；b. <code>1x1</code> conv，stride=2 的下采样，且输出 channel 增大一倍。</li>\n<li>在 ResNet-18 和 ResNet-34 上，使用 identity mapping（以及 zero-padding 维度填充）。</li>\n<li>conv3_1,  conv4_1, conv5_1 进行了下采样，具体而言，对 ResNet-18 和 ResNet-34，conv block 是由两个 <code>3x3</code> conv layer 组成，所以是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的第一个 conv layer 执行了下采样；对于 ResNet-50，ResNet-101 以及 ResNet-152，conv block 是由三个 conv layer 组成的（也叫 bottleneck），这三个conv 按顺序分别是 <code>1x1</code>, <code>3x3</code>, <code>1x1</code>， 所以其实是 conv3_1,  conv4_1, conv5_1 这三个 conv block 中的 <code>3x3</code> 执行了下采样操作。</li>\n<li>整个网络的下采样率为 <code>2x2x2x2x2=32</code>，输入为 <code>224x224</code>，那么最后的 conv 输出 feature 大小为 <code>7x7</code>。对每个 feature map 进行全局均值池化 GAP，那么每个 channel 得到一个值，然后经 full connection 层，得到 1000 个分类得分，最后由 softmax 将其归一化。</li>\n<li>ResNet-x，后缀 <code>x</code> 表示有参数的 layer 数量。</li>\n<li>shortcut 与 residual function 按 channel 维度连接之后再经过 relu 层。</li>\n<li>如果 shortcut 需要下采样，那么令 stride=2 即可。<h2 id=\"实验说明\"><a href=\"#实验说明\" class=\"headerlink\" title=\"实验说明\"></a>实验说明</h2><h3 id=\"plain-vs-resnet\"><a href=\"#plain-vs-resnet\" class=\"headerlink\" title=\"plain vs. resnet\"></a>plain vs. resnet</h3>在 18 layers 和 34 layers 两个网络上，对比 plain 和 ResNet，top-1 错误率如下表</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">plain</th>\n<th align=\"center\">ResNet</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">18 layers</td>\n<td align=\"center\">27.94</td>\n<td align=\"center\">27.88</td>\n</tr>\n<tr>\n<td align=\"center\">34 layers</td>\n<td align=\"center\">28.54</td>\n<td align=\"center\"><b> 25.03 </b></td>\n</tr>\n<tr>\n<td align=\"center\"><center>表 1. plain 与 Resnet 结果比较。ResNet 采用 identity shortcut。ImageNet 验证集上的错误率（10-crop testing）</center></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">说明：</td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<ul>\n<li>plain，随着网络加深，准确率下降。</li>\n<li>ResNet 采用 identity mapping （zero padding 增加channel）作为 shortcut。ResNet-34 比 ResNet-18 好，这表明加深网络导致恶化的问题可通过 ResNet 得到解决，且能获得更高的准确率。</li>\n<li>ResNet-18 比 plain-18 收敛更快。</li>\n</ul>\n<h3 id=\"Projection-Shortcut\"><a href=\"#Projection-Shortcut\" class=\"headerlink\" title=\"Projection Shortcut\"></a>Projection Shortcut</h3><p>使用 <code>1x1</code> conv 代替 identity mapping，输出如下，其中第一项为 conv block 的输出，第二项为 shortcut 的输出，</p>\n<p>$$\\mathbf y = \\mathcal F(\\mathbf x, {W_i}) + W_s \\mathbf x $$<br>以 34 layers 网络为例，对比如下三种情况：</p>\n<ul>\n<li>A：所有 shortcut 均没有参数，即 identity mapping，以及 zero-padding 以增加维度。与上述 ResNet-34 一样。</li>\n<li>B：除了需要增加维度的时候（conv3_1, conv4_1, conv5_1）使用 projection shortcut，其他 shortcut 采用 identity mapping。此时无需 zero-padding。</li>\n<li>C：所有的 shortcuts 均采用 projection shortcut。此时无需 zero-padding。</li>\n</ul>\n<p>实验结果如下：<br>|model|top-1 err | top-5 err|<br>|:–|:–|:–|<br>|plain-34| 28.54| 10.02|<br>|ResNet-34 A|25.03|7.76|<br>|ResNet-34 B|24.52|7.46|<br>|ResNet-34 C|24.19|7.4|<br>|ResNet-50|22.85|6.71|<br>|ResNet-101|21.75|6.05|<br>|ResNet-50|<b>21.43</b>|<b>5.71</b>|</p>\n<center>表 2. projection shortcut 方案比较，以及 ResNet 不同深度的比较。<font color='clan'>ResNet-50/101/152 使用方案 B</font>。ImageNet 验证集上的错误率（10-crop testing）</center>\n\n<p>结果分析：</p>\n<ul>\n<li>错误率：三种情况均比 plain 有所降低。B 比 A 有所降低，C 比 B 轻微降低。由于 A 中没有 projection shortcut，这表明对于解决准确率恶化的问题，projection shortcut 不是本质性地重要；另一方面，C 中由于引入过多的参数，内存占用和计算时间均增大，故综合起来，使用 B 方案。</li>\n</ul>\n<h3 id=\"Bottleneck-结构\"><a href=\"#Bottleneck-结构\" class=\"headerlink\" title=\"Bottleneck 结构\"></a>Bottleneck 结构</h3><p>图 1 中，每个 conv block 是由两层 conv 组成，把它改成 bottleneck 结构，即三层 conv：<code>1x1</code>, <code>3x3</code>, <code>1x1</code>，第一个 <code>1x1</code> 用于降低 channel（维度）（降为 1/4），(H,W) 不变，<code>3x3</code> 执行了下采样，(H,W) 均降为一半，channel 不变，最后一个 <code>1x1</code> 用于增加 channel，增大到 4 倍，这三个 conv layer 形成一个 bottleneck 结构，如下图右侧，<br><img src=\"/images/img_cls/resnet_2.png\" alt=\"\"><center>图 2. ImageNet 上使用的深度残差函数。左：与图 1 中相同的conv 块结构；右：bottleneck 结构，用在 ResNet-50/101/152 中</center></p>\n<p>使用 Bottleneck 时，identity shortcut 显得尤其重要，如果使用 projection，model 大小和时间复杂度都会大大增加，因为此时 shortcut 连接的两端都是高维（即，输入输出的 channel 都增大到原来 4 倍），这使得 shortcut 上的参数量大大增加。</p>\n<h3 id=\"ResNet-50-101-152\"><a href=\"#ResNet-50-101-152\" class=\"headerlink\" title=\"ResNet-50/101/152\"></a>ResNet-50/101/152</h3><p>将 ResNet-34 中的所有 conv block 替换为 bottleneck 结构，就得到 ResNet-50，如图 1，ResNet-34 有 <code>3+4+6+3=16</code> 个 conv block，增加的 layer 数量也就是 16。ResNet-50 使用方案 B。</p>\n<p>在 ResNet-50 基础上增加更多的 bottleneck 块，得到 ResNet-101 和 ResNet152。</p>\n<p>ResNet-50/101/152 比 ResNet-34 改善较多。实验结果如表 2，可见，使用 ResNet 结构可享受深度带来的益处。</p>\n<p>除了单模型，作者还对比了各种模型的集成结果，这个集成结果我也不知道是怎么计算的，也许只是几种不同深度的 ResNet 网络的结果平均，这个<font color=\"red\">有待考证</font>。</p>\n<p>再在其他数据集上研究 ResNet 的表现。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><ol>\n<li>使用 SGD 方法学习</li>\n<li>mini-batch=256</li>\n<li>学习率初始为 <code>0.1</code>，当错误率进入平稳期时，以  <code>1/10</code> 的倍率降低学习率。</li>\n<li>权重衰减因子为 <code>0.0001</code>，momentum=0.9</li>\n<li>由于使用了 BatchNorm，所以不使用 Dropout。</li>\n</ol>\n<h1 id=\"CIFAR-10\"><a href=\"#CIFAR-10\" class=\"headerlink\" title=\"CIFAR-10\"></a>CIFAR-10</h1><p>作者这里的目的是为了研究非常深的网络，而非为了 SOTA 的性能，所以仅使用了简单的框架，以 ResNet-34 和对应的 plain 网络为主体框架进行改造，这是因为输入 size 为 <code>32x32</code>，这比 ImageNet 数据集的输入 size <code>224x224</code> 小很多。</p>\n<h2 id=\"网络结构-1\"><a href=\"#网络结构-1\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>第一个 layer 的 kernel 需要调小（ImageNet 数据集上使用的是 <code>7x7</code>，太大），使用的是 <code>3x3</code> 的 conv，stride=1，无下采样（输出 channel 增大到 16），接着，使用 6n（6 的整数倍）个 conv layer，每 2n 个 conv layer 划分为一组，共 3 组，每组的输出 feature size 分别为 <code>(32, 16, 8)</code>，channel （也就是卷积 filter 的数量）分别为 <code>(16, 32, 64)</code>。最后使用全局均值池化 GAP + full connection + softmax。有参数的 layer 一共 <code>6n+2</code> 个。</p>\n<blockquote>\n<p>第二组和第三组各做了一次 stride=2 的下采样。论文里面没有明说，但我认为可以使用第二、三组中各自第一个 conv layer 来做下采样，与图 1 中一致。</p>\n</blockquote>\n<p>网络结构说明如下：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">输出 map size</th>\n<th align=\"left\">32x32</th>\n<th align=\"left\">16x16</th>\n<th align=\"left\">8x8</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">#layers</td>\n<td align=\"left\">2n+1</td>\n<td align=\"left\">2n</td>\n<td align=\"left\">2n</td>\n</tr>\n<tr>\n<td align=\"left\">#filters</td>\n<td align=\"left\">16</td>\n<td align=\"left\">32</td>\n<td align=\"left\">64</td>\n</tr>\n</tbody></table>\n<p>从这个表格可见，如果采用 identity shortcut，如果不进行下采样，那么这个 identity shortcut layer 的输出与输入完全一样，如果进行 rate=2 的下采样，那么 (H,W) 各变为一半大小，而 channel 增加一倍，变为原来 channel 的两倍，residual function 分支的 channel 也是原来输入 channel 的两倍，两个分支的输出 channel 和 (H,W) 均分别保持相同，才能进行 element-wise 的 add 操作。</p>\n<p>6n 个 conv layer，每两个 conv layer 使用一个 shortcut，共 3n 个 shortcut，作者使用 identity shortcut。</p>\n<h2 id=\"训练说明\"><a href=\"#训练说明\" class=\"headerlink\" title=\"训练说明\"></a>训练说明</h2><ul>\n<li>与 ImageNet 上的训练类似，区别是 mini-batch=128，学习率初始为 <code>0.1</code>，在第 32k 次和第 48k 次迭代时，学习率降为 <code>1/10</code>，训练共迭代 64k 次。</li>\n<li>CIFAR-10 训练集大小为 50k，测试集大小为 10k，类别数量为 10。将训练集按 45k/5k 分割为 train/val 集。</li>\n<li>原图像采用 4 pixel 的填充，然后以 0.5 的概率水平翻转，再随机 crop 一个 32x32 的 patch，作为网络输入。</li>\n</ul>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>当 <code>n={3,5,7,9}</code> 时，分别得到 <code>20, 32, 44, 56</code> 个有参数的 layer 的网络。实验结果如图 3，<br><img src=\"/images/img_cls/resnet_3.png\" alt=\"\"><center>图 3. CIFAR-10 训练结果。细线表示训练错误率，粗线表示测试错误率。左图中 plain-110 的错误率大于 60%，没有在图中显示。</center></p>\n<p>从图 3 中可见，ResNet 可以解决网络深度增加带来的准确率恶化问题。n=18 得到 ResNet-110，此时初始学习率 0.1 过大，导致开始训练时一直难以收敛，故初始化学习率为 0.01，当训练错误率降低到 80% 以下时（大约 400 次迭代），将学习率重置为 0.1 以加快收敛速度继续训练，之后就与训练 ImageNet 一样（阶梯降低学习率）。</p>\n<p>训练结果：</p>\n<p>ResNet-110 表现最好，比其他 deep &amp; thin 的网络有更少的参数和更好的性能，对比如下表，<br>||#layers|#params(M)|error(%)|<br>|:—|:—|:—|:—|<br>|FitNet|19|2.5|8.39|<br>|Highway|19|2.3|7.54(7.72 ± 0.16)|<br>|Highway|32|1.25|8.8|<br>|ResNet|20|0.27|8.75|<br>|ResNet|32|0.46|7.51|<br>|ResNet|44|0.66|7.17|<br>|ResNet|56|0.85|6.97|<br>|ResNet|110|1.7|<b>6.43</b>(6.61±0.16)|<br>|ResNet|1202|19.4|7.93|</p>\n<center>CIFAR-10 测试集上的分类错误率。其中，与 Highway 中类似，对 ResNet-110 试验了 5 次，得到最佳结果 6.43% 的错误率，均值为 6.61%，标准差为0.16</center>\n\n<h2 id=\"Layer-响应分析\"><a href=\"#Layer-响应分析\" class=\"headerlink\" title=\"Layer 响应分析\"></a>Layer 响应分析</h2><p>图 4 是 CIFAR-10 上网络（训练好之后）中各个 <code>3x3</code> layer 响应的标准差，响应指 BN 之后，activate 之前的 layer 响应。<br><img src=\"/images/img_cls/resnet_4.png\" alt=\"\"><center>图 4. 网络（3x3）各层的响应标准差。上：网络原始各层先后顺序；下：按标准差倒序排列。</center></p>\n<p>从图 4中可见，ResNet 有着比 plain 更小的响应标准差，所以 ResNet 中各层的输出更加集中在 0 附近，避免了梯度消失（现在都使用 relu 而非 sigmoid 激活，避免梯度消失这一说还有意义吗？）。同时注意到，更深的 ResNet 的响应幅度更小，这表明当 layer 数量越多时，均摊到单个 layer 上，其对信号的改变越小。</p>\n<h2 id=\"超深网络-layers-gt-1000\"><a href=\"#超深网络-layers-gt-1000\" class=\"headerlink\" title=\"超深网络 layers&gt;1000\"></a>超深网络 layers&gt;1000</h2><p>n=200，此时网络层数 6n+2=1202。训练结果（错误率）如上表 和图 3 中所示，训练集错误率 <code>&lt;0.1%</code>，测试集错误率 <code>7.93%</code>，差强人意，但是比起 ResNet-110，虽然训练错误率差不多，但是测试错误率已经上升，作者认为这是过拟合导致，此时网络太大，而数据集太小，可以使用 <code>maxout</code>，<code>dropout</code> 等手段来改善过拟合问题，但是作者自己并没有这么做，而只是在设计网络架构时，遵循 deep &amp; thin 的原则，简单地将网络正则化，这是因为本论文的重点在于解决（deep 网络的）优化困难，而非网络正则</p>\n<blockquote>\n<p>deep: 网络层数多；thin: 每层的操作少，例如常见的一层为 conv-bn-relu；wide: 每层的 feature size 较大。</p>\n</blockquote>\n<h1 id=\"目标检测\"><a href=\"#目标检测\" class=\"headerlink\" title=\"目标检测\"></a>目标检测</h1><p>检测采用 Faster R-CNN，将 baseline 从 VGG-16 替换为 ResNet-101。</p>\n<h2 id=\"训练方法\"><a href=\"#训练方法\" class=\"headerlink\" title=\"训练方法\"></a>训练方法</h2><ol>\n<li>在 ImageNet 上训练，然后再在目标检测数据上对网络进行 fine-tune。</li>\n</ol>"}],"PostAsset":[],"PostCategory":[{"post_id":"ck9dzciv7001pgga66dc248n0","category_id":"ck9dzcivu001sgga64qfs77vd","_id":"ck9dzciw1001vgga6gqqzgdnv"},{"post_id":"ck9dzcj1o002agga6ffs82n5e","category_id":"ck9dzcivu001sgga64qfs77vd","_id":"ck9dzcj21002hgga65456c3zy"},{"post_id":"ck9dzcj4w002mgga63195h8sz","category_id":"ck9dzcivu001sgga64qfs77vd","_id":"ck9dzcj54002qgga6azmp3jde"},{"post_id":"ck9dzcjg9002ugga675qd6yrm","category_id":"ck9dzcivu001sgga64qfs77vd","_id":"ck9dzcjgz0030gga60h9ydl05"},{"post_id":"ck9dzcjgj002xgga62ozddn7i","category_id":"ck9dzcivu001sgga64qfs77vd","_id":"ck9dzcjh00032gga6a833ezrk"},{"post_id":"ck9dzcjg5002sgga68e1411qa","category_id":"ck9dzcjgh002vgga6gmby7446","_id":"ck9dzcjh10034gga63pnr568a"}],"PostTag":[{"post_id":"ck9dzchum0000gga6hn0q5b6g","tag_id":"ck9dzchvz0003gga65t8hevd6","_id":"ck9dzchwp0008gga6dz2wgf4q"},{"post_id":"ck9dzchwq0009gga6daloav3p","tag_id":"ck9dzchvz0003gga65t8hevd6","_id":"ck9dzchx4000cgga669tr2wzn"},{"post_id":"ck9dzchvr0002gga6f6le5fyl","tag_id":"ck9dzchwn0007gga652cf3ith","_id":"ck9dzchx7000egga65mvg8o5o"},{"post_id":"ck9dzchwc0005gga6hlb6eg33","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzchxi000hgga68h0egukz"},{"post_id":"ck9dzchwk0006gga6e8788jqj","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzchxp000jgga6b4tzgehs"},{"post_id":"ck9dzchwx000agga66fq8463i","tag_id":"ck9dzchxn000igga6h40pee8q","_id":"ck9dzchxr000lgga655vm30dz"},{"post_id":"ck9dzchx5000dgga67s1f5l05","tag_id":"ck9dzchxq000kgga6bt9maf7k","_id":"ck9dzchxt000ngga67fzq3o8p"},{"post_id":"ck9dzchx8000fgga62vo6ej0e","tag_id":"ck9dzchxn000igga6h40pee8q","_id":"ck9dzchxt000ogga62d1p5t9z"},{"post_id":"ck9dzcio3000pgga64t9ca2tj","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcip4000tgga6gvx53bp6"},{"post_id":"ck9dzcioe000rgga66xyghfdd","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcip9000vgga67gj00ma8"},{"post_id":"ck9dzcip5000ugga6gc1t0r5m","tag_id":"ck9dzchwn0007gga652cf3ith","_id":"ck9dzciph000xgga60q7he7fn"},{"post_id":"ck9dzcip9000wgga69soc6hs3","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcipl000zgga6a2pqfixw"},{"post_id":"ck9dzcipi000ygga64z7kh24x","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcipo0011gga68ymr0ttx"},{"post_id":"ck9dzcipm0010gga66f5f2gtr","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcipw0013gga6egjke4tv"},{"post_id":"ck9dzcips0012gga6cg30bkya","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzciq40015gga62b6z40rq"},{"post_id":"ck9dzcipw0014gga6fg3vbir8","tag_id":"ck9dzchvz0003gga65t8hevd6","_id":"ck9dzciqf0017gga6hs2t0bhs"},{"post_id":"ck9dzciq40016gga60l432k8l","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzciqf0019gga602b63l3s"},{"post_id":"ck9dzciqf0018gga6gl2k56v5","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzciql001bgga6aumo94tl"},{"post_id":"ck9dzciqf001agga6bgobcaq5","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzciqq001dgga6bef3g0j0"},{"post_id":"ck9dzciqm001cgga6034132ht","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcir0001fgga65qrbada6"},{"post_id":"ck9dzciqx001egga6220gd7ev","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcir3001hgga630wfh5p8"},{"post_id":"ck9dzciuq001kgga6eg7ravz4","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzciuz001mgga61ygk2g56"},{"post_id":"ck9dzciuw001lgga6cqdf40gu","tag_id":"ck9dzchvz0003gga65t8hevd6","_id":"ck9dzciv6001ogga69pe5d1hz"},{"post_id":"ck9dzciuz001ngga6db7ecnq5","tag_id":"ck9dzciv9001qgga6bypk722m","_id":"ck9dzciw1001wgga6cnat13is"},{"post_id":"ck9dzciuz001ngga6db7ecnq5","tag_id":"ck9dzcivy001tgga6d38h3pfu","_id":"ck9dzciw3001xgga636de51j5"},{"post_id":"ck9dzciv7001pgga66dc248n0","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzciw3001zgga68bc1632e"},{"post_id":"ck9dzcivb001rgga6hkzd7lgz","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzciw50021gga6flcd4yxw"},{"post_id":"ck9dzcivb001rgga6hkzd7lgz","tag_id":"ck9dzciw40020gga67iluc90r","_id":"ck9dzciw50022gga620p66ocs"},{"post_id":"ck9dzcj190023gga61eu2hhsv","tag_id":"ck9dzchxq000kgga6bt9maf7k","_id":"ck9dzcj1d0025gga64trbge2z"},{"post_id":"ck9dzcj1b0024gga69d3n8ess","tag_id":"ck9dzchx0000bgga698isayfw","_id":"ck9dzcj1l0027gga6d54v8s67"},{"post_id":"ck9dzcj1l0028gga6g4ri2yxi","tag_id":"ck9dzciv9001qgga6bypk722m","_id":"ck9dzcj1r002bgga6av3wb8pd"},{"post_id":"ck9dzcj1l0028gga6g4ri2yxi","tag_id":"ck9dzcivy001tgga6d38h3pfu","_id":"ck9dzcj1t002dgga6ak97e2wz"},{"post_id":"ck9dzcj1o002agga6ffs82n5e","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcj1v002fgga60fyz5kzs"},{"post_id":"ck9dzcj1d0026gga62ewrfhlt","tag_id":"ck9dzcj1n0029gga61hkf7rkf","_id":"ck9dzcj22002jgga69rrt3ndh"},{"post_id":"ck9dzcj1t002egga649zs01re","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcj23002kgga67nob6xal"},{"post_id":"ck9dzcj1v002ggga69jso7kei","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcj23002lgga68jtydkvb"},{"post_id":"ck9dzcj4w002mgga63195h8sz","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcj53002ogga65e1je4g8"},{"post_id":"ck9dzcj4y002ngga6gsbm1qbo","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcj54002pgga626mn8uic"},{"post_id":"ck9dzcjg3002rgga6dmdfbe4b","tag_id":"ck9dzciv9001qgga6bypk722m","_id":"ck9dzcjg9002tgga61os34vgi"},{"post_id":"ck9dzcjg3002rgga6dmdfbe4b","tag_id":"ck9dzcivy001tgga6d38h3pfu","_id":"ck9dzcjgi002wgga65or56m13"},{"post_id":"ck9dzcjg5002sgga68e1411qa","tag_id":"ck9dzciv9001qgga6bypk722m","_id":"ck9dzcjgq002ygga69jsd5z4e"},{"post_id":"ck9dzcjg5002sgga68e1411qa","tag_id":"ck9dzcivy001tgga6d38h3pfu","_id":"ck9dzcjgz002zgga6dmfscabf"},{"post_id":"ck9dzcjg9002ugga675qd6yrm","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcjh00031gga6bwcy9gqs"},{"post_id":"ck9dzcjgj002xgga62ozddn7i","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ck9dzcjh00033gga67q4g1ttd"},{"post_id":"ckjmjft0b00006gdj4qg2d2e7","tag_id":"ck9dzchxn000igga6h40pee8q","_id":"ckjmjft0e00026gdjhbki79av"},{"post_id":"ckjmjft0e00036gdj6he8h57y","tag_id":"ck9dzciw40020gga67iluc90r","_id":"ckjmjft0g00046gdj8w3payd6"},{"post_id":"ckjxr7whr0001y4dj8k6ubzq2","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ckjxr7wi00004y4djhvrob0e7"},{"post_id":"ckjxr7whz0003y4djbsp0bnrp","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ckjxr7wi00005y4dj19b2at9a"},{"post_id":"ckjxr7wgn0000y4dj0j506tzu","tag_id":"ckjxr7wht0002y4dj2z56bbj0","_id":"ckjxr7wi10006y4dj46i921d2"},{"post_id":"ckjxr7wi30007y4djcfj42smn","tag_id":"ck9dzcivz001ugga6ctwu3xeo","_id":"ckjxr7wi40009y4dje04g3qqz"},{"post_id":"ckjxr7wi40008y4dj40bc9t56","tag_id":"ckjxr7wht0002y4dj2z56bbj0","_id":"ckjxr7wi5000ay4djaq0ydncv"},{"post_id":"cklbo2bwf0002tsdj2flzhxje","tag_id":"ck9dzchx0000bgga698isayfw","_id":"cklbo2bwo0005tsdjgeyv0jml"},{"post_id":"cklbo2bwn0004tsdjbux43r55","tag_id":"ck9dzcir4001igga6f87964cw","_id":"cklbo2bwp0007tsdjgshf3qpo"},{"post_id":"cklbo2bwe0001tsdjdtp55pab","tag_id":"cklbo2bwg0003tsdjfocw1522","_id":"cklbo2bwu0009tsdjh3b7cht9"},{"post_id":"cklbo2bwp0008tsdja5m2cahk","tag_id":"cklbo2bwu000atsdj3tlzcke4","_id":"cklbo2bwv000btsdj558bdqaz"},{"post_id":"cklbo2bwz000ctsdjbebcd63o","tag_id":"cklbo2bx0000dtsdjaad68nim","_id":"cklbo2bx1000etsdj2owpbvhl"}],"Tag":[{"name":"GAN","_id":"ck9dzchvz0003gga65t8hevd6"},{"name":"DIP","_id":"ck9dzchwn0007gga652cf3ith"},{"name":"object detection","_id":"ck9dzchx0000bgga698isayfw"},{"name":"CV","_id":"ck9dzchxn000igga6h40pee8q"},{"name":"c++","_id":"ck9dzchxq000kgga6bt9maf7k"},{"name":"tool","_id":"ck9dzcir4001igga6f87964cw"},{"name":"math","_id":"ck9dzciv9001qgga6bypk722m"},{"name":"DP","_id":"ck9dzcivy001tgga6d38h3pfu"},{"name":"PyTorch","_id":"ck9dzcivz001ugga6ctwu3xeo"},{"name":"DL","_id":"ck9dzciw40020gga67iluc90r"},{"name":"image classification","_id":"ck9dzcj1n0029gga61hkf7rkf"},{"name":"Deep Learning","_id":"ckjxr7wht0002y4dj2z56bbj0"},{"name":"Deep Learning, CNN","_id":"cklbo2bwg0003tsdjfocw1522"},{"name":"CNN, Deep Learning","_id":"cklbo2bwu000atsdj3tlzcke4"},{"name":"img cls","_id":"cklbo2bx0000dtsdjaad68nim"}]}}