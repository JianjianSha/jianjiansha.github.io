{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/BBox-reg_fig3.png","path":"images/BBox-reg_fig3.png","modified":1,"renderable":0},{"_id":"source/images/BBox-reg_fig4.png","path":"images/BBox-reg_fig4.png","modified":1,"renderable":0},{"_id":"source/images/CGAN_fig1.png","path":"images/CGAN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DP1_fig1.png","path":"images/DP1_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DP2_fig1.png","path":"images/DP2_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DetNet_fig1.png","path":"images/DetNet_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DetNet_fig2.png","path":"images/DetNet_fig2.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig2.png","path":"images/FSAF_fig2.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig3.png","path":"images/FSAF_fig3.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig4.png","path":"images/FSAF_fig4.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig5.png","path":"images/FSAF_fig5.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig6.png","path":"images/FSAF_fig6.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig3.png","path":"images/GA-RPN_fig3.png","modified":1,"renderable":0},{"_id":"source/images/GAN_fig1.png","path":"images/GAN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/GIoU_fig2.png","path":"images/GIoU_fig2.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN_fig1.png","path":"images/Grid-RCNN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/ImprovedGAN_fig1.png","path":"images/ImprovedGAN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(b).png","path":"images/TridentNet_fig1(b).png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig3.png","path":"images/M2Det_fig3.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(c).png","path":"images/TridentNet_fig1(c).png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig3.png","path":"images/libra-rcnn_fig3.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_figa.png","path":"images/libra-rcnn_figa.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig1.png","path":"images/mAP_fig1.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig2.png","path":"images/mAP_fig2.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig3.png","path":"images/mAP_fig3.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig4.png","path":"images/mAP_fig4.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig5.png","path":"images/mAP_fig5.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig3.png","path":"images/mask-rcnn_fig3.png","modified":1,"renderable":0},{"_id":"source/images/BBox-reg_fig2.png","path":"images/BBox-reg_fig2.png","modified":1,"renderable":0},{"_id":"source/images/CGAN_fig2.png","path":"images/CGAN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/DeRPN_fig1.png","path":"images/DeRPN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig1.png","path":"images/GA-RPN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig2.png","path":"images/GA-RPN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig4.png","path":"images/GA-RPN_fig4.png","modified":1,"renderable":0},{"_id":"source/images/GAN_alg1.png","path":"images/GAN_alg1.png","modified":1,"renderable":0},{"_id":"source/images/GAN_fig2.png","path":"images/GAN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/GIoU_fig1.png","path":"images/GIoU_fig1.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN-Plus_fig1.png","path":"images/Grid-RCNN-Plus_fig1.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN_fig3.png","path":"images/Grid-RCNN_fig3.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN_fig4.png","path":"images/Grid-RCNN_fig4.png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig1.png","path":"images/M2Det_fig1.png","modified":1,"renderable":0},{"_id":"source/images/RepPoints_fig2.png","path":"images/RepPoints_fig2.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(a).png","path":"images/TridentNet_fig1(a).png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig4.png","path":"images/M2Det_fig4.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig1.png","path":"images/libra-rcnn_fig1.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig2.png","path":"images/TridentNet_fig2.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig3.png","path":"images/TridentNet_fig3.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig2.png","path":"images/libra-rcnn_fig2.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig4.png","path":"images/libra-rcnn_fig4.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig5.png","path":"images/libra-rcnn_fig5.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig1.png","path":"images/mask-rcnn_fig1.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig4.png","path":"images/mask-rcnn_fig4.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN-Plus_fig2.png","path":"images/Grid-RCNN-Plus_fig2.png","modified":1,"renderable":0},{"_id":"source/images/Grid-RCNN_fig2.png","path":"images/Grid-RCNN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/RepPoints_fig1.png","path":"images/RepPoints_fig1.png","modified":1,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/affix.js","path":"js/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/exturl.js","path":"js/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/js.cookie.js","path":"js/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/post-details.js","path":"js/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scroll-cookie.js","path":"js/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scrollspy.js","path":"js/scrollspy.js","modified":1,"renderable":1},{"_id":"source/images/BBox-reg_fig1.png","path":"images/BBox-reg_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DeRPN_fig2.png","path":"images/DeRPN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig2.png","path":"images/M2Det_fig2.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/images/DSOD_fig1.png","path":"images/DSOD_fig1.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1560516826130},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1560516826130},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1560516826129},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1560516826130},{"_id":"themes/next/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1560516826140},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1560516826139},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1560516826140},{"_id":"themes/next/bower.json","hash":"8076a6e58a99d1188d335a6456a0de0fda163338","modified":1560516826142},{"_id":"themes/next/README.md","hash":"3f72e5a5051ca2bdaccdda684c46dc4fdb4413a6","modified":1560516826141},{"_id":"themes/next/.all-contributorsrc","hash":"43eb0149c78e464c695f0dd758bb8c59353182b3","modified":1560516826129},{"_id":"themes/next/.gitignore","hash":"8bf1bfc917aa8dd2d886fd36f764447a2b561e1e","modified":1560516826139},{"_id":"themes/next/package.json","hash":"037ed50fbce1520918bf8c3a1c14b6e07676783e","modified":1560516826195},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1560516826142},{"_id":"source/_posts/CGAN.md","hash":"eef9ff96d977e8ae948121358c8fe14155c4e1d6","modified":1564566437246},{"_id":"source/_posts/DP1.md","hash":"c8dc6a749682e293d93ff13e1a5b38383d9ca877","modified":1565953374849},{"_id":"source/_posts/BBox-Reg-Uncertainty.md","hash":"af31eca9b7a7e09588671fba50245e1c326943dd","modified":1561713660064},{"_id":"source/_posts/DP2.md","hash":"623a4ada18ac669933e94ecd29beb85fe6485059","modified":1566903127614},{"_id":"source/_posts/DeRPN.md","hash":"4bf5d185b5f5500c82735b9b342afb72198d7751","modified":1563618814404},{"_id":"source/_posts/DP3.md","hash":"90c4e923f595d5c42a3e2989ae5822ea8440f811","modified":1567167680627},{"_id":"source/_posts/DetNet.md","hash":"95e4c7d06f7557ae20bbd39f9559338f94ba14ec","modified":1563619177332},{"_id":"source/_posts/DSOD.md","hash":"1074946adbcf09724075cb4a9a805195b44d2c3f","modified":1563343920637},{"_id":"source/_posts/FSAF.md","hash":"b008fe64013cfaa36df10791f3f42dda0eab7aee","modified":1561637721594},{"_id":"source/_posts/GA-RPN.md","hash":"6e16e51f06c48693e144172fd32859013b1d2a9b","modified":1561637765328},{"_id":"source/_posts/GAN.md","hash":"9b1f856ceaacb1f8a8852e868caf69a74a3c9bc9","modified":1564474621780},{"_id":"source/_posts/GIoU.md","hash":"72416e2a1faca72ce4f4f834e5cc72fc1a8af13e","modified":1564640668166},{"_id":"source/_posts/Grid-RCNN.md","hash":"ff4760b94930ba72296fc87136ea1660ecae9162","modified":1563845003833},{"_id":"source/_posts/Hexo-Sync.md","hash":"b8cc3f0b1e846edd52d10c5d41c4e7afeb20f555","modified":1560518432167},{"_id":"source/_posts/ImprovedGAN.md","hash":"b005293b4790b5fb43fae5b598c91798646d9e8a","modified":1565953704220},{"_id":"source/_posts/M2Det.md","hash":"6eb8aec3c5176af1faf540594c09d1d5c4b70455","modified":1561802397459},{"_id":"source/_posts/PyTorch-1.md","hash":"02541d37b0459041c1ac8e7a0e7980c5d904d6c7","modified":1561025269801},{"_id":"source/_posts/PyTorch-2.md","hash":"a8294968514b9d9c2d4acc8967a0c30403d84cd9","modified":1566782235372},{"_id":"source/_posts/PyTorch-3.md","hash":"b4b8db2712e37127041d8c2fb3a10ba8153eaae7","modified":1566805430509},{"_id":"source/_posts/PyTorch-4.md","hash":"ff7533d0dc2b5cb0775fe6dc53d8090421b5c3c3","modified":1566805645398},{"_id":"source/_posts/PyTorch-5.md","hash":"6c7112e4ab31b6091bf0596d0524b5f3fe75d87a","modified":1566895029451},{"_id":"themes/next/gulpfile.coffee","hash":"6407d9063bd88ede299ff7c2a59cf2c82e079476","modified":1560516826152},{"_id":"source/_posts/RepPoints.md","hash":"3e3d4aa9322e89944463456db26918cf1349949d","modified":1563448962079},{"_id":"source/_posts/TridentNet.md","hash":"de2029038a1b6d9388722a83d5dbd6afa44c654f","modified":1561637849335},{"_id":"source/_posts/WGAN.md","hash":"fdecf7078cba1ab4258a2f03e95631fc21c8773f","modified":1564402946732},{"_id":"source/_posts/cpp-aux-tools.md","hash":"63aeac99af1efeb92814ccb164d313227175e478","modified":1564126682442},{"_id":"source/_posts/cv-mtds.md","hash":"b46e3d21b161a5d3eb590811337ae0bf6a427f48","modified":1563269478679},{"_id":"source/_posts/libra-rcnn.md","hash":"d20c22a87cf5ddb28c447391ff6f4d40da666a28","modified":1562330122631},{"_id":"source/_posts/loss.md","hash":"e2f2848272240eb043f2a492356e6a946143804c","modified":1563328501435},{"_id":"source/_posts/gcc-src.md","hash":"5a6ea3c2c2896aa71ca5f9e6b50fac1070100b6d","modified":1563013967398},{"_id":"source/_posts/mAP.md","hash":"07f0e284d51af27a2c28b57c5bae20aa57da9fcf","modified":1561100541616},{"_id":"source/_posts/mask-rcnn.md","hash":"5c9efbe2d02dc03eb6d39fe4c6d582d5cc6f4c91","modified":1562669847936},{"_id":"source/about/index.md","hash":"07157c341bcfac0afd3e3e9d43c5d1de8144c34b","modified":1563865941258},{"_id":"source/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1561700897878},{"_id":"themes/next/_config.yml","hash":"a87cac157119b7bc9c16d55a965cfd96a9248463","modified":1560652971556},{"_id":"source/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1561700932509},{"_id":"source/images/CGAN_fig1.png","hash":"94d2eb94087c072188200f54dfccb665a4bb0bc3","modified":1564394520745},{"_id":"source/images/DP1_fig1.png","hash":"ed9dd7cbecff94be6778831a494b1295482bfe3e","modified":1565833680807},{"_id":"source/images/DP2_fig1.png","hash":"a90b55aa55d0cfdd147c5b00feaf432300f2854b","modified":1566538841949},{"_id":"source/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1563348004387},{"_id":"source/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1563348028431},{"_id":"source/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1561601705733},{"_id":"source/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1561601750043},{"_id":"source/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1561606224212},{"_id":"source/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1561615069653},{"_id":"source/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1561620213819},{"_id":"source/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1561532150029},{"_id":"source/images/GAN_fig1.png","hash":"c6f2c668d1b4d98d4bd7fdc27851faed5c7ffe9c","modified":1563970898796},{"_id":"source/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1560737748441},{"_id":"source/images/Grid-RCNN_fig1.png","hash":"4c257e0b6f9f569b1a60ac27af54ca440812bc9c","modified":1563618540872},{"_id":"source/images/ImprovedGAN_fig1.png","hash":"9496ef5958922774f45112291e76fc421ba33ad5","modified":1564738411112},{"_id":"source/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1561107751780},{"_id":"source/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1561782174383},{"_id":"source/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1561107868725},{"_id":"source/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1562323541605},{"_id":"source/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1562205999211},{"_id":"source/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1560737862918},{"_id":"source/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1560739283012},{"_id":"source/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1560741089754},{"_id":"source/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1560761461928},{"_id":"source/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1560776786462},{"_id":"source/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1562670075135},{"_id":"source/categories/index.md","hash":"df4e0622c971f2ed6ca0fede98457aef01367ad0","modified":1560518664346},{"_id":"source/tags/index.md","hash":"9b0cad73fc3ec06304fd78b076232fd6c4147bf8","modified":1560518657742},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"f7ddb7faed8031a9f40eae4ee7bb48c1bc50fd14","modified":1560516826131},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"046262c4b2f54b5ed8ac19b0c99aad04968e01e5","modified":1560516826132},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"1e212fe229bd659726b4a3bcf4b5b14e0310ba3a","modified":1560516826132},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"f1631b9bef922e7bc2db1e33badfad70fd88d459","modified":1560516826134},{"_id":"themes/next/.github/auto_assign.yml","hash":"9fe0dbe3f6edc59bf10ea25b14eba0e92e2c8f42","modified":1560516826135},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"e06053d417579ed967a94166deb6bda5ce41d805","modified":1560516826136},{"_id":"themes/next/.github/release-drafter.yml","hash":"d01b1e8f462af114e3934fef2ee654634d86b406","modified":1560516826137},{"_id":"themes/next/.github/config.yml","hash":"cbd06d0c40afa9fdf056765120e9085826b00d20","modified":1560516826136},{"_id":"themes/next/.github/stale.yml","hash":"85975c43d606c39b91c0ad32197154be9d482a09","modified":1560516826138},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1560516826137},{"_id":"themes/next/.github/weekly-digest.yml","hash":"6db3bcad65c3156de298f6a3ffd3ba887af4aa4f","modified":1560516826139},{"_id":"themes/next/.github/topissuebot.yml","hash":"5091c3bc6f3df303d16d853ce65a302601c1e875","modified":1560516826138},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1560516826143},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1560516826143},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1e49c08b446678336b2eacc8bf581faca969f34b","modified":1560516826143},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1560516826144},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1560516826144},{"_id":"themes/next/.github/lock.yml","hash":"4f1070097b614b24050f238694127e3573ce8472","modified":1560516826136},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"721a1aa9feed1b580ab99af8e69ed22699121e88","modified":1560516826145},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1560516826145},{"_id":"themes/next/docs/MATH.md","hash":"7d0330c250082a86897d1c96fbb4ef5df59538af","modified":1560516826146},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1560516826146},{"_id":"themes/next/languages/de.yml","hash":"79b3221344da335743b5ef5a82efa9338d64feb0","modified":1560516826153},{"_id":"themes/next/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1560516826154},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1560516826138},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1560516826154},{"_id":"themes/next/languages/fa.yml","hash":"3227072c7e1bfb16ec0517394b60632f4be921dd","modified":1560516826155},{"_id":"themes/next/languages/fr.yml","hash":"2429c90dad5bb865e3a969be2b373f19b3a77b3b","modified":1560516826155},{"_id":"themes/next/languages/es.yml","hash":"db1a9f2af477212544c830c2dd986400e26ddd6a","modified":1560516826154},{"_id":"themes/next/languages/it.yml","hash":"31eb878b53d60ff47e3e534cdd7a839c8801ac6e","modified":1560516826156},{"_id":"themes/next/languages/id.yml","hash":"f3302a4dfdc9be38a52d6e081411574b1ea01671","modified":1560516826156},{"_id":"themes/next/languages/ja.yml","hash":"3f25eca504ee5a519987b4402731f1bb7f5191c9","modified":1560516826156},{"_id":"themes/next/languages/ko.yml","hash":"75f2fe142f76bf623e34ed3570598226f55f2b8b","modified":1560516826157},{"_id":"themes/next/languages/nl.yml","hash":"08f16ce395dacc88847fc30dc6b985ce22fb8948","modified":1560516826157},{"_id":"themes/next/languages/pt.yml","hash":"ca5072c967e5eb1178ffed91827459eda6e4e6e2","modified":1560516826158},{"_id":"themes/next/languages/pt-BR.yml","hash":"c7de8b77f44e75be4f04423088a1c891537aa601","modified":1560516826157},{"_id":"themes/next/languages/vi.yml","hash":"e2f0dd7f020a36aa6b73ed4d00dcc4259a7e5e9d","modified":1560516826159},{"_id":"themes/next/languages/uk.yml","hash":"6320439c6e9ff81e5b8f8129ca16e9a744b37032","modified":1560516826159},{"_id":"themes/next/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1560516826158},{"_id":"themes/next/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1560516826158},{"_id":"themes/next/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1560516826159},{"_id":"themes/next/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1560516826160},{"_id":"themes/next/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1560516826160},{"_id":"themes/next/layout/category.swig","hash":"ad0ac6a1ff341f8eab9570e7fb443962948c5f9d","modified":1560516826193},{"_id":"themes/next/layout/archive.swig","hash":"61bc56e77e653684fc834f63dcbdadf18687c748","modified":1560516826193},{"_id":"themes/next/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1560516826194},{"_id":"themes/next/layout/post.swig","hash":"af74e97d57cf00cde6f8dbd4364f27910915454e","modified":1560516826194},{"_id":"themes/next/layout/page.swig","hash":"5d06ee8f477ffc39932d0251aa792ffcaf8faf14","modified":1560516826194},{"_id":"themes/next/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1560516826197},{"_id":"themes/next/layout/schedule.swig","hash":"e79f43df0e9a6cf48bbf00882de48c5a58080247","modified":1560516826195},{"_id":"themes/next/layout/_layout.swig","hash":"ba786b1baba49021928e2e508da53f2fd1369b3f","modified":1560516826161},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1560516826198},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1560516826266},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1560516826267},{"_id":"source/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1561700861047},{"_id":"source/images/CGAN_fig2.png","hash":"d690cf996f276f489982f47ed689683d1c48de1a","modified":1564402878126},{"_id":"source/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1563268584724},{"_id":"source/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1561511191938},{"_id":"source/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1561527456160},{"_id":"source/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1561515013760},{"_id":"source/images/GAN_alg1.png","hash":"d535c3b65a4c0c8eb331c79dc5032bf754e7084a","modified":1563970949977},{"_id":"source/images/GAN_fig2.png","hash":"86a432fab3c2df0b7e84f789971e5ced612cc2aa","modified":1564040096252},{"_id":"source/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1560737735216},{"_id":"source/images/Grid-RCNN-Plus_fig1.png","hash":"7255da83a2dadae1b8832adf87b3f60e6ed211d5","modified":1563768382258},{"_id":"source/images/Grid-RCNN_fig3.png","hash":"b5e40a1ca3df03b69c06e573086ce5af7f91eec1","modified":1563618615096},{"_id":"source/images/Grid-RCNN_fig4.png","hash":"f06e5e1250a7e59b4d04fbe316592745e65b6650","modified":1563618676537},{"_id":"source/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1561776113410},{"_id":"source/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1563448839370},{"_id":"source/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1561107716675},{"_id":"source/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1561782128055},{"_id":"source/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1562205321629},{"_id":"source/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1561338611567},{"_id":"source/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1561363621295},{"_id":"source/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1562207542722},{"_id":"source/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1562311073193},{"_id":"source/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1562323605049},{"_id":"source/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1562670019815},{"_id":"source/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1562670127461},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1560516826267},{"_id":"themes/next/layout/tag.swig","hash":"283519d4d5b67814412863a3e0212bac18bcc5a0","modified":1560516826195},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826242},{"_id":"source/images/Grid-RCNN-Plus_fig2.png","hash":"801abc90e532b83bc123e58df681d79b9fc8ad4b","modified":1563768417580},{"_id":"source/images/Grid-RCNN_fig2.png","hash":"65e1e9f69e1d3e7b1b7b8b95d5cbb1859a99e897","modified":1563618590594},{"_id":"source/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1563448770086},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"245917ffaa296bc2d9a85444acf639077ca25944","modified":1560516826133},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"59b2b45e151972bbe08582cde22f398e58832765","modified":1560516826134},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"795b8ddb251da8e2327299d5f7dbf446fb9867c6","modified":1560516826133},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"ae22e700b7c63c60746321719a20d34022ad78d9","modified":1560516826134},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1560516826147},{"_id":"themes/next/docs/ru/README.md","hash":"aeb95129ab1da9ec41786bfa86dc32c739ee6358","modified":1560516826147},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1560516826147},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1560516826148},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"aaf25d304793344e2d026062768c93005723f5c6","modified":1560516826148},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"018a259694f4a8c7c384e1f323531442cba5fbf3","modified":1560516826149},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"16d98708de86efe40ebcb02c02a01af0f160b80a","modified":1560516826149},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1560516826150},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1560516826150},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"b17fc344ff61603f83387c0f9b2b2189aae81d50","modified":1560516826151},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"db2797f161e1e7a4987cbfa3d1be682266dfbba6","modified":1560516826151},{"_id":"themes/next/docs/zh-CN/README.md","hash":"4016948fdb971e4f905efb7a5bb3add3dd58e7a8","modified":1560516826152},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1560516826152},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1560516826161},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1560516826161},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1560516826160},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"91017f58f83d9505ce99109fffdc51c032bf017e","modified":1560516826164},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"891ab67815969dd8736cb22fbbb3f791b8fff4e4","modified":1560516826163},{"_id":"themes/next/layout/_partials/comments.swig","hash":"d0b9e841d55c974d02f43823a06a2627f8e46431","modified":1560516826164},{"_id":"themes/next/layout/_partials/footer.swig","hash":"9a79dde1412b1b1473380e8b6cacfe1930ed321b","modified":1560516826164},{"_id":"themes/next/layout/_macro/post.swig","hash":"c77a7928d65bfe0fb712a2931b4cd7045666508c","modified":1560516826163},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1560516826170},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"1ad13269b43b900356f3bdab7947d6a86f035a2c","modified":1560516826165},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"c2e8f4b3a2bf991320ecc827dcdc227399ad5b51","modified":1560516826175},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"50c3ae6b50f173ae70f8c3312f7c6da1097eb9b6","modified":1560516826175},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1560516826170},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"cccd93d30787675010b1a74ef02eb5b813ec1d96","modified":1560516826174},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"8a992b7fe42b9c1a5eb9d937b0827aed91586d94","modified":1560516826178},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"efb3404a3303622f3be60944d9d1926972c5c248","modified":1560516826175},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"b9b57e1735035319e200c22cf46a38b52b4e0d9c","modified":1560516826178},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1560516826183},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"87182367d7954457cb2498bbfa9445c03c2d619e","modified":1560516826184},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"12bf51c55449d0e838f93a4aae9f6d25c0a27ba2","modified":1560516826187},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"80dfc0879866e6512cb67590a3b2d8741a66f980","modified":1560516826189},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1560516826190},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"7db4ad4a8dd5420dad2f6890f5299945df0af970","modified":1560516826189},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"4ae61c7efb16e962385bfe522a38c4d29cdcccbe","modified":1560516826190},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"87bcb495f7ddd81cc3fe2c2a886e51c08053019b","modified":1560516826183},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"8b1322a091355853db62a5aafb8886fdbd8ab56a","modified":1560516826190},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"c476dc3693a9dd0be2d136a45b0d7fdef55d4d92","modified":1560516826191},{"_id":"themes/next/scripts/filters/exturl.js","hash":"b19c7c1021e57367b3b3bbf5678381017ed5667d","modified":1560516826196},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cdb6152582313268d970ffeef99b4a8a7850f034","modified":1560516826196},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"a40ce6bc852bb4bff8b9f984fa064741dd151e96","modified":1560516826196},{"_id":"themes/next/scripts/tags/button.js","hash":"95a520f6529424a03c7ead6dbfd5e626d672febb","modified":1560516826198},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1560516826198},{"_id":"themes/next/scripts/tags/exturl.js","hash":"f9f25905adecfb8be49def4ff3b0b8bbc6955d84","modified":1560516826199},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a6b2264215c555c553b2c5db85fa90678798d0d5","modified":1560516826199},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"23d839333422375e85d44e476f554faf49973a3c","modified":1560516826199},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1560516826199},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1560516826200},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1560516826200},{"_id":"themes/next/scripts/tags/pdf.js","hash":"ab995f0fc60d60f637220e2651111b775b8a06de","modified":1560516826201},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1560516826201},{"_id":"themes/next/scripts/tags/tabs.js","hash":"72a5adbd8f300bee1d0c289367598ca06b2bed17","modified":1560516826201},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1560516826202},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1560516826243},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1560516826243},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1560516826244},{"_id":"themes/next/source/css/main.styl","hash":"5e7d28bc539e84f8b03e68df82292f7fc0f2d023","modified":1560516826242},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"882cd0b68c493af1b6d945660f9c21085e006ffc","modified":1560516826191},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1560516826244},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1560516826244},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"b44010cd577e4d063c3406772938c4b117ec7b7b","modified":1560516826192},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1560516826246},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1560516826246},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1560516826247},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1560516826247},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560516826247},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1560516826245},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560516826248},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1560516826247},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1560516826248},{"_id":"themes/next/source/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1560516826249},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1560516826248},{"_id":"themes/next/source/js/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1560516826249},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1560516826249},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1560516826246},{"_id":"themes/next/source/js/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1560516826250},{"_id":"themes/next/source/js/js.cookie.js","hash":"f11e84def0352b7dd6393f1b83e55a40ab468686","modified":1560516826250},{"_id":"themes/next/source/js/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1560516826250},{"_id":"themes/next/source/js/next-boot.js","hash":"696a0c2cf158001576d56b48195ec8e39e835b47","modified":1560516826251},{"_id":"themes/next/source/js/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1560516826252},{"_id":"themes/next/source/js/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1560516826253},{"_id":"themes/next/source/js/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1560516826253},{"_id":"source/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1561700827659},{"_id":"source/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1563268633859},{"_id":"source/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1561776076542},{"_id":"themes/next/source/js/utils.js","hash":"fed16cd4fa5fac8cb4a63633d1840792a056f2be","modified":1560516826254},{"_id":"themes/next/source/js/motion.js","hash":"d0a6d9dbcc57159e54bbb1f683b86632ae0b78f0","modified":1560516826251},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826230},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826231},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826231},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826241},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826242},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1560516826162},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"b57bf9c865bed0f22157176a8085de168a1aef77","modified":1560516826165},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"fd079a414ca0f42f4cddd00247a9d5a5f58c4d8e","modified":1560516826166},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"03f669356bbaa70144b743f3312178e1981ac3a8","modified":1560516826167},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1560516826162},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"39c4ad0e36b7c1260da98ba345f7bd72a2ac0f2e","modified":1560516826168},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1560516826167},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"39fa6780b9515bc343898ff615c858206728cc3c","modified":1560516826166},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"e015c7d9b84062b60b15b36be3ef11929dd10943","modified":1560516826168},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"2b905ddd5dea8558c3fd95aacad241da6b6800f4","modified":1560516826168},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"be6683db6a269d83bb0441d7cf74db63a240fa8a","modified":1560516826171},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"f62b801c7999da67b4bdca9c5e373b9b5ed039dc","modified":1560516826171},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"f46699a9daa5fef599733cbab35cb75cf7a05444","modified":1560516826169},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"fb7727e8ec63a58238a7206bf70eb273c8879993","modified":1560516826171},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f14e9e8c27af82f1bfe794e252dec0d7e521f503","modified":1560516826172},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1560516826172},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1560516826173},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c609097b95eb6127c2784f47f2230e6e6efc0be2","modified":1560516826173},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f331ad02beea8990066d32ad6ec9f859672c3615","modified":1560516826171},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a62c93f19429f159bcf0c2e533ffc619aa399755","modified":1560516826176},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"31245e09ce0465b994cebd94223a531585c4eab4","modified":1560516826173},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"3c548934b97cc426544947f7a2ae35c270b5e33f","modified":1560516826176},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"647e8677d1ccfb3f7918dd3ea2ff7078504a845d","modified":1560516826174},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7ef07edd2a97a3774229990d2f0a6eefa31bd015","modified":1560516826177},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cf87ab778618a32119ec1c4ac2079a51385b1913","modified":1560516826176},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"66d562b3778dbc839f7c00103bd0099c5d61602a","modified":1560516826178},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"828eb9c47f34090c841a2e9a0b3f31b0e4ccf40a","modified":1560516826177},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"83dd7df11b100bae38c9faab9a478f92149a0315","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"2e1de38f44af00209129d4051b7ae307cb11ad68","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"335005a9f8b36349f0ad0a7beeba6969c55fc7f7","modified":1560516826180},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"54b43d406cf37932e7b60f46814e864d31b1842c","modified":1560516826174},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"73576c9683d9ad9b124916dc6c660607fe7cc1fa","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"f648e5bf8c5dfc74143233976ed4ff5978deda43","modified":1560516826180},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"d68da660cd1cc8fb3ff0a81178decadb620afc11","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"39928f358dd13d9fc1a4641800e57be157ecd815","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"623e73bedef067ac24a398ef27c8197295da872d","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"4cff8bf5c42c62f7f0ac1f0d70f839dae39ba77a","modified":1560516826183},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"d18c87d7839e7407e39acd2998bcc9e0b34611b0","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a22d1ea29a5ffe46199ab7d108a291a05af8d5b6","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"71fb01bcad43bc9410ab19190373b9f7e59215b5","modified":1560516826182},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"280ff4282396beb53fb3913f58c6b5890bd1c9ef","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"4e86e1ace90a70bb8862f5e6de9dbe7bfc046bee","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"3da014b25f9ac804eda1614591706e3733c0d2c5","modified":1560516826184},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"b3818fd0b3028dadf341b6d0b180e1243683de6a","modified":1560516826186},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"9a4923d2aa5182531ea7a7fb9abe824450026208","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"bc3fc9d053b3d1fc0cd3918bf9a629a6f38f6414","modified":1560516826184},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1a5d94f5779a2ce13abc886dd78e0617f89c34b9","modified":1560516826187},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"43a20fa0e9ae2f4254f04813f9c619dd36b49ae5","modified":1560516826188},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"ea1c136f960667a0a13b334db497b9b19c41f629","modified":1560516826188},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"078bd2d5815eb23e8c5f74467dc0042babea00ae","modified":1560516826192},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"143ef265c96a8ea2fb93c36c5ffb9c5e940f7693","modified":1560516826191},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1560516826230},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"767ba29f258db5d2e5baf875a6f36ac1d44df6a3","modified":1560516826189},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1560516826230},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"1b72c755101c9dfb85da13df9a0abccf37cd1dd2","modified":1560516826187},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"1aabac9e37a8f4451c86d09037b3a1f8b30eaf5e","modified":1560516826231},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"21a14a19149f1cb8e011c477f29dd1352675605b","modified":1560516826231},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1560516826241},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e9b0752f08398709e787546a246baca12b4c557f","modified":1560516826241},{"_id":"themes/next/source/js/schemes/muse.js","hash":"ccc0c5cd4ec6f8159c98990ad83f11a5c0b0234c","modified":1560516826252},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"da7049f3d9a157abe0ecc62611edcf43605ba84d","modified":1560516826241},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1560516826254},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"d45ca53af17d1d83fd27f8ed0917a72f0060e1a9","modified":1560516826192},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1560516826255},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"3eea56cc9ce47bb4760930c4c69cebf847a7fbb2","modified":1560516826253},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1560516826255},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1560516826255},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1560516826255},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1560516826266},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1560516826266},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ebc95eeb8966d17cdc7dd0de009deaef1fe65064","modified":1560516826242},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1560516826265},{"_id":"themes/next/source/lib/jquery/index.js","hash":"b15f7cfa79519756dff1ad22553fd0ed09024343","modified":1560516826262},{"_id":"source/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1562554796933},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1560516826203},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"5c0ca7f801859cff254d2f5b7d1a70d66ff61a8d","modified":1560516826202},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"463817cbbd804ce134cb3e7e721431cb0e1616f2","modified":1560516826203},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"9fd526db0527c71243f05e18086f937dc67b1c3e","modified":1560516826203},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"a4c6ee546a94fd69e5b7a1e4c054ab8cacb73d2a","modified":1560516826211},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"cfa64bd8ee2ff9f943673e339d69341e76fbf031","modified":1560516826216},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"997058180065d986e05df72992cc2cbfd7febd7e","modified":1560516826204},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"afdd21533db18d846e1a2663b1199761b1bd2c1e","modified":1560516826216},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"1a4ac0d119f2126ef8951897338706edce112235","modified":1560516826227},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"fec36a14080104b5862e9f021eab117d87c5f7c5","modified":1560516826229},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"35c6fd7eab3779bd9e38b7ba8825ab0c67a1be7a","modified":1560516826228},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"40144394fdfe05d400f39f6763f66f75479a2e34","modified":1560516826228},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"107f42aa590ec4ba0765a0bc5d735f0f09edc0ff","modified":1560516826230},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9c6194177533619a6f6685dc7e890dcbec456241","modified":1560516826229},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"a609ff811f2b2764f5470236fe2fb1f3aa6ccba5","modified":1560516826232},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1560516826233},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"8da8416213127595dfc4d2b358639194647e7bd3","modified":1560516826233},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1560516826229},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"06d9d00257abd28414ec0b746f866bf9911cf5ec","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"40f266e24af4dedc9497056ab18ebcfda38dd47d","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"04706657af638f2746ae59520e6fc78577c7682c","modified":1560516826236},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"25f05ed8da68d034dce7f06e0f20f6cd55841070","modified":1560516826235},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1560516826233},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"157e6915dcf5990566e463acffa71043b2651c07","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1560516826238},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"b9619c9827f969ca2e2f5878552362a7b858918f","modified":1560516826239},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0b3001909f3446843b226030524ea8498d4d8997","modified":1560516826239},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"234b44cfd03f9c9e3e179ff5fd698ac876341913","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"5b5e0a02a7bf63de9efcd33a4e482939cce5822d","modified":1560516826240},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"adb7379f3b9001840eb38b260434e89365771a81","modified":1560516826240},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1560516826257},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1560516826256},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"e73d6da74c5755442e831d8fd7d922c5b32bd892","modified":1560516826239},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0d6f0df798449b710e1e5dbd43d470089b2a3c95","modified":1560516826240},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"a8f4d4b86acaa34c99111b2dde5d0779cc7e0de6","modified":1560516826205},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"2df409df04fdb52d7234876a9f6e502edd4e3929","modified":1560516826205},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"33200f60bd6a8bbfc66dd49a239bcc75c2f564c1","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"c9cfb4b99e1ec8ec9cf075cb761b8f7fa5fe63fd","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"3a0efe849587b34f20d4e260028dc799215b0bb3","modified":1560516826207},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"b8647d6140141b0a160607f6353e4d4594cca92e","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"e5a5f8747fdf2ca960e4e73c081b8952afd62224","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"9c1a082e6c1f96187a099c3f4cb5424c0c9fd06e","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"9a8fb61bd2d184de9d206e62ba8961d1845c5669","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"024e8ff40ca881c6fbf45712897e22f58a3811ab","modified":1560516826207},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fa1cea6fcc3f552d57cc7d28380a304859139bf6","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1560516826211},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"c27527cdeb9e3a9f447f7238f442a5dc33fde4e6","modified":1560516826210},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"c97c819a65f6967485184399397601e5133deda6","modified":1560516826210},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"f3b0d259e991ac86454ae5eac6bc94dc8691d8c9","modified":1560516826211},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"fc94dd09b4245143b452d6cf2fc4c12134d99d6d","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"f14cefc99309934d4103a3aa785e1258d858813f","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a73346f999b31355075cd58637946a8950cf6f7e","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"13d365ee626c01f17ec664b3f54f51d8b9ee7cf4","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"b6a241626783d2ac115d683fd59ec283af68e5bb","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"4aad8e36178faaa71a767af0084d578df4c09f73","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d77f85d3af2d7090d84b28ab01c6a49f92eec647","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ccd0b1309acff0c676fdcc848a8ae2d05f0369ab","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"539fc0880b2e035e8316d5d4b423703195c1b7ba","modified":1560516826215},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1560516826257},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"981795aad232c8bd3f52a0ed8720db696d18a234","modified":1560516826215},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"c8009fd9598a661b7d23158b5121b6ac266939e9","modified":1560516826215},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"a5484d8436b2b7862faf6e7309a9e7b88cdd0027","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"615fca7dff197a2ca3df674cf963ce70b8525985","modified":1560516826216},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"ab1776c5dc537beabb0ab81a0f04e08bebad070b","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"03a4e75e963e3e7cc393d588b1495a88d52e0e40","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"b36eea093bd4b32056b5de6f370ff57e50b25a49","modified":1560516826218},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"e58bb8b7127aa21e8260493a425ec00fcb25d338","modified":1560516826218},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"0eadef0381f696de7f88d7dc5f0ddc3cd5d309b3","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"7e2ba73073daaea0a18c3d67ff137dd683af7011","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"9204c79c05d620ecd5d411cdf11e27441b6281dc","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"ed3a2960ebce7396d1893bb8e08c99c7d9259140","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"fde59300ec38868676ff5ed495b9dc9b02d07ffc","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"3cb387fa70017f3c24a1a1884461d29deda54585","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"b43421291bf85b589e8d0ec853e238d36ab80631","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"3baeeb51cfe123e99235ee1816d0e1f6a97c7852","modified":1560516826222},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"da7a21f5a2f7dcf4c5a4788d7670159ca4132b65","modified":1560516826223},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"1df9d36e2b0e9c94e0a959acc136026405ae0d73","modified":1560516826223},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"d9c244b1c3a09a7fccd3c3f732e6fb112a8cd565","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"4305813408a1cd6aba764a7769b94b081d383d4f","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"2a1008f1044b450b806adc166754ba9513e68375","modified":1560516826225},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"2fbe52f955da41c7a14eb09918bf86a252e4504f","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"6880467b4f6d7b057fb8291aa10966429a0a3bff","modified":1560516826226},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"ed8a12982c0497eeb9d7642781abeb801428f83d","modified":1560516826225},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"9801977a23268e36c5deefd270423f6f1a0c3bb2","modified":1560516826226},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"35dc9f3990fadff3ea038d4e8ac75923219886ed","modified":1560516826226},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1560516826235},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1560516826261},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1560516826264},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1560516826236},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1560516826238},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"7cf42f96ba6b249c75e00dad251ebacf7de61e6c","modified":1560516826227},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1560516826261},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"020fac447d7a17c03e2802f0f724ae0738088354","modified":1560516826222},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1560516826260},{"_id":"public/about/index.html","hash":"c53cb0b48e84c0c04d2da6b42341992c12fa060a","modified":1567167730939},{"_id":"public/tags/index.html","hash":"cc56ac68b303a4bd2d83a363462bb01e52e91eed","modified":1567167730948},{"_id":"public/categories/index.html","hash":"c8fd1cb7bb002c7c46c6d53fb04a9985fc5bcce0","modified":1567167730949},{"_id":"public/2019/07/25/WGAN/index.html","hash":"d9fcf5b86ce94d43e6fa97d60d8b1f780d896f66","modified":1567167730950},{"_id":"public/2019/07/11/cpp-aux-tools/index.html","hash":"a6c996e84d24ac9ff0589a05aad42fabfd4d768d","modified":1567167730950},{"_id":"public/categories/DL-Framework/index.html","hash":"121036ace414fe5a3a728b189b33e21ca0824140","modified":1567167730950},{"_id":"public/categories/math/index.html","hash":"210dcc45065e9fe7a9c65bcc54746ebbd0390dba","modified":1567167730950},{"_id":"public/archives/2019/08/index.html","hash":"d9910afe33589f0f5f99b56d289cfb5f1254f210","modified":1567167730950},{"_id":"public/tags/GAN/index.html","hash":"51093f65c8a25271ffc97f2f580b55037d3acac6","modified":1567167730950},{"_id":"public/tags/object-detection/page/2/index.html","hash":"fdc1b03a38f358238f3d357d6cf9e031dcd4010c","modified":1567167730950},{"_id":"public/tags/math/index.html","hash":"0911fe943d932446bcb8613c9e9d162e68d2a29d","modified":1567167730950},{"_id":"public/tags/DP/index.html","hash":"32c757d7bd9770b5f2c7e4c6df5544d9d003014e","modified":1567167730950},{"_id":"public/tags/tool/index.html","hash":"622bfe9235e5aba267cd755954a87c73b2845ee8","modified":1567167730951},{"_id":"public/tags/PyTorch/index.html","hash":"a508fc8a1b520ddfecae20d1fbea44746bcba7d9","modified":1567167730951},{"_id":"public/tags/c/index.html","hash":"564fac773f3224766d712611b6b43ff3b64df2ea","modified":1567167730951},{"_id":"public/tags/CV/index.html","hash":"f51af1b4365fca9d4dc84dede6eb7122e0577499","modified":1567167730951},{"_id":"public/2019/08/22/PyTorch-4/index.html","hash":"e4e9b72c33d0bac63d109a2ea0e442efbe9322d7","modified":1567167730951},{"_id":"public/2019/08/14/DP2/index.html","hash":"2cad60af86a7c9b0a78710665f19d5b524121715","modified":1567167730951},{"_id":"public/2019/08/07/DP1/index.html","hash":"e55c945d04b29e3bf67d70c1d5e026349c0cfadd","modified":1567167730951},{"_id":"public/2019/08/01/ImprovedGAN/index.html","hash":"89e75220834d8b16730273c8858a04a215306ee2","modified":1567167730951},{"_id":"public/2019/07/29/CGAN/index.html","hash":"b6fd39b1782da467086c04d0dc8156f4ecae697e","modified":1567167730951},{"_id":"public/2019/07/23/GAN/index.html","hash":"1c73135231d28024c2a7241c02ba9ceb2b7b7083","modified":1567167730952},{"_id":"public/2019/07/19/Grid-RCNN/index.html","hash":"86c0e72c9b5f36b39e4972991541524829294540","modified":1567167730952},{"_id":"public/2019/07/17/RepPoints/index.html","hash":"d75f42b07a02b1ab01cb4c50249eb8ea169d4aea","modified":1567167730952},{"_id":"public/2019/07/17/DetNet/index.html","hash":"59953516016d2fc0e3b6d3572b51ca5706902a5c","modified":1567167730952},{"_id":"public/2019/07/16/loss/index.html","hash":"6e048325a0c402226d3d1c7c04ba5a67a80c1aaa","modified":1567167730952},{"_id":"public/2019/07/15/DeRPN/index.html","hash":"4fc9f3c1f317536daf0a5f6b98397de6da50c46e","modified":1567167730952},{"_id":"public/2019/07/08/mask-rcnn/index.html","hash":"837f875d8bc0cc2dadbaf9ec70db81b277d6d86a","modified":1567167730952},{"_id":"public/2019/07/08/DSOD/index.html","hash":"c22451841963b99d5c2b1cafc24df28e84202f47","modified":1567167730953},{"_id":"public/2019/07/03/libra-rcnn/index.html","hash":"80768620efae0e81a6293b1194f3492a0604c0e5","modified":1567167730953},{"_id":"public/2019/07/02/gcc-src/index.html","hash":"958b2168e778be64e753094709c607fa10492a4f","modified":1567167730953},{"_id":"public/2019/06/28/M2Det/index.html","hash":"acd8ba6f2fd3a5229bd735d99b1d50866e559db3","modified":1567167730953},{"_id":"public/2019/06/28/BBox-Reg-Uncertainty/index.html","hash":"fe5b98f0103bd8e121dadc369d06f18e3942912c","modified":1567167730953},{"_id":"public/2019/06/27/FSAF/index.html","hash":"8feba3276c0a365d8d6c467ef74f2785c9cb4d16","modified":1567167730953},{"_id":"public/2019/06/25/GA-RPN/index.html","hash":"372116a3edc32c8af79b6ecffed38f9fcb354400","modified":1567167730953},{"_id":"public/2019/06/24/cv-mtds/index.html","hash":"60518c671b29fcd953681d070adba48fd2b6b9eb","modified":1567167730953},{"_id":"public/2019/06/21/TridentNet/index.html","hash":"062d050ed6b2c03b6e1efae5c8d5a22ee6798551","modified":1567167730954},{"_id":"public/2019/06/18/PyTorch-3/index.html","hash":"1e2ac92992432700095015f3671804855fc60505","modified":1567167730954},{"_id":"public/2019/06/16/mAP/index.html","hash":"2d3a81c3ac2a3c7d775a11c3f03e13eaccdad9ee","modified":1567167730954},{"_id":"public/2019/06/13/GIoU/index.html","hash":"410d80ea896a9e068467c778682818793a0abe1c","modified":1567167730955},{"_id":"public/2019/06/13/PyTorch-2/index.html","hash":"9b98f6cbf6e86c475b9e03955e598b8616d79fa8","modified":1567167730955},{"_id":"public/2019/06/13/Hexo-Sync/index.html","hash":"7afa6cbd24a68187975feb0a58d08327d9ab5785","modified":1567167730955},{"_id":"public/2019/06/12/PyTorch-1/index.html","hash":"794b580e2d688a313cecfa10746f9c03b62fba07","modified":1567167730955},{"_id":"public/index.html","hash":"bd0939fd1a76f6b88e960314465b7f31d07c4de7","modified":1567167730955},{"_id":"public/page/2/index.html","hash":"67d84176700e95fa992d64aa746c38dd997c1ea2","modified":1567167730955},{"_id":"public/page/3/index.html","hash":"a24570215c3c87a2629e80784c8114c3695fb967","modified":1567167730955},{"_id":"public/archives/index.html","hash":"bb0fd4e40aff736b0ddab7dbdc832c93da251977","modified":1567167730955},{"_id":"public/archives/page/2/index.html","hash":"e17e49c25578a9df0a6073b81f31b5e59b12f710","modified":1567167730955},{"_id":"public/archives/2019/index.html","hash":"7c3f4f2a27132f4ba330ee19503755c85446278e","modified":1567167730955},{"_id":"public/archives/2019/page/2/index.html","hash":"223f775c46056d622cc5754759b4d285fee84049","modified":1567167730955},{"_id":"public/archives/2019/06/index.html","hash":"97c1a33011fcea4772182761adc7a8b5fc6b3478","modified":1567167730955},{"_id":"public/archives/2019/07/index.html","hash":"a572b401a2a7f8e08dd534563e678bac12ca41ca","modified":1567167730956},{"_id":"public/tags/object-detection/index.html","hash":"1842cbf61a8b5c64ab231fa73e735625e6ae377e","modified":1567167730956},{"_id":"public/page/4/index.html","hash":"c4dcead8fa393d98ddfc56998457057e74a4527e","modified":1567167730967},{"_id":"public/2019/08/27/DP3/index.html","hash":"96052503f24e78e1837b0fc987be0a362bb0cc2a","modified":1567167730967},{"_id":"public/2019/08/27/PyTorch-5/index.html","hash":"6f017ce88f9fbb1a561276ea481c60dcf856b416","modified":1567167730967},{"_id":"public/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1567167730979},{"_id":"public/images/CGAN_fig1.png","hash":"94d2eb94087c072188200f54dfccb665a4bb0bc3","modified":1567167730979},{"_id":"public/images/DP1_fig1.png","hash":"ed9dd7cbecff94be6778831a494b1295482bfe3e","modified":1567167730979},{"_id":"public/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1567167730979},{"_id":"public/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1567167730979},{"_id":"public/images/GAN_fig1.png","hash":"c6f2c668d1b4d98d4bd7fdc27851faed5c7ffe9c","modified":1567167730979},{"_id":"public/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1567167730979},{"_id":"public/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1567167730979},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1567167730979},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1567167730979},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1567167730979},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1567167730979},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1567167730979},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1567167730979},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1567167730980},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1567167730980},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1567167730980},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1567167730980},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1567167730980},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1567167730980},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1567167730980},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1567167730980},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1567167730980},{"_id":"public/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1567167730980},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1567167730980},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1567167730980},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1567167730980},{"_id":"public/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1567167731452},{"_id":"public/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1567167731452},{"_id":"public/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1567167731453},{"_id":"public/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1567167731453},{"_id":"public/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1567167731453},{"_id":"public/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1567167731453},{"_id":"public/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1567167731453},{"_id":"public/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1567167731453},{"_id":"public/images/Grid-RCNN_fig1.png","hash":"4c257e0b6f9f569b1a60ac27af54ca440812bc9c","modified":1567167731453},{"_id":"public/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1567167731453},{"_id":"public/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1567167731453},{"_id":"public/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1567167731453},{"_id":"public/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1567167731453},{"_id":"public/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1567167731454},{"_id":"public/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1567167731454},{"_id":"public/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1567167731454},{"_id":"public/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1567167731454},{"_id":"public/images/CGAN_fig2.png","hash":"d690cf996f276f489982f47ed689683d1c48de1a","modified":1567167731454},{"_id":"public/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1567167731454},{"_id":"public/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1567167731454},{"_id":"public/images/GAN_alg1.png","hash":"d535c3b65a4c0c8eb331c79dc5032bf754e7084a","modified":1567167731454},{"_id":"public/images/GAN_fig2.png","hash":"86a432fab3c2df0b7e84f789971e5ced612cc2aa","modified":1567167731454},{"_id":"public/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1567167731454},{"_id":"public/images/Grid-RCNN-Plus_fig1.png","hash":"7255da83a2dadae1b8832adf87b3f60e6ed211d5","modified":1567167731454},{"_id":"public/images/Grid-RCNN_fig3.png","hash":"b5e40a1ca3df03b69c06e573086ce5af7f91eec1","modified":1567167731454},{"_id":"public/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1567167731455},{"_id":"public/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1567167731455},{"_id":"public/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1567167731455},{"_id":"public/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1567167731455},{"_id":"public/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1567167731455},{"_id":"public/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1567167731455},{"_id":"public/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1567167731455},{"_id":"public/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1567167731455},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1567167731455},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1567167731455},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1567167731455},{"_id":"public/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1567167731461},{"_id":"public/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1567167731461},{"_id":"public/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1567167731462},{"_id":"public/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1567167731462},{"_id":"public/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1567167731462},{"_id":"public/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1567167731462},{"_id":"public/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1567167731462},{"_id":"public/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1567167731462},{"_id":"public/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1567167731462},{"_id":"public/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1567167731462},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1567167731462},{"_id":"public/css/main.css","hash":"36076f3b22c0429f0e619a8d512344cdb20c45de","modified":1567167731462},{"_id":"public/images/DP2_fig1.png","hash":"a90b55aa55d0cfdd147c5b00feaf432300f2854b","modified":1567167731463},{"_id":"public/images/ImprovedGAN_fig1.png","hash":"9496ef5958922774f45112291e76fc421ba33ad5","modified":1567167731463},{"_id":"public/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1567167731463},{"_id":"public/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1567167731464},{"_id":"public/images/Grid-RCNN_fig4.png","hash":"f06e5e1250a7e59b4d04fbe316592745e65b6650","modified":1567167731464},{"_id":"public/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1567167731464},{"_id":"public/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1567167731464},{"_id":"public/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1567167731464},{"_id":"public/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1567167731464},{"_id":"public/images/Grid-RCNN-Plus_fig2.png","hash":"801abc90e532b83bc123e58df681d79b9fc8ad4b","modified":1567167731464},{"_id":"public/images/Grid-RCNN_fig2.png","hash":"65e1e9f69e1d3e7b1b7b8b95d5cbb1859a99e897","modified":1567167731464},{"_id":"public/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1567167731464},{"_id":"public/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1567167731465},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1567167731465},{"_id":"public/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1567167731468},{"_id":"public/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1567167731468},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1567167731468},{"_id":"public/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1567167731468},{"_id":"public/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1567167731472},{"_id":"public/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1567167731472},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1567167731476},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1567167731482},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1567167731482},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1567167731482},{"_id":"public/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1567167731485},{"_id":"public/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1567167731489},{"_id":"public/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1567167731490},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1567167731493}],"Category":[{"name":"DL Framework","_id":"cjzy37966000cdgvc242rnacc"},{"name":"math","_id":"cjzy379f4001zdgvchfwipm58"}],"Data":[],"Page":[{"title":"about","date":"2019-07-23T07:12:21.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2019-07-23 15:12:21\n---\n","updated":"2019-07-23T07:12:21.258Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjzy3795r0001dgvcvcwrl9bn","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2019-06-14T13:23:22.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: \ndate: 2019-06-14 21:23:22\ntype: tags\n---\n","updated":"2019-06-14T13:24:17.742Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjzy3795u0003dgvczg0bnjt5","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2019-06-14T13:23:43.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: \ndate: 2019-06-14 21:23:43\ntype: categories\n---\n","updated":"2019-06-14T13:24:24.346Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjzy3795y0006dgvcsthy4bfw","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"CGAN/DCGAN","date":"2019-07-29T09:00:43.000Z","mathjax":true,"_content":"# CGAN\n [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n\n [GAN](2019/07/23/GAN)  GAN  G  D  G  mnist  G G  1GAN GAN  G  CGAN \n\n## Conditional Adversarial Nets\nGAN  G  D  y  CGAN y  mnist  y\n\n z   y  G  GAN  x  y  D G  D  GAN  fc  conv/deconv\n\n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$\n\n 1  CGAN \n![](/images/CGAN_fig1.png)\n\n\n```python\nimport tensorflow as tf\ny_dim=10    # one-hot vector for mnist-label\nz_dim=100   # length of noise input vector\ny=tf.placeholder(tf.float32, shape=[None,y_dim], name='label')\nx=tf.placeholder(tf.float32, shape=[None,28,28,1], name='real_img')\nz=tf.placeholder(tf.float32, shape=[None,z_dim], name='noise')\n\n# G  noise  label  vector  100  110\nx_for_g=tf.concat([z,y], axis=1)    # [batch_size, 100+10]\n#  GAN  G \n\n# D  real_img  label \nnew_y=tf.reshape(y,[batch_size,1,1,y_dim])\nnew_y=new_y*tf.ones([batch_size,28,28,y_dim])   # [batch_size,28,28,10]\nx_for_d=tf.concat([x,new_y],axis=-1)    # [batch_size,28,28,1+10]\n#  GAN  D \n```\n\n## \n### Unimodal\n mnist  y  10  one-hot CGAN  2 \n![](/images/CGAN_fig2.png)\n\n### Multimodal\n Flickr  UGMuser-generated metadataUGM word embedding\n\n CGAN  tag-vectors  AlexNet  ImageNet  fc  4096  YFCC100M  metadata  user-tagstitle  descriptions  skip-gram  200  200  247465 G  tag  y  4096 \n\n MIR Flickr 25000 AlexNetskip-gram tag  15000  tag  tag tag \n\nevaluation  100 tag  20  100  2000  top 10  tags\n\n# DCGAN\n [Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n\nBN  ReLU  GAN  github  [DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)","source":"_posts/CGAN.md","raw":"---\ntitle: CGAN/DCGAN\ndate: 2019-07-29 17:00:43\ntags: GAN\nmathjax: true\n---\n# CGAN\n [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n\n [GAN](2019/07/23/GAN)  GAN  G  D  G  mnist  G G  1GAN GAN  G  CGAN \n\n## Conditional Adversarial Nets\nGAN  G  D  y  CGAN y  mnist  y\n\n z   y  G  GAN  x  y  D G  D  GAN  fc  conv/deconv\n\n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$\n\n 1  CGAN \n![](/images/CGAN_fig1.png)\n\n\n```python\nimport tensorflow as tf\ny_dim=10    # one-hot vector for mnist-label\nz_dim=100   # length of noise input vector\ny=tf.placeholder(tf.float32, shape=[None,y_dim], name='label')\nx=tf.placeholder(tf.float32, shape=[None,28,28,1], name='real_img')\nz=tf.placeholder(tf.float32, shape=[None,z_dim], name='noise')\n\n# G  noise  label  vector  100  110\nx_for_g=tf.concat([z,y], axis=1)    # [batch_size, 100+10]\n#  GAN  G \n\n# D  real_img  label \nnew_y=tf.reshape(y,[batch_size,1,1,y_dim])\nnew_y=new_y*tf.ones([batch_size,28,28,y_dim])   # [batch_size,28,28,10]\nx_for_d=tf.concat([x,new_y],axis=-1)    # [batch_size,28,28,1+10]\n#  GAN  D \n```\n\n## \n### Unimodal\n mnist  y  10  one-hot CGAN  2 \n![](/images/CGAN_fig2.png)\n\n### Multimodal\n Flickr  UGMuser-generated metadataUGM word embedding\n\n CGAN  tag-vectors  AlexNet  ImageNet  fc  4096  YFCC100M  metadata  user-tagstitle  descriptions  skip-gram  200  200  247465 G  tag  y  4096 \n\n MIR Flickr 25000 AlexNetskip-gram tag  15000  tag  tag tag \n\nevaluation  100 tag  20  100  2000  top 10  tags\n\n# DCGAN\n [Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n\nBN  ReLU  GAN  github  [DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)","slug":"CGAN","published":1,"updated":"2019-07-31T09:47:17.246Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy3795l0000dgvcowedm8pb","content":"<h1 id=\"CGAN\"><a href=\"#CGAN\" class=\"headerlink\" title=\"CGAN\"></a>CGAN</h1><p> <a href=\"https://arxiv.org/abs/1411.1784\" target=\"_blank\" rel=\"noopener\">Conditional Generative Adversarial Nets</a></p>\n<p> <a href=\"2019/07/23/GAN\">GAN</a>  GAN  G  D  G  mnist  G G  1GAN GAN  G  CGAN </p>\n<h2 id=\"Conditional-Adversarial-Nets\"><a href=\"#Conditional-Adversarial-Nets\" class=\"headerlink\" title=\"Conditional Adversarial Nets\"></a>Conditional Adversarial Nets</h2><p>GAN  G  D  y  CGAN y  mnist  y</p>\n<p> z   y  G  GAN  x  y  D G  D  GAN  fc  conv/deconv</p>\n<p><br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$</p>\n<p> 1  CGAN <br><img src=\"/images/CGAN_fig1.png\" alt></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\">y_dim=<span class=\"number\">10</span>    <span class=\"comment\"># one-hot vector for mnist-label</span></span><br><span class=\"line\">z_dim=<span class=\"number\">100</span>   <span class=\"comment\"># length of noise input vector</span></span><br><span class=\"line\">y=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,y_dim], name=<span class=\"string\">'label'</span>)</span><br><span class=\"line\">x=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>], name=<span class=\"string\">'real_img'</span>)</span><br><span class=\"line\">z=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,z_dim], name=<span class=\"string\">'noise'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># G  noise  label  vector  100  110</span></span><br><span class=\"line\">x_for_g=tf.concat([z,y], axis=<span class=\"number\">1</span>)    <span class=\"comment\"># [batch_size, 100+10]</span></span><br><span class=\"line\"><span class=\"comment\">#  GAN  G </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># D  real_img  label </span></span><br><span class=\"line\">new_y=tf.reshape(y,[batch_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>,y_dim])</span><br><span class=\"line\">new_y=new_y*tf.ones([batch_size,<span class=\"number\">28</span>,<span class=\"number\">28</span>,y_dim])   <span class=\"comment\"># [batch_size,28,28,10]</span></span><br><span class=\"line\">x_for_d=tf.concat([x,new_y],axis=<span class=\"number\">-1</span>)    <span class=\"comment\"># [batch_size,28,28,1+10]</span></span><br><span class=\"line\"><span class=\"comment\">#  GAN  D </span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Unimodal\"><a href=\"#Unimodal\" class=\"headerlink\" title=\"Unimodal\"></a>Unimodal</h3><p> mnist  y  10  one-hot CGAN  2 <br><img src=\"/images/CGAN_fig2.png\" alt></p>\n<h3 id=\"Multimodal\"><a href=\"#Multimodal\" class=\"headerlink\" title=\"Multimodal\"></a>Multimodal</h3><p> Flickr  UGMuser-generated metadataUGM word embedding</p>\n<p> CGAN  tag-vectors  AlexNet  ImageNet  fc  4096  YFCC100M  metadata  user-tagstitle  descriptions  skip-gram  200  200  247465 G  tag  y  4096 </p>\n<p> MIR Flickr 25000 AlexNetskip-gram tag  15000  tag  tag tag </p>\n<p>evaluation  100 tag  20  100  2000  top 10  tags</p>\n<h1 id=\"DCGAN\"><a href=\"#DCGAN\" class=\"headerlink\" title=\"DCGAN\"></a>DCGAN</h1><p> <a href=\"https://arxiv.org/abs/1511.06434\" target=\"_blank\" rel=\"noopener\">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks</a></p>\n<p>BN  ReLU  GAN  github  <a href=\"https://github.com/carpedm20/DCGAN-tensorflow\" target=\"_blank\" rel=\"noopener\">DCGAN-tensorflow</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"CGAN\"><a href=\"#CGAN\" class=\"headerlink\" title=\"CGAN\"></a>CGAN</h1><p> <a href=\"https://arxiv.org/abs/1411.1784\" target=\"_blank\" rel=\"noopener\">Conditional Generative Adversarial Nets</a></p>\n<p> <a href=\"2019/07/23/GAN\">GAN</a>  GAN  G  D  G  mnist  G G  1GAN GAN  G  CGAN </p>\n<h2 id=\"Conditional-Adversarial-Nets\"><a href=\"#Conditional-Adversarial-Nets\" class=\"headerlink\" title=\"Conditional Adversarial Nets\"></a>Conditional Adversarial Nets</h2><p>GAN  G  D  y  CGAN y  mnist  y</p>\n<p> z   y  G  GAN  x  y  D G  D  GAN  fc  conv/deconv</p>\n<p><br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)} [\\log D(x|y)] + \\Bbb E_{z \\sim p_z(z)}[1-\\log (1-D(G(z|y)))]$$</p>\n<p> 1  CGAN <br><img src=\"/images/CGAN_fig1.png\" alt></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\">y_dim=<span class=\"number\">10</span>    <span class=\"comment\"># one-hot vector for mnist-label</span></span><br><span class=\"line\">z_dim=<span class=\"number\">100</span>   <span class=\"comment\"># length of noise input vector</span></span><br><span class=\"line\">y=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,y_dim], name=<span class=\"string\">'label'</span>)</span><br><span class=\"line\">x=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>], name=<span class=\"string\">'real_img'</span>)</span><br><span class=\"line\">z=tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>,z_dim], name=<span class=\"string\">'noise'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># G  noise  label  vector  100  110</span></span><br><span class=\"line\">x_for_g=tf.concat([z,y], axis=<span class=\"number\">1</span>)    <span class=\"comment\"># [batch_size, 100+10]</span></span><br><span class=\"line\"><span class=\"comment\">#  GAN  G </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># D  real_img  label </span></span><br><span class=\"line\">new_y=tf.reshape(y,[batch_size,<span class=\"number\">1</span>,<span class=\"number\">1</span>,y_dim])</span><br><span class=\"line\">new_y=new_y*tf.ones([batch_size,<span class=\"number\">28</span>,<span class=\"number\">28</span>,y_dim])   <span class=\"comment\"># [batch_size,28,28,10]</span></span><br><span class=\"line\">x_for_d=tf.concat([x,new_y],axis=<span class=\"number\">-1</span>)    <span class=\"comment\"># [batch_size,28,28,1+10]</span></span><br><span class=\"line\"><span class=\"comment\">#  GAN  D </span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Unimodal\"><a href=\"#Unimodal\" class=\"headerlink\" title=\"Unimodal\"></a>Unimodal</h3><p> mnist  y  10  one-hot CGAN  2 <br><img src=\"/images/CGAN_fig2.png\" alt></p>\n<h3 id=\"Multimodal\"><a href=\"#Multimodal\" class=\"headerlink\" title=\"Multimodal\"></a>Multimodal</h3><p> Flickr  UGMuser-generated metadataUGM word embedding</p>\n<p> CGAN  tag-vectors  AlexNet  ImageNet  fc  4096  YFCC100M  metadata  user-tagstitle  descriptions  skip-gram  200  200  247465 G  tag  y  4096 </p>\n<p> MIR Flickr 25000 AlexNetskip-gram tag  15000  tag  tag tag </p>\n<p>evaluation  100 tag  20  100  2000  top 10  tags</p>\n<h1 id=\"DCGAN\"><a href=\"#DCGAN\" class=\"headerlink\" title=\"DCGAN\"></a>DCGAN</h1><p> <a href=\"https://arxiv.org/abs/1511.06434\" target=\"_blank\" rel=\"noopener\">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks</a></p>\n<p>BN  ReLU  GAN  github  <a href=\"https://github.com/carpedm20/DCGAN-tensorflow\" target=\"_blank\" rel=\"noopener\">DCGAN-tensorflow</a></p>\n"},{"title":"DetNet","date":"2019-07-17T02:05:50.000Z","mathjax":true,"_content":" [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n\n ImageNet  finetune  backbone  backbone  RF FPN  RetinaNet extra stage DetNet backbone\n\nDetNet  FPN extra stage FPN  ImageNet DetNet  dilated bottleneck \n\n# DetNet\n 1(A)  FPN \n![](/images/DetNet_fig1.png)<center>A.  backbone  FPN B.  backboneC. DetNet  backbone FPN </center>\n\n1.  stage  5  stages stage  2 32  FPN  stages P6  RetinaNet  P6  P7\n2.  32  feature map FPN  layer \n3.  FPN  layer  layer  layer  layer  1 A  layer  context \n\nDetNet \n1.  stage \n2.  stage  6~7  stage deep layer\n\n## DetNet \n ResNet-50  baseline ResNet-50  DetNet-59 ResNet-101  DetNetDetNet  stage 1,2,3,4  ResNet-50  stage 1,2,3,4  ResNet-50  stage \n\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1 & 64 \\\\\\\\ 3 \\times 3 & 64 \\\\\\\\ 1 \\times 1 & 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1 & 128 \\\\\\\\ 3 \\times 3 & 128 \\\\\\\\ 1 \\times 1 & 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1 & 256 \\\\\\\\ 3 \\times 3 & 256 \\\\\\\\ 1 \\times 1 & 1024\\end{bmatrix} \\times 6$|\n\n stage  DetNet 2 D DetNet-59 \n![](/images/DetNet_fig2.png)<center>fig 2. DetNet </center>\n\n1.  backbone  extra stage P6 FPN  stage 4  16 stage \n2.  stage 4  dilated bottleneck  1x1  stage  2 B\n3. bottleneck  dilated conv  dilated conv  stage 5  6  channel  stage 4 256 backbone  backbone  stage  channel  stage  ResNet-50  64->128->256->512\n\nDetNet  backbone / feature pyramid  FPN  backbone  FPN  stage 4  stage  stage 4,5,6  2 E\n\n# \n\n","source":"_posts/DetNet.md","raw":"---\ntitle: DetNet\ndate: 2019-07-17 10:05:50\ntags: object detection\nmathjax: true\n---\n [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n\n ImageNet  finetune  backbone  backbone  RF FPN  RetinaNet extra stage DetNet backbone\n\nDetNet  FPN extra stage FPN  ImageNet DetNet  dilated bottleneck \n\n# DetNet\n 1(A)  FPN \n![](/images/DetNet_fig1.png)<center>A.  backbone  FPN B.  backboneC. DetNet  backbone FPN </center>\n\n1.  stage  5  stages stage  2 32  FPN  stages P6  RetinaNet  P6  P7\n2.  32  feature map FPN  layer \n3.  FPN  layer  layer  layer  layer  1 A  layer  context \n\nDetNet \n1.  stage \n2.  stage  6~7  stage deep layer\n\n## DetNet \n ResNet-50  baseline ResNet-50  DetNet-59 ResNet-101  DetNetDetNet  stage 1,2,3,4  ResNet-50  stage 1,2,3,4  ResNet-50  stage \n\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1 & 64 \\\\\\\\ 3 \\times 3 & 64 \\\\\\\\ 1 \\times 1 & 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1 & 128 \\\\\\\\ 3 \\times 3 & 128 \\\\\\\\ 1 \\times 1 & 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1 & 256 \\\\\\\\ 3 \\times 3 & 256 \\\\\\\\ 1 \\times 1 & 1024\\end{bmatrix} \\times 6$|\n\n stage  DetNet 2 D DetNet-59 \n![](/images/DetNet_fig2.png)<center>fig 2. DetNet </center>\n\n1.  backbone  extra stage P6 FPN  stage 4  16 stage \n2.  stage 4  dilated bottleneck  1x1  stage  2 B\n3. bottleneck  dilated conv  dilated conv  stage 5  6  channel  stage 4 256 backbone  backbone  stage  channel  stage  ResNet-50  64->128->256->512\n\nDetNet  backbone / feature pyramid  FPN  backbone  FPN  stage 4  stage  stage 4,5,6  2 E\n\n# \n\n","slug":"DetNet","published":1,"updated":"2019-07-20T10:39:37.332Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy3795s0002dgvchi1va9xb","content":"<p> <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>\n<p> ImageNet  finetune  backbone  backbone  RF FPN  RetinaNet extra stage DetNet backbone</p>\n<p>DetNet  FPN extra stage FPN  ImageNet DetNet  dilated bottleneck </p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p> 1(A)  FPN <br><img src=\"/images/DetNet_fig1.png\" alt><center>A.  backbone  FPN B.  backboneC. DetNet  backbone FPN </center></p>\n<ol>\n<li> stage  5  stages stage  2 32  FPN  stages P6  RetinaNet  P6  P7</li>\n<li> 32  feature map FPN  layer </li>\n<li> FPN  layer  layer  layer  layer  1 A  layer  context </li>\n</ol>\n<p>DetNet </p>\n<ol>\n<li> stage </li>\n<li> stage  6~7  stage deep layer</li>\n</ol>\n<h2 id=\"DetNet-\"><a href=\"#DetNet-\" class=\"headerlink\" title=\"DetNet \"></a>DetNet </h2><p> ResNet-50  baseline ResNet-50  DetNet-59 ResNet-101  DetNetDetNet  stage 1,2,3,4  ResNet-50  stage 1,2,3,4  ResNet-50  stage </p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">ResNet</th>\n<th align=\"center\">output size</th>\n<th align=\"center\">50-layer</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">conv1</td>\n<td align=\"center\">112x112</td>\n<td align=\"center\">7x7,64, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">maxpool</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">3x3, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">conv2_x</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 64 \\\\ 3 \\times 3 &amp; 64 \\\\ 1 \\times 1 &amp; 256\\end{bmatrix} \\times 3$</td>\n</tr>\n<tr>\n<td align=\"center\">conv3_x</td>\n<td align=\"center\">28x28</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 128 \\\\ 3 \\times 3 &amp; 128 \\\\ 1 \\times 1 &amp; 512\\end{bmatrix} \\times 4$</td>\n</tr>\n<tr>\n<td align=\"center\">conv4_x</td>\n<td align=\"center\">14x14</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 256 \\\\ 3 \\times 3 &amp; 256 \\\\ 1 \\times 1 &amp; 1024\\end{bmatrix} \\times 6$</td>\n</tr>\n</tbody></table>\n<p> stage  DetNet 2 D DetNet-59 <br><img src=\"/images/DetNet_fig2.png\" alt><center>fig 2. DetNet </center></p>\n<ol>\n<li> backbone  extra stage P6 FPN  stage 4  16 stage </li>\n<li> stage 4  dilated bottleneck  1x1  stage  2 B</li>\n<li>bottleneck  dilated conv  dilated conv  stage 5  6  channel  stage 4 256 backbone  backbone  stage  channel  stage  ResNet-50  64-&gt;128-&gt;256-&gt;512</li>\n</ol>\n<p>DetNet  backbone / feature pyramid  FPN  backbone  FPN  stage 4  stage  stage 4,5,6  2 E</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>\n<p> ImageNet  finetune  backbone  backbone  RF FPN  RetinaNet extra stage DetNet backbone</p>\n<p>DetNet  FPN extra stage FPN  ImageNet DetNet  dilated bottleneck </p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p> 1(A)  FPN <br><img src=\"/images/DetNet_fig1.png\" alt><center>A.  backbone  FPN B.  backboneC. DetNet  backbone FPN </center></p>\n<ol>\n<li> stage  5  stages stage  2 32  FPN  stages P6  RetinaNet  P6  P7</li>\n<li> 32  feature map FPN  layer </li>\n<li> FPN  layer  layer  layer  layer  1 A  layer  context </li>\n</ol>\n<p>DetNet </p>\n<ol>\n<li> stage </li>\n<li> stage  6~7  stage deep layer</li>\n</ol>\n<h2 id=\"DetNet-\"><a href=\"#DetNet-\" class=\"headerlink\" title=\"DetNet \"></a>DetNet </h2><p> ResNet-50  baseline ResNet-50  DetNet-59 ResNet-101  DetNetDetNet  stage 1,2,3,4  ResNet-50  stage 1,2,3,4  ResNet-50  stage </p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">ResNet</th>\n<th align=\"center\">output size</th>\n<th align=\"center\">50-layer</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">conv1</td>\n<td align=\"center\">112x112</td>\n<td align=\"center\">7x7,64, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">maxpool</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">3x3, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">conv2_x</td>\n<td align=\"center\">56x56</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 64 \\\\ 3 \\times 3 &amp; 64 \\\\ 1 \\times 1 &amp; 256\\end{bmatrix} \\times 3$</td>\n</tr>\n<tr>\n<td align=\"center\">conv3_x</td>\n<td align=\"center\">28x28</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 128 \\\\ 3 \\times 3 &amp; 128 \\\\ 1 \\times 1 &amp; 512\\end{bmatrix} \\times 4$</td>\n</tr>\n<tr>\n<td align=\"center\">conv4_x</td>\n<td align=\"center\">14x14</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; 256 \\\\ 3 \\times 3 &amp; 256 \\\\ 1 \\times 1 &amp; 1024\\end{bmatrix} \\times 6$</td>\n</tr>\n</tbody></table>\n<p> stage  DetNet 2 D DetNet-59 <br><img src=\"/images/DetNet_fig2.png\" alt><center>fig 2. DetNet </center></p>\n<ol>\n<li> backbone  extra stage P6 FPN  stage 4  16 stage </li>\n<li> stage 4  dilated bottleneck  1x1  stage  2 B</li>\n<li>bottleneck  dilated conv  dilated conv  stage 5  6  channel  stage 4 256 backbone  backbone  stage  channel  stage  ResNet-50  64-&gt;128-&gt;256-&gt;512</li>\n</ol>\n<p>DetNet  backbone / feature pyramid  FPN  backbone  FPN  stage 4  stage  stage 4,5,6  2 E</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n"},{"title":"Dynamic Programming (3)","date":"2019-08-27T11:03:44.000Z","mathjax":true,"_content":" [DP2](2019/08/14/DP2)  DP \n\n##  FLOWSHOP\n A  BB  A A B  $i=\\{0,1,2,3\\}$ A  $p_i=\\{3,4,8,10\\}$B  $q_i=\\{6,2,9,15\\}$ $0,1,2,3$\n\n| processor 1 | $A_0: 0-3$ | $A_1:3-7$ | $A_2:7-15$ | $A_3:15-25$ | |\n|:---------:|:--------:| :-----------: | :-------:  | :--------:  |:--:|\n| __processor 2__  |  |$B_0: 3-9$ | $B_1:9-11$ | $B_2:15-24$ | $B_3:25-40$ |\n\n B  40 DP  d  (k,S) k  A B S k  0 d  $k \\le p_d$ $B_d$  $A_d$  $B_d$ __ k'  $q_d$__ $B_d$   $A_d$  $k-p_d$  k'   $k-p_d+q_d$\n\n k = 0 $d_1=0$ $k=0<p_0$ $B_0$  $A_0$  $d_2=1$  k  $q_0=6$ $k>p_1$ $B_1$  $k-p_1=6-4=2$ $B_1$  $A_1$  7  2  $d_3=2$  k  $k:=k-p_1+q_1=6-4+2=4$ $k<p_2$ $B_2$  $A_2$  15  $d_4=3$  k  $k:=q_2=9$ $k<p_3$ $B_3$  $A_3$  25  k  $k=q_3=15$ $f(k,S)=k  \\  S=\\emptyset$\n\nDPFE \n$$f(k,S)=\\min_{d \\in S} \\{p_d + f(\\max (k-p_d,0)+q_d, S-\\{d\\})\\}$$\n $f(k,\\emptyset)=k$ $f(0,S^{\\ast})$$S^{\\ast}$ \n","source":"_posts/DP3.md","raw":"---\ntitle: Dynamic Programming (3)\ndate: 2019-08-27 19:03:44\ntags: \n    - math\n    - DP\nmathjax: true\n---\n [DP2](2019/08/14/DP2)  DP \n\n##  FLOWSHOP\n A  BB  A A B  $i=\\{0,1,2,3\\}$ A  $p_i=\\{3,4,8,10\\}$B  $q_i=\\{6,2,9,15\\}$ $0,1,2,3$\n\n| processor 1 | $A_0: 0-3$ | $A_1:3-7$ | $A_2:7-15$ | $A_3:15-25$ | |\n|:---------:|:--------:| :-----------: | :-------:  | :--------:  |:--:|\n| __processor 2__  |  |$B_0: 3-9$ | $B_1:9-11$ | $B_2:15-24$ | $B_3:25-40$ |\n\n B  40 DP  d  (k,S) k  A B S k  0 d  $k \\le p_d$ $B_d$  $A_d$  $B_d$ __ k'  $q_d$__ $B_d$   $A_d$  $k-p_d$  k'   $k-p_d+q_d$\n\n k = 0 $d_1=0$ $k=0<p_0$ $B_0$  $A_0$  $d_2=1$  k  $q_0=6$ $k>p_1$ $B_1$  $k-p_1=6-4=2$ $B_1$  $A_1$  7  2  $d_3=2$  k  $k:=k-p_1+q_1=6-4+2=4$ $k<p_2$ $B_2$  $A_2$  15  $d_4=3$  k  $k:=q_2=9$ $k<p_3$ $B_3$  $A_3$  25  k  $k=q_3=15$ $f(k,S)=k  \\  S=\\emptyset$\n\nDPFE \n$$f(k,S)=\\min_{d \\in S} \\{p_d + f(\\max (k-p_d,0)+q_d, S-\\{d\\})\\}$$\n $f(k,\\emptyset)=k$ $f(0,S^{\\ast})$$S^{\\ast}$ \n","slug":"DP3","published":1,"updated":"2019-08-30T12:21:20.627Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy3795x0005dgvcr20nvfez","content":"<p> <a href=\"2019/08/14/DP2\">DP2</a>  DP </p>\n<h2 id=\"-FLOWSHOP\"><a href=\"#-FLOWSHOP\" class=\"headerlink\" title=\" FLOWSHOP\"></a> FLOWSHOP</h2><p> A  BB  A A B  $i={0,1,2,3}$ A  $p_i={3,4,8,10}$B  $q_i={6,2,9,15}$ $0,1,2,3$</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">processor 1</th>\n<th align=\"center\">$A_0: 0-3$</th>\n<th align=\"center\">$A_1:3-7$</th>\n<th align=\"center\">$A_2:7-15$</th>\n<th align=\"center\">$A_3:15-25$</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>processor 2</strong></td>\n<td align=\"center\"></td>\n<td align=\"center\">$B_0: 3-9$</td>\n<td align=\"center\">$B_1:9-11$</td>\n<td align=\"center\">$B_2:15-24$</td>\n<td align=\"center\">$B_3:25-40$</td>\n</tr>\n</tbody></table>\n<p> B  40 DP  d  (k,S) k  A B S k  0 d  $k \\le p_d$ $B_d$  $A_d$  $B_d$ <strong> k  $q_d$</strong> $B_d$   $A_d$  $k-p_d$  k   $k-p_d+q_d$</p>\n<p> k = 0 $d_1=0$ $k=0&lt;p_0$ $B_0$  $A_0$  $d_2=1$  k  $q_0=6$ $k&gt;p_1$ $B_1$  $k-p_1=6-4=2$ $B_1$  $A_1$  7  2  $d_3=2$  k  $k:=k-p_1+q_1=6-4+2=4$ $k&lt;p_2$ $B_2$  $A_2$  15  $d_4=3$  k  $k:=q_2=9$ $k&lt;p_3$ $B_3$  $A_3$  25  k  $k=q_3=15$ $f(k,S)=k  \\  S=\\emptyset$</p>\n<p>DPFE <br>$$f(k,S)=\\min_{d \\in S} {p_d + f(\\max (k-p_d,0)+q_d, S-{d})}$$<br> $f(k,\\emptyset)=k$ $f(0,S^{\\ast})$$S^{\\ast}$ </p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"2019/08/14/DP2\">DP2</a>  DP </p>\n<h2 id=\"-FLOWSHOP\"><a href=\"#-FLOWSHOP\" class=\"headerlink\" title=\" FLOWSHOP\"></a> FLOWSHOP</h2><p> A  BB  A A B  $i={0,1,2,3}$ A  $p_i={3,4,8,10}$B  $q_i={6,2,9,15}$ $0,1,2,3$</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">processor 1</th>\n<th align=\"center\">$A_0: 0-3$</th>\n<th align=\"center\">$A_1:3-7$</th>\n<th align=\"center\">$A_2:7-15$</th>\n<th align=\"center\">$A_3:15-25$</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>processor 2</strong></td>\n<td align=\"center\"></td>\n<td align=\"center\">$B_0: 3-9$</td>\n<td align=\"center\">$B_1:9-11$</td>\n<td align=\"center\">$B_2:15-24$</td>\n<td align=\"center\">$B_3:25-40$</td>\n</tr>\n</tbody></table>\n<p> B  40 DP  d  (k,S) k  A B S k  0 d  $k \\le p_d$ $B_d$  $A_d$  $B_d$ <strong> k  $q_d$</strong> $B_d$   $A_d$  $k-p_d$  k   $k-p_d+q_d$</p>\n<p> k = 0 $d_1=0$ $k=0&lt;p_0$ $B_0$  $A_0$  $d_2=1$  k  $q_0=6$ $k&gt;p_1$ $B_1$  $k-p_1=6-4=2$ $B_1$  $A_1$  7  2  $d_3=2$  k  $k:=k-p_1+q_1=6-4+2=4$ $k&lt;p_2$ $B_2$  $A_2$  15  $d_4=3$  k  $k:=q_2=9$ $k&lt;p_3$ $B_3$  $A_3$  25  k  $k=q_3=15$ $f(k,S)=k  \\  S=\\emptyset$</p>\n<p>DPFE <br>$$f(k,S)=\\min_{d \\in S} {p_d + f(\\max (k-p_d,0)+q_d, S-{d})}$$<br> $f(k,\\emptyset)=k$ $f(0,S^{\\ast})$$S^{\\ast}$ </p>\n"},{"title":"Hexo Sync","date":"2019-06-13T01:57:11.000Z","_content":"\n\n```\nA, BHexo\n```\n computer B hexo https://shajian.github.io computer B hexo path/to/myblog/\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\ngithub shajian.github.io branch\"hexo\"\"mater\"hexo/\"hexo\"hexo/\n\n computer A  clone  hexo \n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n shajian.github.io / path/to/myblog\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n\n```\nhexo new \"<title>\"\n```\nsource/_posts\n\n```\nhexo g -d\n```\nhexo\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n https://shajian.github.io /\n\n computer B  path/to/myblog  clone  hexo \n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n .gitignore  node_modules \n```\n$ npm install\n```\n computer A\n\ncomputer A  B \n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n hexo \n\n .depoly_git  master git  .git  .deploy_git / computer B  clone \n```\nhexo g -d\n```\n hexo  .deploy_git / hexo ","source":"_posts/Hexo-Sync.md","raw":"---\ntitle: Hexo Sync\ndate: 2019-06-13 9:57:11\ntags: tool\n---\n\n\n```\nA, BHexo\n```\n computer B hexo https://shajian.github.io computer B hexo path/to/myblog/\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\ngithub shajian.github.io branch\"hexo\"\"mater\"hexo/\"hexo\"hexo/\n\n computer A  clone  hexo \n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n shajian.github.io / path/to/myblog\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n\n```\nhexo new \"<title>\"\n```\nsource/_posts\n\n```\nhexo g -d\n```\nhexo\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n https://shajian.github.io /\n\n computer B  path/to/myblog  clone  hexo \n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n .gitignore  node_modules \n```\n$ npm install\n```\n computer A\n\ncomputer A  B \n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n hexo \n\n .depoly_git  master git  .git  .deploy_git / computer B  clone \n```\nhexo g -d\n```\n hexo  .deploy_git / hexo ","slug":"Hexo-Sync","published":1,"updated":"2019-06-14T13:20:32.167Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy3795z0007dgvc28xd127q","content":"<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A, BHexo</span><br></pre></td></tr></table></figure>\n\n<p> computer B hexo <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> computer B hexo path/to/myblog/</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n\n<p>github shajian.github.io branchhexomaterhexo/hexohexo/</p>\n<p> computer A  clone  hexo </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n\n<p> shajian.github.io / path/to/myblog</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path/to/myblog/* ./&quot; which ignores hidden files/directories</span><br><span class=\"line\">$ cp -R path/to/myblog/. ./</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>source/_posts<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>hexo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &apos;title&apos;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p> <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> /</p>\n<p> computer B  path/to/myblog  clone  hexo </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n\n<p> .gitignore  node_modules </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n\n<p> computer A</p>\n<p>computer A  B </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n\n<p> hexo </p>\n<p> .depoly_git  master git  .git  .deploy_git / computer B  clone </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p> hexo  .deploy_git / hexo </p>\n","site":{"data":{}},"excerpt":"","more":"<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A, BHexo</span><br></pre></td></tr></table></figure>\n\n<p> computer B hexo <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> computer B hexo path/to/myblog/</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n\n<p>github shajian.github.io branchhexomaterhexo/hexohexo/</p>\n<p> computer A  clone  hexo </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n\n<p> shajian.github.io / path/to/myblog</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path/to/myblog/* ./&quot; which ignores hidden files/directories</span><br><span class=\"line\">$ cp -R path/to/myblog/. ./</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>source/_posts<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>hexo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &apos;title&apos;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p> <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> /</p>\n<p> computer B  path/to/myblog  clone  hexo </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n\n<p> .gitignore  node_modules </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n\n<p> computer A</p>\n<p>computer A  B </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n\n<p> hexo </p>\n<p> .depoly_git  master git  .git  .deploy_git / computer B  clone </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p> hexo  .deploy_git / hexo </p>\n"},{"title":"PyTorch-5","date":"2019-08-27T06:07:27.000Z","_content":" PyTorch  Tensor  `requires_grad=True`\n```python\nx=torch.ones(1, requires_grad=True)\n```\n torch.empty  C++  torch.ones  C++  torch/csrc/autograd/generated/python_torch_functions.cpp  THPVariable_ones \n```c++\nauto size = r.intlist(0);\nauto dtype = r.scalartype(2);\nauto device = r.device(4);\nconst auto options = TensorOptions()\n    .dtype(dtype)\n    .device(device)\n    .layout(r.layout(3).layout)\n    .requires_grad(r.toBool(5));\nreturn wrap(dispatch_ones(size, options));\n```\n`wrap`  C++  Tensor  python  Tensor  `torch.Tensor`dispatch_ones  torch::ones \n```c++\nat::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(false));\nauto result = autograd::make_variable(tensor, options.requires_grad());\n```\n 1  Tensor Tensor  VariableVariable  TensorTensor  c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>  `impl_`Variable::Impl  TensorImpl Variable  `impl_`  Variable::Impl  Variable::Impl  `requires_grad_`  Variable \n\n```python\ny=torch.ones(1) # y.requires_grad False\nz=x+y           # z.requires_grad True\n```\n\ntorch.Tensor  torch._C._TensorBase  torch/csrc/autograd/generated/python_variable_methods.cpp  variable_methods `__add__`  THPVariable_add\n```\nTHPVariable_add -> dispatch_add -> Tensor::add -> TypeDefault::add -> native::add -> native::add_out\n```\n\n\n","source":"_posts/PyTorch-5.md","raw":"---\ntitle: PyTorch-5\ndate: 2019-08-27 14:07:27\ntags: PyTorch\ncategories: DL Framework\n---\n PyTorch  Tensor  `requires_grad=True`\n```python\nx=torch.ones(1, requires_grad=True)\n```\n torch.empty  C++  torch.ones  C++  torch/csrc/autograd/generated/python_torch_functions.cpp  THPVariable_ones \n```c++\nauto size = r.intlist(0);\nauto dtype = r.scalartype(2);\nauto device = r.device(4);\nconst auto options = TensorOptions()\n    .dtype(dtype)\n    .device(device)\n    .layout(r.layout(3).layout)\n    .requires_grad(r.toBool(5));\nreturn wrap(dispatch_ones(size, options));\n```\n`wrap`  C++  Tensor  python  Tensor  `torch.Tensor`dispatch_ones  torch::ones \n```c++\nat::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(false));\nauto result = autograd::make_variable(tensor, options.requires_grad());\n```\n 1  Tensor Tensor  VariableVariable  TensorTensor  c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>  `impl_`Variable::Impl  TensorImpl Variable  `impl_`  Variable::Impl  Variable::Impl  `requires_grad_`  Variable \n\n```python\ny=torch.ones(1) # y.requires_grad False\nz=x+y           # z.requires_grad True\n```\n\ntorch.Tensor  torch._C._TensorBase  torch/csrc/autograd/generated/python_variable_methods.cpp  variable_methods `__add__`  THPVariable_add\n```\nTHPVariable_add -> dispatch_add -> Tensor::add -> TypeDefault::add -> native::add -> native::add_out\n```\n\n\n","slug":"PyTorch-5","published":1,"updated":"2019-08-27T08:37:09.451Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379620008dgvcss88f34e","content":"<p> PyTorch  Tensor  <code>requires_grad=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.ones(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p> torch.empty  C++  torch.ones  C++  torch/csrc/autograd/generated/python_torch_functions.cpp  THPVariable_ones </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> size = r.intlist(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = r.scalartype(<span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> device = r.device(<span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">auto</span> options = TensorOptions()</span><br><span class=\"line\">    .dtype(dtype)</span><br><span class=\"line\">    .device(device)</span><br><span class=\"line\">    .layout(r.layout(<span class=\"number\">3</span>).layout)</span><br><span class=\"line\">    .requires_grad(r.toBool(<span class=\"number\">5</span>));</span><br><span class=\"line\"><span class=\"keyword\">return</span> wrap(dispatch_ones(size, options));</span><br></pre></td></tr></table></figure>\n\n<p><code>wrap</code>  C++  Tensor  python  Tensor  <code>torch.Tensor</code>dispatch_ones  torch::ones </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\"><span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad());</span><br></pre></td></tr></table></figure>\n\n<p> 1  Tensor Tensor  VariableVariable  TensorTensor  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;  <code>impl_</code>Variable::Impl  TensorImpl Variable  <code>impl_</code>  Variable::Impl  Variable::Impl  <code>requires_grad_</code>  Variable </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y=torch.ones(<span class=\"number\">1</span>) <span class=\"comment\"># y.requires_grad False</span></span><br><span class=\"line\">z=x+y           <span class=\"comment\"># z.requires_grad True</span></span><br></pre></td></tr></table></figure>\n\n<p>torch.Tensor  torch._C._TensorBase  torch/csrc/autograd/generated/python_variable_methods.cpp  variable_methods <code>__add__</code>  THPVariable_add</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPVariable_add -&gt; dispatch_add -&gt; Tensor::add -&gt; TypeDefault::add -&gt; native::add -&gt; native::add_out</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<p> PyTorch  Tensor  <code>requires_grad=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.ones(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p> torch.empty  C++  torch.ones  C++  torch/csrc/autograd/generated/python_torch_functions.cpp  THPVariable_ones </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> size = r.intlist(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = r.scalartype(<span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> device = r.device(<span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">auto</span> options = TensorOptions()</span><br><span class=\"line\">    .dtype(dtype)</span><br><span class=\"line\">    .device(device)</span><br><span class=\"line\">    .layout(r.layout(<span class=\"number\">3</span>).layout)</span><br><span class=\"line\">    .requires_grad(r.toBool(<span class=\"number\">5</span>));</span><br><span class=\"line\"><span class=\"keyword\">return</span> wrap(dispatch_ones(size, options));</span><br></pre></td></tr></table></figure>\n\n<p><code>wrap</code>  C++  Tensor  python  Tensor  <code>torch.Tensor</code>dispatch_ones  torch::ones </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::Tensor tensor = at::ones(size, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\"><span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad());</span><br></pre></td></tr></table></figure>\n\n<p> 1  Tensor Tensor  VariableVariable  TensorTensor  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;  <code>impl_</code>Variable::Impl  TensorImpl Variable  <code>impl_</code>  Variable::Impl  Variable::Impl  <code>requires_grad_</code>  Variable </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y=torch.ones(<span class=\"number\">1</span>) <span class=\"comment\"># y.requires_grad False</span></span><br><span class=\"line\">z=x+y           <span class=\"comment\"># z.requires_grad True</span></span><br></pre></td></tr></table></figure>\n\n<p>torch.Tensor  torch._C._TensorBase  torch/csrc/autograd/generated/python_variable_methods.cpp  variable_methods <code>__add__</code>  THPVariable_add</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPVariable_add -&gt; dispatch_add -&gt; Tensor::add -&gt; TypeDefault::add -&gt; native::add -&gt; native::add_out</span><br></pre></td></tr></table></figure>\n\n"},{"title":"RepPoints","date":"2019-07-17T08:05:41.000Z","mathjax":true,"_content":" [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n\n bbox  bbox  RepPointsrepresentative pointsRepPoints  gt RepPoints  anchor-free  anchor  anchor \n\n 1\n![](/images/RepPoints_fig1.png)\n\nRepPoints ~~RepPoints  bottom-up  box RepPoints  top-down  image/~~\n\n# RepPoints\n## BBox \n bbox  $\\mathcal B=(x,y,w,h)$ multi-stage  stage\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n scale  aspect ratio  anchorsanchor / bbox  proposal (S1) stage S1  RoIpooling/RoIAlign two-stage S1  box  multi-stage S1  S2 stage  bbox target\n\n 4-d  $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$ bbox proposal  $\\mathcal B_p=(x_p,y_p,w_p,h_p)$  bbox\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n gt  $\\mathcal B_t=(x_t,y_t,w_t,h_t)$gt gt target\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n smooth L1 \n\n## RepPoints\n4-d bbox RepPoints \n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n n  9\n\nRepPoints \n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ refine \n\n__RepPoints  gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$ $\\mathcal R_P$  P  RepPoints$\\mathcal T(\\mathcal R_p)$  box\n1. Min-max function  \n   $\\mathcal {T=T_1}$ RepPoints  $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$ RepPoints  $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$ RepPoints  $\\mathcal B_p$  scale scale  $\\lambda_x, \\ \\lambda_y$ box RepPoints RepPoints  size RepPoint  point loss \n\n__RepPoints __  RepPoints  pseudo box gt box  smooth L1 \n\n# RPDet\nmulti-stage  RepPoints \n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector)  2\n![](/images/RepPoints_fig2.png)\n\n N  RepPoints \n\n FPN backbone feature maps 3x3  offset field feature maps  spatial size  2N-channel  feature vector  2N-d  offsetsoffsets  deformable conv RepPoints deformable conv  offsets  offsets  pseudo box gt box  point losssmooth L1 \n\n offsets  deformable conv offsets (5)  refinement  RepPoints offsets  $\\Delta x, \\Delta y$ score maps\n\n##  RoI pooling \n RoI pooling  RepPoints  RoI pooling  proposals RoI pooling  RoI pooling  proposals RoI pooling \n\n# \n\n\n# \n","source":"_posts/RepPoints.md","raw":"---\ntitle: RepPoints\ndate: 2019-07-17 16:05:41\ntags: object detection\nmathjax: true\n---\n [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n\n bbox  bbox  RepPointsrepresentative pointsRepPoints  gt RepPoints  anchor-free  anchor  anchor \n\n 1\n![](/images/RepPoints_fig1.png)\n\nRepPoints ~~RepPoints  bottom-up  box RepPoints  top-down  image/~~\n\n# RepPoints\n## BBox \n bbox  $\\mathcal B=(x,y,w,h)$ multi-stage  stage\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n scale  aspect ratio  anchorsanchor / bbox  proposal (S1) stage S1  RoIpooling/RoIAlign two-stage S1  box  multi-stage S1  S2 stage  bbox target\n\n 4-d  $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$ bbox proposal  $\\mathcal B_p=(x_p,y_p,w_p,h_p)$  bbox\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n gt  $\\mathcal B_t=(x_t,y_t,w_t,h_t)$gt gt target\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n smooth L1 \n\n## RepPoints\n4-d bbox RepPoints \n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n n  9\n\nRepPoints \n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ refine \n\n__RepPoints  gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$ $\\mathcal R_P$  P  RepPoints$\\mathcal T(\\mathcal R_p)$  box\n1. Min-max function  \n   $\\mathcal {T=T_1}$ RepPoints  $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$ RepPoints  $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$ RepPoints  $\\mathcal B_p$  scale scale  $\\lambda_x, \\ \\lambda_y$ box RepPoints RepPoints  size RepPoint  point loss \n\n__RepPoints __  RepPoints  pseudo box gt box  smooth L1 \n\n# RPDet\nmulti-stage  RepPoints \n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector)  2\n![](/images/RepPoints_fig2.png)\n\n N  RepPoints \n\n FPN backbone feature maps 3x3  offset field feature maps  spatial size  2N-channel  feature vector  2N-d  offsetsoffsets  deformable conv RepPoints deformable conv  offsets  offsets  pseudo box gt box  point losssmooth L1 \n\n offsets  deformable conv offsets (5)  refinement  RepPoints offsets  $\\Delta x, \\Delta y$ score maps\n\n##  RoI pooling \n RoI pooling  RepPoints  RoI pooling  proposals RoI pooling  RoI pooling  proposals RoI pooling \n\n# \n\n\n# \n","slug":"RepPoints","published":1,"updated":"2019-07-18T11:22:42.079Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy37964000bdgvcjmis7zg5","content":"<p> <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>\n<p> bbox  bbox  RepPointsrepresentative pointsRepPoints  gt RepPoints  anchor-free  anchor  anchor </p>\n<p> 1<br><img src=\"/images/RepPoints_fig1.png\" alt></p>\n<p>RepPoints <del>RepPoints  bottom-up  box RepPoints  top-down  image/</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-\"><a href=\"#BBox-\" class=\"headerlink\" title=\"BBox \"></a>BBox </h2><p> bbox  $\\mathcal B=(x,y,w,h)$ multi-stage  stage</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             <br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p> scale  aspect ratio  anchorsanchor / bbox  proposal (S1) stage S1  RoIpooling/RoIAlign two-stage S1  box  multi-stage S1  S2 stage  bbox target</p>\n<p> 4-d  $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$ bbox proposal  $\\mathcal B_p=(x_p,y_p,w_p,h_p)$  bbox<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p> gt  $\\mathcal B_t=(x_t,y_t,w_t,h_t)$gt gt target<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p> smooth L1 </p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>4-d bbox RepPoints <br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br> n  9</p>\n<p>RepPoints <br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br> ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ refine </p>\n<p><strong>RepPoints  gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$ $\\mathcal R_P$  P  RepPoints$\\mathcal T(\\mathcal R_p)$  box</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$ RepPoints  $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$ RepPoints  $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$ RepPoints  $\\mathcal B_p$  scale scale  $\\lambda_x, \\ \\lambda_y$ box RepPoints RepPoints  size RepPoint  point loss </p>\n</li>\n</ol>\n<p><strong>RepPoints </strong>  RepPoints  pseudo box gt box  smooth L1 </p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>multi-stage  RepPoints </p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2)  -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector)  2<br><img src=\"/images/RepPoints_fig2.png\" alt></p>\n<p> N  RepPoints </p>\n<p> FPN backbone feature maps 3x3  offset field feature maps  spatial size  2N-channel  feature vector  2N-d  offsetsoffsets  deformable conv RepPoints deformable conv  offsets  offsets  pseudo box gt box  point losssmooth L1 </p>\n<p> offsets  deformable conv offsets (5)  refinement  RepPoints offsets  $\\Delta x, \\Delta y$ score maps</p>\n<h2 id=\"-RoI-pooling-\"><a href=\"#-RoI-pooling-\" class=\"headerlink\" title=\" RoI pooling \"></a> RoI pooling </h2><p> RoI pooling  RepPoints  RoI pooling  proposals RoI pooling  RoI pooling  proposals RoI pooling </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>\n<p> bbox  bbox  RepPointsrepresentative pointsRepPoints  gt RepPoints  anchor-free  anchor  anchor </p>\n<p> 1<br><img src=\"/images/RepPoints_fig1.png\" alt></p>\n<p>RepPoints <del>RepPoints  bottom-up  box RepPoints  top-down  image/</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-\"><a href=\"#BBox-\" class=\"headerlink\" title=\"BBox \"></a>BBox </h2><p> bbox  $\\mathcal B=(x,y,w,h)$ multi-stage  stage</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             <br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p> scale  aspect ratio  anchorsanchor / bbox  proposal (S1) stage S1  RoIpooling/RoIAlign two-stage S1  box  multi-stage S1  S2 stage  bbox target</p>\n<p> 4-d  $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$ bbox proposal  $\\mathcal B_p=(x_p,y_p,w_p,h_p)$  bbox<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p> gt  $\\mathcal B_t=(x_t,y_t,w_t,h_t)$gt gt target<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p> smooth L1 </p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>4-d bbox RepPoints <br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br> n  9</p>\n<p>RepPoints <br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br> ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ refine </p>\n<p><strong>RepPoints  gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$ $\\mathcal R_P$  P  RepPoints$\\mathcal T(\\mathcal R_p)$  box</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$ RepPoints  $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$ RepPoints  $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$ RepPoints  $\\mathcal B_p$  scale scale  $\\lambda_x, \\ \\lambda_y$ box RepPoints RepPoints  size RepPoint  point loss </p>\n</li>\n</ol>\n<p><strong>RepPoints </strong>  RepPoints  pseudo box gt box  smooth L1 </p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>multi-stage  RepPoints </p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2)  -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector)  2<br><img src=\"/images/RepPoints_fig2.png\" alt></p>\n<p> N  RepPoints </p>\n<p> FPN backbone feature maps 3x3  offset field feature maps  spatial size  2N-channel  feature vector  2N-d  offsetsoffsets  deformable conv RepPoints deformable conv  offsets  offsets  pseudo box gt box  point losssmooth L1 </p>\n<p> offsets  deformable conv offsets (5)  refinement  RepPoints offsets  $\\Delta x, \\Delta y$ score maps</p>\n<h2 id=\"-RoI-pooling-\"><a href=\"#-RoI-pooling-\" class=\"headerlink\" title=\" RoI pooling \"></a> RoI pooling </h2><p> RoI pooling  RepPoints  RoI pooling  proposals RoI pooling  RoI pooling  proposals RoI pooling </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n"},{"title":"cpp-aux-tools","date":"2019-07-11T11:16:23.000Z","_content":" c++ \n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n\n```\ngcc test.cpp -o test.o\n```\n f  low-level assembler name mangling\n```\nnm test.o | grep f\n// \n// 000000000000008b T _Z4fi\n```\n\n```\nc++filt -n _Z4fi\n// \n// f(int)\n```\n\n","source":"_posts/cpp-aux-tools.md","raw":"---\ntitle: cpp-aux-tools\ndate: 2019-07-11 19:16:23\ntags: c++\n---\n c++ \n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n\n```\ngcc test.cpp -o test.o\n```\n f  low-level assembler name mangling\n```\nnm test.o | grep f\n// \n// 000000000000008b T _Z4fi\n```\n\n```\nc++filt -n _Z4fi\n// \n// f(int)\n```\n\n","slug":"cpp-aux-tools","published":1,"updated":"2019-07-26T07:38:02.442Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy37966000ddgvcu4700weq","content":"<p> c++ </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n\n<p> f  low-level assembler name mangling</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">// </span><br><span class=\"line\">// 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">// </span><br><span class=\"line\">// f(int)</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<p> c++ </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n\n<p> f  low-level assembler name mangling</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">// </span><br><span class=\"line\">// 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">// </span><br><span class=\"line\">// f(int)</span><br></pre></td></tr></table></figure>\n\n"},{"title":"WGAN","date":"2019-07-25T12:24:41.000Z","mathjax":true,"_content":" [Wasserstein GAN](https://arxiv.org/abs/1701.07875)","source":"_posts/WGAN.md","raw":"---\ntitle: WGAN\ndate: 2019-07-25 20:24:41\ntags: GAN\nmathjax: true\n---\n [Wasserstein GAN](https://arxiv.org/abs/1701.07875)","slug":"WGAN","published":1,"updated":"2019-07-29T12:22:26.732Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy37968000gdgvc27iowwy7","content":"<p> <a href=\"https://arxiv.org/abs/1701.07875\" target=\"_blank\" rel=\"noopener\">Wasserstein GAN</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1701.07875\" target=\"_blank\" rel=\"noopener\">Wasserstein GAN</a></p>\n"},{"title":"CV ","date":"2019-06-24T09:33:22.000Z","mathjax":true,"_content":" CV  CV\n# RF\n k layer \n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n$s_i$  i layer $f_k$  k layer $l_0=1, \\ s_0=1$\n# NMS\n Faster R-CNN Test ProposalLayer  anchorsRPN  anchors  anchors  proposalsproposals  anchors \n1. proposal  image  [0,w-1], [0,h-1] proposal  clip  proposal \n2.  proposalproposal  image  box  16\n3.  proposals  top N1  proposalsN1 \n4.  NMS\n5.  proposals  top N2  proposalsN2 \n\nNMS \n1.  proposals  P A proposals  I proposals  K \n2.  proposal I[0] K `K.append(I[0])` I  proposal  IOUs I  IOU  proposals  I[0]  proposal  I[0]  K \n3.  2 I  \n4. K  NMS  proposals \n\n# Soft-NMS\nNMS boxes  box gt boxes box  recall  soft-NMS [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS  NMS  NMS  box  0 Soft-NMS  box  rank list \n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   *  boxes scoresNMS 0.7 $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n Soft-NMS  f  NMS  Soft-NMS box  $s_i=s_i f(i)$ f \n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n    iou  iou=N<sub>t</sub> \n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n    iou=0  $\\sigma$ $\\sigma$  iou \n\n# Deconvolution\n CV  semantic segmentation  bilinear interpolation\n(to be continued...)","source":"_posts/cv-mtds.md","raw":"---\ntitle: CV \ndate: 2019-06-24 17:33:22\ntags: CV\nmathjax: true\n---\n CV  CV\n# RF\n k layer \n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n$s_i$  i layer $f_k$  k layer $l_0=1, \\ s_0=1$\n# NMS\n Faster R-CNN Test ProposalLayer  anchorsRPN  anchors  anchors  proposalsproposals  anchors \n1. proposal  image  [0,w-1], [0,h-1] proposal  clip  proposal \n2.  proposalproposal  image  box  16\n3.  proposals  top N1  proposalsN1 \n4.  NMS\n5.  proposals  top N2  proposalsN2 \n\nNMS \n1.  proposals  P A proposals  I proposals  K \n2.  proposal I[0] K `K.append(I[0])` I  proposal  IOUs I  IOU  proposals  I[0]  proposal  I[0]  K \n3.  2 I  \n4. K  NMS  proposals \n\n# Soft-NMS\nNMS boxes  box gt boxes box  recall  soft-NMS [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS  NMS  NMS  box  0 Soft-NMS  box  rank list \n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   *  boxes scoresNMS 0.7 $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n Soft-NMS  f  NMS  Soft-NMS box  $s_i=s_i f(i)$ f \n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n    iou  iou=N<sub>t</sub> \n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n    iou=0  $\\sigma$ $\\sigma$  iou \n\n# Deconvolution\n CV  semantic segmentation  bilinear interpolation\n(to be continued...)","slug":"cv-mtds","published":1,"updated":"2019-07-16T09:31:18.679Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy37969000idgvchkwkaz1h","content":"<p> CV  CV</p>\n<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p> k layer <br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>$s_i$  i layer $f_k$  k layer $l_0=1, \\ s_0=1$</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p> Faster R-CNN Test ProposalLayer  anchorsRPN  anchors  anchors  proposalsproposals  anchors </p>\n<ol>\n<li>proposal  image  [0,w-1], [0,h-1] proposal  clip  proposal </li>\n<li> proposalproposal  image  box  16</li>\n<li> proposals  top N1  proposalsN1 </li>\n<li> NMS</li>\n<li> proposals  top N2  proposalsN2 </li>\n</ol>\n<p>NMS </p>\n<ol>\n<li> proposals  P A proposals  I proposals  K </li>\n<li> proposal I[0] K <code>K.append(I[0])</code> I  proposal  IOUs I  IOU  proposals  I[0]  proposal  I[0]  K </li>\n<li> 2 I  </li>\n<li>K  NMS  proposals </li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS boxes  box gt boxes box  recall  soft-NMS <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS  NMS  NMS  box  0 Soft-NMS  box  rank list </p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,,b_N}, \\mathcal S = {s_1,,S_N}, N_t, m$</li>\n<li> boxes scoresNMS 0.7 $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p> Soft-NMS  f  NMS  Soft-NMS box  $s_i=s_i f(i)$ f </p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br> iou  iou=N<sub>t</sub> <br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br> iou=0  $\\sigma$ $\\sigma$  iou </p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p> CV  semantic segmentation  bilinear interpolation<br>(to be continued)</p>\n","site":{"data":{}},"excerpt":"","more":"<p> CV  CV</p>\n<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p> k layer <br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>$s_i$  i layer $f_k$  k layer $l_0=1, \\ s_0=1$</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p> Faster R-CNN Test ProposalLayer  anchorsRPN  anchors  anchors  proposalsproposals  anchors </p>\n<ol>\n<li>proposal  image  [0,w-1], [0,h-1] proposal  clip  proposal </li>\n<li> proposalproposal  image  box  16</li>\n<li> proposals  top N1  proposalsN1 </li>\n<li> NMS</li>\n<li> proposals  top N2  proposalsN2 </li>\n</ol>\n<p>NMS </p>\n<ol>\n<li> proposals  P A proposals  I proposals  K </li>\n<li> proposal I[0] K <code>K.append(I[0])</code> I  proposal  IOUs I  IOU  proposals  I[0]  proposal  I[0]  K </li>\n<li> 2 I  </li>\n<li>K  NMS  proposals </li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS boxes  box gt boxes box  recall  soft-NMS <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS  NMS  NMS  box  0 Soft-NMS  box  rank list </p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,,b_N}, \\mathcal S = {s_1,,S_N}, N_t, m$</li>\n<li> boxes scoresNMS 0.7 $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p> Soft-NMS  f  NMS  Soft-NMS box  $s_i=s_i f(i)$ f </p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br> iou  iou=N<sub>t</sub> <br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br> iou=0  $\\sigma$ $\\sigma$  iou </p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p> CV  semantic segmentation  bilinear interpolation<br>(to be continued)</p>\n"},{"title":"loss","date":"2019-07-16T09:32:26.000Z","mathjax":true,"_content":" CV CV  CV \n\n# Cross-Entropy Loss\n C softmax  $P=(p_1,...,p_C)$ $p_i$  i  $\\sum_i^C p_i=1$ c gt target  $T=(t_1,...,t_C)$\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n C=2  p t$t \\in \\{0,1\\}$\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\nlong-tail distribution 1  0  1  1 t=1  $\\alpha$t=0  $1-\\alpha$$\\alpha \\in [0,1]$ $\\alpha$  $\\alpha$ RetinaNet  0.25\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n balanced cross-entropy loss  $\\alpha$  $p_t \\gg 0.5$  Focal loss\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n $(1-p_t)^{\\gamma}$ \n\nFocal loss \n1. $p_t$  $(1-p_t)^{\\gamma}$ \n2. $p_t$  $(1-p_t)^{\\gamma}$ \n\n# MSE\n\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n n  L2  $Y_i, \\hat Y_i$  i \n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n $|Y_i-\\hat Y_i|>1$ \n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n $|Y_i-\\hat Y_i|<1$ \n## Smooth L1 Loss\n Smooth L1 \n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n L1  L2 \n\n","source":"_posts/loss.md","raw":"---\ntitle: loss\ndate: 2019-07-16 17:32:26\ntags: CV\nmathjax: true\n---\n CV CV  CV \n\n# Cross-Entropy Loss\n C softmax  $P=(p_1,...,p_C)$ $p_i$  i  $\\sum_i^C p_i=1$ c gt target  $T=(t_1,...,t_C)$\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n C=2  p t$t \\in \\{0,1\\}$\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\nlong-tail distribution 1  0  1  1 t=1  $\\alpha$t=0  $1-\\alpha$$\\alpha \\in [0,1]$ $\\alpha$  $\\alpha$ RetinaNet  0.25\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n balanced cross-entropy loss  $\\alpha$  $p_t \\gg 0.5$  Focal loss\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n $(1-p_t)^{\\gamma}$ \n\nFocal loss \n1. $p_t$  $(1-p_t)^{\\gamma}$ \n2. $p_t$  $(1-p_t)^{\\gamma}$ \n\n# MSE\n\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n n  L2  $Y_i, \\hat Y_i$  i \n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n $|Y_i-\\hat Y_i|>1$ \n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n $|Y_i-\\hat Y_i|<1$ \n## Smooth L1 Loss\n Smooth L1 \n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n L1  L2 \n\n","slug":"loss","published":1,"updated":"2019-07-17T01:55:01.435Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy3796b000ldgvcs131qnkc","content":"<p> CV CV  CV </p>\n<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p> C softmax  $P=(p_1,,p_C)$ $p_i$  i  $\\sum_i^C p_i=1$ c gt target  $T=(t_1,,t_C)$<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br><br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p> C=2  p t$t \\in {0,1}$<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br><br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br><br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>long-tail distribution 1  0  1  1 t=1  $\\alpha$t=0  $1-\\alpha$$\\alpha \\in [0,1]$ $\\alpha$  $\\alpha$ RetinaNet  0.25<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p> balanced cross-entropy loss  $\\alpha$  $p_t \\gg 0.5$  Focal loss<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br> $(1-p_t)^{\\gamma}$ </p>\n<p>Focal loss </p>\n<ol>\n<li>$p_t$  $(1-p_t)^{\\gamma}$ </li>\n<li>$p_t$  $(1-p_t)^{\\gamma}$ </li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p><br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br> n  L2  $Y_i, \\hat Y_i$  i </p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br> $|Y_i-\\hat Y_i|&gt;1$ </p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br> $|Y_i-\\hat Y_i|&lt;1$ </p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p> Smooth L1 <br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p> L1  L2 </p>\n","site":{"data":{}},"excerpt":"","more":"<p> CV CV  CV </p>\n<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p> C softmax  $P=(p_1,,p_C)$ $p_i$  i  $\\sum_i^C p_i=1$ c gt target  $T=(t_1,,t_C)$<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br><br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p> C=2  p t$t \\in {0,1}$<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br><br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br><br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>long-tail distribution 1  0  1  1 t=1  $\\alpha$t=0  $1-\\alpha$$\\alpha \\in [0,1]$ $\\alpha$  $\\alpha$ RetinaNet  0.25<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p> balanced cross-entropy loss  $\\alpha$  $p_t \\gg 0.5$  Focal loss<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br> $(1-p_t)^{\\gamma}$ </p>\n<p>Focal loss </p>\n<ol>\n<li>$p_t$  $(1-p_t)^{\\gamma}$ </li>\n<li>$p_t$  $(1-p_t)^{\\gamma}$ </li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p><br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br> n  L2  $Y_i, \\hat Y_i$  i </p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br> $|Y_i-\\hat Y_i|&gt;1$ </p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br> $|Y_i-\\hat Y_i|&lt;1$ </p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p> Smooth L1 <br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p> L1  L2 </p>\n"},{"title":"DeRPN","date":"2019-07-15T07:04:18.000Z","mathjax":true,"_content":" [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n\ntwo-stage SOTA  anchor Faster R-CNN  RPN anchor  scale  aspect ratio K-means  anchor DeRPN  RPN  1(b)\n![](/images/DeRPN_fig1.png)\n\nDeRPN  anchor strings anchor \n\n# \n## \n CNN  $\\mathbf x$ anchor box $B_a$ sigmoid softmax $\\sigma$ bbox \n$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$\n $\\mathbf {W_r, b_r}$ $\\mathbf {W_c, b_c}$ $\\psi$  box  Faster R-CNN  $\\mathbf t$  region proposals  box \n\nanchor  anchor  anchor  gt box !  anchor string$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$anchor string  $(S_w(x,w), S_h(y,h))$  $(P_s^w, P_s^h)$\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n bbox \n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\nf g  bbox \n### \n n  $n^2$  anchor box  $O(n^2)$n  anchor string  $O(n)$\n\n## \n### Anchor strings\nRPN  anchor string DeRPN  box  anchor string anchor string object  anchor string  $\\{a_n\\}$ (16,32,64,128,256,512,1024) $[8\\sqrt 2,1024 \\sqrt 2]$ $\\sqrt 2$ anchor string  $a_i$ anchor string $[a_i/\\sqrt 2, a_i\\sqrt 2]$2 $[8\\sqrt 2,1024 \\sqrt 2]$\n\n 2  DeRPN \n![](/images/DeRPN_fig2.png) <center>(a)  anchor string  anchor string(b)  anchor string  anchor string(c)  bbox(d)  NMS  region proposals</center>\n\n anchor string RPN  anchor box  gt box  IoU  anchor  anchor  IoU  0.7 gt  IoU  anchor  DeRPN  anchor string  anchor string \n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$  j  anchor string $e_j$ N  $\\{a_n\\}$ q  2\n\n anchor string $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$ $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$  i $\\beta$  $e_j$  i  i+1  anchor string \n\n $a_i$  $[a_i/ \\sqrt q,a_i\\sqrt q]$ $e_j$  i 0 gt  i  gt  anchor string  anchor string $a_i, a_{i+1}$  $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$ $a_i \\sqrt q$ $e_j$  i  i+1 \n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n\n\n anchor string  $\\sqrt q$ $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$ $\\sqrt q$  DeRPN  RPN  IoU  anchor box  gtRPN \n\n### Label assignment\n anchor string  feature map  (9) anchor string observe-to-distribute  anchor string1.  anchor string / region proposal region proposal  gt  IoU 0.6 anchor string  anchor string \n\n### Consistent network\nDeRPN  RPN  two-stage  2  3x3  1x1  DeRPN  anchor string  $\\{a_n\\}$ N anchor string $2\\times 2N$  anchor string anchor string  $(x,w)$ $(y,h)$ $2 \\times 2N$ \n\n### Scale-sensitive loss function\n\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\nN M  batch sizes  anchor string$p_i$  i  anchor string $p_i^*$  gt label anchor string  1  0$t_i$ $t_i^*$  gt A  anchor string $R_j$  anchor string j  $a_j$$G_j$  anchor string j  $a_j$ anchor string  smooth L1 \n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n t  Fast/Faster R-CNN  box \n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# \nDeRPN  region proposal  bbox region proposal\n\n____  t  anchor string  W top-N $W_N$ top-N  (x,w) $p^W$ (x,w)  top-k  $(y^{(k)},h^{(k)})$ bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$ bbox \n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n $p^W$  (x,w) $p^H$  $(y^{(k)},h^{(k)})$ \n\n top-N  $H_N$ $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$ $B=B_w \\cup B_h$  NMS top-M  region proposals bbox stage \n\n# \n\n\n# \n1.  DeRPN\n2. ","source":"_posts/DeRPN.md","raw":"---\ntitle: DeRPN\ndate: 2019-07-15 15:04:18\ntags: object detection\nmathjax: true\n---\n [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n\ntwo-stage SOTA  anchor Faster R-CNN  RPN anchor  scale  aspect ratio K-means  anchor DeRPN  RPN  1(b)\n![](/images/DeRPN_fig1.png)\n\nDeRPN  anchor strings anchor \n\n# \n## \n CNN  $\\mathbf x$ anchor box $B_a$ sigmoid softmax $\\sigma$ bbox \n$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$\n $\\mathbf {W_r, b_r}$ $\\mathbf {W_c, b_c}$ $\\psi$  box  Faster R-CNN  $\\mathbf t$  region proposals  box \n\nanchor  anchor  anchor  gt box !  anchor string$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$anchor string  $(S_w(x,w), S_h(y,h))$  $(P_s^w, P_s^h)$\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n bbox \n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\nf g  bbox \n### \n n  $n^2$  anchor box  $O(n^2)$n  anchor string  $O(n)$\n\n## \n### Anchor strings\nRPN  anchor string DeRPN  box  anchor string anchor string object  anchor string  $\\{a_n\\}$ (16,32,64,128,256,512,1024) $[8\\sqrt 2,1024 \\sqrt 2]$ $\\sqrt 2$ anchor string  $a_i$ anchor string $[a_i/\\sqrt 2, a_i\\sqrt 2]$2 $[8\\sqrt 2,1024 \\sqrt 2]$\n\n 2  DeRPN \n![](/images/DeRPN_fig2.png) <center>(a)  anchor string  anchor string(b)  anchor string  anchor string(c)  bbox(d)  NMS  region proposals</center>\n\n anchor string RPN  anchor box  gt box  IoU  anchor  anchor  IoU  0.7 gt  IoU  anchor  DeRPN  anchor string  anchor string \n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$  j  anchor string $e_j$ N  $\\{a_n\\}$ q  2\n\n anchor string $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$ $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$  i $\\beta$  $e_j$  i  i+1  anchor string \n\n $a_i$  $[a_i/ \\sqrt q,a_i\\sqrt q]$ $e_j$  i 0 gt  i  gt  anchor string  anchor string $a_i, a_{i+1}$  $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$ $a_i \\sqrt q$ $e_j$  i  i+1 \n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n\n\n anchor string  $\\sqrt q$ $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$ $\\sqrt q$  DeRPN  RPN  IoU  anchor box  gtRPN \n\n### Label assignment\n anchor string  feature map  (9) anchor string observe-to-distribute  anchor string1.  anchor string / region proposal region proposal  gt  IoU 0.6 anchor string  anchor string \n\n### Consistent network\nDeRPN  RPN  two-stage  2  3x3  1x1  DeRPN  anchor string  $\\{a_n\\}$ N anchor string $2\\times 2N$  anchor string anchor string  $(x,w)$ $(y,h)$ $2 \\times 2N$ \n\n### Scale-sensitive loss function\n\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\nN M  batch sizes  anchor string$p_i$  i  anchor string $p_i^*$  gt label anchor string  1  0$t_i$ $t_i^*$  gt A  anchor string $R_j$  anchor string j  $a_j$$G_j$  anchor string j  $a_j$ anchor string  smooth L1 \n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n t  Fast/Faster R-CNN  box \n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# \nDeRPN  region proposal  bbox region proposal\n\n____  t  anchor string  W top-N $W_N$ top-N  (x,w) $p^W$ (x,w)  top-k  $(y^{(k)},h^{(k)})$ bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$ bbox \n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n $p^W$  (x,w) $p^H$  $(y^{(k)},h^{(k)})$ \n\n top-N  $H_N$ $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$ $B=B_w \\cup B_h$  NMS top-M  region proposals bbox stage \n\n# \n\n\n# \n1.  DeRPN\n2. ","slug":"DeRPN","published":1,"updated":"2019-07-20T10:33:34.404Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379br000zdgvcajs3c9oo","content":"<p> <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>\n<p>two-stage SOTA  anchor Faster R-CNN  RPN anchor  scale  aspect ratio K-means  anchor DeRPN  RPN  1(b)<br><img src=\"/images/DeRPN_fig1.png\" alt></p>\n<p>DeRPN  anchor strings anchor </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> CNN  $\\mathbf x$ anchor box $B_a$ sigmoid softmax $\\sigma$ bbox <br>$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$<br> $\\mathbf {W_r, b_r}$ $\\mathbf {W_c, b_c}$ $\\psi$  box  Faster R-CNN  $\\mathbf t$  region proposals  box </p>\n<p>anchor  anchor  anchor  gt box !  anchor string$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$anchor string  $(S_w(x,w), S_h(y,h))$  $(P_s^w, P_s^h)$<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br> bbox <br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>f g  bbox </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> n  $n^2$  anchor box  $O(n^2)$n  anchor string  $O(n)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN  anchor string DeRPN  box  anchor string anchor string object  anchor string  ${a_n}$ (16,32,64,128,256,512,1024) $[8\\sqrt 2,1024 \\sqrt 2]$ $\\sqrt 2$ anchor string  $a_i$ anchor string $[a_i/\\sqrt 2, a_i\\sqrt 2]$2 $[8\\sqrt 2,1024 \\sqrt 2]$</p>\n<p> 2  DeRPN <br><img src=\"/images/DeRPN_fig2.png\" alt> <center>(a)  anchor string  anchor string(b)  anchor string  anchor string(c)  bbox(d)  NMS  region proposals</center></p>\n<p> anchor string RPN  anchor box  gt box  IoU  anchor  anchor  IoU  0.7 gt  IoU  anchor  DeRPN  anchor string  anchor string <br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,,N) \\quad(9)$$<br>$M_j$  j  anchor string $e_j$ N  ${a_n}$ q  2</p>\n<p> anchor string $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$ $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$  i $\\beta$  $e_j$  i  i+1  anchor string </p>\n<p> $a_i$  $[a_i/ \\sqrt q,a_i\\sqrt q]$ $e_j$  i 0 gt  i  gt  anchor string  anchor string $a_i, a_{i+1}$  $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$ $a_i \\sqrt q$ $e_j$  i  i+1 <br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br></p>\n<p> anchor string  $\\sqrt q$ $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$ $\\sqrt q$  DeRPN  RPN  IoU  anchor box  gtRPN </p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p> anchor string  feature map  (9) anchor string observe-to-distribute  anchor string1.  anchor string / region proposal region proposal  gt  IoU 0.6 anchor string  anchor string </p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN  RPN  two-stage  2  3x3  1x1  DeRPN  anchor string  ${a_n}$ N anchor string $2\\times 2N$  anchor string anchor string  $(x,w)$ $(y,h)$ $2 \\times 2N$ </p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p><br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,,M} \\quad (12)$$</p>\n<p>N M  batch sizes  anchor string$p_i$  i  anchor string $p_i^<em>$  gt label anchor string  1  0$t_i$ $t_i^</em>$  gt A  anchor string $R_j$  anchor string j  $a_j$$G_j$  anchor string j  $a_j$ anchor string  smooth L1 <br>$$L_{cls}(p_i,p_i^<em>)=- p_i^</em>\\log p_i-(1-p_i^<em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^</em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j*})$$</p>\n<p> t  Fast/Faster R-CNN  box <br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>DeRPN  region proposal  bbox region proposal</p>\n<p><strong></strong>  t  anchor string  W top-N $W_N$ top-N  (x,w) $p^W$ (x,w)  top-k  $(y^{(k)},h^{(k)})$ bbox $B_w={(x,y^{(k)},w,h^{(k)}}$ bbox <br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br> $p^W$  (x,w) $p^H$  $(y^{(k)},h^{(k)})$ </p>\n<p> top-N  $H_N$ $B_h={(x^{(k)},y,w^{(k)},h}$ $B=B_w \\cup B_h$  NMS top-M  region proposals bbox stage </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li> DeRPN</li>\n<li></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>\n<p>two-stage SOTA  anchor Faster R-CNN  RPN anchor  scale  aspect ratio K-means  anchor DeRPN  RPN  1(b)<br><img src=\"/images/DeRPN_fig1.png\" alt></p>\n<p>DeRPN  anchor strings anchor </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> CNN  $\\mathbf x$ anchor box $B_a$ sigmoid softmax $\\sigma$ bbox <br>$$\\mathbf t = \\mathbf {W}_t \\mathbf x+ \\mathbf {b}_r<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma (\\mathbf {W}_c \\mathbf x + \\mathbf {b}_c)$$<br> $\\mathbf {W_r, b_r}$ $\\mathbf {W_c, b_c}$ $\\psi$  box  Faster R-CNN  $\\mathbf t$  region proposals  box </p>\n<p>anchor  anchor  anchor  gt box !  anchor string$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$anchor string  $(S_w(x,w), S_h(y,h))$  $(P_s^w, P_s^h)$<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br> bbox <br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>f g  bbox </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> n  $n^2$  anchor box  $O(n^2)$n  anchor string  $O(n)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN  anchor string DeRPN  box  anchor string anchor string object  anchor string  ${a_n}$ (16,32,64,128,256,512,1024) $[8\\sqrt 2,1024 \\sqrt 2]$ $\\sqrt 2$ anchor string  $a_i$ anchor string $[a_i/\\sqrt 2, a_i\\sqrt 2]$2 $[8\\sqrt 2,1024 \\sqrt 2]$</p>\n<p> 2  DeRPN <br><img src=\"/images/DeRPN_fig2.png\" alt> <center>(a)  anchor string  anchor string(b)  anchor string  anchor string(c)  bbox(d)  NMS  region proposals</center></p>\n<p> anchor string RPN  anchor box  gt box  IoU  anchor  anchor  IoU  0.7 gt  IoU  anchor  DeRPN  anchor string  anchor string <br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,,N) \\quad(9)$$<br>$M_j$  j  anchor string $e_j$ N  ${a_n}$ q  2</p>\n<p> anchor string $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$ $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$  i $\\beta$  $e_j$  i  i+1  anchor string </p>\n<p> $a_i$  $[a_i/ \\sqrt q,a_i\\sqrt q]$ $e_j$  i 0 gt  i  gt  anchor string  anchor string $a_i, a_{i+1}$  $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$ $a_i \\sqrt q$ $e_j$  i  i+1 <br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br></p>\n<p> anchor string  $\\sqrt q$ $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$ $\\sqrt q$  DeRPN  RPN  IoU  anchor box  gtRPN </p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p> anchor string  feature map  (9) anchor string observe-to-distribute  anchor string1.  anchor string / region proposal region proposal  gt  IoU 0.6 anchor string  anchor string </p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN  RPN  two-stage  2  3x3  1x1  DeRPN  anchor string  ${a_n}$ N anchor string $2\\times 2N$  anchor string anchor string  $(x,w)$ $(y,h)$ $2 \\times 2N$ </p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p><br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,,M} \\quad (12)$$</p>\n<p>N M  batch sizes  anchor string$p_i$  i  anchor string $p_i^<em>$  gt label anchor string  1  0$t_i$ $t_i^</em>$  gt A  anchor string $R_j$  anchor string j  $a_j$$G_j$  anchor string j  $a_j$ anchor string  smooth L1 <br>$$L_{cls}(p_i,p_i^<em>)=- p_i^</em>\\log p_i-(1-p_i^<em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^</em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j*})$$</p>\n<p> t  Fast/Faster R-CNN  box <br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>DeRPN  region proposal  bbox region proposal</p>\n<p><strong></strong>  t  anchor string  W top-N $W_N$ top-N  (x,w) $p^W$ (x,w)  top-k  $(y^{(k)},h^{(k)})$ bbox $B_w={(x,y^{(k)},w,h^{(k)}}$ bbox <br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br> $p^W$  (x,w) $p^H$  $(y^{(k)},h^{(k)})$ </p>\n<p> top-N  $H_N$ $B_h={(x^{(k)},y,w^{(k)},h}$ $B=B_w \\cup B_h$  NMS top-M  region proposals bbox stage </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li> DeRPN</li>\n<li></li>\n</ol>\n"},{"title":"BBox-Reg-Uncertainty","date":"2019-06-28T01:23:16.000Z","mathjax":true,"_content":"[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n# \n ImageNetMS-COCO  CrowdHuman  ground truth bounding box gt bbox  bbox  1\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO  gt bbox (a)(c) (b) (d) </center>\n\n SOTA  Faster R-CNNCascade R-CNN  Mask R-CNN  bbox  bbox  smooth-L1  gt box  bbox  Inference  2 bbox  __ loss __\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO  VGG-16 Faster R-CNN (a) (b)  bbox </center>\n\n bbox  KL loss bbox  __ loss __ gt box  box  gt box  Gaussian  Dirac delta KL loss  gt  KL  KL KL  P(x) Q(x)  KL \n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n p(x)  q(x) \n P,Q  KL  0\n\n KL loss  bbox \n1.  bbox\n2.  var voting (variance voting) box  box  Fig 2 \n3.  box \n\n\n\n KL loss  var voting PASCAL VOC 2007  MS-COCO  benchmark VGG-CNN-M-1024, VGG-16, ResNet-5-FPN  Mask R-CNN Faster R-CNN\n\n# \n## BBox \n Faster R-CNN  Mask R-CNN  3 bbx  Box  shape  (N, 84) N  proposals  batch size84  21  4  PASCAL VOC  21 Box std  shape  (N, 84) 21  4  $\\sigma$not class-agnostic box \n![](/images/BBox-reg_fig3.png)\n\n $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$  bbox $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ \n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n *  gt offset *  offset$(x_{1a},y_{1a},x_{2a},y_{2a})$  anchor box x x $\\{x_1,y_1,x_2,y_2\\}$\n\n bbox \n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n $\\Theta$ $x_e$  bbox  $\\sigma$  $\\sigma \\rightarrow 0$ bbox \n\n~~ Faster R-CNN bbox  blob 4   4 $\\Theta$ ~~\n\ngt box  0 $\\sigma \\rightarrow 0$ Dirac delta \n$$P_D(x)=\\delta(x-x_g)$$\n $x_g$  gt box  x \n\n##  KL Loss  BBox \n $P_{\\Theta}(x)$  $P_D(x)$  KL  $\\hat \\Theta$ KL \n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n N x  4 KL \n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n$H(P_D(x))$  Dirac delta \n\n 4\n![](/images/BBox-reg_fig4.png)\n\n box  $x_e$  $\\sigma^2$  $L_{reg}$ $H(P_D(x)), \\log (2\\pi)/2$  $\\Theta$ \n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n $\\sigma=1$KL Loss \n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n $x_e$  $\\sigma$ \n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n $\\sigma$  $\\alpha=\\log \\sigma^2$  $\\sigma$ 3  Box std  $\\alpha$\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n $L_{reg}$  $\\alpha$  $\\alpha$  $\\sigma$ Box std  $\\sigma=\\sqrt{e^{\\alpha}}$ \n\n $|x_g - x_e| > 1$  smooth-L1  $x_g,x_e$ \n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n bbox  offset  $\\sigma$ $\\sigma$  $\\alpha$$\\alpha$  Gaussian  Gaussian  0.0001 0\n\n## Variance Voting\n $\\sigma^2$  bbox  box IoU>0 box Variance Voting  Fig 2 \n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$  Nx4  boxes\n\n$\\mathcal S$  N \n\n$\\mathcal C$  Nx4 \n\n$\\mathcal D$ $\\sigma_t$  var voting \n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$  $\\arg \\max \\mathcal S$\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n box  box  box  IoU  boxNMS  box soft-NMS  NMS  box  $f(IoU(b_m,b_i))$  [CV ](/2019/06/24/cv-mtds)\n\n 1  box b $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$ soft-NMS  boxes IoU>0 boxes boxes $\\sigma$  box  box  box box  x  x<sub>1</sub> x<sub>i</sub>  i  box \n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n box  boxes  box  box  box IoU  p<sub>i</sub>  voting  box  box   $\\sigma^2$  voting  box \n\n# \n\n\n# \n gt box  SOTA  bbox  KL Loss  var voting  box \n\n Faster R-CNN/Mask R-CNN  KL Loss  smooth L1 Loss var voting  1  $\\mathcal B$ Faster R-CNN ","source":"_posts/BBox-Reg-Uncertainty.md","raw":"---\ntitle: BBox-Reg-Uncertainty\ndate: 2019-06-28 09:23:16\ntags: object detection\nmathjax: true\n---\n[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n# \n ImageNetMS-COCO  CrowdHuman  ground truth bounding box gt bbox  bbox  1\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO  gt bbox (a)(c) (b) (d) </center>\n\n SOTA  Faster R-CNNCascade R-CNN  Mask R-CNN  bbox  bbox  smooth-L1  gt box  bbox  Inference  2 bbox  __ loss __\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO  VGG-16 Faster R-CNN (a) (b)  bbox </center>\n\n bbox  KL loss bbox  __ loss __ gt box  box  gt box  Gaussian  Dirac delta KL loss  gt  KL  KL KL  P(x) Q(x)  KL \n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n p(x)  q(x) \n P,Q  KL  0\n\n KL loss  bbox \n1.  bbox\n2.  var voting (variance voting) box  box  Fig 2 \n3.  box \n\n\n\n KL loss  var voting PASCAL VOC 2007  MS-COCO  benchmark VGG-CNN-M-1024, VGG-16, ResNet-5-FPN  Mask R-CNN Faster R-CNN\n\n# \n## BBox \n Faster R-CNN  Mask R-CNN  3 bbx  Box  shape  (N, 84) N  proposals  batch size84  21  4  PASCAL VOC  21 Box std  shape  (N, 84) 21  4  $\\sigma$not class-agnostic box \n![](/images/BBox-reg_fig3.png)\n\n $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$  bbox $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ \n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n *  gt offset *  offset$(x_{1a},y_{1a},x_{2a},y_{2a})$  anchor box x x $\\{x_1,y_1,x_2,y_2\\}$\n\n bbox \n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n $\\Theta$ $x_e$  bbox  $\\sigma$  $\\sigma \\rightarrow 0$ bbox \n\n~~ Faster R-CNN bbox  blob 4   4 $\\Theta$ ~~\n\ngt box  0 $\\sigma \\rightarrow 0$ Dirac delta \n$$P_D(x)=\\delta(x-x_g)$$\n $x_g$  gt box  x \n\n##  KL Loss  BBox \n $P_{\\Theta}(x)$  $P_D(x)$  KL  $\\hat \\Theta$ KL \n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n N x  4 KL \n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n$H(P_D(x))$  Dirac delta \n\n 4\n![](/images/BBox-reg_fig4.png)\n\n box  $x_e$  $\\sigma^2$  $L_{reg}$ $H(P_D(x)), \\log (2\\pi)/2$  $\\Theta$ \n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n $\\sigma=1$KL Loss \n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n $x_e$  $\\sigma$ \n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n $\\sigma$  $\\alpha=\\log \\sigma^2$  $\\sigma$ 3  Box std  $\\alpha$\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n $L_{reg}$  $\\alpha$  $\\alpha$  $\\sigma$ Box std  $\\sigma=\\sqrt{e^{\\alpha}}$ \n\n $|x_g - x_e| > 1$  smooth-L1  $x_g,x_e$ \n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n bbox  offset  $\\sigma$ $\\sigma$  $\\alpha$$\\alpha$  Gaussian  Gaussian  0.0001 0\n\n## Variance Voting\n $\\sigma^2$  bbox  box IoU>0 box Variance Voting  Fig 2 \n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$  Nx4  boxes\n\n$\\mathcal S$  N \n\n$\\mathcal C$  Nx4 \n\n$\\mathcal D$ $\\sigma_t$  var voting \n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$  $\\arg \\max \\mathcal S$\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n box  box  box  IoU  boxNMS  box soft-NMS  NMS  box  $f(IoU(b_m,b_i))$  [CV ](/2019/06/24/cv-mtds)\n\n 1  box b $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$ soft-NMS  boxes IoU>0 boxes boxes $\\sigma$  box  box  box box  x  x<sub>1</sub> x<sub>i</sub>  i  box \n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n box  boxes  box  box  box IoU  p<sub>i</sub>  voting  box  box   $\\sigma^2$  voting  box \n\n# \n\n\n# \n gt box  SOTA  bbox  KL Loss  var voting  box \n\n Faster R-CNN/Mask R-CNN  KL Loss  smooth L1 Loss var voting  1  $\\mathcal B$ Faster R-CNN ","slug":"BBox-Reg-Uncertainty","published":1,"updated":"2019-06-28T09:21:00.064Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379bs0010dgvcd8yiapcv","content":"<p><a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> ImageNetMS-COCO  CrowdHuman  ground truth bounding box gt bbox  bbox  1<br><img src=\"/images/BBox-reg_fig1.png\" alt> <center>Fig 1 MS-COCO  gt bbox (a)(c) (b) (d) </center></p>\n<p> SOTA  Faster R-CNNCascade R-CNN  Mask R-CNN  bbox  bbox  smooth-L1  gt box  bbox  Inference  2 bbox  <strong> loss </strong>\n<img src=\"/images/BBox-reg_fig2.png\" alt> <center>Fig 2 MS-COCO  VGG-16 Faster R-CNN (a) (b)  bbox </center></p>\n<p> bbox  KL loss bbox  <strong> loss </strong> gt box  box  gt box  Gaussian  Dirac delta KL loss  gt  KL  KL KL  P(x) Q(x)  KL <br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br><br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br> p(x)  q(x) <br> P,Q  KL  0</p>\n<p> KL loss  bbox </p>\n<ol>\n<li> bbox</li>\n<li> var voting (variance voting) box  box  Fig 2 </li>\n<li> box </li>\n</ol>\n<p> KL loss  var voting PASCAL VOC 2007  MS-COCO  benchmark VGG-CNN-M-1024, VGG-16, ResNet-5-FPN  Mask R-CNN Faster R-CNN</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"BBox-\"><a href=\"#BBox-\" class=\"headerlink\" title=\"BBox \"></a>BBox </h2><p> Faster R-CNN  Mask R-CNN  3 bbx  Box  shape  (N, 84) N  proposals  batch size84  21  4  PASCAL VOC  21 Box std  shape  (N, 84) 21  4  $\\sigma$not class-agnostic box <br><img src=\"/images/BBox-reg_fig3.png\" alt></p>\n<p> $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$  bbox ${t_i| i=x_1,y_1,x_2,y_2}$ </p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p> *  gt offset *  offset$(x_{1a},y_{1a},x_{2a},y_{2a})$  anchor box x x ${x_1,y_1,x_2,y_2}$</p>\n<p> bbox <br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br> $\\Theta$ $x_e$  bbox  $\\sigma$  $\\sigma \\rightarrow 0$ bbox </p>\n<p><del> Faster R-CNN bbox  blob 4   4 $\\Theta$ </del></p>\n<p>gt box  0 $\\sigma \\rightarrow 0$ Dirac delta <br>$$P_D(x)=\\delta(x-x_g)$$<br> $x_g$  gt box  x </p>\n<h2 id=\"-KL-Loss--BBox-\"><a href=\"#-KL-Loss--BBox-\" class=\"headerlink\" title=\" KL Loss  BBox \"></a> KL Loss  BBox </h2><p> $P_{\\Theta}(x)$  $P_D(x)$  KL  $\\hat \\Theta$ KL <br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br> N x  4 KL <br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>$H(P_D(x))$  Dirac delta </p>\n<p> 4<br><img src=\"/images/BBox-reg_fig4.png\" alt></p>\n<p> box  $x_e$  $\\sigma^2$  $L_{reg}$ $H(P_D(x)), \\log (2\\pi)/2$  $\\Theta$ <br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br> $\\sigma=1$KL Loss <br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br> $x_e$  $\\sigma$ <br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p> $\\sigma$  $\\alpha=\\log \\sigma^2$  $\\sigma$ 3  Box std  $\\alpha$<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br> $L_{reg}$  $\\alpha$  $\\alpha$  $\\sigma$ Box std  $\\sigma=\\sqrt{e^{\\alpha}}$ </p>\n<p> $|x_g - x_e| &gt; 1$  smooth-L1  $x_g,x_e$ <br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p> bbox  offset  $\\sigma$ $\\sigma$  $\\alpha$$\\alpha$  Gaussian  Gaussian  0.0001 0</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p> $\\sigma^2$  bbox  box IoU&gt;0 box Variance Voting  Fig 2 </p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$  Nx4  boxes</p>\n<p>$\\mathcal S$  N </p>\n<p>$\\mathcal C$  Nx4 </p>\n<p>$\\mathcal D$ $\\sigma_t$  var voting </p>\n<p>$\\mathcal B={b_1,,b_N}, \\ \\mathcal S={s_1,,s_N}, \\ \\mathcal C={\\sigma_1^2,,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$  $\\arg \\max \\mathcal S$</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color=\"cyan\">$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color=\"gree\">$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color=\"gree\"> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color=\"gree\"> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p> box  box  box  IoU  boxNMS  box soft-NMS  NMS  box  $f(IoU(b_m,b_i))$  <a href=\"/2019/06/24/cv-mtds\">CV </a></p>\n<p> 1  box b ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$ soft-NMS  boxes IoU&gt;0 boxes boxes $\\sigma$  box  box  box box  x  x<sub>1</sub> x<sub>i</sub>  i  box <br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br> box  boxes  box  box  box IoU  p<sub>i</sub>  voting  box  box   $\\sigma^2$  voting  box </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> gt box  SOTA  bbox  KL Loss  var voting  box </p>\n<p> Faster R-CNN/Mask R-CNN  KL Loss  smooth L1 Loss var voting  1  $\\mathcal B$ Faster R-CNN </p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> ImageNetMS-COCO  CrowdHuman  ground truth bounding box gt bbox  bbox  1<br><img src=\"/images/BBox-reg_fig1.png\" alt> <center>Fig 1 MS-COCO  gt bbox (a)(c) (b) (d) </center></p>\n<p> SOTA  Faster R-CNNCascade R-CNN  Mask R-CNN  bbox  bbox  smooth-L1  gt box  bbox  Inference  2 bbox  <strong> loss </strong>\n<img src=\"/images/BBox-reg_fig2.png\" alt> <center>Fig 2 MS-COCO  VGG-16 Faster R-CNN (a) (b)  bbox </center></p>\n<p> bbox  KL loss bbox  <strong> loss </strong> gt box  box  gt box  Gaussian  Dirac delta KL loss  gt  KL  KL KL  P(x) Q(x)  KL <br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br><br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br> p(x)  q(x) <br> P,Q  KL  0</p>\n<p> KL loss  bbox </p>\n<ol>\n<li> bbox</li>\n<li> var voting (variance voting) box  box  Fig 2 </li>\n<li> box </li>\n</ol>\n<p> KL loss  var voting PASCAL VOC 2007  MS-COCO  benchmark VGG-CNN-M-1024, VGG-16, ResNet-5-FPN  Mask R-CNN Faster R-CNN</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"BBox-\"><a href=\"#BBox-\" class=\"headerlink\" title=\"BBox \"></a>BBox </h2><p> Faster R-CNN  Mask R-CNN  3 bbx  Box  shape  (N, 84) N  proposals  batch size84  21  4  PASCAL VOC  21 Box std  shape  (N, 84) 21  4  $\\sigma$not class-agnostic box <br><img src=\"/images/BBox-reg_fig3.png\" alt></p>\n<p> $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$  bbox ${t_i| i=x_1,y_1,x_2,y_2}$ </p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p> *  gt offset *  offset$(x_{1a},y_{1a},x_{2a},y_{2a})$  anchor box x x ${x_1,y_1,x_2,y_2}$</p>\n<p> bbox <br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br> $\\Theta$ $x_e$  bbox  $\\sigma$  $\\sigma \\rightarrow 0$ bbox </p>\n<p><del> Faster R-CNN bbox  blob 4   4 $\\Theta$ </del></p>\n<p>gt box  0 $\\sigma \\rightarrow 0$ Dirac delta <br>$$P_D(x)=\\delta(x-x_g)$$<br> $x_g$  gt box  x </p>\n<h2 id=\"-KL-Loss--BBox-\"><a href=\"#-KL-Loss--BBox-\" class=\"headerlink\" title=\" KL Loss  BBox \"></a> KL Loss  BBox </h2><p> $P_{\\Theta}(x)$  $P_D(x)$  KL  $\\hat \\Theta$ KL <br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br> N x  4 KL <br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>$H(P_D(x))$  Dirac delta </p>\n<p> 4<br><img src=\"/images/BBox-reg_fig4.png\" alt></p>\n<p> box  $x_e$  $\\sigma^2$  $L_{reg}$ $H(P_D(x)), \\log (2\\pi)/2$  $\\Theta$ <br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br> $\\sigma=1$KL Loss <br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br> $x_e$  $\\sigma$ <br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p> $\\sigma$  $\\alpha=\\log \\sigma^2$  $\\sigma$ 3  Box std  $\\alpha$<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br> $L_{reg}$  $\\alpha$  $\\alpha$  $\\sigma$ Box std  $\\sigma=\\sqrt{e^{\\alpha}}$ </p>\n<p> $|x_g - x_e| &gt; 1$  smooth-L1  $x_g,x_e$ <br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p> bbox  offset  $\\sigma$ $\\sigma$  $\\alpha$$\\alpha$  Gaussian  Gaussian  0.0001 0</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p> $\\sigma^2$  bbox  box IoU&gt;0 box Variance Voting  Fig 2 </p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$  Nx4  boxes</p>\n<p>$\\mathcal S$  N </p>\n<p>$\\mathcal C$  Nx4 </p>\n<p>$\\mathcal D$ $\\sigma_t$  var voting </p>\n<p>$\\mathcal B={b_1,,b_N}, \\ \\mathcal S={s_1,,s_N}, \\ \\mathcal C={\\sigma_1^2,,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$  $\\arg \\max \\mathcal S$</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color=\"cyan\">$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color=\"gree\">$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color=\"gree\"> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color=\"gree\"> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p> box  box  box  IoU  boxNMS  box soft-NMS  NMS  box  $f(IoU(b_m,b_i))$  <a href=\"/2019/06/24/cv-mtds\">CV </a></p>\n<p> 1  box b ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$ soft-NMS  boxes IoU&gt;0 boxes boxes $\\sigma$  box  box  box box  x  x<sub>1</sub> x<sub>i</sub>  i  box <br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br> box  boxes  box  box  box IoU  p<sub>i</sub>  voting  box  box   $\\sigma^2$  voting  box </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> gt box  SOTA  bbox  KL Loss  var voting  box </p>\n<p> Faster R-CNN/Mask R-CNN  KL Loss  smooth L1 Loss var voting  1  $\\mathcal B$ Faster R-CNN </p>\n"},{"title":"DSOD","date":"2019-07-08T01:14:40.000Z","mathjax":true,"_content":" [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n# Introduction\n CNN  InceptionResNet  DenseNet  CV  backbone backbone  benchmark  ImageNet  fine-tune\n1.  ImageNet  backbone \n2. /\n3. fine-tuning ImageNet\n\n train from scratch DSOD \n\n# DSOD\n## \nDSOD  SSD  proposalone-stageDSOD  backbone response mapsbackbone  DenseNet  ImageNet  fine-tune DenseNet  stem block dense block transition layer  transition layerdense 1  DSOD  SSD  maps \n![](/images/DSOD_fig1.png)<center>Fig 1:  SSD  dense </center>\n\n DSOD  1 \n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD  </center>\n\nDSOD \n###  Proposal\n SOTA \n1. R-CNN  Fast R-CNN proposal  selective search\n2. Faster R-CNN  R-FCN  RPN  region proposals\n3. YOLO  SSD single-shot  proposalsproposal-free\n\nproposal-free RoI pooling  region proposal  pooling  region  conv feature  proposal  RoI pooling  layers  train from scratch  layers  layers \n\ntraining from scratch  proposal-free \n\n### \n DenseNets  block  layer  layers  dense blockDenseNet  layers  skip connections  DenseNet  transition layer  dense block  layers Transition w/o pooling layer \n\n### Stem Block\nStem block  3x3  2x2  2 stem block  DenseNet 7x7  2 2  3x3 stem block  image \n\n### \n 1 1.  SSD 2.  image  300x3006  feature maps  Scale-1 feature maps  backbone  feature maps  38x38 feature maps  backbone  1  feature maps  transition layer  transition layer  bottleneck  1x1  previous scale  feature maps  3x3  next scale  feature maps\n\n 1  SSD  feature maps  DSOD  scale-1 feature maps  conv  conv  1  feature maps  feature maps  1  2x2  2  max pooling 1x1  1  conv max pooling  feature maps  concatenate  1x1 conv  feature maps max pooling  1x1 conv  scale  feature maps previous feature maps\n\n# Experiments\n\n\n# Conclusion\n DSOD  training from scratch single-shot  SSD  DenseNet  backbone DenseNet ","source":"_posts/DSOD.md","raw":"---\ntitle: DSOD\ndate: 2019-07-08 09:14:40\ntags: object detection\nmathjax: true\n---\n [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n# Introduction\n CNN  InceptionResNet  DenseNet  CV  backbone backbone  benchmark  ImageNet  fine-tune\n1.  ImageNet  backbone \n2. /\n3. fine-tuning ImageNet\n\n train from scratch DSOD \n\n# DSOD\n## \nDSOD  SSD  proposalone-stageDSOD  backbone response mapsbackbone  DenseNet  ImageNet  fine-tune DenseNet  stem block dense block transition layer  transition layerdense 1  DSOD  SSD  maps \n![](/images/DSOD_fig1.png)<center>Fig 1:  SSD  dense </center>\n\n DSOD  1 \n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD  </center>\n\nDSOD \n###  Proposal\n SOTA \n1. R-CNN  Fast R-CNN proposal  selective search\n2. Faster R-CNN  R-FCN  RPN  region proposals\n3. YOLO  SSD single-shot  proposalsproposal-free\n\nproposal-free RoI pooling  region proposal  pooling  region  conv feature  proposal  RoI pooling  layers  train from scratch  layers  layers \n\ntraining from scratch  proposal-free \n\n### \n DenseNets  block  layer  layers  dense blockDenseNet  layers  skip connections  DenseNet  transition layer  dense block  layers Transition w/o pooling layer \n\n### Stem Block\nStem block  3x3  2x2  2 stem block  DenseNet 7x7  2 2  3x3 stem block  image \n\n### \n 1 1.  SSD 2.  image  300x3006  feature maps  Scale-1 feature maps  backbone  feature maps  38x38 feature maps  backbone  1  feature maps  transition layer  transition layer  bottleneck  1x1  previous scale  feature maps  3x3  next scale  feature maps\n\n 1  SSD  feature maps  DSOD  scale-1 feature maps  conv  conv  1  feature maps  feature maps  1  2x2  2  max pooling 1x1  1  conv max pooling  feature maps  concatenate  1x1 conv  feature maps max pooling  1x1 conv  scale  feature maps previous feature maps\n\n# Experiments\n\n\n# Conclusion\n DSOD  training from scratch single-shot  SSD  DenseNet  backbone DenseNet ","slug":"DSOD","published":1,"updated":"2019-07-17T06:12:00.637Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379bu0012dgvcjjxidx7g","content":"<p> <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> CNN  InceptionResNet  DenseNet  CV  backbone backbone  benchmark  ImageNet  fine-tune</p>\n<ol>\n<li> ImageNet  backbone </li>\n<li>/</li>\n<li>fine-tuning ImageNet</li>\n</ol>\n<p> train from scratch DSOD </p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>DSOD  SSD  proposalone-stageDSOD  backbone response mapsbackbone  DenseNet  ImageNet  fine-tune DenseNet  stem block dense block transition layer  transition layerdense 1  DSOD  SSD  maps <br><img src=\"/images/DSOD_fig1.png\" alt><center>Fig 1:  SSD  dense </center></p>\n<p> DSOD  1 </p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD  </center>\n\n<p>DSOD </p>\n<h3 id=\"-Proposal\"><a href=\"#-Proposal\" class=\"headerlink\" title=\" Proposal\"></a> Proposal</h3><p> SOTA </p>\n<ol>\n<li>R-CNN  Fast R-CNN proposal  selective search</li>\n<li>Faster R-CNN  R-FCN  RPN  region proposals</li>\n<li>YOLO  SSD single-shot  proposalsproposal-free</li>\n</ol>\n<p>proposal-free RoI pooling  region proposal  pooling  region  conv feature  proposal  RoI pooling  layers  train from scratch  layers  layers </p>\n<p>training from scratch  proposal-free </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> DenseNets  block  layer  layers  dense blockDenseNet  layers  skip connections  DenseNet  transition layer  dense block  layers Transition w/o pooling layer </p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block  3x3  2x2  2 stem block  DenseNet 7x7  2 2  3x3 stem block  image </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 1 1.  SSD 2.  image  300x3006  feature maps  Scale-1 feature maps  backbone  feature maps  38x38 feature maps  backbone  1  feature maps  transition layer  transition layer  bottleneck  1x1  previous scale  feature maps  3x3  next scale  feature maps</p>\n<p> 1  SSD  feature maps  DSOD  scale-1 feature maps  conv  conv  1  feature maps  feature maps  1  2x2  2  max pooling 1x1  1  conv max pooling  feature maps  concatenate  1x1 conv  feature maps max pooling  1x1 conv  scale  feature maps previous feature maps</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p> DSOD  training from scratch single-shot  SSD  DenseNet  backbone DenseNet </p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> CNN  InceptionResNet  DenseNet  CV  backbone backbone  benchmark  ImageNet  fine-tune</p>\n<ol>\n<li> ImageNet  backbone </li>\n<li>/</li>\n<li>fine-tuning ImageNet</li>\n</ol>\n<p> train from scratch DSOD </p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>DSOD  SSD  proposalone-stageDSOD  backbone response mapsbackbone  DenseNet  ImageNet  fine-tune DenseNet  stem block dense block transition layer  transition layerdense 1  DSOD  SSD  maps <br><img src=\"/images/DSOD_fig1.png\" alt><center>Fig 1:  SSD  dense </center></p>\n<p> DSOD  1 </p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD  </center>\n\n<p>DSOD </p>\n<h3 id=\"-Proposal\"><a href=\"#-Proposal\" class=\"headerlink\" title=\" Proposal\"></a> Proposal</h3><p> SOTA </p>\n<ol>\n<li>R-CNN  Fast R-CNN proposal  selective search</li>\n<li>Faster R-CNN  R-FCN  RPN  region proposals</li>\n<li>YOLO  SSD single-shot  proposalsproposal-free</li>\n</ol>\n<p>proposal-free RoI pooling  region proposal  pooling  region  conv feature  proposal  RoI pooling  layers  train from scratch  layers  layers </p>\n<p>training from scratch  proposal-free </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> DenseNets  block  layer  layers  dense blockDenseNet  layers  skip connections  DenseNet  transition layer  dense block  layers Transition w/o pooling layer </p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block  3x3  2x2  2 stem block  DenseNet 7x7  2 2  3x3 stem block  image </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 1 1.  SSD 2.  image  300x3006  feature maps  Scale-1 feature maps  backbone  feature maps  38x38 feature maps  backbone  1  feature maps  transition layer  transition layer  bottleneck  1x1  previous scale  feature maps  3x3  next scale  feature maps</p>\n<p> 1  SSD  feature maps  DSOD  scale-1 feature maps  conv  conv  1  feature maps  feature maps  1  2x2  2  max pooling 1x1  1  conv max pooling  feature maps  concatenate  1x1 conv  feature maps max pooling  1x1 conv  scale  feature maps previous feature maps</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p> DSOD  training from scratch single-shot  SSD  DenseNet  backbone DenseNet </p>\n"},{"title":"FSAF","date":"2019-06-27T01:14:42.000Z","mathjax":true,"_content":"[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n\nSOTA  feature pyramid  image pyramid feature pyramid  level  feature  anchor level  feature  anchor 2 level  feature  level  feature \n1. \n2.  overlap  anchor \n\n 1  level  feature  feature level  2  IoU  anchor \n\n![](/images/FSAF_fig2.png)\n\n feature selective anchor-free (FSAF) feature level 3\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF  anchor pyramid level</center>\n\n feature pyramid  level  anchor-free  anchor-based  3  level  anchor-free  box  feature level feature level Inference  FSAF  anchor-based anchor-free  FSAF  FSAF \n\n# FSAF \n FSAF  feature pyramid  single-shot  SSD, DSSD, RetinaNet FSAF  SOTA  RetinaNet\n1.  anchor-free \n2.  anchor-free GT target\n3.  feature level\n4. / anchor-free  anchor-based \n\n## \n 4  FSAF  RetinaNet Retina  backbone  backbone  feature pyramid level  $\\{P_l|l\\in [3,7]\\}$$P_l$  image  $1/2^l$  4  level  feature pyramid level  scale  feature level \n\n RetinaNetFSAF  feature level  4\n\n![](/images/FSAF_fig4.png)<center>Fig 4  FSAF  RetinaNet </center>\n\n anchor-free  3x3  K  sigmoid  anchor-based  K  3x3  4  ReLu  anchor-free  box anchor-free  anchor-based  level  feature\n\n## Ground-truth and Loss\n k  bbox  b=[x,y,w,h] feature level $P_l$  box  $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$ $P_l$  image  $1/2^l$ $b_p^l=b/2^l$ box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$  ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$ $b_p^l$  $\\epsilon_e, \\ \\epsilon_i$\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n [GA-RPN](/2019/06/25/GA-RPN)  anchor-free  GA-RPN  anchor-free  scale  aspect ratio  anchor anchor-free  anchor  shape  anchor-based  scale  aspect ratio\n\n 5  car  GT GT target\n\n![](/images/FSAF_fig5.png)\n\n____  GT output  K-channel maps map  k k  GT map \n-  $b_e^l$  1 5 \n-  $b_i^l - b_e^l$  5 \n-  feature level $b_i^{l-1}, b_i^{l+1}$ \n\n feature level \n\n box gt map  0 5  Focal loss\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free  ignore box  focal loss  box \n\n__Box __  gt  4-channal maps 4  4K-channel  gt maps  $b_e^l$  $b_e^l$  (i,j) 4-d  $b_p^l$\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n $d_t^l,d_l^l,d_b^l,d_r^l$  (i,j)  $b_p^l$  top,left,bottom,right  [FCOS](FCOS)  anchor-free  (i,j)  4-d  $\\mathbf d_{i,j}^l/S$ S=4 box  IoU anchor-free  box  IoU  box  IoU \n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n UnitBox\n\nInference  box (i,j) $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$ $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$ $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$ $2^l$  image  maps  (i,j)  K-d \n\n## \nFSAF  level  feature $P_l$ anchor-based  box \n\n $I$ $P_l$  $L_{FL}^I(l), \\ L_{IoU}^I(l)$\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n$N(b_e^l)$  box  $I$ \n\n 6 \n\n![](/images/FSAF_fig6.png) <center>Fig 6 levelanchor-freegt target</center>\n\n $I$  feature pyramid anchor-free  $L_{FL}^I(l) + L_{IoU}^I(l)$  feature pyramid leve $P_l$\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n level  level Inference \n\n box size FPN  $I$  $P_{l'}$\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n(w,h)  size224  ImageNet 224x224  $l_0$  target level  $P_l$  image  $1/2^l$\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n scale  $1/2^l$  $l_0=5$ ResNet  conv5_x  feature map \n\n## Joint Inference and Training\n FSAF  RetinaNet  4 anchor-based \n\n__Inference:__ FSAF  anchor-free  pyramid level  0.05  top 1k  box level  box  anchor-based  box  NMSNMS  0.5\n\n____ backbone  ImageNet1k RetinaNet  layers  RetinaNet  layers FSAF   layers  $\\sigma=0.01$ bias  $-\\log((1-\\pi)/\\pi)$ $\\pi$  $\\pi$  RetineNet  $\\pi=0.01$ layers  b=0.1 $\\sigma=0.01$\n\n____  anchor-free  anchor-based  RetinaNet  $L^{ab}$ $L_{cls}^{af}, \\ L_{reg}^{af}$  anchor-free  $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$ $\\lambda$  $\\lambda=0.5$\n\n# \n\n\n#\n feature pyramid  anchor-based single-shot  FSAF FSAF  anchor-free inference  SOTA single-shot ","source":"_posts/FSAF.md","raw":"---\ntitle: FSAF\ndate: 2019-06-27 09:14:42\ntags: object detection\nmathjax: true\n---\n[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n\nSOTA  feature pyramid  image pyramid feature pyramid  level  feature  anchor level  feature  anchor 2 level  feature  level  feature \n1. \n2.  overlap  anchor \n\n 1  level  feature  feature level  2  IoU  anchor \n\n![](/images/FSAF_fig2.png)\n\n feature selective anchor-free (FSAF) feature level 3\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF  anchor pyramid level</center>\n\n feature pyramid  level  anchor-free  anchor-based  3  level  anchor-free  box  feature level feature level Inference  FSAF  anchor-based anchor-free  FSAF  FSAF \n\n# FSAF \n FSAF  feature pyramid  single-shot  SSD, DSSD, RetinaNet FSAF  SOTA  RetinaNet\n1.  anchor-free \n2.  anchor-free GT target\n3.  feature level\n4. / anchor-free  anchor-based \n\n## \n 4  FSAF  RetinaNet Retina  backbone  backbone  feature pyramid level  $\\{P_l|l\\in [3,7]\\}$$P_l$  image  $1/2^l$  4  level  feature pyramid level  scale  feature level \n\n RetinaNetFSAF  feature level  4\n\n![](/images/FSAF_fig4.png)<center>Fig 4  FSAF  RetinaNet </center>\n\n anchor-free  3x3  K  sigmoid  anchor-based  K  3x3  4  ReLu  anchor-free  box anchor-free  anchor-based  level  feature\n\n## Ground-truth and Loss\n k  bbox  b=[x,y,w,h] feature level $P_l$  box  $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$ $P_l$  image  $1/2^l$ $b_p^l=b/2^l$ box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$  ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$ $b_p^l$  $\\epsilon_e, \\ \\epsilon_i$\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n [GA-RPN](/2019/06/25/GA-RPN)  anchor-free  GA-RPN  anchor-free  scale  aspect ratio  anchor anchor-free  anchor  shape  anchor-based  scale  aspect ratio\n\n 5  car  GT GT target\n\n![](/images/FSAF_fig5.png)\n\n____  GT output  K-channel maps map  k k  GT map \n-  $b_e^l$  1 5 \n-  $b_i^l - b_e^l$  5 \n-  feature level $b_i^{l-1}, b_i^{l+1}$ \n\n feature level \n\n box gt map  0 5  Focal loss\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free  ignore box  focal loss  box \n\n__Box __  gt  4-channal maps 4  4K-channel  gt maps  $b_e^l$  $b_e^l$  (i,j) 4-d  $b_p^l$\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n $d_t^l,d_l^l,d_b^l,d_r^l$  (i,j)  $b_p^l$  top,left,bottom,right  [FCOS](FCOS)  anchor-free  (i,j)  4-d  $\\mathbf d_{i,j}^l/S$ S=4 box  IoU anchor-free  box  IoU  box  IoU \n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n UnitBox\n\nInference  box (i,j) $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$ $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$ $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$ $2^l$  image  maps  (i,j)  K-d \n\n## \nFSAF  level  feature $P_l$ anchor-based  box \n\n $I$ $P_l$  $L_{FL}^I(l), \\ L_{IoU}^I(l)$\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n$N(b_e^l)$  box  $I$ \n\n 6 \n\n![](/images/FSAF_fig6.png) <center>Fig 6 levelanchor-freegt target</center>\n\n $I$  feature pyramid anchor-free  $L_{FL}^I(l) + L_{IoU}^I(l)$  feature pyramid leve $P_l$\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n level  level Inference \n\n box size FPN  $I$  $P_{l'}$\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n(w,h)  size224  ImageNet 224x224  $l_0$  target level  $P_l$  image  $1/2^l$\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n scale  $1/2^l$  $l_0=5$ ResNet  conv5_x  feature map \n\n## Joint Inference and Training\n FSAF  RetinaNet  4 anchor-based \n\n__Inference:__ FSAF  anchor-free  pyramid level  0.05  top 1k  box level  box  anchor-based  box  NMSNMS  0.5\n\n____ backbone  ImageNet1k RetinaNet  layers  RetinaNet  layers FSAF   layers  $\\sigma=0.01$ bias  $-\\log((1-\\pi)/\\pi)$ $\\pi$  $\\pi$  RetineNet  $\\pi=0.01$ layers  b=0.1 $\\sigma=0.01$\n\n____  anchor-free  anchor-based  RetinaNet  $L^{ab}$ $L_{cls}^{af}, \\ L_{reg}^{af}$  anchor-free  $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$ $\\lambda$  $\\lambda=0.5$\n\n# \n\n\n#\n feature pyramid  anchor-based single-shot  FSAF FSAF  anchor-free inference  SOTA single-shot ","slug":"FSAF","published":1,"updated":"2019-06-27T12:15:21.594Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379bv0014dgvcdyyigmr5","content":"<p><a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>\n<p>SOTA  feature pyramid  image pyramid feature pyramid  level  feature  anchor level  feature  anchor 2 level  feature  level  feature </p>\n<ol>\n<li></li>\n<li> overlap  anchor </li>\n</ol>\n<p> 1  level  feature  feature level  2  IoU  anchor </p>\n<p><img src=\"/images/FSAF_fig2.png\" alt></p>\n<p> feature selective anchor-free (FSAF) feature level 3</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt> <center>Fig 3 FSAF  anchor pyramid level</center></p>\n<p> feature pyramid  level  anchor-free  anchor-based  3  level  anchor-free  box  feature level feature level Inference  FSAF  anchor-based anchor-free  FSAF  FSAF </p>\n<h1 id=\"FSAF-\"><a href=\"#FSAF-\" class=\"headerlink\" title=\"FSAF \"></a>FSAF </h1><p> FSAF  feature pyramid  single-shot  SSD, DSSD, RetinaNet FSAF  SOTA  RetinaNet</p>\n<ol>\n<li> anchor-free </li>\n<li> anchor-free GT target</li>\n<li> feature level</li>\n<li>/ anchor-free  anchor-based </li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 4  FSAF  RetinaNet Retina  backbone  backbone  feature pyramid level  ${P_l|l\\in [3,7]}$$P_l$  image  $1/2^l$  4  level  feature pyramid level  scale  feature level </p>\n<p> RetinaNetFSAF  feature level  4</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt><center>Fig 4  FSAF  RetinaNet </center></p>\n<p> anchor-free  3x3  K  sigmoid  anchor-based  K  3x3  4  ReLu  anchor-free  box anchor-free  anchor-based  level  feature</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p> k  bbox  b=[x,y,w,h] feature level $P_l$  box  $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$ $P_l$  image  $1/2^l$ $b_p^l=b/2^l$ box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$  ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$ $b_p^l$  $\\epsilon_e, \\ \\epsilon_i$<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br> <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a>  anchor-free  GA-RPN  anchor-free  scale  aspect ratio  anchor anchor-free  anchor  shape  anchor-based  scale  aspect ratio</p>\n<p> 5  car  GT GT target</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt></p>\n<p><strong></strong>  GT output  K-channel maps map  k k  GT map </p>\n<ul>\n<li> $b_e^l$  1 5 </li>\n<li> $b_i^l - b_e^l$  5 </li>\n<li> feature level $b_i^{l-1}, b_i^{l+1}$ </li>\n</ul>\n<p> feature level </p>\n<p> box gt map  0 5  Focal loss<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free  ignore box  focal loss  box </p>\n<p><strong>Box </strong>  gt  4-channal maps 4  4K-channel  gt maps  $b_e^l$  $b_e^l$  (i,j) 4-d  $b_p^l$<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br> $d_t^l,d_l^l,d_b^l,d_r^l$  (i,j)  $b_p^l$  top,left,bottom,right  <a href=\"FCOS\">FCOS</a>  anchor-free  (i,j)  4-d  $\\mathbf d_{i,j}^l/S$ S=4 box  IoU anchor-free  box  IoU  box  IoU <br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br> UnitBox</p>\n<p>Inference  box (i,j) $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$ $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$ $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$ $2^l$  image  maps  (i,j)  K-d </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>FSAF  level  feature $P_l$ anchor-based  box </p>\n<p> $I$ $P_l$  $L_{FL}^I(l), \\ L_{IoU}^I(l)$<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>$N(b_e^l)$  box  $I$ </p>\n<p> 6 </p>\n<p><img src=\"/images/FSAF_fig6.png\" alt> <center>Fig 6 levelanchor-freegt target</center></p>\n<p> $I$  feature pyramid anchor-free  $L_{FL}^I(l) + L_{IoU}^I(l)$  feature pyramid leve $P_l$<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br> level  level Inference </p>\n<p> box size FPN  $I$  $P_{l}$<br>$$l = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>(w,h)  size224  ImageNet 224x224  $l_0$  target level  $P_l$  image  $1/2^l$<br>$$ \\sqrt{wh}/2^{l} \\approx 224/2^{l_0}$$<br> scale  $1/2^l$  $l_0=5$ ResNet  conv5_x  feature map </p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p> FSAF  RetinaNet  4 anchor-based </p>\n<p><strong>Inference:</strong> FSAF  anchor-free  pyramid level  0.05  top 1k  box level  box  anchor-based  box  NMSNMS  0.5</p>\n<p><strong></strong> backbone  ImageNet1k RetinaNet  layers  RetinaNet  layers FSAF   layers  $\\sigma=0.01$ bias  $-\\log((1-\\pi)/\\pi)$ $\\pi$  $\\pi$  RetineNet  $\\pi=0.01$ layers  b=0.1 $\\sigma=0.01$</p>\n<p><strong></strong>  anchor-free  anchor-based  RetinaNet  $L^{ab}$ $L_{cls}^{af}, \\ L_{reg}^{af}$  anchor-free  $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$ $\\lambda$  $\\lambda=0.5$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p>#<br> feature pyramid  anchor-based single-shot  FSAF FSAF  anchor-free inference  SOTA single-shot </p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>\n<p>SOTA  feature pyramid  image pyramid feature pyramid  level  feature  anchor level  feature  anchor 2 level  feature  level  feature </p>\n<ol>\n<li></li>\n<li> overlap  anchor </li>\n</ol>\n<p> 1  level  feature  feature level  2  IoU  anchor </p>\n<p><img src=\"/images/FSAF_fig2.png\" alt></p>\n<p> feature selective anchor-free (FSAF) feature level 3</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt> <center>Fig 3 FSAF  anchor pyramid level</center></p>\n<p> feature pyramid  level  anchor-free  anchor-based  3  level  anchor-free  box  feature level feature level Inference  FSAF  anchor-based anchor-free  FSAF  FSAF </p>\n<h1 id=\"FSAF-\"><a href=\"#FSAF-\" class=\"headerlink\" title=\"FSAF \"></a>FSAF </h1><p> FSAF  feature pyramid  single-shot  SSD, DSSD, RetinaNet FSAF  SOTA  RetinaNet</p>\n<ol>\n<li> anchor-free </li>\n<li> anchor-free GT target</li>\n<li> feature level</li>\n<li>/ anchor-free  anchor-based </li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 4  FSAF  RetinaNet Retina  backbone  backbone  feature pyramid level  ${P_l|l\\in [3,7]}$$P_l$  image  $1/2^l$  4  level  feature pyramid level  scale  feature level </p>\n<p> RetinaNetFSAF  feature level  4</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt><center>Fig 4  FSAF  RetinaNet </center></p>\n<p> anchor-free  3x3  K  sigmoid  anchor-based  K  3x3  4  ReLu  anchor-free  box anchor-free  anchor-based  level  feature</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p> k  bbox  b=[x,y,w,h] feature level $P_l$  box  $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$ $P_l$  image  $1/2^l$ $b_p^l=b/2^l$ box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$  ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$ $b_p^l$  $\\epsilon_e, \\ \\epsilon_i$<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br> <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a>  anchor-free  GA-RPN  anchor-free  scale  aspect ratio  anchor anchor-free  anchor  shape  anchor-based  scale  aspect ratio</p>\n<p> 5  car  GT GT target</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt></p>\n<p><strong></strong>  GT output  K-channel maps map  k k  GT map </p>\n<ul>\n<li> $b_e^l$  1 5 </li>\n<li> $b_i^l - b_e^l$  5 </li>\n<li> feature level $b_i^{l-1}, b_i^{l+1}$ </li>\n</ul>\n<p> feature level </p>\n<p> box gt map  0 5  Focal loss<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free  ignore box  focal loss  box </p>\n<p><strong>Box </strong>  gt  4-channal maps 4  4K-channel  gt maps  $b_e^l$  $b_e^l$  (i,j) 4-d  $b_p^l$<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br> $d_t^l,d_l^l,d_b^l,d_r^l$  (i,j)  $b_p^l$  top,left,bottom,right  <a href=\"FCOS\">FCOS</a>  anchor-free  (i,j)  4-d  $\\mathbf d_{i,j}^l/S$ S=4 box  IoU anchor-free  box  IoU  box  IoU <br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br> UnitBox</p>\n<p>Inference  box (i,j) $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$ $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$ $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$ $2^l$  image  maps  (i,j)  K-d </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>FSAF  level  feature $P_l$ anchor-based  box </p>\n<p> $I$ $P_l$  $L_{FL}^I(l), \\ L_{IoU}^I(l)$<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>$N(b_e^l)$  box  $I$ </p>\n<p> 6 </p>\n<p><img src=\"/images/FSAF_fig6.png\" alt> <center>Fig 6 levelanchor-freegt target</center></p>\n<p> $I$  feature pyramid anchor-free  $L_{FL}^I(l) + L_{IoU}^I(l)$  feature pyramid leve $P_l$<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br> level  level Inference </p>\n<p> box size FPN  $I$  $P_{l}$<br>$$l = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>(w,h)  size224  ImageNet 224x224  $l_0$  target level  $P_l$  image  $1/2^l$<br>$$ \\sqrt{wh}/2^{l} \\approx 224/2^{l_0}$$<br> scale  $1/2^l$  $l_0=5$ ResNet  conv5_x  feature map </p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p> FSAF  RetinaNet  4 anchor-based </p>\n<p><strong>Inference:</strong> FSAF  anchor-free  pyramid level  0.05  top 1k  box level  box  anchor-based  box  NMSNMS  0.5</p>\n<p><strong></strong> backbone  ImageNet1k RetinaNet  layers  RetinaNet  layers FSAF   layers  $\\sigma=0.01$ bias  $-\\log((1-\\pi)/\\pi)$ $\\pi$  $\\pi$  RetineNet  $\\pi=0.01$ layers  b=0.1 $\\sigma=0.01$</p>\n<p><strong></strong>  anchor-free  anchor-based  RetinaNet  $L^{ab}$ $L_{cls}^{af}, \\ L_{reg}^{af}$  anchor-free  $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$ $\\lambda$  $\\lambda=0.5$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p>#<br> feature pyramid  anchor-based single-shot  FSAF FSAF  anchor-free inference  SOTA single-shot </p>\n"},{"title":"GA-RPN","date":"2019-06-25T09:01:57.000Z","mathjax":true,"_content":"[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n\n anchor  proposaltwo-stage anchor one-stage two-stage  Faster R-CNN  feature map  anchors proposals proposals \n\n anchor alignment  consistency\n1. anchor  feature map \n2.  RF  anchor \n\n anchor  anchors feature map  scale  aspect ratio  k  anchors anchor \n1.  scale  aspect ratio\n2.  recall anchors anchors non-object anchors \n\n anchor \n1. image \n2.  scale  image \n   \n anchor guided anchoring\n1. \n2.  shape\n\n anchor shape  consistency  anchor shape  RF   scope  scale  aspect ratio  feature map  anchor shape  anchor shape  consistency  anchor shape  features  feature adaptation \n\n guided anchoring  feature adaptation  Guided Anchoring Region Proposal Network GP-RPN anchorsrecall  RPNbaseline anchor 9.1% anchors  90% scale  aspect ratio  aspect ratio  1/2   2 / region proposals guided anchoring  anchor  SSD anchor  boxguided anchoring  COCO  GA-Fast-RCNNGA-Faster-RCNN  GA-RetinaNet  mAP  baseline  2.2%, 2.7%  1.2%\n\n# Guided Anchoring\n location  shape  (x,y,w,h)  (x,y) (w,h)  image $I$ location  shape \n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n\n1.  image\n2. shape  scale  aspect ratio  anchor \n   \n anchor  1\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 feature map  anchor  anchor  shape feature  feature map  feature map anchor</center>\n\n image $I$ feature map $F_I$ $F_I$   mapshape  shape w,h shape anchor anchor  shape  feature  anchor shape  feature \n\n leve  feature maps  FPN  RetinaNet 1 level  anchor  level  anchor \n\n## Anchor \nanchor  map $p(\\cdot|F_I)$  feature map $F_I$  $p(i,j|F_I)$  image $I$  $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$ s   $F_I$  $I$  anchor $p(i,j|F_I)$  $F_I$  (i,j) \n\n $\\mathcal N_L$  $p(i,j|F_I)$$\\mathcal N_L$  1x1  element-wise  sigmoid  $\\mathcal N_L$  $\\mathcal N_L$ \n\n $\\epsilon_L$ map  region anchor  shape region 90%  region  recall RPN  4(b) region  region masked convolution \n\n## Anchor shape \n anchor  anchor  shape 1 bbox  anchor  alignment  $F_I$ shape (w,h) shape  anchor  gt box \n\n w,h  [-1,1] \n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n shape  (dw,dh) s  $F_I$  $I$ $\\sigma$  $\\sigma=8$ $\\mathcal N_S$  shape $\\mathcal N_S$  1x1  2  element-wise  2  dw map  dh map w map  h map\n\n anchor  anchor  anchor  anchor shape  scale  aspect ratio  k  anchor anchor shape  location  recall aspect ratio  shape anchor \n\n## Anchor-Guided \n RPN  one-stage  anchors  feature map  anchor  shape/scale  feature map  anchor  shape  feature map  anchor  RPN  one-stage  anchor  feature  region  anchor  feature  anchor-guided  anchor shape  feature \n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n$\\mathbf f_i$   i  feature$(w_i,h_i)$  anchor shape 3x3  $\\mathcal N_T$ 1 anchor shape  offset feature map  feature map  bbox \n\n## \n### \n anchor  shape \n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n anchor  image   label map 1 0  label map  gt box  anchor anchor gt box $(x_g,y_g,w_g,h_g)$  feature map $F_I$ $(x_g',y_g',w_g',h_g')$ $\\mathcal R(x,y,w,h)$ Anchors  gt box  IOU gt box \n1.  \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$ gt box  gt box  $\\sigma_1$ CR  1positive\n\n2. \n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$$\\sigma_2 > \\sigma_1$IR  `ignore` Faster R-CNN  RPNanchor  gt box  IOU  0.7  1 positive 0.3  0 negative `[0.3,0.7]`  -1 -1  anchor \n\n3. \n   \n   $OR=F_I \\setminus IR$OR  0negative\n\n level features level  feature map  scale  feature map  scale  CR IR  2 CR  IR IR  OR CR, IR, OR  recall CR  feature map  Focal Loss  anchor \n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n shape target\n1.  anchor  gt box \n2.  anchor  gt box\n\nFaster R-CNN  anchor  IOU  gt box anchor  gt box  $(t_x,t_y,t_w,t_h)$  target anchor  $(x,y,w,h)$ \n\n anchor  anchor  w,h  shape  $(w_p,h_p)$  target $(t_w,t_h)$  anchor  $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$  gt box $gt=(x_g,y_g,w_g,h_g)$  IoU  vIoU\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n anchor  $(x_0,y_0)$  gt box $gt$ $(x_0,y_0)$ w  h  w  h  w  h anchor  gt box  IoU IoU  $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$  9  (w,h)  vIoU 9  (w,h)  RetinaNet  scales  aspect ratios  (w,h)  vIoU  bounded iou loss  shape \n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n (w,h)  anchor shape(w<sub>g</sub>,h<sub>g</sub>)  anchor  vIoU  gt box  shape $\\min(\\frac w {w_g}, \\frac {w_g} w)$  $\\min(\\frac h {h_g}, \\frac {h_g} h)$ w  w<sub>g</sub>h  h<sub>g</sub>\n\n\n1.  9  (w,h)\n2.  (x<sub>0</sub>,y<sub>0</sub>) anchor  gt box  vIoU vIoU  9  (w,h)\n3.  vIoU  gt box  anchor \n4. shape  (w,h)  anchor  gt box  (w<sub>g</sub>,h<sub>g</sub>)  shape \n\n shape  (w,h)  gt box  IoU IoU  gt box  anchor  gt box \n\n shape  (w,h)  gt box anchor  gt box  anchor  target \n\n##  proposals\n guided anchoring  RPNGA-RPN proposals proposals two-stage  RPN  GA-RPN  proposals  IoU  3\n![](/images/GA-RPN_fig3.png) <center>Fig 3  IoU  proposals </center>\n\n RPNGA-RPN \n1.  proposals \n2.  IoU  proposals \n\n RPN  GA-RPN  1  proposals  proposal \n\nGA-RPN  two-stage  proposal  RPN GA-RPN proposals  epochs 3  epochsGA-RPN proposals  inference epochs\n\n![](/images/GA-RPN_fig4.png)\n\n# \n\n\n# \n Guided Anchoring  shape  anchor","source":"_posts/GA-RPN.md","raw":"---\ntitle: GA-RPN\ndate: 2019-06-25 17:01:57\ntags: object detection\nmathjax: true\n---\n[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n\n anchor  proposaltwo-stage anchor one-stage two-stage  Faster R-CNN  feature map  anchors proposals proposals \n\n anchor alignment  consistency\n1. anchor  feature map \n2.  RF  anchor \n\n anchor  anchors feature map  scale  aspect ratio  k  anchors anchor \n1.  scale  aspect ratio\n2.  recall anchors anchors non-object anchors \n\n anchor \n1. image \n2.  scale  image \n   \n anchor guided anchoring\n1. \n2.  shape\n\n anchor shape  consistency  anchor shape  RF   scope  scale  aspect ratio  feature map  anchor shape  anchor shape  consistency  anchor shape  features  feature adaptation \n\n guided anchoring  feature adaptation  Guided Anchoring Region Proposal Network GP-RPN anchorsrecall  RPNbaseline anchor 9.1% anchors  90% scale  aspect ratio  aspect ratio  1/2   2 / region proposals guided anchoring  anchor  SSD anchor  boxguided anchoring  COCO  GA-Fast-RCNNGA-Faster-RCNN  GA-RetinaNet  mAP  baseline  2.2%, 2.7%  1.2%\n\n# Guided Anchoring\n location  shape  (x,y,w,h)  (x,y) (w,h)  image $I$ location  shape \n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n\n1.  image\n2. shape  scale  aspect ratio  anchor \n   \n anchor  1\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 feature map  anchor  anchor  shape feature  feature map  feature map anchor</center>\n\n image $I$ feature map $F_I$ $F_I$   mapshape  shape w,h shape anchor anchor  shape  feature  anchor shape  feature \n\n leve  feature maps  FPN  RetinaNet 1 level  anchor  level  anchor \n\n## Anchor \nanchor  map $p(\\cdot|F_I)$  feature map $F_I$  $p(i,j|F_I)$  image $I$  $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$ s   $F_I$  $I$  anchor $p(i,j|F_I)$  $F_I$  (i,j) \n\n $\\mathcal N_L$  $p(i,j|F_I)$$\\mathcal N_L$  1x1  element-wise  sigmoid  $\\mathcal N_L$  $\\mathcal N_L$ \n\n $\\epsilon_L$ map  region anchor  shape region 90%  region  recall RPN  4(b) region  region masked convolution \n\n## Anchor shape \n anchor  anchor  shape 1 bbox  anchor  alignment  $F_I$ shape (w,h) shape  anchor  gt box \n\n w,h  [-1,1] \n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n shape  (dw,dh) s  $F_I$  $I$ $\\sigma$  $\\sigma=8$ $\\mathcal N_S$  shape $\\mathcal N_S$  1x1  2  element-wise  2  dw map  dh map w map  h map\n\n anchor  anchor  anchor  anchor shape  scale  aspect ratio  k  anchor anchor shape  location  recall aspect ratio  shape anchor \n\n## Anchor-Guided \n RPN  one-stage  anchors  feature map  anchor  shape/scale  feature map  anchor  shape  feature map  anchor  RPN  one-stage  anchor  feature  region  anchor  feature  anchor-guided  anchor shape  feature \n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n$\\mathbf f_i$   i  feature$(w_i,h_i)$  anchor shape 3x3  $\\mathcal N_T$ 1 anchor shape  offset feature map  feature map  bbox \n\n## \n### \n anchor  shape \n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n anchor  image   label map 1 0  label map  gt box  anchor anchor gt box $(x_g,y_g,w_g,h_g)$  feature map $F_I$ $(x_g',y_g',w_g',h_g')$ $\\mathcal R(x,y,w,h)$ Anchors  gt box  IOU gt box \n1.  \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$ gt box  gt box  $\\sigma_1$ CR  1positive\n\n2. \n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$$\\sigma_2 > \\sigma_1$IR  `ignore` Faster R-CNN  RPNanchor  gt box  IOU  0.7  1 positive 0.3  0 negative `[0.3,0.7]`  -1 -1  anchor \n\n3. \n   \n   $OR=F_I \\setminus IR$OR  0negative\n\n level features level  feature map  scale  feature map  scale  CR IR  2 CR  IR IR  OR CR, IR, OR  recall CR  feature map  Focal Loss  anchor \n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n shape target\n1.  anchor  gt box \n2.  anchor  gt box\n\nFaster R-CNN  anchor  IOU  gt box anchor  gt box  $(t_x,t_y,t_w,t_h)$  target anchor  $(x,y,w,h)$ \n\n anchor  anchor  w,h  shape  $(w_p,h_p)$  target $(t_w,t_h)$  anchor  $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$  gt box $gt=(x_g,y_g,w_g,h_g)$  IoU  vIoU\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n anchor  $(x_0,y_0)$  gt box $gt$ $(x_0,y_0)$ w  h  w  h  w  h anchor  gt box  IoU IoU  $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$  9  (w,h)  vIoU 9  (w,h)  RetinaNet  scales  aspect ratios  (w,h)  vIoU  bounded iou loss  shape \n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n (w,h)  anchor shape(w<sub>g</sub>,h<sub>g</sub>)  anchor  vIoU  gt box  shape $\\min(\\frac w {w_g}, \\frac {w_g} w)$  $\\min(\\frac h {h_g}, \\frac {h_g} h)$ w  w<sub>g</sub>h  h<sub>g</sub>\n\n\n1.  9  (w,h)\n2.  (x<sub>0</sub>,y<sub>0</sub>) anchor  gt box  vIoU vIoU  9  (w,h)\n3.  vIoU  gt box  anchor \n4. shape  (w,h)  anchor  gt box  (w<sub>g</sub>,h<sub>g</sub>)  shape \n\n shape  (w,h)  gt box  IoU IoU  gt box  anchor  gt box \n\n shape  (w,h)  gt box anchor  gt box  anchor  target \n\n##  proposals\n guided anchoring  RPNGA-RPN proposals proposals two-stage  RPN  GA-RPN  proposals  IoU  3\n![](/images/GA-RPN_fig3.png) <center>Fig 3  IoU  proposals </center>\n\n RPNGA-RPN \n1.  proposals \n2.  IoU  proposals \n\n RPN  GA-RPN  1  proposals  proposal \n\nGA-RPN  two-stage  proposal  RPN GA-RPN proposals  epochs 3  epochsGA-RPN proposals  inference epochs\n\n![](/images/GA-RPN_fig4.png)\n\n# \n\n\n# \n Guided Anchoring  shape  anchor","slug":"GA-RPN","published":1,"updated":"2019-06-27T12:16:05.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379bx0016dgvc9wfkx7iv","content":"<p><a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>\n<p> anchor  proposaltwo-stage anchor one-stage two-stage  Faster R-CNN  feature map  anchors proposals proposals </p>\n<p> anchor alignment  consistency</p>\n<ol>\n<li>anchor  feature map </li>\n<li> RF  anchor </li>\n</ol>\n<p> anchor  anchors feature map  scale  aspect ratio  k  anchors anchor </p>\n<ol>\n<li> scale  aspect ratio</li>\n<li> recall anchors anchors non-object anchors </li>\n</ol>\n<p> anchor </p>\n<ol>\n<li>image </li>\n<li> scale  image </li>\n</ol>\n<p> anchor guided anchoring</p>\n<ol>\n<li></li>\n<li> shape</li>\n</ol>\n<p> anchor shape  consistency  anchor shape  RF   scope  scale  aspect ratio  feature map  anchor shape  anchor shape  consistency  anchor shape  features  feature adaptation </p>\n<p> guided anchoring  feature adaptation  Guided Anchoring Region Proposal Network GP-RPN anchorsrecall  RPNbaseline anchor 9.1% anchors  90% scale  aspect ratio  aspect ratio  1/2   2 / region proposals guided anchoring  anchor  SSD anchor  boxguided anchoring  COCO  GA-Fast-RCNNGA-Faster-RCNN  GA-RetinaNet  mAP  baseline  2.2%, 2.7%  1.2%</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p> location  shape  (x,y,w,h)  (x,y) (w,h)  image $I$ location  shape <br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br></p>\n<ol>\n<li> image</li>\n<li>shape  scale  aspect ratio  anchor </li>\n</ol>\n<p> anchor  1</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt> <center>Fig 1 feature map  anchor  anchor  shape feature  feature map  feature map anchor</center></p>\n<p> image $I$ feature map $F_I$ $F_I$   mapshape  shape w,h shape anchor anchor  shape  feature  anchor shape  feature </p>\n<p> leve  feature maps  FPN  RetinaNet 1 level  anchor  level  anchor </p>\n<h2 id=\"Anchor-\"><a href=\"#Anchor-\" class=\"headerlink\" title=\"Anchor \"></a>Anchor </h2><p>anchor  map $p(\\cdot|F_I)$  feature map $F_I$  $p(i,j|F_I)$  image $I$  $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$ s   $F_I$  $I$  anchor $p(i,j|F_I)$  $F_I$  (i,j) </p>\n<p> $\\mathcal N_L$  $p(i,j|F_I)$$\\mathcal N_L$  1x1  element-wise  sigmoid  $\\mathcal N_L$  $\\mathcal N_L$ </p>\n<p> $\\epsilon_L$ map  region anchor  shape region 90%  region  recall RPN  4(b) region  region masked convolution </p>\n<h2 id=\"Anchor-shape-\"><a href=\"#Anchor-shape-\" class=\"headerlink\" title=\"Anchor shape \"></a>Anchor shape </h2><p> anchor  anchor  shape 1 bbox  anchor  alignment  $F_I$ shape (w,h) shape  anchor  gt box </p>\n<p> w,h  [-1,1] <br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br> shape  (dw,dh) s  $F_I$  $I$ $\\sigma$  $\\sigma=8$ $\\mathcal N_S$  shape $\\mathcal N_S$  1x1  2  element-wise  2  dw map  dh map w map  h map</p>\n<p> anchor  anchor  anchor  anchor shape  scale  aspect ratio  k  anchor anchor shape  location  recall aspect ratio  shape anchor </p>\n<h2 id=\"Anchor-Guided-\"><a href=\"#Anchor-Guided-\" class=\"headerlink\" title=\"Anchor-Guided \"></a>Anchor-Guided </h2><p> RPN  one-stage  anchors  feature map  anchor  shape/scale  feature map  anchor  shape  feature map  anchor  RPN  one-stage  anchor  feature  region  anchor  feature  anchor-guided  anchor shape  feature <br>$$\\mathbf f_i= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>$\\mathbf f_i$   i  feature$(w_i,h_i)$  anchor shape 3x3  $\\mathcal N_T$ 1 anchor shape  offset feature map  feature map  bbox </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> anchor  shape <br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p> anchor  image   label map 1 0  label map  gt box  anchor anchor gt box $(x_g,y_g,w_g,h_g)$  feature map $F_I$ $(x_g,y_g,w_g,h_g)$ $\\mathcal R(x,y,w,h)$ Anchors  gt box  IOU gt box </p>\n<ol>\n<li><p> </p>\n<p>$CR=\\mathcal R(x_g,y_g,\\sigma_1 w_g, \\sigma_1 h_g)$ gt box  gt box  $\\sigma_1$ CR  1positive</p>\n</li>\n<li><p></p>\n<p>$IR=\\mathcal R(x_g,y_g,\\sigma_2 w_g, \\sigma_2 h_g) \\setminus CR$$\\sigma_2 &gt; \\sigma_1$IR  <code>ignore</code> Faster R-CNN  RPNanchor  gt box  IOU  0.7  1 positive 0.3  0 negative <code>[0.3,0.7]</code>  -1 -1  anchor </p>\n</li>\n<li><p></p>\n<p>$OR=F_I \\setminus IR$OR  0negative</p>\n</li>\n</ol>\n<p> level features level  feature map  scale  feature map  scale  CR IR  2 CR  IR IR  OR CR, IR, OR  recall CR  feature map  Focal Loss  anchor <br><img src=\"/images/GA-RPN_fig2.png\" alt></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p> shape target</p>\n<ol>\n<li> anchor  gt box </li>\n<li> anchor  gt box</li>\n</ol>\n<p>Faster R-CNN  anchor  IOU  gt box anchor  gt box  $(t_x,t_y,t_w,t_h)$  target anchor  $(x,y,w,h)$ </p>\n<p> anchor  anchor  w,h  shape  $(w_p,h_p)$  target $(t_w,t_h)$  anchor  $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$  gt box $gt=(x_g,y_g,w_g,h_g)$  IoU  vIoU<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br> anchor  $(x_0,y_0)$  gt box $gt$ $(x_0,y_0)$ w  h  w  h  w  h anchor  gt box  IoU IoU  $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$  9  (w,h)  vIoU 9  (w,h)  RetinaNet  scales  aspect ratios  (w,h)  vIoU  bounded iou loss  shape <br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br> (w,h)  anchor shape(w<sub>g</sub>,h<sub>g</sub>)  anchor  vIoU  gt box  shape $\\min(\\frac w {w_g}, \\frac {w_g} w)$  $\\min(\\frac h {h_g}, \\frac {h_g} h)$ w  w<sub>g</sub>h  h<sub>g</sub></p>\n<p></p>\n<ol>\n<li> 9  (w,h)</li>\n<li> (x<sub>0</sub>,y<sub>0</sub>) anchor  gt box  vIoU vIoU  9  (w,h)</li>\n<li> vIoU  gt box  anchor </li>\n<li>shape  (w,h)  anchor  gt box  (w<sub>g</sub>,h<sub>g</sub>)  shape </li>\n</ol>\n<p> shape  (w,h)  gt box  IoU IoU  gt box  anchor  gt box </p>\n<p> shape  (w,h)  gt box anchor  gt box  anchor  target </p>\n<h2 id=\"-proposals\"><a href=\"#-proposals\" class=\"headerlink\" title=\" proposals\"></a> proposals</h2><p> guided anchoring  RPNGA-RPN proposals proposals two-stage  RPN  GA-RPN  proposals  IoU  3<br><img src=\"/images/GA-RPN_fig3.png\" alt> <center>Fig 3  IoU  proposals </center></p>\n<p> RPNGA-RPN </p>\n<ol>\n<li> proposals </li>\n<li> IoU  proposals </li>\n</ol>\n<p> RPN  GA-RPN  1  proposals  proposal </p>\n<p>GA-RPN  two-stage  proposal  RPN GA-RPN proposals  epochs 3  epochsGA-RPN proposals  inference epochs</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Guided Anchoring  shape  anchor</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>\n<p> anchor  proposaltwo-stage anchor one-stage two-stage  Faster R-CNN  feature map  anchors proposals proposals </p>\n<p> anchor alignment  consistency</p>\n<ol>\n<li>anchor  feature map </li>\n<li> RF  anchor </li>\n</ol>\n<p> anchor  anchors feature map  scale  aspect ratio  k  anchors anchor </p>\n<ol>\n<li> scale  aspect ratio</li>\n<li> recall anchors anchors non-object anchors </li>\n</ol>\n<p> anchor </p>\n<ol>\n<li>image </li>\n<li> scale  image </li>\n</ol>\n<p> anchor guided anchoring</p>\n<ol>\n<li></li>\n<li> shape</li>\n</ol>\n<p> anchor shape  consistency  anchor shape  RF   scope  scale  aspect ratio  feature map  anchor shape  anchor shape  consistency  anchor shape  features  feature adaptation </p>\n<p> guided anchoring  feature adaptation  Guided Anchoring Region Proposal Network GP-RPN anchorsrecall  RPNbaseline anchor 9.1% anchors  90% scale  aspect ratio  aspect ratio  1/2   2 / region proposals guided anchoring  anchor  SSD anchor  boxguided anchoring  COCO  GA-Fast-RCNNGA-Faster-RCNN  GA-RetinaNet  mAP  baseline  2.2%, 2.7%  1.2%</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p> location  shape  (x,y,w,h)  (x,y) (w,h)  image $I$ location  shape <br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br></p>\n<ol>\n<li> image</li>\n<li>shape  scale  aspect ratio  anchor </li>\n</ol>\n<p> anchor  1</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt> <center>Fig 1 feature map  anchor  anchor  shape feature  feature map  feature map anchor</center></p>\n<p> image $I$ feature map $F_I$ $F_I$   mapshape  shape w,h shape anchor anchor  shape  feature  anchor shape  feature </p>\n<p> leve  feature maps  FPN  RetinaNet 1 level  anchor  level  anchor </p>\n<h2 id=\"Anchor-\"><a href=\"#Anchor-\" class=\"headerlink\" title=\"Anchor \"></a>Anchor </h2><p>anchor  map $p(\\cdot|F_I)$  feature map $F_I$  $p(i,j|F_I)$  image $I$  $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$ s   $F_I$  $I$  anchor $p(i,j|F_I)$  $F_I$  (i,j) </p>\n<p> $\\mathcal N_L$  $p(i,j|F_I)$$\\mathcal N_L$  1x1  element-wise  sigmoid  $\\mathcal N_L$  $\\mathcal N_L$ </p>\n<p> $\\epsilon_L$ map  region anchor  shape region 90%  region  recall RPN  4(b) region  region masked convolution </p>\n<h2 id=\"Anchor-shape-\"><a href=\"#Anchor-shape-\" class=\"headerlink\" title=\"Anchor shape \"></a>Anchor shape </h2><p> anchor  anchor  shape 1 bbox  anchor  alignment  $F_I$ shape (w,h) shape  anchor  gt box </p>\n<p> w,h  [-1,1] <br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br> shape  (dw,dh) s  $F_I$  $I$ $\\sigma$  $\\sigma=8$ $\\mathcal N_S$  shape $\\mathcal N_S$  1x1  2  element-wise  2  dw map  dh map w map  h map</p>\n<p> anchor  anchor  anchor  anchor shape  scale  aspect ratio  k  anchor anchor shape  location  recall aspect ratio  shape anchor </p>\n<h2 id=\"Anchor-Guided-\"><a href=\"#Anchor-Guided-\" class=\"headerlink\" title=\"Anchor-Guided \"></a>Anchor-Guided </h2><p> RPN  one-stage  anchors  feature map  anchor  shape/scale  feature map  anchor  shape  feature map  anchor  RPN  one-stage  anchor  feature  region  anchor  feature  anchor-guided  anchor shape  feature <br>$$\\mathbf f_i= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>$\\mathbf f_i$   i  feature$(w_i,h_i)$  anchor shape 3x3  $\\mathcal N_T$ 1 anchor shape  offset feature map  feature map  bbox </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> anchor  shape <br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p> anchor  image   label map 1 0  label map  gt box  anchor anchor gt box $(x_g,y_g,w_g,h_g)$  feature map $F_I$ $(x_g,y_g,w_g,h_g)$ $\\mathcal R(x,y,w,h)$ Anchors  gt box  IOU gt box </p>\n<ol>\n<li><p> </p>\n<p>$CR=\\mathcal R(x_g,y_g,\\sigma_1 w_g, \\sigma_1 h_g)$ gt box  gt box  $\\sigma_1$ CR  1positive</p>\n</li>\n<li><p></p>\n<p>$IR=\\mathcal R(x_g,y_g,\\sigma_2 w_g, \\sigma_2 h_g) \\setminus CR$$\\sigma_2 &gt; \\sigma_1$IR  <code>ignore</code> Faster R-CNN  RPNanchor  gt box  IOU  0.7  1 positive 0.3  0 negative <code>[0.3,0.7]</code>  -1 -1  anchor </p>\n</li>\n<li><p></p>\n<p>$OR=F_I \\setminus IR$OR  0negative</p>\n</li>\n</ol>\n<p> level features level  feature map  scale  feature map  scale  CR IR  2 CR  IR IR  OR CR, IR, OR  recall CR  feature map  Focal Loss  anchor <br><img src=\"/images/GA-RPN_fig2.png\" alt></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p> shape target</p>\n<ol>\n<li> anchor  gt box </li>\n<li> anchor  gt box</li>\n</ol>\n<p>Faster R-CNN  anchor  IOU  gt box anchor  gt box  $(t_x,t_y,t_w,t_h)$  target anchor  $(x,y,w,h)$ </p>\n<p> anchor  anchor  w,h  shape  $(w_p,h_p)$  target $(t_w,t_h)$  anchor  $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$  gt box $gt=(x_g,y_g,w_g,h_g)$  IoU  vIoU<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br> anchor  $(x_0,y_0)$  gt box $gt$ $(x_0,y_0)$ w  h  w  h  w  h anchor  gt box  IoU IoU  $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$  9  (w,h)  vIoU 9  (w,h)  RetinaNet  scales  aspect ratios  (w,h)  vIoU  bounded iou loss  shape <br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br> (w,h)  anchor shape(w<sub>g</sub>,h<sub>g</sub>)  anchor  vIoU  gt box  shape $\\min(\\frac w {w_g}, \\frac {w_g} w)$  $\\min(\\frac h {h_g}, \\frac {h_g} h)$ w  w<sub>g</sub>h  h<sub>g</sub></p>\n<p></p>\n<ol>\n<li> 9  (w,h)</li>\n<li> (x<sub>0</sub>,y<sub>0</sub>) anchor  gt box  vIoU vIoU  9  (w,h)</li>\n<li> vIoU  gt box  anchor </li>\n<li>shape  (w,h)  anchor  gt box  (w<sub>g</sub>,h<sub>g</sub>)  shape </li>\n</ol>\n<p> shape  (w,h)  gt box  IoU IoU  gt box  anchor  gt box </p>\n<p> shape  (w,h)  gt box anchor  gt box  anchor  target </p>\n<h2 id=\"-proposals\"><a href=\"#-proposals\" class=\"headerlink\" title=\" proposals\"></a> proposals</h2><p> guided anchoring  RPNGA-RPN proposals proposals two-stage  RPN  GA-RPN  proposals  IoU  3<br><img src=\"/images/GA-RPN_fig3.png\" alt> <center>Fig 3  IoU  proposals </center></p>\n<p> RPNGA-RPN </p>\n<ol>\n<li> proposals </li>\n<li> IoU  proposals </li>\n</ol>\n<p> RPN  GA-RPN  1  proposals  proposal </p>\n<p>GA-RPN  two-stage  proposal  RPN GA-RPN proposals  epochs 3  epochsGA-RPN proposals  inference epochs</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Guided Anchoring  shape  anchor</p>\n"},{"title":"Grid-RCNN","date":"2019-07-19T09:44:19.000Z","mathjax":true,"_content":"# Grid R-CNN\n [Grid R-CNN](https://arxiv.org/abs/1811.12030)\n\n [Grid R-CNN](https://github.com/STVIR/Grid-R-CNN)\n\n## 1. \n conv  fc  4 x,y,w,h  4*(C+1) fc \n![](/images/Grid-RCNN_fig1.png)\n\n Grid R-CNN R-CNN  R-CNN  1  grid points  RoI  FCN  grid points\n1. FCN  RoI  grid points\n2.  grid points  bboxExtremeNet  CornerNet  grid points  3x3  1(b)top-middle\n\n grid points  grid point  feature maps grid point grid point  feature maps  grid point  feature map feature map  grid point \n\n## 2. Grid R-CNN\n 2  region proposals RoIAlign  RoI  FCN  feature maps sigmoid probability heatmapheatmaps  target map  grid points 2.1 2.2 \n![](/images/Grid-RCNN_fig2.png)\n\n### 2.1 Grid Guided Localization\n NxN  grid points  bbox  3x3  grid points  bbox \n\n RoIAlign  RoI  14x14  feature 8  3x3  2x  56x56  feature NxN pixel-wise  sigmoid  NxN  heatmap heatmap  feature map  grid point supervision map supervision map  target grid point  5  $t_i=1$ $t_i=0$target grid point  supervision map  2.3  binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$ $p_i$  pixel \n\nInference  heatmap  $(H_x,H_y)$ image  $(I_x,I_y)$\n$$I_x=P_x+ \\frac {H_x}{w_o} w_p\n\\\\\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$\n $(P_x,P_y)$  region proposal  image $(w_p,h_p)$  proposal $(w_o,h_o)$  heatmap  ____ heatmap  proposal    \n grid points  bbox  $B=(x_l,y_u,x_r,y_b)$ j  grid point  $g_j$ $(x_j,y_j)$ $p_j$ j  heatmap  $E_i$  i  grid points  $g_j$   i  $j \\in E_i$ grid points  B\n$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j\n\\\\\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$\n\n### 2.2 Grid Points Feature Fusion\n FCN  heatmap  RoI  grid points  3x3 grid points  point grid points  feature maps \n\n grid points  feature maps NxN  filters  feature map  grid points  feature map  grid point  i  point  feature map  $F_i$\n\n grid point L1  1  points  points  source points $S_i$ $S_i$  point j feature maps $F_j$  5x5  $T_{j \\rightarrow i}$$S_i$  source points  features  $F_i$  $F_i'$\n$$F_i'=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$\n![](/images/Grid-RCNN_fig3.png)\n\n 3(a)  point  $F_i'$  source point  $F_i'$  $T_{j \\rightarrow i}^+$ $F_i''$ heatmap grid point  L1  2  points  features  grid point  3(b)\n\n____  feature maps  sigmoid  heatmaps supervision maps  loss Binary Cross-Entropy Loss\n\n### 2.3 Extended Region Mapping\nGrid  heatmap  pixel  grid point  RoI  FCN heatmap  region proposal  image region proposal  gt grid point  region proposal  supervision map  target grid point inference  heatmap  pixel  grid point region proposal  grid point  gt grid points  region proposals  4proposal gt box 9  grid points  7  heatmap \n\n proposal gt grid points  proposal    proposal  heatmap  image  region  proposal RoI  feature map  region  proposal heatmap  region  grid points  4  supervision map  heatmap  gt box  supervision map  gt grid point  pixels gt grid point  pixels 5  pixels \n![](/images/Grid-RCNN_fig4.png)\n\n\n$$I_x'=P_x+\\frac {4H_x-w_o}{2w_o}w_p\n\\\\\\\\ I_y'=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$\n\n 4   \n heatmap  image  region proposal  R' $(I_x,I_y)$ \n$$I_x'=P_x'+\\frac {H_x}{w_o} 2 w_p, \\quad I_y'=P_y'+\\frac {H_y}{h_o} 2 h_p$$\n $(P_x',P_y')$  R'  image  proposal \n$$2(x_c-P_x)=x_c-P_x'=w_p, \\quad 2(y_c-P_y)=y_c-P_y'=h_p$$\n $(x_c,y_c)$  proposal  R'  image \n\n### 2.4 Implementation Details\n\n\n# Grid R-CNN Plus\n [Grid R-CNN Plus: Faster and Better](https://arxiv.org/abs/1906.05688)\n\n Grid R-CNN  two-stage  mAP Grid R-CNN  inference  Grid R-CNN Plus\n\n Grid R-CNN Plus grid pointGrid R-CNN  supervision map grid point  supervision map  Grid R-CNN Plus  1/4  grid  feature maps  grid point  1/4grid point \n\nNMS \n\n##  Grid R-CNN\n 1  Grid R-CNN  two-stage Grid R-CNN  RPN  R-CNN region propopsals RoIAlign  CNN backbone  feature maps  RoI  RoI  bbox Grid R-CNN  R-CNN  grid points Grid  FCN  heatmaps heatmaps  grid points\n![](/images/Grid-RCNN-Plus_fig1.png)\n\nGrid R-CNN  8  3x3  2x  heatmap RPN  1000  proposals NMS  top 100  proposals grid \n\n grid points  gt grid points  proposal \n\n## Grid R-CNN Plus\n### Grid Point \n IoU > 0.5  proposals Grid  supervision map  gt grid point  2  grid point  gt label  3x3 grid points  point  gt label  supervision map  grid points  scale  center pixel  grid point \n![](/images/Grid-RCNN-Plus_fig2.png)\n\n grid point  56x56 28x28 1/4 heatmap Grid point  2  point   point  gt label \n\n### Light Grid Head\n heatmap  grid  features  14x14  7x7 RoIAlign  RoI  14x14  3x3 stride=2  size  7x7 7  3x3 stride=1  7x7 N  9 grid point grid points  2  2x  28x28  heatmaps\n\ngrid point  grid points  grid points  Grid R-CNN Plus  5x5 depth-wise depth-wise  Grid R-CNN  5x5  grid \n\n### \ngrid branch IoU > 0.5  positive proposals grid  Grid R-CNN Plus  96  192 \n\n###  NMS\nGrid R-CNN proposals  IoU  0.5  NMS top 125  proposals  grid  NMS  NMS  proposals 80 COCO  NMS  Grid R-CNN Plus  NMS\n\n## \n","source":"_posts/Grid-RCNN.md","raw":"---\ntitle: Grid-RCNN\ndate: 2019-07-19 17:44:19\ntags: object detection\nmathjax: true\n---\n# Grid R-CNN\n [Grid R-CNN](https://arxiv.org/abs/1811.12030)\n\n [Grid R-CNN](https://github.com/STVIR/Grid-R-CNN)\n\n## 1. \n conv  fc  4 x,y,w,h  4*(C+1) fc \n![](/images/Grid-RCNN_fig1.png)\n\n Grid R-CNN R-CNN  R-CNN  1  grid points  RoI  FCN  grid points\n1. FCN  RoI  grid points\n2.  grid points  bboxExtremeNet  CornerNet  grid points  3x3  1(b)top-middle\n\n grid points  grid point  feature maps grid point grid point  feature maps  grid point  feature map feature map  grid point \n\n## 2. Grid R-CNN\n 2  region proposals RoIAlign  RoI  FCN  feature maps sigmoid probability heatmapheatmaps  target map  grid points 2.1 2.2 \n![](/images/Grid-RCNN_fig2.png)\n\n### 2.1 Grid Guided Localization\n NxN  grid points  bbox  3x3  grid points  bbox \n\n RoIAlign  RoI  14x14  feature 8  3x3  2x  56x56  feature NxN pixel-wise  sigmoid  NxN  heatmap heatmap  feature map  grid point supervision map supervision map  target grid point  5  $t_i=1$ $t_i=0$target grid point  supervision map  2.3  binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$ $p_i$  pixel \n\nInference  heatmap  $(H_x,H_y)$ image  $(I_x,I_y)$\n$$I_x=P_x+ \\frac {H_x}{w_o} w_p\n\\\\\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$\n $(P_x,P_y)$  region proposal  image $(w_p,h_p)$  proposal $(w_o,h_o)$  heatmap  ____ heatmap  proposal    \n grid points  bbox  $B=(x_l,y_u,x_r,y_b)$ j  grid point  $g_j$ $(x_j,y_j)$ $p_j$ j  heatmap  $E_i$  i  grid points  $g_j$   i  $j \\in E_i$ grid points  B\n$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j\n\\\\\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$\n\n### 2.2 Grid Points Feature Fusion\n FCN  heatmap  RoI  grid points  3x3 grid points  point grid points  feature maps \n\n grid points  feature maps NxN  filters  feature map  grid points  feature map  grid point  i  point  feature map  $F_i$\n\n grid point L1  1  points  points  source points $S_i$ $S_i$  point j feature maps $F_j$  5x5  $T_{j \\rightarrow i}$$S_i$  source points  features  $F_i$  $F_i'$\n$$F_i'=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$\n![](/images/Grid-RCNN_fig3.png)\n\n 3(a)  point  $F_i'$  source point  $F_i'$  $T_{j \\rightarrow i}^+$ $F_i''$ heatmap grid point  L1  2  points  features  grid point  3(b)\n\n____  feature maps  sigmoid  heatmaps supervision maps  loss Binary Cross-Entropy Loss\n\n### 2.3 Extended Region Mapping\nGrid  heatmap  pixel  grid point  RoI  FCN heatmap  region proposal  image region proposal  gt grid point  region proposal  supervision map  target grid point inference  heatmap  pixel  grid point region proposal  grid point  gt grid points  region proposals  4proposal gt box 9  grid points  7  heatmap \n\n proposal gt grid points  proposal    proposal  heatmap  image  region  proposal RoI  feature map  region  proposal heatmap  region  grid points  4  supervision map  heatmap  gt box  supervision map  gt grid point  pixels gt grid point  pixels 5  pixels \n![](/images/Grid-RCNN_fig4.png)\n\n\n$$I_x'=P_x+\\frac {4H_x-w_o}{2w_o}w_p\n\\\\\\\\ I_y'=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$\n\n 4   \n heatmap  image  region proposal  R' $(I_x,I_y)$ \n$$I_x'=P_x'+\\frac {H_x}{w_o} 2 w_p, \\quad I_y'=P_y'+\\frac {H_y}{h_o} 2 h_p$$\n $(P_x',P_y')$  R'  image  proposal \n$$2(x_c-P_x)=x_c-P_x'=w_p, \\quad 2(y_c-P_y)=y_c-P_y'=h_p$$\n $(x_c,y_c)$  proposal  R'  image \n\n### 2.4 Implementation Details\n\n\n# Grid R-CNN Plus\n [Grid R-CNN Plus: Faster and Better](https://arxiv.org/abs/1906.05688)\n\n Grid R-CNN  two-stage  mAP Grid R-CNN  inference  Grid R-CNN Plus\n\n Grid R-CNN Plus grid pointGrid R-CNN  supervision map grid point  supervision map  Grid R-CNN Plus  1/4  grid  feature maps  grid point  1/4grid point \n\nNMS \n\n##  Grid R-CNN\n 1  Grid R-CNN  two-stage Grid R-CNN  RPN  R-CNN region propopsals RoIAlign  CNN backbone  feature maps  RoI  RoI  bbox Grid R-CNN  R-CNN  grid points Grid  FCN  heatmaps heatmaps  grid points\n![](/images/Grid-RCNN-Plus_fig1.png)\n\nGrid R-CNN  8  3x3  2x  heatmap RPN  1000  proposals NMS  top 100  proposals grid \n\n grid points  gt grid points  proposal \n\n## Grid R-CNN Plus\n### Grid Point \n IoU > 0.5  proposals Grid  supervision map  gt grid point  2  grid point  gt label  3x3 grid points  point  gt label  supervision map  grid points  scale  center pixel  grid point \n![](/images/Grid-RCNN-Plus_fig2.png)\n\n grid point  56x56 28x28 1/4 heatmap Grid point  2  point   point  gt label \n\n### Light Grid Head\n heatmap  grid  features  14x14  7x7 RoIAlign  RoI  14x14  3x3 stride=2  size  7x7 7  3x3 stride=1  7x7 N  9 grid point grid points  2  2x  28x28  heatmaps\n\ngrid point  grid points  grid points  Grid R-CNN Plus  5x5 depth-wise depth-wise  Grid R-CNN  5x5  grid \n\n### \ngrid branch IoU > 0.5  positive proposals grid  Grid R-CNN Plus  96  192 \n\n###  NMS\nGrid R-CNN proposals  IoU  0.5  NMS top 125  proposals  grid  NMS  NMS  proposals 80 COCO  NMS  Grid R-CNN Plus  NMS\n\n## \n","slug":"Grid-RCNN","published":1,"updated":"2019-07-23T01:23:23.833Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379by0018dgvc8zweroo8","content":"<h1 id=\"Grid-R-CNN\"><a href=\"#Grid-R-CNN\" class=\"headerlink\" title=\"Grid R-CNN\"></a>Grid R-CNN</h1><p> <a href=\"https://arxiv.org/abs/1811.12030\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<p> <a href=\"https://github.com/STVIR/Grid-R-CNN\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<h2 id=\"1-\"><a href=\"#1-\" class=\"headerlink\" title=\"1. \"></a>1. </h2><p> conv  fc  4 x,y,w,h  4*(C+1) fc <br><img src=\"/images/Grid-RCNN_fig1.png\" alt></p>\n<p> Grid R-CNN R-CNN  R-CNN  1  grid points  RoI  FCN  grid points</p>\n<ol>\n<li>FCN  RoI  grid points</li>\n<li> grid points  bboxExtremeNet  CornerNet  grid points  3x3  1(b)top-middle</li>\n</ol>\n<p> grid points  grid point  feature maps grid point grid point  feature maps  grid point  feature map feature map  grid point </p>\n<h2 id=\"2-Grid-R-CNN\"><a href=\"#2-Grid-R-CNN\" class=\"headerlink\" title=\"2. Grid R-CNN\"></a>2. Grid R-CNN</h2><p> 2  region proposals RoIAlign  RoI  FCN  feature maps sigmoid probability heatmapheatmaps  target map  grid points 2.1 2.2 <br><img src=\"/images/Grid-RCNN_fig2.png\" alt></p>\n<h3 id=\"2-1-Grid-Guided-Localization\"><a href=\"#2-1-Grid-Guided-Localization\" class=\"headerlink\" title=\"2.1 Grid Guided Localization\"></a>2.1 Grid Guided Localization</h3><p> NxN  grid points  bbox  3x3  grid points  bbox </p>\n<p> RoIAlign  RoI  14x14  feature 8  3x3  2x  56x56  feature NxN pixel-wise  sigmoid  NxN  heatmap heatmap  feature map  grid point supervision map supervision map  target grid point  5  $t_i=1$ $t_i=0$target grid point  supervision map  2.3  binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$ $p_i$  pixel </p>\n<p>Inference  heatmap  $(H_x,H_y)$ image  $(I_x,I_y)$<br>$$I_x=P_x+ \\frac {H_x}{w_o} w_p<br>\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$<br> $(P_x,P_y)$  region proposal  image $(w_p,h_p)$  proposal $(w_o,h_o)$  heatmap  <strong></strong> heatmap  proposal <br> grid points  bbox  $B=(x_l,y_u,x_r,y_b)$ j  grid point  $g_j$ $(x_j,y_j)$ $p_j$ j  heatmap  $E_i$  i  grid points  $g_j$   i  $j \\in E_i$ grid points  B<br>$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j<br>\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$</p>\n<h3 id=\"2-2-Grid-Points-Feature-Fusion\"><a href=\"#2-2-Grid-Points-Feature-Fusion\" class=\"headerlink\" title=\"2.2 Grid Points Feature Fusion\"></a>2.2 Grid Points Feature Fusion</h3><p> FCN  heatmap  RoI  grid points  3x3 grid points  point grid points  feature maps </p>\n<p> grid points  feature maps NxN  filters  feature map  grid points  feature map  grid point  i  point  feature map  $F_i$</p>\n<p> grid point L1  1  points  points  source points $S_i$ $S_i$  point j feature maps $F_j$  5x5  $T_{j \\rightarrow i}$$S_i$  source points  features  $F_i$  $F_i$<br>$$F_i=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$<br><img src=\"/images/Grid-RCNN_fig3.png\" alt></p>\n<p> 3(a)  point  $F_i$  source point  $F_i$  $T_{j \\rightarrow i}^+$ $F_i$ heatmap grid point  L1  2  points  features  grid point  3(b)</p>\n<p><strong></strong>  feature maps  sigmoid  heatmaps supervision maps  loss Binary Cross-Entropy Loss</p>\n<h3 id=\"2-3-Extended-Region-Mapping\"><a href=\"#2-3-Extended-Region-Mapping\" class=\"headerlink\" title=\"2.3 Extended Region Mapping\"></a>2.3 Extended Region Mapping</h3><p>Grid  heatmap  pixel  grid point  RoI  FCN heatmap  region proposal  image region proposal  gt grid point  region proposal  supervision map  target grid point inference  heatmap  pixel  grid point region proposal  grid point  gt grid points  region proposals  4proposal gt box 9  grid points  7  heatmap </p>\n<p> proposal gt grid points  proposal    proposal  heatmap  image  region  proposal RoI  feature map  region  proposal heatmap  region  grid points  4  supervision map  heatmap  gt box  supervision map  gt grid point  pixels gt grid point  pixels 5  pixels <br><img src=\"/images/Grid-RCNN_fig4.png\" alt></p>\n<p><br>$$I_x=P_x+\\frac {4H_x-w_o}{2w_o}w_p<br>\\\\ I_y=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$</p>\n<p> 4 <br> heatmap  image  region proposal  R $(I_x,I_y)$ <br>$$I_x=P_x+\\frac {H_x}{w_o} 2 w_p, \\quad I_y=P_y+\\frac {H_y}{h_o} 2 h_p$$<br> $(P_x,P_y)$  R  image  proposal <br>$$2(x_c-P_x)=x_c-P_x=w_p, \\quad 2(y_c-P_y)=y_c-P_y=h_p$$<br> $(x_c,y_c)$  proposal  R  image </p>\n<h3 id=\"2-4-Implementation-Details\"><a href=\"#2-4-Implementation-Details\" class=\"headerlink\" title=\"2.4 Implementation Details\"></a>2.4 Implementation Details</h3><p></p>\n<h1 id=\"Grid-R-CNN-Plus\"><a href=\"#Grid-R-CNN-Plus\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h1><p> <a href=\"https://arxiv.org/abs/1906.05688\" target=\"_blank\" rel=\"noopener\">Grid R-CNN Plus: Faster and Better</a></p>\n<p> Grid R-CNN  two-stage  mAP Grid R-CNN  inference  Grid R-CNN Plus</p>\n<p> Grid R-CNN Plus grid pointGrid R-CNN  supervision map grid point  supervision map  Grid R-CNN Plus  1/4  grid  feature maps  grid point  1/4grid point </p>\n<p>NMS </p>\n<h2 id=\"-Grid-R-CNN\"><a href=\"#-Grid-R-CNN\" class=\"headerlink\" title=\" Grid R-CNN\"></a> Grid R-CNN</h2><p> 1  Grid R-CNN  two-stage Grid R-CNN  RPN  R-CNN region propopsals RoIAlign  CNN backbone  feature maps  RoI  RoI  bbox Grid R-CNN  R-CNN  grid points Grid  FCN  heatmaps heatmaps  grid points<br><img src=\"/images/Grid-RCNN-Plus_fig1.png\" alt></p>\n<p>Grid R-CNN  8  3x3  2x  heatmap RPN  1000  proposals NMS  top 100  proposals grid </p>\n<p> grid points  gt grid points  proposal </p>\n<h2 id=\"Grid-R-CNN-Plus-1\"><a href=\"#Grid-R-CNN-Plus-1\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h2><h3 id=\"Grid-Point-\"><a href=\"#Grid-Point-\" class=\"headerlink\" title=\"Grid Point \"></a>Grid Point </h3><p> IoU &gt; 0.5  proposals Grid  supervision map  gt grid point  2  grid point  gt label  3x3 grid points  point  gt label  supervision map  grid points  scale  center pixel  grid point <br><img src=\"/images/Grid-RCNN-Plus_fig2.png\" alt></p>\n<p> grid point  56x56 28x28 1/4 heatmap Grid point  2  point   point  gt label </p>\n<h3 id=\"Light-Grid-Head\"><a href=\"#Light-Grid-Head\" class=\"headerlink\" title=\"Light Grid Head\"></a>Light Grid Head</h3><p> heatmap  grid  features  14x14  7x7 RoIAlign  RoI  14x14  3x3 stride=2  size  7x7 7  3x3 stride=1  7x7 N  9 grid point grid points  2  2x  28x28  heatmaps</p>\n<p>grid point  grid points  grid points  Grid R-CNN Plus  5x5 depth-wise depth-wise  Grid R-CNN  5x5  grid </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>grid branch IoU &gt; 0.5  positive proposals grid  Grid R-CNN Plus  96  192 </p>\n<h3 id=\"-NMS\"><a href=\"#-NMS\" class=\"headerlink\" title=\" NMS\"></a> NMS</h3><p>Grid R-CNN proposals  IoU  0.5  NMS top 125  proposals  grid  NMS  NMS  proposals 80 COCO  NMS  Grid R-CNN Plus  NMS</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Grid-R-CNN\"><a href=\"#Grid-R-CNN\" class=\"headerlink\" title=\"Grid R-CNN\"></a>Grid R-CNN</h1><p> <a href=\"https://arxiv.org/abs/1811.12030\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<p> <a href=\"https://github.com/STVIR/Grid-R-CNN\" target=\"_blank\" rel=\"noopener\">Grid R-CNN</a></p>\n<h2 id=\"1-\"><a href=\"#1-\" class=\"headerlink\" title=\"1. \"></a>1. </h2><p> conv  fc  4 x,y,w,h  4*(C+1) fc <br><img src=\"/images/Grid-RCNN_fig1.png\" alt></p>\n<p> Grid R-CNN R-CNN  R-CNN  1  grid points  RoI  FCN  grid points</p>\n<ol>\n<li>FCN  RoI  grid points</li>\n<li> grid points  bboxExtremeNet  CornerNet  grid points  3x3  1(b)top-middle</li>\n</ol>\n<p> grid points  grid point  feature maps grid point grid point  feature maps  grid point  feature map feature map  grid point </p>\n<h2 id=\"2-Grid-R-CNN\"><a href=\"#2-Grid-R-CNN\" class=\"headerlink\" title=\"2. Grid R-CNN\"></a>2. Grid R-CNN</h2><p> 2  region proposals RoIAlign  RoI  FCN  feature maps sigmoid probability heatmapheatmaps  target map  grid points 2.1 2.2 <br><img src=\"/images/Grid-RCNN_fig2.png\" alt></p>\n<h3 id=\"2-1-Grid-Guided-Localization\"><a href=\"#2-1-Grid-Guided-Localization\" class=\"headerlink\" title=\"2.1 Grid Guided Localization\"></a>2.1 Grid Guided Localization</h3><p> NxN  grid points  bbox  3x3  grid points  bbox </p>\n<p> RoIAlign  RoI  14x14  feature 8  3x3  2x  56x56  feature NxN pixel-wise  sigmoid  NxN  heatmap heatmap  feature map  grid point supervision map supervision map  target grid point  5  $t_i=1$ $t_i=0$target grid point  supervision map  2.3  binary cross-entropy loss $CE=\\sum_{i=1}^{56 \\times 56} [-t_i\\log p_i-(1-t_i)\\log(1-p_i)]$ $p_i$  pixel </p>\n<p>Inference  heatmap  $(H_x,H_y)$ image  $(I_x,I_y)$<br>$$I_x=P_x+ \\frac {H_x}{w_o} w_p<br>\\\\ I_y=P_y+ \\frac {H_y}{h_o} h_p$$<br> $(P_x,P_y)$  region proposal  image $(w_p,h_p)$  proposal $(w_o,h_o)$  heatmap  <strong></strong> heatmap  proposal <br> grid points  bbox  $B=(x_l,y_u,x_r,y_b)$ j  grid point  $g_j$ $(x_j,y_j)$ $p_j$ j  heatmap  $E_i$  i  grid points  $g_j$   i  $j \\in E_i$ grid points  B<br>$$x_l=\\frac 1 N \\sum_{j \\in E_1} x_j p_j, \\quad y_u=\\frac 1 N \\sum_{j \\in E_2} y_j p_j<br>\\\\ x_r=\\frac 1 N \\sum_{j \\in E_3} x_j p_j, \\quad y_b=\\frac 1 N \\sum_{j \\in E_4} y_j p_j$$</p>\n<h3 id=\"2-2-Grid-Points-Feature-Fusion\"><a href=\"#2-2-Grid-Points-Feature-Fusion\" class=\"headerlink\" title=\"2.2 Grid Points Feature Fusion\"></a>2.2 Grid Points Feature Fusion</h3><p> FCN  heatmap  RoI  grid points  3x3 grid points  point grid points  feature maps </p>\n<p> grid points  feature maps NxN  filters  feature map  grid points  feature map  grid point  i  point  feature map  $F_i$</p>\n<p> grid point L1  1  points  points  source points $S_i$ $S_i$  point j feature maps $F_j$  5x5  $T_{j \\rightarrow i}$$S_i$  source points  features  $F_i$  $F_i$<br>$$F_i=F_i + \\sum_{j \\in S_i} T_{j \\rightarrow i} (F_j)$$<br><img src=\"/images/Grid-RCNN_fig3.png\" alt></p>\n<p> 3(a)  point  $F_i$  source point  $F_i$  $T_{j \\rightarrow i}^+$ $F_i$ heatmap grid point  L1  2  points  features  grid point  3(b)</p>\n<p><strong></strong>  feature maps  sigmoid  heatmaps supervision maps  loss Binary Cross-Entropy Loss</p>\n<h3 id=\"2-3-Extended-Region-Mapping\"><a href=\"#2-3-Extended-Region-Mapping\" class=\"headerlink\" title=\"2.3 Extended Region Mapping\"></a>2.3 Extended Region Mapping</h3><p>Grid  heatmap  pixel  grid point  RoI  FCN heatmap  region proposal  image region proposal  gt grid point  region proposal  supervision map  target grid point inference  heatmap  pixel  grid point region proposal  grid point  gt grid points  region proposals  4proposal gt box 9  grid points  7  heatmap </p>\n<p> proposal gt grid points  proposal    proposal  heatmap  image  region  proposal RoI  feature map  region  proposal heatmap  region  grid points  4  supervision map  heatmap  gt box  supervision map  gt grid point  pixels gt grid point  pixels 5  pixels <br><img src=\"/images/Grid-RCNN_fig4.png\" alt></p>\n<p><br>$$I_x=P_x+\\frac {4H_x-w_o}{2w_o}w_p<br>\\\\ I_y=P_y+\\frac {4H_y-h_o}{2h_o}h_p$$</p>\n<p> 4 <br> heatmap  image  region proposal  R $(I_x,I_y)$ <br>$$I_x=P_x+\\frac {H_x}{w_o} 2 w_p, \\quad I_y=P_y+\\frac {H_y}{h_o} 2 h_p$$<br> $(P_x,P_y)$  R  image  proposal <br>$$2(x_c-P_x)=x_c-P_x=w_p, \\quad 2(y_c-P_y)=y_c-P_y=h_p$$<br> $(x_c,y_c)$  proposal  R  image </p>\n<h3 id=\"2-4-Implementation-Details\"><a href=\"#2-4-Implementation-Details\" class=\"headerlink\" title=\"2.4 Implementation Details\"></a>2.4 Implementation Details</h3><p></p>\n<h1 id=\"Grid-R-CNN-Plus\"><a href=\"#Grid-R-CNN-Plus\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h1><p> <a href=\"https://arxiv.org/abs/1906.05688\" target=\"_blank\" rel=\"noopener\">Grid R-CNN Plus: Faster and Better</a></p>\n<p> Grid R-CNN  two-stage  mAP Grid R-CNN  inference  Grid R-CNN Plus</p>\n<p> Grid R-CNN Plus grid pointGrid R-CNN  supervision map grid point  supervision map  Grid R-CNN Plus  1/4  grid  feature maps  grid point  1/4grid point </p>\n<p>NMS </p>\n<h2 id=\"-Grid-R-CNN\"><a href=\"#-Grid-R-CNN\" class=\"headerlink\" title=\" Grid R-CNN\"></a> Grid R-CNN</h2><p> 1  Grid R-CNN  two-stage Grid R-CNN  RPN  R-CNN region propopsals RoIAlign  CNN backbone  feature maps  RoI  RoI  bbox Grid R-CNN  R-CNN  grid points Grid  FCN  heatmaps heatmaps  grid points<br><img src=\"/images/Grid-RCNN-Plus_fig1.png\" alt></p>\n<p>Grid R-CNN  8  3x3  2x  heatmap RPN  1000  proposals NMS  top 100  proposals grid </p>\n<p> grid points  gt grid points  proposal </p>\n<h2 id=\"Grid-R-CNN-Plus-1\"><a href=\"#Grid-R-CNN-Plus-1\" class=\"headerlink\" title=\"Grid R-CNN Plus\"></a>Grid R-CNN Plus</h2><h3 id=\"Grid-Point-\"><a href=\"#Grid-Point-\" class=\"headerlink\" title=\"Grid Point \"></a>Grid Point </h3><p> IoU &gt; 0.5  proposals Grid  supervision map  gt grid point  2  grid point  gt label  3x3 grid points  point  gt label  supervision map  grid points  scale  center pixel  grid point <br><img src=\"/images/Grid-RCNN-Plus_fig2.png\" alt></p>\n<p> grid point  56x56 28x28 1/4 heatmap Grid point  2  point   point  gt label </p>\n<h3 id=\"Light-Grid-Head\"><a href=\"#Light-Grid-Head\" class=\"headerlink\" title=\"Light Grid Head\"></a>Light Grid Head</h3><p> heatmap  grid  features  14x14  7x7 RoIAlign  RoI  14x14  3x3 stride=2  size  7x7 7  3x3 stride=1  7x7 N  9 grid point grid points  2  2x  28x28  heatmaps</p>\n<p>grid point  grid points  grid points  Grid R-CNN Plus  5x5 depth-wise depth-wise  Grid R-CNN  5x5  grid </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>grid branch IoU &gt; 0.5  positive proposals grid  Grid R-CNN Plus  96  192 </p>\n<h3 id=\"-NMS\"><a href=\"#-NMS\" class=\"headerlink\" title=\" NMS\"></a> NMS</h3><p>Grid R-CNN proposals  IoU  0.5  NMS top 125  proposals  grid  NMS  NMS  proposals 80 COCO  NMS  Grid R-CNN Plus  NMS</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n"},{"title":"M2Det","date":"2019-06-28T09:59:08.000Z","mathjax":true,"_content":"[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n\n# Introduction\nimage pyramid  feature pyramid size  image  image  level  feature scale  feature maps feature pyramid feature pyramid  backbone  multi-scale feature maps feature maps  feature pyramid 1\n![](/images/M2Det_fig1.png)\n\nSSD  backbone  layers 2  4  layers 6  layers  feature pyramidSTDN  DenseNet  block  pooling  scale-transfer  feature pyramidFPN  top-down  feature pyramid\n1. pyramid  feature  backbone  backbone \n2. pyramid  level  feature  scale  feature FPN  FPN  backbone  layer  feature  level \n\nsize  size  level  feature maps \n\n feature pyramid  scale  2\n![](/images/M2Det_fig2.png)\n\n backbone  multi-level features layers  base feature base feature  Thinned U-shape Modules(TUM)  Feature Fusion Modules(FFM)  block 2  multi-level multi-scale featuresmulti-leve  shallow, medium, deep  levelmulti-scale  level  features U  Scale-wise Feature Aggregation ModuleSFAM  level  scale  feature pyramid SFAM  multi-level multi-scale  multi-scale multi-level scale  feature  feature pyramid  backbone  layers  feature pyramid  Multi-Level Feature Pyramid NetworkMLFPN\n\n MLFPN  one-stage  M2Det MLFPN  SSD M2Det  SOTA  inference FPS=11.8AP=41.0 inference AP  44.2 MS-COCO  one-stage \n\n# Method\nM2Det  2 backbone  MLFPN  feature pyramid SSD  bbox  NMS MLFPN FFM, TUM  SFAMFFMv1  base feature  backbone  feature maps TUM  TUM  FFMv2  multi-level multi scale featuresSFAM  scale  level  featuresconcatenate features M2Det \n\n## MLFPN\n 2FFMv1  base feature VGG  conv4_3  conv5_3  TUM  FFMv2 TUM  scale  feature mapsFFMv2  base feature  TUM  scale  feature feature maps  TUM  TUM  base feature  multi-level multi-scale features \n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n$\\mathbf X_{base}$  base feature$x_i^l$  $l$  TUM  $i$  scale  featureL  TUM $\\mathbf T_l$   $l$  TUM $\\mathbf F$  FFMv2 \n\n### FFM\nFFM  feature  1x1  features concatenation  features FFMv1  backbone  scale  features  upsample  scale  concatenation TUM  ____  FFMv2  base feature   TUM  feature  scale concatenate  TUM FFMv1  FFMv2  4 (a)(b)\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM block  size</center>\n\n### TUM\nTUM  Thin U-shape  4(c)encoder  stride=2  3x3 decoder  feature maps  FPN  backbone  layer  upsample  element-wise sum  1x1  TUM  decoder  multi-level multi-scale features TUM  multi-scale features TUM  multi-scale features TUM  multi-scale features\n\n### SFAM\nSFAM  TUM  multi-level multi-scale features 3\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM  scale  channel  concatenate  SE attention </center>\n\n scale  features  concatenate feature pyramid  \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$  $i$  scale $W_i \\times H_i$  $i$  scale  feature map  size scale  level  feature maps  $C$  4  $C=128$ concatenate  features features  SE block squeeze global average pooling $\\mathbf z \\in \\mathcal R^C$ excitation  fc \n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n$\\sigma$  ReLu$\\delta$  sigmoid$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$ r  r=16\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$\n\n### \n VGG  ResNet  M2Det  backbonebackbone  ImageNet2012 MLFPN  8  TUM TUM  5  convs  5  6  scale  featuresTUM  scale  256  4 (c)  SSD, RefineDet  RetinaNet 320, 512  800\n\nMLFPN  6  pyramid featuresscale  1x13x35x510x1020x2040x40 scale  pyramid features 6  pyramid features  anchor(prior) box  scale  aspect ratio SSD  m  features m = 6 k  features  anchor box  scale \n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n$s_{min}=0.2, \\ s_{max}=0.9$ features  image  anchor  scale\n\n pyramidal features  6  anchors 3  aspect ratios SSD 0.05  [soft-NMS](/2019/06/24/cv-mtds) \n\n# \n\n\n# \n MLFPN  multi-scale  M2Det  SOTA  one-stage ","source":"_posts/M2Det.md","raw":"---\ntitle: M2Det\ndate: 2019-06-28 17:59:08\ntags: object detection\nmathjax: true\n---\n[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n\n# Introduction\nimage pyramid  feature pyramid size  image  image  level  feature scale  feature maps feature pyramid feature pyramid  backbone  multi-scale feature maps feature maps  feature pyramid 1\n![](/images/M2Det_fig1.png)\n\nSSD  backbone  layers 2  4  layers 6  layers  feature pyramidSTDN  DenseNet  block  pooling  scale-transfer  feature pyramidFPN  top-down  feature pyramid\n1. pyramid  feature  backbone  backbone \n2. pyramid  level  feature  scale  feature FPN  FPN  backbone  layer  feature  level \n\nsize  size  level  feature maps \n\n feature pyramid  scale  2\n![](/images/M2Det_fig2.png)\n\n backbone  multi-level features layers  base feature base feature  Thinned U-shape Modules(TUM)  Feature Fusion Modules(FFM)  block 2  multi-level multi-scale featuresmulti-leve  shallow, medium, deep  levelmulti-scale  level  features U  Scale-wise Feature Aggregation ModuleSFAM  level  scale  feature pyramid SFAM  multi-level multi-scale  multi-scale multi-level scale  feature  feature pyramid  backbone  layers  feature pyramid  Multi-Level Feature Pyramid NetworkMLFPN\n\n MLFPN  one-stage  M2Det MLFPN  SSD M2Det  SOTA  inference FPS=11.8AP=41.0 inference AP  44.2 MS-COCO  one-stage \n\n# Method\nM2Det  2 backbone  MLFPN  feature pyramid SSD  bbox  NMS MLFPN FFM, TUM  SFAMFFMv1  base feature  backbone  feature maps TUM  TUM  FFMv2  multi-level multi scale featuresSFAM  scale  level  featuresconcatenate features M2Det \n\n## MLFPN\n 2FFMv1  base feature VGG  conv4_3  conv5_3  TUM  FFMv2 TUM  scale  feature mapsFFMv2  base feature  TUM  scale  feature feature maps  TUM  TUM  base feature  multi-level multi-scale features \n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n$\\mathbf X_{base}$  base feature$x_i^l$  $l$  TUM  $i$  scale  featureL  TUM $\\mathbf T_l$   $l$  TUM $\\mathbf F$  FFMv2 \n\n### FFM\nFFM  feature  1x1  features concatenation  features FFMv1  backbone  scale  features  upsample  scale  concatenation TUM  ____  FFMv2  base feature   TUM  feature  scale concatenate  TUM FFMv1  FFMv2  4 (a)(b)\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM block  size</center>\n\n### TUM\nTUM  Thin U-shape  4(c)encoder  stride=2  3x3 decoder  feature maps  FPN  backbone  layer  upsample  element-wise sum  1x1  TUM  decoder  multi-level multi-scale features TUM  multi-scale features TUM  multi-scale features TUM  multi-scale features\n\n### SFAM\nSFAM  TUM  multi-level multi-scale features 3\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM  scale  channel  concatenate  SE attention </center>\n\n scale  features  concatenate feature pyramid  \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$  $i$  scale $W_i \\times H_i$  $i$  scale  feature map  size scale  level  feature maps  $C$  4  $C=128$ concatenate  features features  SE block squeeze global average pooling $\\mathbf z \\in \\mathcal R^C$ excitation  fc \n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n$\\sigma$  ReLu$\\delta$  sigmoid$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$ r  r=16\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$\n\n### \n VGG  ResNet  M2Det  backbonebackbone  ImageNet2012 MLFPN  8  TUM TUM  5  convs  5  6  scale  featuresTUM  scale  256  4 (c)  SSD, RefineDet  RetinaNet 320, 512  800\n\nMLFPN  6  pyramid featuresscale  1x13x35x510x1020x2040x40 scale  pyramid features 6  pyramid features  anchor(prior) box  scale  aspect ratio SSD  m  features m = 6 k  features  anchor box  scale \n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n$s_{min}=0.2, \\ s_{max}=0.9$ features  image  anchor  scale\n\n pyramidal features  6  anchors 3  aspect ratios SSD 0.05  [soft-NMS](/2019/06/24/cv-mtds) \n\n# \n\n\n# \n MLFPN  multi-scale  M2Det  SOTA  one-stage ","slug":"M2Det","published":1,"updated":"2019-06-29T09:59:57.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379bz001adgvct874b4zz","content":"<p><a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>image pyramid  feature pyramid size  image  image  level  feature scale  feature maps feature pyramid feature pyramid  backbone  multi-scale feature maps feature maps  feature pyramid 1<br><img src=\"/images/M2Det_fig1.png\" alt></p>\n<p>SSD  backbone  layers 2  4  layers 6  layers  feature pyramidSTDN  DenseNet  block  pooling  scale-transfer  feature pyramidFPN  top-down  feature pyramid</p>\n<ol>\n<li>pyramid  feature  backbone  backbone </li>\n<li>pyramid  level  feature  scale  feature FPN  FPN  backbone  layer  feature  level </li>\n</ol>\n<p>size  size  level  feature maps </p>\n<p> feature pyramid  scale  2<br><img src=\"/images/M2Det_fig2.png\" alt></p>\n<p> backbone  multi-level features layers  base feature base feature  Thinned U-shape Modules(TUM)  Feature Fusion Modules(FFM)  block 2  multi-level multi-scale featuresmulti-leve  shallow, medium, deep  levelmulti-scale  level  features U  Scale-wise Feature Aggregation ModuleSFAM  level  scale  feature pyramid SFAM  multi-level multi-scale  multi-scale multi-level scale  feature  feature pyramid  backbone  layers  feature pyramid  Multi-Level Feature Pyramid NetworkMLFPN</p>\n<p> MLFPN  one-stage  M2Det MLFPN  SSD M2Det  SOTA  inference FPS=11.8AP=41.0 inference AP  44.2 MS-COCO  one-stage </p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det  2 backbone  MLFPN  feature pyramid SSD  bbox  NMS MLFPN FFM, TUM  SFAMFFMv1  base feature  backbone  feature maps TUM  TUM  FFMv2  multi-level multi scale featuresSFAM  scale  level  featuresconcatenate features M2Det </p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p> 2FFMv1  base feature VGG  conv4_3  conv5_3  TUM  FFMv2 TUM  scale  feature mapsFFMv2  base feature  TUM  scale  feature feature maps  TUM  TUM  base feature  multi-level multi-scale features <br>$$[x_1^l,x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,L \\end{cases}$$<br>$\\mathbf X_{base}$  base feature$x_i^l$  $l$  TUM  $i$  scale  featureL  TUM $\\mathbf T_l$   $l$  TUM $\\mathbf F$  FFMv2 </p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM  feature  1x1  features concatenation  features FFMv1  backbone  scale  features  upsample  scale  concatenation TUM  <strong></strong>  FFMv2  base feature   TUM  feature  scale concatenate  TUM FFMv1  FFMv2  4 (a)(b)<br><img src=\"/images/M2Det_fig4.png\" alt> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM block  size</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM  Thin U-shape  4(c)encoder  stride=2  3x3 decoder  feature maps  FPN  backbone  layer  upsample  element-wise sum  1x1  TUM  decoder  multi-level multi-scale features TUM  multi-scale features TUM  multi-scale features TUM  multi-scale features</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM  TUM  multi-level multi-scale features 3<br><img src=\"/images/M2Det_fig3.png\" alt><center>Fig 3 SFAM  scale  channel  concatenate  SE attention </center></p>\n<p> scale  features  concatenate feature pyramid <br>$$\\mathbf X=[\\mathbf X_1,,\\mathbf X_i]$$<br> $\\mathbf X_i=Concat(x_i^1,x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$  $i$  scale $W_i \\times H_i$  $i$  scale  feature map  size scale  level  feature maps  $C$  4  $C=128$ concatenate  features features  SE block squeeze global average pooling $\\mathbf z \\in \\mathcal R^C$ excitation  fc <br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>$\\sigma$  ReLu$\\delta$  sigmoid$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$ r  r=16</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p> $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},,\\tilde {\\mathbf X_i^C}]$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> VGG  ResNet  M2Det  backbonebackbone  ImageNet2012 MLFPN  8  TUM TUM  5  convs  5  6  scale  featuresTUM  scale  256  4 (c)  SSD, RefineDet  RetinaNet 320, 512  800</p>\n<p>MLFPN  6  pyramid featuresscale  1x13x35x510x1020x2040x40 scale  pyramid features 6  pyramid features  anchor(prior) box  scale  aspect ratio SSD  m  features m = 6 k  features  anchor box  scale <br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>$s_{min}=0.2, \\ s_{max}=0.9$ features  image  anchor  scale</p>\n<p> pyramidal features  6  anchors 3  aspect ratios SSD 0.05  <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> MLFPN  multi-scale  M2Det  SOTA  one-stage </p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>image pyramid  feature pyramid size  image  image  level  feature scale  feature maps feature pyramid feature pyramid  backbone  multi-scale feature maps feature maps  feature pyramid 1<br><img src=\"/images/M2Det_fig1.png\" alt></p>\n<p>SSD  backbone  layers 2  4  layers 6  layers  feature pyramidSTDN  DenseNet  block  pooling  scale-transfer  feature pyramidFPN  top-down  feature pyramid</p>\n<ol>\n<li>pyramid  feature  backbone  backbone </li>\n<li>pyramid  level  feature  scale  feature FPN  FPN  backbone  layer  feature  level </li>\n</ol>\n<p>size  size  level  feature maps </p>\n<p> feature pyramid  scale  2<br><img src=\"/images/M2Det_fig2.png\" alt></p>\n<p> backbone  multi-level features layers  base feature base feature  Thinned U-shape Modules(TUM)  Feature Fusion Modules(FFM)  block 2  multi-level multi-scale featuresmulti-leve  shallow, medium, deep  levelmulti-scale  level  features U  Scale-wise Feature Aggregation ModuleSFAM  level  scale  feature pyramid SFAM  multi-level multi-scale  multi-scale multi-level scale  feature  feature pyramid  backbone  layers  feature pyramid  Multi-Level Feature Pyramid NetworkMLFPN</p>\n<p> MLFPN  one-stage  M2Det MLFPN  SSD M2Det  SOTA  inference FPS=11.8AP=41.0 inference AP  44.2 MS-COCO  one-stage </p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det  2 backbone  MLFPN  feature pyramid SSD  bbox  NMS MLFPN FFM, TUM  SFAMFFMv1  base feature  backbone  feature maps TUM  TUM  FFMv2  multi-level multi scale featuresSFAM  scale  level  featuresconcatenate features M2Det </p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p> 2FFMv1  base feature VGG  conv4_3  conv5_3  TUM  FFMv2 TUM  scale  feature mapsFFMv2  base feature  TUM  scale  feature feature maps  TUM  TUM  base feature  multi-level multi-scale features <br>$$[x_1^l,x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,L \\end{cases}$$<br>$\\mathbf X_{base}$  base feature$x_i^l$  $l$  TUM  $i$  scale  featureL  TUM $\\mathbf T_l$   $l$  TUM $\\mathbf F$  FFMv2 </p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM  feature  1x1  features concatenation  features FFMv1  backbone  scale  features  upsample  scale  concatenation TUM  <strong></strong>  FFMv2  base feature   TUM  feature  scale concatenate  TUM FFMv1  FFMv2  4 (a)(b)<br><img src=\"/images/M2Det_fig4.png\" alt> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM block  size</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM  Thin U-shape  4(c)encoder  stride=2  3x3 decoder  feature maps  FPN  backbone  layer  upsample  element-wise sum  1x1  TUM  decoder  multi-level multi-scale features TUM  multi-scale features TUM  multi-scale features TUM  multi-scale features</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM  TUM  multi-level multi-scale features 3<br><img src=\"/images/M2Det_fig3.png\" alt><center>Fig 3 SFAM  scale  channel  concatenate  SE attention </center></p>\n<p> scale  features  concatenate feature pyramid <br>$$\\mathbf X=[\\mathbf X_1,,\\mathbf X_i]$$<br> $\\mathbf X_i=Concat(x_i^1,x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$  $i$  scale $W_i \\times H_i$  $i$  scale  feature map  size scale  level  feature maps  $C$  4  $C=128$ concatenate  features features  SE block squeeze global average pooling $\\mathbf z \\in \\mathcal R^C$ excitation  fc <br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>$\\sigma$  ReLu$\\delta$  sigmoid$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$ r  r=16</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p> $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},,\\tilde {\\mathbf X_i^C}]$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> VGG  ResNet  M2Det  backbonebackbone  ImageNet2012 MLFPN  8  TUM TUM  5  convs  5  6  scale  featuresTUM  scale  256  4 (c)  SSD, RefineDet  RetinaNet 320, 512  800</p>\n<p>MLFPN  6  pyramid featuresscale  1x13x35x510x1020x2040x40 scale  pyramid features 6  pyramid features  anchor(prior) box  scale  aspect ratio SSD  m  features m = 6 k  features  anchor box  scale <br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>$s_{min}=0.2, \\ s_{max}=0.9$ features  image  anchor  scale</p>\n<p> pyramidal features  6  anchors 3  aspect ratios SSD 0.05  <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> MLFPN  multi-scale  M2Det  SOTA  one-stage </p>\n"},{"title":"ImprovedGAN","date":"2019-08-01T07:48:46.000Z","mathjax":true,"_content":" [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n\n [improved_gan](https://github.com/openai/improved_gan)\n\n# \nGAN GAN  [DCGAN](2019/07/23/GAN)  GAN  conv+BN+ReLU  GAN  Nash  Nash \n1. \n2. \n3. \n\n# GAN \n GAN  Nash  $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$  $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$Nash  $J^{(D)}$  $\\theta^{(D)}$  $J^{(G)}$  $\\theta^{(G)}$  Nash  Nash  G  D  $\\theta^{(D)}$  $J^{(D)}$  $J^{(G)}$ $\\theta^{(G)}$  $J^{(G)}$  $J^{(D)}$ x  xy y  -xy x  y  x=y=0\n\n## \n GAN  D G  D  G  G  D  $\\mathbf {f(x)}$  D  G \n$$\\|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))\\|_2^2$$\nG \n\n## \nGAN  ____  ____ \n\n  \n $\\mathbf {f(x_i)} \\in \\Bbb R^A$  $\\mathbf x_i$  D  $T \\in \\Bbb R^{A \\times B \\times C}$ $M_i \\in \\Bbb R^{B \\times C}$ $i \\in \\{1,...,n\\}$ $\\{M_i |i=1,...,n\\}$ L1 \n$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-\\|M_{i,b}-M_{j,b}\\| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in \\{1,...,n\\}, \\quad b \\in \\{1,...,B\\}$$\n\n b  row index 1minibatch layer  $\\mathbf x_i$ \n$$\\begin{aligned} &o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R\n\\\\\\\\ &o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,...o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B\n\\\\\\\\ &o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$\n\n minibatch layer  $o(\\mathbf x_i)$  minibatch layer  $\\mathbf {f(x_i)}$ concatenate  D  layer  minibatch layer \n![](/images/ImprovedGAN_fig1.png)\n\n## \nG  D $\\|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]\\|^2$ $\\theta[i]$  i \n\n## \nLabel  target  0  1  0.9  0.1 target  $\\alpha$ target  $\\beta$\n$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$\n\n-  $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\alpha$\n-  $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\beta$\n\n [GAN](2019/7/23/GAN)  $D^{\\ast}$\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$\n\n target  target $\\alpha > \\beta$ (1)  D  $\\beta < D(x) < \\alpha$\n\n $p_{model}$ $p_{data} \\rightarrow 0$ $p_{model}$  $p_{model}$  label  $\\alpha$ label  0\n\n## \nDCGAN  BN  $\\mathbf x$ minibatch  $\\mathbf x'$ VBN $\\mathbf x$  reference batch  $\\mathbf x$ reference batch reference batch  VBN  G \n\n# \nGAN  Inception  label  $p(y|\\mathbf x)$ label  $p(y|\\mathbf x)$  $\\mathbf x$Inception  y  c zG  Inception  $\\int p(y|\\mathbf x=G(z)) dz$  KL \n$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)\\|p(y)) ]$$\n\n\n\n# \n $\\mathbf x$ K  K  $[l_1,...,l_K]$ softmax \n$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$\n\n log \n\n G G  y=K+1 K+1 $p_{model}(y=K+1|\\mathbf x)$  $\\mathbf x$  GAN  $1-D(\\mathbf x)$  K  $\\log p_{model}(y \\in \\{1,...,K\\}|\\mathbf x)$log \n$$\\begin{aligned} &L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}\n\\\\\\\\ &L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y <K+1)\n\\\\\\\\ &L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$\n\n $L_{unsupervised}$  GAN  objective $L_{unsupervised}$  $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$,\n$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$\n\n $L_{supervised}$  $L_{unsupervised}$  $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$  $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$ $c(\\mathbf x)$  G  GAN objective D G  GAN  G\n\n K+1  $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$ softmax  $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$ $L_{supervised}$  K  D  $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$ $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$","source":"_posts/ImprovedGAN.md","raw":"---\ntitle: ImprovedGAN\ndate: 2019-08-01 15:48:46\ntags: GAN\nmathjax: true\n---\n [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n\n [improved_gan](https://github.com/openai/improved_gan)\n\n# \nGAN GAN  [DCGAN](2019/07/23/GAN)  GAN  conv+BN+ReLU  GAN  Nash  Nash \n1. \n2. \n3. \n\n# GAN \n GAN  Nash  $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$  $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$Nash  $J^{(D)}$  $\\theta^{(D)}$  $J^{(G)}$  $\\theta^{(G)}$  Nash  Nash  G  D  $\\theta^{(D)}$  $J^{(D)}$  $J^{(G)}$ $\\theta^{(G)}$  $J^{(G)}$  $J^{(D)}$ x  xy y  -xy x  y  x=y=0\n\n## \n GAN  D G  D  G  G  D  $\\mathbf {f(x)}$  D  G \n$$\\|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))\\|_2^2$$\nG \n\n## \nGAN  ____  ____ \n\n  \n $\\mathbf {f(x_i)} \\in \\Bbb R^A$  $\\mathbf x_i$  D  $T \\in \\Bbb R^{A \\times B \\times C}$ $M_i \\in \\Bbb R^{B \\times C}$ $i \\in \\{1,...,n\\}$ $\\{M_i |i=1,...,n\\}$ L1 \n$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-\\|M_{i,b}-M_{j,b}\\| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in \\{1,...,n\\}, \\quad b \\in \\{1,...,B\\}$$\n\n b  row index 1minibatch layer  $\\mathbf x_i$ \n$$\\begin{aligned} &o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R\n\\\\\\\\ &o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,...o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B\n\\\\\\\\ &o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$\n\n minibatch layer  $o(\\mathbf x_i)$  minibatch layer  $\\mathbf {f(x_i)}$ concatenate  D  layer  minibatch layer \n![](/images/ImprovedGAN_fig1.png)\n\n## \nG  D $\\|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]\\|^2$ $\\theta[i]$  i \n\n## \nLabel  target  0  1  0.9  0.1 target  $\\alpha$ target  $\\beta$\n$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$\n\n-  $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\alpha$\n-  $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\beta$\n\n [GAN](2019/7/23/GAN)  $D^{\\ast}$\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$\n\n target  target $\\alpha > \\beta$ (1)  D  $\\beta < D(x) < \\alpha$\n\n $p_{model}$ $p_{data} \\rightarrow 0$ $p_{model}$  $p_{model}$  label  $\\alpha$ label  0\n\n## \nDCGAN  BN  $\\mathbf x$ minibatch  $\\mathbf x'$ VBN $\\mathbf x$  reference batch  $\\mathbf x$ reference batch reference batch  VBN  G \n\n# \nGAN  Inception  label  $p(y|\\mathbf x)$ label  $p(y|\\mathbf x)$  $\\mathbf x$Inception  y  c zG  Inception  $\\int p(y|\\mathbf x=G(z)) dz$  KL \n$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)\\|p(y)) ]$$\n\n\n\n# \n $\\mathbf x$ K  K  $[l_1,...,l_K]$ softmax \n$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$\n\n log \n\n G G  y=K+1 K+1 $p_{model}(y=K+1|\\mathbf x)$  $\\mathbf x$  GAN  $1-D(\\mathbf x)$  K  $\\log p_{model}(y \\in \\{1,...,K\\}|\\mathbf x)$log \n$$\\begin{aligned} &L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}\n\\\\\\\\ &L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y <K+1)\n\\\\\\\\ &L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$\n\n $L_{unsupervised}$  GAN  objective $L_{unsupervised}$  $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$,\n$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$\n\n $L_{supervised}$  $L_{unsupervised}$  $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$  $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$ $c(\\mathbf x)$  G  GAN objective D G  GAN  G\n\n K+1  $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$ softmax  $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$ $L_{supervised}$  K  D  $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$ $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$","slug":"ImprovedGAN","published":1,"updated":"2019-08-16T11:08:24.220Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379c1001cdgvcieym9jpw","content":"<p> <a href=\"https://arxiv.org/abs/1606.03498\" target=\"_blank\" rel=\"noopener\">Improved Techniques for Training GANs</a></p>\n<p> <a href=\"https://github.com/openai/improved_gan\" target=\"_blank\" rel=\"noopener\">improved_gan</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN GAN  <a href=\"2019/07/23/GAN\">DCGAN</a>  GAN  conv+BN+ReLU  GAN  Nash  Nash </p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<h1 id=\"GAN-\"><a href=\"#GAN-\" class=\"headerlink\" title=\"GAN \"></a>GAN </h1><p> GAN  Nash  $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$  $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$Nash  $J^{(D)}$  $\\theta^{(D)}$  $J^{(G)}$  $\\theta^{(G)}$  Nash  Nash  G  D  $\\theta^{(D)}$  $J^{(D)}$  $J^{(G)}$ $\\theta^{(G)}$  $J^{(G)}$  $J^{(D)}$ x  xy y  -xy x  y  x=y=0</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> GAN  D G  D  G  G  D  $\\mathbf {f(x)}$  D  G <br>$$|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))|_2^2$$<br>G </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>GAN  <strong></strong>  <strong></strong> </p>\n<p><br> $\\mathbf {f(x_i)} \\in \\Bbb R^A$  $\\mathbf x_i$  D  $T \\in \\Bbb R^{A \\times B \\times C}$ $M_i \\in \\Bbb R^{B \\times C}$ $i \\in {1,,n}$ ${M_i |i=1,,n}$ L1 <br>$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-|M_{i,b}-M_{j,b}| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in {1,,n}, \\quad b \\in {1,,B}$$</p>\n<p> b  row index 1minibatch layer  $\\mathbf x_i$ <br>$$\\begin{aligned} &amp;o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R<br>\\\\ &amp;o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B<br>\\\\ &amp;o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$</p>\n<p> minibatch layer  $o(\\mathbf x_i)$  minibatch layer  $\\mathbf {f(x_i)}$ concatenate  D  layer  minibatch layer <br><img src=\"/images/ImprovedGAN_fig1.png\" alt></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>G  D $|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]|^2$ $\\theta[i]$  i </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>Label  target  0  1  0.9  0.1 target  $\\alpha$ target  $\\beta$<br>$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$</p>\n<ul>\n<li> $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\alpha$</li>\n<li> $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\beta$</li>\n</ul>\n<p> <a href=\"2019/7/23/GAN\">GAN</a>  $D^{\\ast}$<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$</p>\n<p> target  target $\\alpha &gt; \\beta$ (1)  D  $\\beta &lt; D(x) &lt; \\alpha$</p>\n<p> $p_{model}$ $p_{data} \\rightarrow 0$ $p_{model}$  $p_{model}$  label  $\\alpha$ label  0</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>DCGAN  BN  $\\mathbf x$ minibatch  $\\mathbf x$ VBN $\\mathbf x$  reference batch  $\\mathbf x$ reference batch reference batch  VBN  G </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  Inception  label  $p(y|\\mathbf x)$ label  $p(y|\\mathbf x)$  $\\mathbf x$Inception  y  c zG  Inception  $\\int p(y|\\mathbf x=G(z)) dz$  KL <br>$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)|p(y)) ]$$</p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\mathbf x$ K  K  $[l_1,,l_K]$ softmax <br>$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$</p>\n<p> log </p>\n<p> G G  y=K+1 K+1 $p_{model}(y=K+1|\\mathbf x)$  $\\mathbf x$  GAN  $1-D(\\mathbf x)$  K  $\\log p_{model}(y \\in {1,,K}|\\mathbf x)$log <br>$$\\begin{aligned} &amp;L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}<br>\\\\ &amp;L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y &lt;K+1)<br>\\\\ &amp;L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$</p>\n<p> $L_{unsupervised}$  GAN  objective $L_{unsupervised}$  $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$,<br>$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$</p>\n<p> $L_{supervised}$  $L_{unsupervised}$  $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$  $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$ $c(\\mathbf x)$  G  GAN objective D G  GAN  G</p>\n<p> K+1  $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$ softmax  $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$ $L_{supervised}$  K  D  $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$ $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$</p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1606.03498\" target=\"_blank\" rel=\"noopener\">Improved Techniques for Training GANs</a></p>\n<p> <a href=\"https://github.com/openai/improved_gan\" target=\"_blank\" rel=\"noopener\">improved_gan</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN GAN  <a href=\"2019/07/23/GAN\">DCGAN</a>  GAN  conv+BN+ReLU  GAN  Nash  Nash </p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<h1 id=\"GAN-\"><a href=\"#GAN-\" class=\"headerlink\" title=\"GAN \"></a>GAN </h1><p> GAN  Nash  $J^{(G)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$  $J^{(D)}(\\mathbf {\\theta}^{(D)}, \\mathbf {\\theta}^{G})$Nash  $J^{(D)}$  $\\theta^{(D)}$  $J^{(G)}$  $\\theta^{(G)}$  Nash  Nash  G  D  $\\theta^{(D)}$  $J^{(D)}$  $J^{(G)}$ $\\theta^{(G)}$  $J^{(G)}$  $J^{(D)}$ x  xy y  -xy x  y  x=y=0</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> GAN  D G  D  G  G  D  $\\mathbf {f(x)}$  D  G <br>$$|\\Bbb E_{\\mathbf x \\sim p_{data}} \\mathbf {f(x)}-\\Bbb E_{\\mathbf z \\sim p_{\\mathbf z}}\\mathbf f(G(\\mathbf z))|_2^2$$<br>G </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>GAN  <strong></strong>  <strong></strong> </p>\n<p><br> $\\mathbf {f(x_i)} \\in \\Bbb R^A$  $\\mathbf x_i$  D  $T \\in \\Bbb R^{A \\times B \\times C}$ $M_i \\in \\Bbb R^{B \\times C}$ $i \\in {1,,n}$ ${M_i |i=1,,n}$ L1 <br>$$c_b(\\mathbf x_i, \\mathbf x_j)=\\exp(-|M_{i,b}-M_{j,b}| _ {L_1}) \\in \\Bbb R, \\quad i,j \\in {1,,n}, \\quad b \\in {1,,B}$$</p>\n<p> b  row index 1minibatch layer  $\\mathbf x_i$ <br>$$\\begin{aligned} &amp;o(\\mathbf x_i) _ b = \\sum_{j=1}^n c _ b(\\mathbf x_i, \\mathbf x_j) \\in \\Bbb R<br>\\\\ &amp;o(\\mathbf x_i)=\\left[o(\\mathbf x_i) _ 1,o(\\mathbf x_i) _ B \\right] \\in \\Bbb R^B<br>\\\\ &amp;o(\\mathbf X) \\in \\Bbb R^{n \\times B} \\end{aligned}$$</p>\n<p> minibatch layer  $o(\\mathbf x_i)$  minibatch layer  $\\mathbf {f(x_i)}$ concatenate  D  layer  minibatch layer <br><img src=\"/images/ImprovedGAN_fig1.png\" alt></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>G  D $|\\mathbf \\theta -\\frac 1 t \\sum_{i=1}^t \\theta[i]|^2$ $\\theta[i]$  i </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>Label  target  0  1  0.9  0.1 target  $\\alpha$ target  $\\beta$<br>$$D(\\mathbf x)=\\frac {\\alpha p_{data}(\\mathbf x) + \\beta p_{model}(\\mathbf x)}{p_{data}(\\mathbf x)+p_{model}(\\mathbf x)}$$</p>\n<ul>\n<li> $p_{data}(\\mathbf x) \\gg p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\alpha$</li>\n<li> $p_{data}(\\mathbf x) \\ll p_{model}(\\mathbf x)$ $D(\\mathbf x) \\rightarrow \\beta$</li>\n</ul>\n<p> <a href=\"2019/7/23/GAN\">GAN</a>  $D^{\\ast}$<br>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log (D(x)-\\beta)] + \\Bbb E_{z \\sim p_z(z)}[\\log(\\alpha-D(G(z)))] \\quad (1)$$</p>\n<p> target  target $\\alpha &gt; \\beta$ (1)  D  $\\beta &lt; D(x) &lt; \\alpha$</p>\n<p> $p_{model}$ $p_{data} \\rightarrow 0$ $p_{model}$  $p_{model}$  label  $\\alpha$ label  0</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>DCGAN  BN  $\\mathbf x$ minibatch  $\\mathbf x$ VBN $\\mathbf x$  reference batch  $\\mathbf x$ reference batch reference batch  VBN  G </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  Inception  label  $p(y|\\mathbf x)$ label  $p(y|\\mathbf x)$  $\\mathbf x$Inception  y  c zG  Inception  $\\int p(y|\\mathbf x=G(z)) dz$  KL <br>$$\\exp [\\Bbb E_{\\mathbf x} \\mathbf {KL}(p(y|\\mathbf x)|p(y)) ]$$</p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\mathbf x$ K  K  $[l_1,,l_K]$ softmax <br>$$p_{model}(y=j|\\mathbf x)=\\frac {\\exp l_j} {\\sum_{k=1}^K \\exp l_k}$$</p>\n<p> log </p>\n<p> G G  y=K+1 K+1 $p_{model}(y=K+1|\\mathbf x)$  $\\mathbf x$  GAN  $1-D(\\mathbf x)$  K  $\\log p_{model}(y \\in {1,,K}|\\mathbf x)$log <br>$$\\begin{aligned} &amp;L=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)}[\\log p_{model}(y|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]=L_{supervised}+L_{unsupervised}<br>\\\\ &amp;L_{supervised}=-\\Bbb E_{\\mathbf x,y \\sim p_{data}(\\mathbf x,y)} \\log p_{model}(y|\\mathbf x, y &lt;K+1)<br>\\\\ &amp;L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log[1- p_{model}(y=K+1|\\mathbf x)] - \\Bbb E_{\\mathbf x \\sim G} [\\log p_{model}(y=K+1|\\mathbf x)]\\end{aligned}$$</p>\n<p> $L_{unsupervised}$  GAN  objective $L_{unsupervised}$  $D(\\mathbf x)=1-p_{model}(y=K+1|\\mathbf x)$,<br>$$L_{unsupervised}=-\\Bbb E_{\\mathbf x \\sim p_{data}(\\mathbf x)} \\log D(\\mathbf x) - \\Bbb E_{z \\sim noise} \\log (1-D(G(z)))$$</p>\n<p> $L_{supervised}$  $L_{unsupervised}$  $\\exp[l_j(\\mathbf x)]=c(\\mathbf x) p(y=j,\\mathbf x), \\ \\forall j \\in K+1$  $\\exp[l_{K+1}(\\mathbf x)]=c(\\mathbf x) p_G(\\mathbf x)$ $c(\\mathbf x)$  G  GAN objective D G  GAN  G</p>\n<p> K+1  $l_j(\\mathbf x)\\leftarrow l_j(\\mathbf x)-f(\\mathbf x)$ softmax  $l_{K+1}(\\mathbf x)=0, \\ \\forall \\mathbf x$ $L_{supervised}$  K  D  $D(\\mathbf x)=\\frac {Z(\\mathbf x)} {Z(\\mathbf x)+1}$ $Z(\\mathbf x)=\\sum_{k=1}^K \\exp [l_k(\\mathbf x)]$</p>\n"},{"title":"PyTorch-4","date":"2019-08-22T06:34:33.000Z","_content":"## Tensor\ntorch  TensorFloatTensorDoubleTensorHalfTensorByteTensor  Tensor  `torch/__init__.py`  `_C._initExtension(manager_path())` manager_path  torch_shm_manager shm Domain Socket_initExtension  torch/csrc/Module.cpp  _C  _C  c++  THPModule_initExtension\n### initializeLayouts\n c10/core/Layout.h \n1. Strided\n2. Sparse\n3. Mkldnn Intel  Mkldnn  CPU  Mkldnn \n\n Strided  THPLayout_New  THPLayoutType/THPLayout  layout  Stridedname  \"torch.strided\" __ torch __\n- CPU, CUDA, MSNPU, XLA, QuantizedCPU -> strided_layout\n- SparseCPU, SparseCUDA -> sparse_coo_layout\n- MkldnnCPU -> mkldnn_layout\n  \n Backend  Layout  Backend  Layout\n\n### initializeMemoryFormats\n Tensor Preserve, Contiguous  ChannelsLast ChannelsLast  NHWC NCHW  sizes ChannelsLast  strides \n```c++\nstrides[1]=1;           // ChannelsLast  C  1\nstrides[3]=sizes[1];    // ChannelsLast  W  C  sizes[1]\nstrides[2]=strides[3]*sizes[3]; // ChannelsLast  H  W*C\nstrides[0]=strides[2]*sizes[2]; // ChannelsLast  N  H*W*C\n```\n strides  sizes  NCHW\n preserve_format, contiguous_format, channels_last  __ torch __\n\n### initializeQScheme\n inference  [Introducing-Quantized-Tensor](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor) 5  __ torch __\n\n### initializeDtypes\n\n```c++\n#define DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,\nat:ScalarType all_scalar_type[] = {\n    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n```\n AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS  complex  quantization \n```\nat::ScalarType::Byte,\nat::ScalarType::Char,\nat::ScalarType::Short,\nat::ScalarType::Int,\nat::ScalarType::Long,\nat::ScalarType::Half,\nat::ScalarType::Float,\nat::ScalarType::Double,\nat::ScalarType::ComplexHalf,\nat::ScalarType::ComplexFloat,\nat::ScalarType::ComplexDouble,\nat::ScalarType::Bool,\nat::ScalarType::QInt8,\nat::ScalarType::QUInt8,\nat::ScalarType::QInt32\nat::ScalarType::BFloat16\n```\n `\"\"` at::ScalarType  at::ScalarType \n```c++\nstd::tie(primary_name, legacy_name) = getDtypeName(scalarType);\nPyObject *dtype = THPDtype_New(scalarType, primary_name);\ntorch::registerDtypeObject((THPDtype*)dtype, scalarType);\n```\nTHPDtypeType/THPDtype __ torch __ torch \n\n### initialize_python_bindings\n python \n#### initialize_aten_types\n all_declared_types Backend  `CPU, CUDA, SparseCPU, SparseCUDA` ScalarType  Complex  Quantization \n```\nByte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16\n```\n Backend  ScalarType  (SparseCUDA|SparseCPU,Bool)  4*10-2=38  38  PyTensorType  PyTensorType \n```c++\nstruct PyTensorType {\n    PyTypeObject py_type;   // python \n    THPDtype* dtype;        //  initializeDtypes \n    THPLayout* layout;      //  initializeLayouts \n    bool is_cuda;           //  cuda  cpu\n    char name[64];          // tensor \n    int backend;            // CPU, CUDA, SparseCPU, SparseCUDA \n    int scalar_type;        // Byte \n};\n```\n 38  PyTensorType Python  Tensor FloatTensor  PyTensorType \n- layout backend initializeLayouts  Backend  Layout \n- is_cuda backend = CUDA|SparseCUD  true\n- name `[].[ScalarType]Tensor`  \n  \n  ```\n  CPU -> torch\n  CUDA -> torch.cuda\n  SparseCPU -> torch.sparse\n  SparseCUDA -> torch.cuda.sparse\n  ```\n  ScalarType  ScalarType  Byte -> \"Byte\", Float -> \"Float\"  (CPU, Float)  PyTensorType  \"torch.FloatTensor\"(SparseCUDA, Double)  PyTensorType  \"torch.cuda.sparse.DoubleTensor\"\n\n\n```c++\nif (backend==Backend::CPU && scalar_type==at::kFloat) {\n    set_default_tensor_type(&tensor_type);\n}\n```\n Tensor  torch.FloatTensor Backend  CPU CUDA  torch.cuda.FloatTensor\n\ninitialize_aten_types  38  PyTensorType  tensor_types  vector \n\n#### py_initialize_metaclass(metaclass)\n python  PyTypeObject python  \"torch.tensortype\" tensor tensor  torch.FloatTensor \n- dtype  \n     initializeDtypes  THPDtype \n- layout  \n     initializeLayouts  THPLayout \n- is_cuda  \n     cuda\n- is_sparse  \n    \n\n\n- `__instancecheck__`  Tensor  tensor  type_id  scalar_type \n\nPyTensorType  python  Tensor  python  metaclass\n\n PyTypeObject  metaclass tensor \n\n#### get_tensor_dict\n torch.Tensor  _C._TensorBase \n\n#### py_initialize_tensor_type\n 38  PyTensorType  py_type py_type  PyTypeObject python  metaclass torch.Tensor \n```\ndir(torch.Tensor)\ndir(torch.FloatTensor)\n```\n\n\n#### py_bind_tensor_types\n 38  PyTensorType  \" torch \" PyTensorType  \"\" \"torch.FloatTensor\" PyTensorType  FloatTensor  python  torch \"torch.cuda.sparse.DoubleTensor\"  PyTensorType  DoubleTensor  python  torch.cuda.sparse  `.` \n\n torch.FloatTensor, torch.IntTensor  torch.Tensor \n```python\na=torch.empty(1,2,dtype=torch.int)\nisinstance(a, torch.IntTensor)  # True\nisinstance(a, torch.Tensor)     # True\nissubclass(torch.IntTensor, torch.Tensor)   # False\nissubclass(torch.Tensor, torch.IntTensor)   # False\n```\n [Pytorch-3](2019/06/18/Pytorch-3)  torch.empty  THPVariable_Wrap  c++ Variable  python  torch.Tensor  torch.IntTensor  THPVariable_Wrap  torch.Tensor \n```\n>>> type(torch.IntTensor([1,2]))\n<class 'torch.Tensor'>\n```\n torch.Tensor  torch.IntTensor torch.IntTensor  38  Tensor  torch.Tensor \n```\n>>> torch.IntTensor.__bases__\n(<class 'object'>)\n>>> torch.Tensor.__bases__\n(<class 'torch._C._TensorBases'>)\n```\n Tensor  torch.Tensor `isinstance(a, torch.IntTensor)`  True  `isinstance`  `__instancecheck__`  metaclass  c++  Tensor_instancecheck\n```c++\nstatic PyObject *Tensor_instancecheck(PyTensorType *self, PyObject * arg) {\n    try{\n        if(THPVariable_Check(arg)) {    //  THPVariable \n            auto& var = ((THPVariable*)arg)->cdata; //  Variable \n            if (var.type_id() == self->get_type_id() &&\n                var.scalar_type() == static_cast<ScalarType>(self->scalar_type)) {\n                Py_RETURN_TRUE;     //  type_id  ScalarType  True\n            }\n        }\n        Py_RETURN_FALSE;\n    } catch(python_error & e){\n        return nullptr;\n    }\n}\n```\n `isinstance(a, torch.IntTensor)=True`\n\n__ [PyTorch-2]()  PyTorch  Tensor  torch.Tensor `torch._C.Tensor` Tensor  `torch._C.Tensor` __\n\n THPxxxStorage_postInit  [PyTorch-2](2019/06/13/PyTorch-2) THPxxxStorage_init\n1. \n    ```c++\n    #define THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) //torch/csrc/Storage.h\n    bool THPStorage_(postInit)(PyObject *module);   // torch/csrc/generic/Storage.h\n    ```\n    THPStorage_(NAME)  THPxxxStorage_init  Real  ScalarTypeNAME  init THPxxxStorage_init(PyObject *module);\n\n2. torch/csrc/generic/Storage.cpp \n   ```c++\n   PyObject *THPStorageClass = nullptr;\n   bool THPStorage_(postInit)(PyObject *module){\n       //  torch  RealStorage  Real  Float, Bool, Double  ScalarType\n       THPStorageClass = PyObject_GetAttrString(module, (char*)TH_CONCAT_STRING_2(Real, Storage));\n       at::Backend backend = at::Backend::CPU;\n       #ifdef THC_GENERIC_FILE\n       backend = at::Backend::CUDA;\n       #endif\n       #ifdef THQUANTIZED\n       backend = at::Backend::QuantizedCPU;\n       #endif\n       torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));\n   }\n   ```\n    `torch/__init__.py`  FloatStorage \n   ```python\n   class FloatStorage(_C.FloatStorageBase, _StorageBase):\n       pass\n   ```\n   torch._C.FloatStorageBase  THPxxxStorage_init  torch._C THPxxxStorage_postInit  torch  RealStorage  RealStorage  (Backend, ScalarType)  (Backend, ScalarType)  RealStorage \n\n","source":"_posts/PyTorch-4.md","raw":"---\ntitle: PyTorch-4\ndate: 2019-08-22 14:34:33\ntags: PyTorch\ncategories: DL Framework\n---\n## Tensor\ntorch  TensorFloatTensorDoubleTensorHalfTensorByteTensor  Tensor  `torch/__init__.py`  `_C._initExtension(manager_path())` manager_path  torch_shm_manager shm Domain Socket_initExtension  torch/csrc/Module.cpp  _C  _C  c++  THPModule_initExtension\n### initializeLayouts\n c10/core/Layout.h \n1. Strided\n2. Sparse\n3. Mkldnn Intel  Mkldnn  CPU  Mkldnn \n\n Strided  THPLayout_New  THPLayoutType/THPLayout  layout  Stridedname  \"torch.strided\" __ torch __\n- CPU, CUDA, MSNPU, XLA, QuantizedCPU -> strided_layout\n- SparseCPU, SparseCUDA -> sparse_coo_layout\n- MkldnnCPU -> mkldnn_layout\n  \n Backend  Layout  Backend  Layout\n\n### initializeMemoryFormats\n Tensor Preserve, Contiguous  ChannelsLast ChannelsLast  NHWC NCHW  sizes ChannelsLast  strides \n```c++\nstrides[1]=1;           // ChannelsLast  C  1\nstrides[3]=sizes[1];    // ChannelsLast  W  C  sizes[1]\nstrides[2]=strides[3]*sizes[3]; // ChannelsLast  H  W*C\nstrides[0]=strides[2]*sizes[2]; // ChannelsLast  N  H*W*C\n```\n strides  sizes  NCHW\n preserve_format, contiguous_format, channels_last  __ torch __\n\n### initializeQScheme\n inference  [Introducing-Quantized-Tensor](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor) 5  __ torch __\n\n### initializeDtypes\n\n```c++\n#define DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,\nat:ScalarType all_scalar_type[] = {\n    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n```\n AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS  complex  quantization \n```\nat::ScalarType::Byte,\nat::ScalarType::Char,\nat::ScalarType::Short,\nat::ScalarType::Int,\nat::ScalarType::Long,\nat::ScalarType::Half,\nat::ScalarType::Float,\nat::ScalarType::Double,\nat::ScalarType::ComplexHalf,\nat::ScalarType::ComplexFloat,\nat::ScalarType::ComplexDouble,\nat::ScalarType::Bool,\nat::ScalarType::QInt8,\nat::ScalarType::QUInt8,\nat::ScalarType::QInt32\nat::ScalarType::BFloat16\n```\n `\"\"` at::ScalarType  at::ScalarType \n```c++\nstd::tie(primary_name, legacy_name) = getDtypeName(scalarType);\nPyObject *dtype = THPDtype_New(scalarType, primary_name);\ntorch::registerDtypeObject((THPDtype*)dtype, scalarType);\n```\nTHPDtypeType/THPDtype __ torch __ torch \n\n### initialize_python_bindings\n python \n#### initialize_aten_types\n all_declared_types Backend  `CPU, CUDA, SparseCPU, SparseCUDA` ScalarType  Complex  Quantization \n```\nByte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16\n```\n Backend  ScalarType  (SparseCUDA|SparseCPU,Bool)  4*10-2=38  38  PyTensorType  PyTensorType \n```c++\nstruct PyTensorType {\n    PyTypeObject py_type;   // python \n    THPDtype* dtype;        //  initializeDtypes \n    THPLayout* layout;      //  initializeLayouts \n    bool is_cuda;           //  cuda  cpu\n    char name[64];          // tensor \n    int backend;            // CPU, CUDA, SparseCPU, SparseCUDA \n    int scalar_type;        // Byte \n};\n```\n 38  PyTensorType Python  Tensor FloatTensor  PyTensorType \n- layout backend initializeLayouts  Backend  Layout \n- is_cuda backend = CUDA|SparseCUD  true\n- name `[].[ScalarType]Tensor`  \n  \n  ```\n  CPU -> torch\n  CUDA -> torch.cuda\n  SparseCPU -> torch.sparse\n  SparseCUDA -> torch.cuda.sparse\n  ```\n  ScalarType  ScalarType  Byte -> \"Byte\", Float -> \"Float\"  (CPU, Float)  PyTensorType  \"torch.FloatTensor\"(SparseCUDA, Double)  PyTensorType  \"torch.cuda.sparse.DoubleTensor\"\n\n\n```c++\nif (backend==Backend::CPU && scalar_type==at::kFloat) {\n    set_default_tensor_type(&tensor_type);\n}\n```\n Tensor  torch.FloatTensor Backend  CPU CUDA  torch.cuda.FloatTensor\n\ninitialize_aten_types  38  PyTensorType  tensor_types  vector \n\n#### py_initialize_metaclass(metaclass)\n python  PyTypeObject python  \"torch.tensortype\" tensor tensor  torch.FloatTensor \n- dtype  \n     initializeDtypes  THPDtype \n- layout  \n     initializeLayouts  THPLayout \n- is_cuda  \n     cuda\n- is_sparse  \n    \n\n\n- `__instancecheck__`  Tensor  tensor  type_id  scalar_type \n\nPyTensorType  python  Tensor  python  metaclass\n\n PyTypeObject  metaclass tensor \n\n#### get_tensor_dict\n torch.Tensor  _C._TensorBase \n\n#### py_initialize_tensor_type\n 38  PyTensorType  py_type py_type  PyTypeObject python  metaclass torch.Tensor \n```\ndir(torch.Tensor)\ndir(torch.FloatTensor)\n```\n\n\n#### py_bind_tensor_types\n 38  PyTensorType  \" torch \" PyTensorType  \"\" \"torch.FloatTensor\" PyTensorType  FloatTensor  python  torch \"torch.cuda.sparse.DoubleTensor\"  PyTensorType  DoubleTensor  python  torch.cuda.sparse  `.` \n\n torch.FloatTensor, torch.IntTensor  torch.Tensor \n```python\na=torch.empty(1,2,dtype=torch.int)\nisinstance(a, torch.IntTensor)  # True\nisinstance(a, torch.Tensor)     # True\nissubclass(torch.IntTensor, torch.Tensor)   # False\nissubclass(torch.Tensor, torch.IntTensor)   # False\n```\n [Pytorch-3](2019/06/18/Pytorch-3)  torch.empty  THPVariable_Wrap  c++ Variable  python  torch.Tensor  torch.IntTensor  THPVariable_Wrap  torch.Tensor \n```\n>>> type(torch.IntTensor([1,2]))\n<class 'torch.Tensor'>\n```\n torch.Tensor  torch.IntTensor torch.IntTensor  38  Tensor  torch.Tensor \n```\n>>> torch.IntTensor.__bases__\n(<class 'object'>)\n>>> torch.Tensor.__bases__\n(<class 'torch._C._TensorBases'>)\n```\n Tensor  torch.Tensor `isinstance(a, torch.IntTensor)`  True  `isinstance`  `__instancecheck__`  metaclass  c++  Tensor_instancecheck\n```c++\nstatic PyObject *Tensor_instancecheck(PyTensorType *self, PyObject * arg) {\n    try{\n        if(THPVariable_Check(arg)) {    //  THPVariable \n            auto& var = ((THPVariable*)arg)->cdata; //  Variable \n            if (var.type_id() == self->get_type_id() &&\n                var.scalar_type() == static_cast<ScalarType>(self->scalar_type)) {\n                Py_RETURN_TRUE;     //  type_id  ScalarType  True\n            }\n        }\n        Py_RETURN_FALSE;\n    } catch(python_error & e){\n        return nullptr;\n    }\n}\n```\n `isinstance(a, torch.IntTensor)=True`\n\n__ [PyTorch-2]()  PyTorch  Tensor  torch.Tensor `torch._C.Tensor` Tensor  `torch._C.Tensor` __\n\n THPxxxStorage_postInit  [PyTorch-2](2019/06/13/PyTorch-2) THPxxxStorage_init\n1. \n    ```c++\n    #define THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) //torch/csrc/Storage.h\n    bool THPStorage_(postInit)(PyObject *module);   // torch/csrc/generic/Storage.h\n    ```\n    THPStorage_(NAME)  THPxxxStorage_init  Real  ScalarTypeNAME  init THPxxxStorage_init(PyObject *module);\n\n2. torch/csrc/generic/Storage.cpp \n   ```c++\n   PyObject *THPStorageClass = nullptr;\n   bool THPStorage_(postInit)(PyObject *module){\n       //  torch  RealStorage  Real  Float, Bool, Double  ScalarType\n       THPStorageClass = PyObject_GetAttrString(module, (char*)TH_CONCAT_STRING_2(Real, Storage));\n       at::Backend backend = at::Backend::CPU;\n       #ifdef THC_GENERIC_FILE\n       backend = at::Backend::CUDA;\n       #endif\n       #ifdef THQUANTIZED\n       backend = at::Backend::QuantizedCPU;\n       #endif\n       torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));\n   }\n   ```\n    `torch/__init__.py`  FloatStorage \n   ```python\n   class FloatStorage(_C.FloatStorageBase, _StorageBase):\n       pass\n   ```\n   torch._C.FloatStorageBase  THPxxxStorage_init  torch._C THPxxxStorage_postInit  torch  RealStorage  RealStorage  (Backend, ScalarType)  (Backend, ScalarType)  RealStorage \n\n","slug":"PyTorch-4","published":1,"updated":"2019-08-26T07:47:25.398Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379c2001edgvc1wahyx27","content":"<h2 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h2><p>torch  TensorFloatTensorDoubleTensorHalfTensorByteTensor  Tensor  <code>torch/__init__.py</code>  <code>_C._initExtension(manager_path())</code> manager_path  torch_shm_manager shm Domain Socket_initExtension  torch/csrc/Module.cpp  _C  _C  c++  THPModule_initExtension</p>\n<h3 id=\"initializeLayouts\"><a href=\"#initializeLayouts\" class=\"headerlink\" title=\"initializeLayouts\"></a>initializeLayouts</h3><p> c10/core/Layout.h </p>\n<ol>\n<li>Strided</li>\n<li>Sparse</li>\n<li>Mkldnn Intel  Mkldnn  CPU  Mkldnn </li>\n</ol>\n<p> Strided  THPLayout_New  THPLayoutType/THPLayout  layout  Stridedname  torch.strided <strong> torch </strong></p>\n<ul>\n<li>CPU, CUDA, MSNPU, XLA, QuantizedCPU -&gt; strided_layout</li>\n<li>SparseCPU, SparseCUDA -&gt; sparse_coo_layout</li>\n<li>MkldnnCPU -&gt; mkldnn_layout</li>\n</ul>\n<p> Backend  Layout  Backend  Layout</p>\n<h3 id=\"initializeMemoryFormats\"><a href=\"#initializeMemoryFormats\" class=\"headerlink\" title=\"initializeMemoryFormats\"></a>initializeMemoryFormats</h3><p> Tensor Preserve, Contiguous  ChannelsLast ChannelsLast  NHWC NCHW  sizes ChannelsLast  strides </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">strides[<span class=\"number\">1</span>]=<span class=\"number\">1</span>;           <span class=\"comment\">// ChannelsLast  C  1</span></span><br><span class=\"line\">strides[<span class=\"number\">3</span>]=sizes[<span class=\"number\">1</span>];    <span class=\"comment\">// ChannelsLast  W  C  sizes[1]</span></span><br><span class=\"line\">strides[<span class=\"number\">2</span>]=strides[<span class=\"number\">3</span>]*sizes[<span class=\"number\">3</span>]; <span class=\"comment\">// ChannelsLast  H  W*C</span></span><br><span class=\"line\">strides[<span class=\"number\">0</span>]=strides[<span class=\"number\">2</span>]*sizes[<span class=\"number\">2</span>]; <span class=\"comment\">// ChannelsLast  N  H*W*C</span></span><br></pre></td></tr></table></figure>\n\n<p> strides  sizes  NCHW<br> preserve_format, contiguous_format, channels_last  <strong> torch </strong></p>\n<h3 id=\"initializeQScheme\"><a href=\"#initializeQScheme\" class=\"headerlink\" title=\"initializeQScheme\"></a>initializeQScheme</h3><p> inference  <a href=\"https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor\" target=\"_blank\" rel=\"noopener\">Introducing-Quantized-Tensor</a> 5  <strong> torch </strong></p>\n<h3 id=\"initializeDtypes\"><a href=\"#initializeDtypes\" class=\"headerlink\" title=\"initializeDtypes\"></a>initializeDtypes</h3><p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,</span></span><br><span class=\"line\">at:ScalarType all_scalar_type[] = &#123;</span><br><span class=\"line\">    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS  complex  quantization </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::ScalarType::Byte,</span><br><span class=\"line\">at::ScalarType::Char,</span><br><span class=\"line\">at::ScalarType::Short,</span><br><span class=\"line\">at::ScalarType::Int,</span><br><span class=\"line\">at::ScalarType::Long,</span><br><span class=\"line\">at::ScalarType::Half,</span><br><span class=\"line\">at::ScalarType::Float,</span><br><span class=\"line\">at::ScalarType::Double,</span><br><span class=\"line\">at::ScalarType::ComplexHalf,</span><br><span class=\"line\">at::ScalarType::ComplexFloat,</span><br><span class=\"line\">at::ScalarType::ComplexDouble,</span><br><span class=\"line\">at::ScalarType::Bool,</span><br><span class=\"line\">at::ScalarType::QInt8,</span><br><span class=\"line\">at::ScalarType::QUInt8,</span><br><span class=\"line\">at::ScalarType::QInt32</span><br><span class=\"line\">at::ScalarType::BFloat16</span><br></pre></td></tr></table></figure>\n\n<p> <code>&quot;&quot;</code> at::ScalarType  at::ScalarType </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">std</span>::tie(primary_name, legacy_name) = getDtypeName(scalarType);</span><br><span class=\"line\">PyObject *dtype = THPDtype_New(scalarType, primary_name);</span><br><span class=\"line\">torch::registerDtypeObject((THPDtype*)dtype, scalarType);</span><br></pre></td></tr></table></figure>\n\n<p>THPDtypeType/THPDtype <strong> torch </strong> torch </p>\n<h3 id=\"initialize-python-bindings\"><a href=\"#initialize-python-bindings\" class=\"headerlink\" title=\"initialize_python_bindings\"></a>initialize_python_bindings</h3><p> python </p>\n<h4 id=\"initialize-aten-types\"><a href=\"#initialize-aten-types\" class=\"headerlink\" title=\"initialize_aten_types\"></a>initialize_aten_types</h4><p> all_declared_types Backend  <code>CPU, CUDA, SparseCPU, SparseCUDA</code> ScalarType  Complex  Quantization </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Byte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16</span><br></pre></td></tr></table></figure>\n\n<p> Backend  ScalarType  (SparseCUDA|SparseCPU,Bool)  4*10-2=38  38  PyTensorType  PyTensorType </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">PyTensorType</span> &#123;</span></span><br><span class=\"line\">    PyTypeObject py_type;   <span class=\"comment\">// python </span></span><br><span class=\"line\">    THPDtype* dtype;        <span class=\"comment\">//  initializeDtypes </span></span><br><span class=\"line\">    THPLayout* layout;      <span class=\"comment\">//  initializeLayouts </span></span><br><span class=\"line\">    <span class=\"keyword\">bool</span> is_cuda;           <span class=\"comment\">//  cuda  cpu</span></span><br><span class=\"line\">    <span class=\"keyword\">char</span> name[<span class=\"number\">64</span>];          <span class=\"comment\">// tensor </span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> backend;            <span class=\"comment\">// CPU, CUDA, SparseCPU, SparseCUDA </span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> scalar_type;        <span class=\"comment\">// Byte </span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> 38  PyTensorType Python  Tensor FloatTensor  PyTensorType </p>\n<ul>\n<li><p>layout backend initializeLayouts  Backend  Layout </p>\n</li>\n<li><p>is_cuda backend = CUDA|SparseCUD  true</p>\n</li>\n<li><p>name <code>[].[ScalarType]Tensor</code>  \n</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPU -&gt; torch</span><br><span class=\"line\">CUDA -&gt; torch.cuda</span><br><span class=\"line\">SparseCPU -&gt; torch.sparse</span><br><span class=\"line\">SparseCUDA -&gt; torch.cuda.sparse</span><br></pre></td></tr></table></figure>\n\n<p>ScalarType  ScalarType  Byte -&gt; Byte, Float -&gt; Float  (CPU, Float)  PyTensorType  torch.FloatTensor(SparseCUDA, Double)  PyTensorType  torch.cuda.sparse.DoubleTensor</p>\n</li>\n</ul>\n<p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (backend==Backend::CPU &amp;&amp; scalar_type==at::kFloat) &#123;</span><br><span class=\"line\">    set_default_tensor_type(&amp;tensor_type);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  torch.FloatTensor Backend  CPU CUDA  torch.cuda.FloatTensor</p>\n<p>initialize_aten_types  38  PyTensorType  tensor_types  vector </p>\n<h4 id=\"py-initialize-metaclass-metaclass\"><a href=\"#py-initialize-metaclass-metaclass\" class=\"headerlink\" title=\"py_initialize_metaclass(metaclass)\"></a>py_initialize_metaclass(metaclass)</h4><p> python  PyTypeObject python  torch.tensortype tensor tensor  torch.FloatTensor </p>\n<ul>\n<li>dtype<br>   initializeDtypes  THPDtype </li>\n<li>layout<br>   initializeLayouts  THPLayout </li>\n<li>is_cuda<br>   cuda</li>\n<li>is_sparse<br>  </li>\n</ul>\n<p></p>\n<ul>\n<li><code>__instancecheck__</code>  Tensor  tensor  type_id  scalar_type </li>\n</ul>\n<p>PyTensorType  python  Tensor  python  metaclass</p>\n<p> PyTypeObject  metaclass tensor </p>\n<h4 id=\"get-tensor-dict\"><a href=\"#get-tensor-dict\" class=\"headerlink\" title=\"get_tensor_dict\"></a>get_tensor_dict</h4><p> torch.Tensor  _C._TensorBase </p>\n<h4 id=\"py-initialize-tensor-type\"><a href=\"#py-initialize-tensor-type\" class=\"headerlink\" title=\"py_initialize_tensor_type\"></a>py_initialize_tensor_type</h4><p> 38  PyTensorType  py_type py_type  PyTypeObject python  metaclass torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dir(torch.Tensor)</span><br><span class=\"line\">dir(torch.FloatTensor)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<h4 id=\"py-bind-tensor-types\"><a href=\"#py-bind-tensor-types\" class=\"headerlink\" title=\"py_bind_tensor_types\"></a>py_bind_tensor_types</h4><p> 38  PyTensorType   torch  PyTensorType   torch.FloatTensor PyTensorType  FloatTensor  python  torch torch.cuda.sparse.DoubleTensor  PyTensorType  DoubleTensor  python  torch.cuda.sparse  <code>.</code> </p>\n<p> torch.FloatTensor, torch.IntTensor  torch.Tensor </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,dtype=torch.int)</span><br><span class=\"line\">isinstance(a, torch.IntTensor)  <span class=\"comment\"># True</span></span><br><span class=\"line\">isinstance(a, torch.Tensor)     <span class=\"comment\"># True</span></span><br><span class=\"line\">issubclass(torch.IntTensor, torch.Tensor)   <span class=\"comment\"># False</span></span><br><span class=\"line\">issubclass(torch.Tensor, torch.IntTensor)   <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure>\n\n<p> <a href=\"2019/06/18/Pytorch-3\">Pytorch-3</a>  torch.empty  THPVariable_Wrap  c++ Variable  python  torch.Tensor  torch.IntTensor  THPVariable_Wrap  torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; type(torch.IntTensor([1,2]))</span><br><span class=\"line\">&lt;class &apos;torch.Tensor&apos;&gt;</span><br></pre></td></tr></table></figure>\n\n<p> torch.Tensor  torch.IntTensor torch.IntTensor  38  Tensor  torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; torch.IntTensor.__bases__</span><br><span class=\"line\">(&lt;class &apos;object&apos;&gt;)</span><br><span class=\"line\">&gt;&gt;&gt; torch.Tensor.__bases__</span><br><span class=\"line\">(&lt;class &apos;torch._C._TensorBases&apos;&gt;)</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  torch.Tensor <code>isinstance(a, torch.IntTensor)</code>  True  <code>isinstance</code>  <code>__instancecheck__</code>  metaclass  c++  Tensor_instancecheck</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject *<span class=\"title\">Tensor_instancecheck</span><span class=\"params\">(PyTensorType *self, PyObject * arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(THPVariable_Check(arg)) &#123;    <span class=\"comment\">//  THPVariable </span></span><br><span class=\"line\">            <span class=\"keyword\">auto</span>&amp; var = ((THPVariable*)arg)-&gt;cdata; <span class=\"comment\">//  Variable </span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (var.type_id() == self-&gt;get_type_id() &amp;&amp;</span><br><span class=\"line\">                var.scalar_type() == <span class=\"keyword\">static_cast</span>&lt;ScalarType&gt;(self-&gt;scalar_type)) &#123;</span><br><span class=\"line\">                Py_RETURN_TRUE;     <span class=\"comment\">//  type_id  ScalarType  True</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Py_RETURN_FALSE;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span>(python_error &amp; e)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> <code>isinstance(a, torch.IntTensor)=True</code></p>\n<p><strong> <a href>PyTorch-2</a>  PyTorch  Tensor  torch.Tensor <code>torch._C.Tensor</code> Tensor  <code>torch._C.Tensor</code> </strong></p>\n<p> THPxxxStorage_postInit  <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> THPxxxStorage_init</p>\n<ol>\n<li><p></p>\n <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) <span class=\"comment\">//torch/csrc/Storage.h</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>;   <span class=\"comment\">// torch/csrc/generic/Storage.h</span></span><br></pre></td></tr></table></figure>\n\n<p> THPStorage_(NAME)  THPxxxStorage_init  Real  ScalarTypeNAME  init THPxxxStorage_init(PyObject *module);</p>\n</li>\n<li><p>torch/csrc/generic/Storage.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyObject *THPStorageClass = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//  torch  RealStorage  Real  Float, Bool, Double  ScalarType</span></span><br><span class=\"line\">    THPStorageClass = PyObject_GetAttrString(<span class=\"keyword\">module</span>, (<span class=\"keyword\">char</span>*)TH_CONCAT_STRING_2(Real, Storage));</span><br><span class=\"line\">    at::Backend backend = at::Backend::CPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THC_GENERIC_FILE</span></span><br><span class=\"line\">    backend = at::Backend::CUDA;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THQUANTIZED</span></span><br><span class=\"line\">    backend = at::Backend::QuantizedCPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> <code>torch/__init__.py</code>  FloatStorage </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FloatStorage</span><span class=\"params\">(_C.FloatStorageBase, _StorageBase)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>torch._C.FloatStorageBase  THPxxxStorage_init  torch._C THPxxxStorage_postInit  torch  RealStorage  RealStorage  (Backend, ScalarType)  (Backend, ScalarType)  RealStorage </p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h2><p>torch  TensorFloatTensorDoubleTensorHalfTensorByteTensor  Tensor  <code>torch/__init__.py</code>  <code>_C._initExtension(manager_path())</code> manager_path  torch_shm_manager shm Domain Socket_initExtension  torch/csrc/Module.cpp  _C  _C  c++  THPModule_initExtension</p>\n<h3 id=\"initializeLayouts\"><a href=\"#initializeLayouts\" class=\"headerlink\" title=\"initializeLayouts\"></a>initializeLayouts</h3><p> c10/core/Layout.h </p>\n<ol>\n<li>Strided</li>\n<li>Sparse</li>\n<li>Mkldnn Intel  Mkldnn  CPU  Mkldnn </li>\n</ol>\n<p> Strided  THPLayout_New  THPLayoutType/THPLayout  layout  Stridedname  torch.strided <strong> torch </strong></p>\n<ul>\n<li>CPU, CUDA, MSNPU, XLA, QuantizedCPU -&gt; strided_layout</li>\n<li>SparseCPU, SparseCUDA -&gt; sparse_coo_layout</li>\n<li>MkldnnCPU -&gt; mkldnn_layout</li>\n</ul>\n<p> Backend  Layout  Backend  Layout</p>\n<h3 id=\"initializeMemoryFormats\"><a href=\"#initializeMemoryFormats\" class=\"headerlink\" title=\"initializeMemoryFormats\"></a>initializeMemoryFormats</h3><p> Tensor Preserve, Contiguous  ChannelsLast ChannelsLast  NHWC NCHW  sizes ChannelsLast  strides </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">strides[<span class=\"number\">1</span>]=<span class=\"number\">1</span>;           <span class=\"comment\">// ChannelsLast  C  1</span></span><br><span class=\"line\">strides[<span class=\"number\">3</span>]=sizes[<span class=\"number\">1</span>];    <span class=\"comment\">// ChannelsLast  W  C  sizes[1]</span></span><br><span class=\"line\">strides[<span class=\"number\">2</span>]=strides[<span class=\"number\">3</span>]*sizes[<span class=\"number\">3</span>]; <span class=\"comment\">// ChannelsLast  H  W*C</span></span><br><span class=\"line\">strides[<span class=\"number\">0</span>]=strides[<span class=\"number\">2</span>]*sizes[<span class=\"number\">2</span>]; <span class=\"comment\">// ChannelsLast  N  H*W*C</span></span><br></pre></td></tr></table></figure>\n\n<p> strides  sizes  NCHW<br> preserve_format, contiguous_format, channels_last  <strong> torch </strong></p>\n<h3 id=\"initializeQScheme\"><a href=\"#initializeQScheme\" class=\"headerlink\" title=\"initializeQScheme\"></a>initializeQScheme</h3><p> inference  <a href=\"https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor\" target=\"_blank\" rel=\"noopener\">Introducing-Quantized-Tensor</a> 5  <strong> torch </strong></p>\n<h3 id=\"initializeDtypes\"><a href=\"#initializeDtypes\" class=\"headerlink\" title=\"initializeDtypes\"></a>initializeDtypes</h3><p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> DEFINE_SCALAR_TYPE(_1, n) at::ScalarType::n,</span></span><br><span class=\"line\">at:ScalarType all_scalar_type[] = &#123;</span><br><span class=\"line\">    AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS  complex  quantization </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">at::ScalarType::Byte,</span><br><span class=\"line\">at::ScalarType::Char,</span><br><span class=\"line\">at::ScalarType::Short,</span><br><span class=\"line\">at::ScalarType::Int,</span><br><span class=\"line\">at::ScalarType::Long,</span><br><span class=\"line\">at::ScalarType::Half,</span><br><span class=\"line\">at::ScalarType::Float,</span><br><span class=\"line\">at::ScalarType::Double,</span><br><span class=\"line\">at::ScalarType::ComplexHalf,</span><br><span class=\"line\">at::ScalarType::ComplexFloat,</span><br><span class=\"line\">at::ScalarType::ComplexDouble,</span><br><span class=\"line\">at::ScalarType::Bool,</span><br><span class=\"line\">at::ScalarType::QInt8,</span><br><span class=\"line\">at::ScalarType::QUInt8,</span><br><span class=\"line\">at::ScalarType::QInt32</span><br><span class=\"line\">at::ScalarType::BFloat16</span><br></pre></td></tr></table></figure>\n\n<p> <code>&quot;&quot;</code> at::ScalarType  at::ScalarType </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">std</span>::tie(primary_name, legacy_name) = getDtypeName(scalarType);</span><br><span class=\"line\">PyObject *dtype = THPDtype_New(scalarType, primary_name);</span><br><span class=\"line\">torch::registerDtypeObject((THPDtype*)dtype, scalarType);</span><br></pre></td></tr></table></figure>\n\n<p>THPDtypeType/THPDtype <strong> torch </strong> torch </p>\n<h3 id=\"initialize-python-bindings\"><a href=\"#initialize-python-bindings\" class=\"headerlink\" title=\"initialize_python_bindings\"></a>initialize_python_bindings</h3><p> python </p>\n<h4 id=\"initialize-aten-types\"><a href=\"#initialize-aten-types\" class=\"headerlink\" title=\"initialize_aten_types\"></a>initialize_aten_types</h4><p> all_declared_types Backend  <code>CPU, CUDA, SparseCPU, SparseCUDA</code> ScalarType  Complex  Quantization </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Byte, Char, Double, Float, Int, Long, Short, Half, Bool, BFloat16</span><br></pre></td></tr></table></figure>\n\n<p> Backend  ScalarType  (SparseCUDA|SparseCPU,Bool)  4*10-2=38  38  PyTensorType  PyTensorType </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">PyTensorType</span> &#123;</span></span><br><span class=\"line\">    PyTypeObject py_type;   <span class=\"comment\">// python </span></span><br><span class=\"line\">    THPDtype* dtype;        <span class=\"comment\">//  initializeDtypes </span></span><br><span class=\"line\">    THPLayout* layout;      <span class=\"comment\">//  initializeLayouts </span></span><br><span class=\"line\">    <span class=\"keyword\">bool</span> is_cuda;           <span class=\"comment\">//  cuda  cpu</span></span><br><span class=\"line\">    <span class=\"keyword\">char</span> name[<span class=\"number\">64</span>];          <span class=\"comment\">// tensor </span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> backend;            <span class=\"comment\">// CPU, CUDA, SparseCPU, SparseCUDA </span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> scalar_type;        <span class=\"comment\">// Byte </span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> 38  PyTensorType Python  Tensor FloatTensor  PyTensorType </p>\n<ul>\n<li><p>layout backend initializeLayouts  Backend  Layout </p>\n</li>\n<li><p>is_cuda backend = CUDA|SparseCUD  true</p>\n</li>\n<li><p>name <code>[].[ScalarType]Tensor</code>  \n</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPU -&gt; torch</span><br><span class=\"line\">CUDA -&gt; torch.cuda</span><br><span class=\"line\">SparseCPU -&gt; torch.sparse</span><br><span class=\"line\">SparseCUDA -&gt; torch.cuda.sparse</span><br></pre></td></tr></table></figure>\n\n<p>ScalarType  ScalarType  Byte -&gt; Byte, Float -&gt; Float  (CPU, Float)  PyTensorType  torch.FloatTensor(SparseCUDA, Double)  PyTensorType  torch.cuda.sparse.DoubleTensor</p>\n</li>\n</ul>\n<p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (backend==Backend::CPU &amp;&amp; scalar_type==at::kFloat) &#123;</span><br><span class=\"line\">    set_default_tensor_type(&amp;tensor_type);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  torch.FloatTensor Backend  CPU CUDA  torch.cuda.FloatTensor</p>\n<p>initialize_aten_types  38  PyTensorType  tensor_types  vector </p>\n<h4 id=\"py-initialize-metaclass-metaclass\"><a href=\"#py-initialize-metaclass-metaclass\" class=\"headerlink\" title=\"py_initialize_metaclass(metaclass)\"></a>py_initialize_metaclass(metaclass)</h4><p> python  PyTypeObject python  torch.tensortype tensor tensor  torch.FloatTensor </p>\n<ul>\n<li>dtype<br>   initializeDtypes  THPDtype </li>\n<li>layout<br>   initializeLayouts  THPLayout </li>\n<li>is_cuda<br>   cuda</li>\n<li>is_sparse<br>  </li>\n</ul>\n<p></p>\n<ul>\n<li><code>__instancecheck__</code>  Tensor  tensor  type_id  scalar_type </li>\n</ul>\n<p>PyTensorType  python  Tensor  python  metaclass</p>\n<p> PyTypeObject  metaclass tensor </p>\n<h4 id=\"get-tensor-dict\"><a href=\"#get-tensor-dict\" class=\"headerlink\" title=\"get_tensor_dict\"></a>get_tensor_dict</h4><p> torch.Tensor  _C._TensorBase </p>\n<h4 id=\"py-initialize-tensor-type\"><a href=\"#py-initialize-tensor-type\" class=\"headerlink\" title=\"py_initialize_tensor_type\"></a>py_initialize_tensor_type</h4><p> 38  PyTensorType  py_type py_type  PyTypeObject python  metaclass torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dir(torch.Tensor)</span><br><span class=\"line\">dir(torch.FloatTensor)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<h4 id=\"py-bind-tensor-types\"><a href=\"#py-bind-tensor-types\" class=\"headerlink\" title=\"py_bind_tensor_types\"></a>py_bind_tensor_types</h4><p> 38  PyTensorType   torch  PyTensorType   torch.FloatTensor PyTensorType  FloatTensor  python  torch torch.cuda.sparse.DoubleTensor  PyTensorType  DoubleTensor  python  torch.cuda.sparse  <code>.</code> </p>\n<p> torch.FloatTensor, torch.IntTensor  torch.Tensor </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,dtype=torch.int)</span><br><span class=\"line\">isinstance(a, torch.IntTensor)  <span class=\"comment\"># True</span></span><br><span class=\"line\">isinstance(a, torch.Tensor)     <span class=\"comment\"># True</span></span><br><span class=\"line\">issubclass(torch.IntTensor, torch.Tensor)   <span class=\"comment\"># False</span></span><br><span class=\"line\">issubclass(torch.Tensor, torch.IntTensor)   <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure>\n\n<p> <a href=\"2019/06/18/Pytorch-3\">Pytorch-3</a>  torch.empty  THPVariable_Wrap  c++ Variable  python  torch.Tensor  torch.IntTensor  THPVariable_Wrap  torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; type(torch.IntTensor([1,2]))</span><br><span class=\"line\">&lt;class &apos;torch.Tensor&apos;&gt;</span><br></pre></td></tr></table></figure>\n\n<p> torch.Tensor  torch.IntTensor torch.IntTensor  38  Tensor  torch.Tensor </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; torch.IntTensor.__bases__</span><br><span class=\"line\">(&lt;class &apos;object&apos;&gt;)</span><br><span class=\"line\">&gt;&gt;&gt; torch.Tensor.__bases__</span><br><span class=\"line\">(&lt;class &apos;torch._C._TensorBases&apos;&gt;)</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  torch.Tensor <code>isinstance(a, torch.IntTensor)</code>  True  <code>isinstance</code>  <code>__instancecheck__</code>  metaclass  c++  Tensor_instancecheck</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject *<span class=\"title\">Tensor_instancecheck</span><span class=\"params\">(PyTensorType *self, PyObject * arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(THPVariable_Check(arg)) &#123;    <span class=\"comment\">//  THPVariable </span></span><br><span class=\"line\">            <span class=\"keyword\">auto</span>&amp; var = ((THPVariable*)arg)-&gt;cdata; <span class=\"comment\">//  Variable </span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (var.type_id() == self-&gt;get_type_id() &amp;&amp;</span><br><span class=\"line\">                var.scalar_type() == <span class=\"keyword\">static_cast</span>&lt;ScalarType&gt;(self-&gt;scalar_type)) &#123;</span><br><span class=\"line\">                Py_RETURN_TRUE;     <span class=\"comment\">//  type_id  ScalarType  True</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Py_RETURN_FALSE;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span>(python_error &amp; e)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> <code>isinstance(a, torch.IntTensor)=True</code></p>\n<p><strong> <a href>PyTorch-2</a>  PyTorch  Tensor  torch.Tensor <code>torch._C.Tensor</code> Tensor  <code>torch._C.Tensor</code> </strong></p>\n<p> THPxxxStorage_postInit  <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a> THPxxxStorage_init</p>\n<ol>\n<li><p></p>\n <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> THPStorage_(NAME) TH_CONCAT_4(THP,Real,Storage_,NAME) <span class=\"comment\">//torch/csrc/Storage.h</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>;   <span class=\"comment\">// torch/csrc/generic/Storage.h</span></span><br></pre></td></tr></table></figure>\n\n<p> THPStorage_(NAME)  THPxxxStorage_init  Real  ScalarTypeNAME  init THPxxxStorage_init(PyObject *module);</p>\n</li>\n<li><p>torch/csrc/generic/Storage.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyObject *THPStorageClass = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">THPStorage_</span><span class=\"params\">(postInit)</span><span class=\"params\">(PyObject *<span class=\"keyword\">module</span>)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//  torch  RealStorage  Real  Float, Bool, Double  ScalarType</span></span><br><span class=\"line\">    THPStorageClass = PyObject_GetAttrString(<span class=\"keyword\">module</span>, (<span class=\"keyword\">char</span>*)TH_CONCAT_STRING_2(Real, Storage));</span><br><span class=\"line\">    at::Backend backend = at::Backend::CPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THC_GENERIC_FILE</span></span><br><span class=\"line\">    backend = at::Backend::CUDA;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> THQUANTIZED</span></span><br><span class=\"line\">    backend = at::Backend::QuantizedCPU;</span><br><span class=\"line\">    <span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, TH_CONCAT_2(at::k, Real));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> <code>torch/__init__.py</code>  FloatStorage </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FloatStorage</span><span class=\"params\">(_C.FloatStorageBase, _StorageBase)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>torch._C.FloatStorageBase  THPxxxStorage_init  torch._C THPxxxStorage_postInit  torch  RealStorage  RealStorage  (Backend, ScalarType)  (Backend, ScalarType)  RealStorage </p>\n</li>\n</ol>\n"},{"title":"libra-rcnn","date":"2019-07-03T12:07:44.000Z","mathjax":true,"_content":" [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n\n# Introduction\n one-stage  two-stage image  Region  region \n1.  region \n2. \n3. \n\n 1\n![](/images/libra-rcnn_fig1.png) <center>  a. b. c.  </center>\n\n\n\n## \n 1:3 region  OHEM [1] OHEM  OHEM Focal loss  Focal loss  one-stage  two-stage  first stage  1:3 Focal loss \n\n## \nFPN  PANet  top-down  feature pyramid \n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n a'  b,c \n\n## \n\n\n Libra R-CNN R-CNNLibra R-CNN \n1. IoU  gt box  IoU \n2.  feature pyramid ____ \n3.  L1 loss 1) 2)  3)  \n\n# Methodology\nLibra R-CNN  2\n![](/images/libra-rcnn_fig2.png)\n\n\n\n## IoU-balanced Sampling\n region  gt box  3  region  IoU \n![](/images/libra-rcnn_fig3.png)\n\n 3  60%  0.05  IoU 3  IoU  0.05  37% 30%  IoU  0.05 IoU  [0,0.05) \n\n IoU-balanced  IoU  IoU  M  N \n$$p=\\frac N M$$\n\n IoU  K  N/K  IoU-balanced sampling  k \n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n$M_k$  k  K=3\n\nIoU-balanced sampling  3 gt box \n\n### SOURCE CODE\n IoU balanced sampling \n1.  proposals  IoU `max_overlaps`\n2.  proposals  IoU `max_iou=max_overlaps.max()`\n3.  `floor_thr` `(floor_thr, max_iou)`  proposals  IoU balanced sampling\n4. bin K N N/K IoU  `(max_iou-floor_thr)/K`\n5.  k  IoU  `[sk,ek)`\n6.  k  proposals  index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7.  k  index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set  floor_thr<iou<0.5  proposals  index\n   ```\n8.  7  k  index N/K \n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling \n1.  proposals  gt  index `gt_inds`\n2.  1 `unique_gt_inds=gt_inds.unique()` gt  index\n   \n    gt  gt  proposals \n3.  gt  `num_gts=len(unique_gt_indx)` N  gt  `num_per_gt=N/num_gts` \n4.  i  gt `num_per_gt` \n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n\n\n## Balanced Feature Pyramid\n ____  multi-level features 4\n![](/images/libra-rcnn_fig4.png)\n\n### \n l  $C_l$ L $l_{min}, l_{max}$ 4 $\\{C_2,C_3,C_4,C_5\\}$ resize  $C_4$ resize \n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n rescale  4  Identity\n\n### \n non-local [2]\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$  FPN \n\n## Balanced L1 Loss\n Fast R-CNN \n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n target  p  ut<sup>u</sup> v  target$\\lambda$  1.0  outliers 1.0  inliers\n\n target  $\\lambda$  outliers outliers Inliers  outliers inliers  30%  L1  smooth L1  $L_b$ inliers  outliers 1.0  outliers  5(a) ,\n![](/images/libra-rcnn_fig5.png)<center> regression error  |x|</center>\n\n L1  inliers  L1 \n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\nw x  $t_i^u - v_i$ smooth L1  smooth L1 \n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n smooth L1 \n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n |x| regression error inliers  |x|<1   |x|<1  smooth L1  |x|<1  $\\nabla_{|x|} L = |x|$  (0,0)  (1,1)  y=x (0,0) target  $|x| \\ge 1$  (1,1)  ____ \n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n $\\alpha$  inliers $\\gamma$  outliers $\\gamma$  5(a) b  |x|=1 \n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n |x|=1 \n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n C  $C=\\frac \\alpha b \\ln(b+1) -\\alpha$\n$$\\alpha \\ln (b+1)=\\gamma$$\n\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n 5(b) \n\n# Experiments\n\n\n# Conclusion\n Libra R-CNN\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","source":"_posts/libra-rcnn.md","raw":"---\ntitle: libra-rcnn\ndate: 2019-07-03 20:07:44\ntags: object detection\nmathjax: true\n---\n [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n\n# Introduction\n one-stage  two-stage image  Region  region \n1.  region \n2. \n3. \n\n 1\n![](/images/libra-rcnn_fig1.png) <center>  a. b. c.  </center>\n\n\n\n## \n 1:3 region  OHEM [1] OHEM  OHEM Focal loss  Focal loss  one-stage  two-stage  first stage  1:3 Focal loss \n\n## \nFPN  PANet  top-down  feature pyramid \n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n a'  b,c \n\n## \n\n\n Libra R-CNN R-CNNLibra R-CNN \n1. IoU  gt box  IoU \n2.  feature pyramid ____ \n3.  L1 loss 1) 2)  3)  \n\n# Methodology\nLibra R-CNN  2\n![](/images/libra-rcnn_fig2.png)\n\n\n\n## IoU-balanced Sampling\n region  gt box  3  region  IoU \n![](/images/libra-rcnn_fig3.png)\n\n 3  60%  0.05  IoU 3  IoU  0.05  37% 30%  IoU  0.05 IoU  [0,0.05) \n\n IoU-balanced  IoU  IoU  M  N \n$$p=\\frac N M$$\n\n IoU  K  N/K  IoU-balanced sampling  k \n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n$M_k$  k  K=3\n\nIoU-balanced sampling  3 gt box \n\n### SOURCE CODE\n IoU balanced sampling \n1.  proposals  IoU `max_overlaps`\n2.  proposals  IoU `max_iou=max_overlaps.max()`\n3.  `floor_thr` `(floor_thr, max_iou)`  proposals  IoU balanced sampling\n4. bin K N N/K IoU  `(max_iou-floor_thr)/K`\n5.  k  IoU  `[sk,ek)`\n6.  k  proposals  index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7.  k  index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set  floor_thr<iou<0.5  proposals  index\n   ```\n8.  7  k  index N/K \n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling \n1.  proposals  gt  index `gt_inds`\n2.  1 `unique_gt_inds=gt_inds.unique()` gt  index\n   \n    gt  gt  proposals \n3.  gt  `num_gts=len(unique_gt_indx)` N  gt  `num_per_gt=N/num_gts` \n4.  i  gt `num_per_gt` \n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n\n\n## Balanced Feature Pyramid\n ____  multi-level features 4\n![](/images/libra-rcnn_fig4.png)\n\n### \n l  $C_l$ L $l_{min}, l_{max}$ 4 $\\{C_2,C_3,C_4,C_5\\}$ resize  $C_4$ resize \n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n rescale  4  Identity\n\n### \n non-local [2]\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$  FPN \n\n## Balanced L1 Loss\n Fast R-CNN \n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n target  p  ut<sup>u</sup> v  target$\\lambda$  1.0  outliers 1.0  inliers\n\n target  $\\lambda$  outliers outliers Inliers  outliers inliers  30%  L1  smooth L1  $L_b$ inliers  outliers 1.0  outliers  5(a) ,\n![](/images/libra-rcnn_fig5.png)<center> regression error  |x|</center>\n\n L1  inliers  L1 \n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\nw x  $t_i^u - v_i$ smooth L1  smooth L1 \n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n smooth L1 \n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n |x| regression error inliers  |x|<1   |x|<1  smooth L1  |x|<1  $\\nabla_{|x|} L = |x|$  (0,0)  (1,1)  y=x (0,0) target  $|x| \\ge 1$  (1,1)  ____ \n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n $\\alpha$  inliers $\\gamma$  outliers $\\gamma$  5(a) b  |x|=1 \n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n |x|=1 \n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n C  $C=\\frac \\alpha b \\ln(b+1) -\\alpha$\n$$\\alpha \\ln (b+1)=\\gamma$$\n\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n 5(b) \n\n# Experiments\n\n\n# Conclusion\n Libra R-CNN\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","slug":"libra-rcnn","published":1,"updated":"2019-07-05T12:35:22.631Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379c7001gdgvcixoiqipn","content":"<p> <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> one-stage  two-stage image  Region  region </p>\n<ol>\n<li> region </li>\n<li></li>\n<li></li>\n</ol>\n<p> 1<br><img src=\"/images/libra-rcnn_fig1.png\" alt> <center>  a. b. c.  </center></p>\n<p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 1:3 region  OHEM [1] OHEM  OHEM Focal loss  Focal loss  one-stage  two-stage  first stage  1:3 Focal loss </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>FPN  PANet  top-down  feature pyramid <br><img src=\"/images/libra-rcnn_figa.png\" alt><center>FPN</center></p>\n<p> a  b,c </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p> Libra R-CNN R-CNNLibra R-CNN </p>\n<ol>\n<li>IoU  gt box  IoU </li>\n<li> feature pyramid <strong></strong> </li>\n<li> L1 loss 1) 2)  3)  </li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN  2<br><img src=\"/images/libra-rcnn_fig2.png\" alt></p>\n<p></p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p> region  gt box  3  region  IoU <br><img src=\"/images/libra-rcnn_fig3.png\" alt></p>\n<p> 3  60%  0.05  IoU 3  IoU  0.05  37% 30%  IoU  0.05 IoU  [0,0.05) </p>\n<p> IoU-balanced  IoU  IoU  M  N <br>$$p=\\frac N M$$</p>\n<p> IoU  K  N/K  IoU-balanced sampling  k <br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>$M_k$  k  K=3</p>\n<p>IoU-balanced sampling  3 gt box </p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p> IoU balanced sampling </p>\n<ol>\n<li><p> proposals  IoU <code>max_overlaps</code></p>\n</li>\n<li><p> proposals  IoU <code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p> <code>floor_thr</code> <code>(floor_thr, max_iou)</code>  proposals  IoU balanced sampling</p>\n</li>\n<li><p>bin K N N/K IoU  <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p> k  IoU  <code>[sk,ek)</code></p>\n</li>\n<li><p> k  proposals  index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> k  index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set  floor_thr&lt;iou&lt;0.5  proposals  index</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p> 7  k  index N/K </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>IoU balanced sampling </p>\n<ol>\n<li><p> proposals  gt  index <code>gt_inds</code></p>\n</li>\n<li><p> 1 <code>unique_gt_inds=gt_inds.unique()</code> gt  index</p>\n<p> gt  gt  proposals </p>\n</li>\n<li><p> gt  <code>num_gts=len(unique_gt_indx)</code> N  gt  <code>num_per_gt=N/num_gts</code> </p>\n</li>\n<li><p> i  gt <code>num_per_gt</code> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p></p>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p> <strong></strong>  multi-level features 4<br><img src=\"/images/libra-rcnn_fig4.png\" alt></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> l  $C_l$ L $l_{min}, l_{max}$ 4 ${C_2,C_3,C_4,C_5}$ resize  $C_4$ resize <br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p> rescale  4  Identity</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> non-local [2]</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$  FPN </p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p> Fast R-CNN <br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br> target  p  ut<sup>u</sup> v  target$\\lambda$  1.0  outliers 1.0  inliers</p>\n<p> target  $\\lambda$  outliers outliers Inliers  outliers inliers  30%  L1  smooth L1  $L_b$ inliers  outliers 1.0  outliers  5(a) ,<br><img src=\"/images/libra-rcnn_fig5.png\" alt><center> regression error  |x|</center></p>\n<p> L1  inliers  L1 <br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br><br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>w x  $t_i^u - v_i$ smooth L1  smooth L1 <br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br><br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br> smooth L1 <br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br> |x| regression error inliers  |x|&lt;1   |x|&lt;1  smooth L1  |x|&lt;1  $\\nabla_{|x|} L = |x|$  (0,0)  (1,1)  y=x (0,0) target  $|x| \\ge 1$  (1,1)  <strong></strong> <br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br> $\\alpha$  inliers $\\gamma$  outliers $\\gamma$  5(a) b  |x|=1 <br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br> |x|=1 <br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br> C  $C=\\frac \\alpha b \\ln(b+1) -\\alpha$<br>$$\\alpha \\ln (b+1)=\\gamma$$<br><br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p> 5(b) </p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p> Libra R-CNN</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> one-stage  two-stage image  Region  region </p>\n<ol>\n<li> region </li>\n<li></li>\n<li></li>\n</ol>\n<p> 1<br><img src=\"/images/libra-rcnn_fig1.png\" alt> <center>  a. b. c.  </center></p>\n<p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 1:3 region  OHEM [1] OHEM  OHEM Focal loss  Focal loss  one-stage  two-stage  first stage  1:3 Focal loss </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>FPN  PANet  top-down  feature pyramid <br><img src=\"/images/libra-rcnn_figa.png\" alt><center>FPN</center></p>\n<p> a  b,c </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p> Libra R-CNN R-CNNLibra R-CNN </p>\n<ol>\n<li>IoU  gt box  IoU </li>\n<li> feature pyramid <strong></strong> </li>\n<li> L1 loss 1) 2)  3)  </li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN  2<br><img src=\"/images/libra-rcnn_fig2.png\" alt></p>\n<p></p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p> region  gt box  3  region  IoU <br><img src=\"/images/libra-rcnn_fig3.png\" alt></p>\n<p> 3  60%  0.05  IoU 3  IoU  0.05  37% 30%  IoU  0.05 IoU  [0,0.05) </p>\n<p> IoU-balanced  IoU  IoU  M  N <br>$$p=\\frac N M$$</p>\n<p> IoU  K  N/K  IoU-balanced sampling  k <br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>$M_k$  k  K=3</p>\n<p>IoU-balanced sampling  3 gt box </p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p> IoU balanced sampling </p>\n<ol>\n<li><p> proposals  IoU <code>max_overlaps</code></p>\n</li>\n<li><p> proposals  IoU <code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p> <code>floor_thr</code> <code>(floor_thr, max_iou)</code>  proposals  IoU balanced sampling</p>\n</li>\n<li><p>bin K N N/K IoU  <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p> k  IoU  <code>[sk,ek)</code></p>\n</li>\n<li><p> k  proposals  index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> k  index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set  floor_thr&lt;iou&lt;0.5  proposals  index</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p> 7  k  index N/K </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>IoU balanced sampling </p>\n<ol>\n<li><p> proposals  gt  index <code>gt_inds</code></p>\n</li>\n<li><p> 1 <code>unique_gt_inds=gt_inds.unique()</code> gt  index</p>\n<p> gt  gt  proposals </p>\n</li>\n<li><p> gt  <code>num_gts=len(unique_gt_indx)</code> N  gt  <code>num_per_gt=N/num_gts</code> </p>\n</li>\n<li><p> i  gt <code>num_per_gt</code> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p></p>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p> <strong></strong>  multi-level features 4<br><img src=\"/images/libra-rcnn_fig4.png\" alt></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> l  $C_l$ L $l_{min}, l_{max}$ 4 ${C_2,C_3,C_4,C_5}$ resize  $C_4$ resize <br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p> rescale  4  Identity</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> non-local [2]</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$  FPN </p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p> Fast R-CNN <br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br> target  p  ut<sup>u</sup> v  target$\\lambda$  1.0  outliers 1.0  inliers</p>\n<p> target  $\\lambda$  outliers outliers Inliers  outliers inliers  30%  L1  smooth L1  $L_b$ inliers  outliers 1.0  outliers  5(a) ,<br><img src=\"/images/libra-rcnn_fig5.png\" alt><center> regression error  |x|</center></p>\n<p> L1  inliers  L1 <br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br><br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>w x  $t_i^u - v_i$ smooth L1  smooth L1 <br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br><br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br> smooth L1 <br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br> |x| regression error inliers  |x|&lt;1   |x|&lt;1  smooth L1  |x|&lt;1  $\\nabla_{|x|} L = |x|$  (0,0)  (1,1)  y=x (0,0) target  $|x| \\ge 1$  (1,1)  <strong></strong> <br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br> $\\alpha$  inliers $\\gamma$  outliers $\\gamma$  5(a) b  |x|=1 <br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br> |x|=1 <br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br> C  $C=\\frac \\alpha b \\ln(b+1) -\\alpha$<br>$$\\alpha \\ln (b+1)=\\gamma$$<br><br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p> 5(b) </p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p> Libra R-CNN</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>\n"},{"title":"mask-rcnn","date":"2019-07-08T09:39:57.000Z","mathjax":true,"_content":" [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n\n# Introduction\n Mask R-CNN   \n\nMask R-CNN  Faster R-CNN  RoI segmentation masks 1mask  RoI  pixel-to-pixel  segmentation mask mask  FCN mask   \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN  RoIPool RoI  image  feature map  1/16 mask  pixel-wise  RoIAlign  10%~50%  mask   \n\n binary mask  binary mask  RoI  FCN \n\n# Mask R-CNN\nFaster R-CNN Mask R-CNN  binary mask Mask R-CNN \n\n__Faster R-CNN:__  Faster R-CNN two-stage  stage  RPN proposals stage  Fast R-CNN RoIPooling  proposal  bbox \n\n__Mask R-CNN:__  Faster R-CNN  stage  RoI  binary mask\n\n RoI  $L=L_{cls}+L_{box}+L_{mask}$ $L_{cls}$  $L_{box}$  Fast/Faster R-CNN \n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n log lossproposal  gt  u$p_u$  proposal  u \n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n smooth L1 loss$t_u$  u  bbox v  gt box  proposal  target  \nmask  RoI  $Km^2$  pixel-wise sigmoid K  binary mask mask  $m \\times m$ K  $L_{mark}$  RoI  gt  k$L_{mark}$  k  binary mask  K-1  binary mask  $L_{mark}$ \n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n $f(\\cdot)$  sigmoid\n\n__Mask Representation:__  RoI  bbox  fc  mask  pixel-to-pixel  RoI  FCN  RoI  $m \\times m$  maskpixel-to-pixel  RoI  RoIAlign \n\n__RoIAlign:__ RoIPool  RoI  7x7 RoI  feature map RoI  bins bin  bin  bin \n\n RoI  x  image  Faster R-CNN  stride  16 RoI  $[x/16]$ $[\\cdot]$  bin  RoI  $(x_1,y_1,x_2,y_2)$ RPN  anchor  RoI RoI  7x7  bins RoI \n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI    bin \n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n (i,j)  bin\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n $0 \\le i<7, \\ 0\\le j<7$ bin \n\n RoI  pixel-to-pixel  mask  RoIAlign  RoIAlign  RoIPool 3\n![](/images/mask-rcnn_fig3.png)\n\n RoI  $x/16$ bin  4  bin max  average \n\n__Network Architecture:__ Mask R-CNN  1.  backbone2. network head bbox  mask \n\nBackbone  ResNet  ResNeXt 50 101Faster R-CNN  ResNet  4-th stage  conv  C4 ResNet  ResNet-50  backbone  ResNet-50-C4\n\n backbone  FPNFPN  top-down  feature pyramid ResNet-FPN  backbone Mask R-CNN \n\n Network head 4\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4  backbone  head  ResNet  5-th stage 9  conv  res5ResNet-FPN  backbone  backbone  res5 head   \n 4 res5  ResNet  5-th stage 7x7  RoI feature maps  conv  stride  1 ResNet  conv conv4_x 14x14 feature maps  conv  stride  2\n# Experiments\n\n\n# Appendix\n mask  4mask  $(R,K,m,m)$ bbox  box  $(R,4)$ R  box K mxm  mask  i  $0 \\le i < R$ box  $(x_1,y_1,x_2,y_2)$ k  mask map  $M_i^k$\n1.  i  box   \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2.  mask map resize  box   \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3.  mask map  mask  pixel-wise sigmoid  (0,1)   \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4.  binary mask  image  image  (W,H) mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","source":"_posts/mask-rcnn.md","raw":"---\ntitle: mask-rcnn\ndate: 2019-07-08 17:39:57\ntags: object detection\nmathjax: true\n---\n [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n\n# Introduction\n Mask R-CNN   \n\nMask R-CNN  Faster R-CNN  RoI segmentation masks 1mask  RoI  pixel-to-pixel  segmentation mask mask  FCN mask   \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN  RoIPool RoI  image  feature map  1/16 mask  pixel-wise  RoIAlign  10%~50%  mask   \n\n binary mask  binary mask  RoI  FCN \n\n# Mask R-CNN\nFaster R-CNN Mask R-CNN  binary mask Mask R-CNN \n\n__Faster R-CNN:__  Faster R-CNN two-stage  stage  RPN proposals stage  Fast R-CNN RoIPooling  proposal  bbox \n\n__Mask R-CNN:__  Faster R-CNN  stage  RoI  binary mask\n\n RoI  $L=L_{cls}+L_{box}+L_{mask}$ $L_{cls}$  $L_{box}$  Fast/Faster R-CNN \n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n log lossproposal  gt  u$p_u$  proposal  u \n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n smooth L1 loss$t_u$  u  bbox v  gt box  proposal  target  \nmask  RoI  $Km^2$  pixel-wise sigmoid K  binary mask mask  $m \\times m$ K  $L_{mark}$  RoI  gt  k$L_{mark}$  k  binary mask  K-1  binary mask  $L_{mark}$ \n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n $f(\\cdot)$  sigmoid\n\n__Mask Representation:__  RoI  bbox  fc  mask  pixel-to-pixel  RoI  FCN  RoI  $m \\times m$  maskpixel-to-pixel  RoI  RoIAlign \n\n__RoIAlign:__ RoIPool  RoI  7x7 RoI  feature map RoI  bins bin  bin  bin \n\n RoI  x  image  Faster R-CNN  stride  16 RoI  $[x/16]$ $[\\cdot]$  bin  RoI  $(x_1,y_1,x_2,y_2)$ RPN  anchor  RoI RoI  7x7  bins RoI \n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI    bin \n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n (i,j)  bin\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n $0 \\le i<7, \\ 0\\le j<7$ bin \n\n RoI  pixel-to-pixel  mask  RoIAlign  RoIAlign  RoIPool 3\n![](/images/mask-rcnn_fig3.png)\n\n RoI  $x/16$ bin  4  bin max  average \n\n__Network Architecture:__ Mask R-CNN  1.  backbone2. network head bbox  mask \n\nBackbone  ResNet  ResNeXt 50 101Faster R-CNN  ResNet  4-th stage  conv  C4 ResNet  ResNet-50  backbone  ResNet-50-C4\n\n backbone  FPNFPN  top-down  feature pyramid ResNet-FPN  backbone Mask R-CNN \n\n Network head 4\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4  backbone  head  ResNet  5-th stage 9  conv  res5ResNet-FPN  backbone  backbone  res5 head   \n 4 res5  ResNet  5-th stage 7x7  RoI feature maps  conv  stride  1 ResNet  conv conv4_x 14x14 feature maps  conv  stride  2\n# Experiments\n\n\n# Appendix\n mask  4mask  $(R,K,m,m)$ bbox  box  $(R,4)$ R  box K mxm  mask  i  $0 \\le i < R$ box  $(x_1,y_1,x_2,y_2)$ k  mask map  $M_i^k$\n1.  i  box   \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2.  mask map resize  box   \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3.  mask map  mask  pixel-wise sigmoid  (0,1)   \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4.  binary mask  image  image  (W,H) mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","slug":"mask-rcnn","published":1,"updated":"2019-07-09T10:57:27.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379cb001idgvc6e83ruww","content":"<p> <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> Mask R-CNN   </p>\n<p>Mask R-CNN  Faster R-CNN  RoI segmentation masks 1mask  RoI  pixel-to-pixel  segmentation mask mask  FCN mask <br><img src=\"/images/mask-rcnn_fig1.png\" alt></p>\n<p>Faster R-CNN  RoIPool RoI  image  feature map  1/16 mask  pixel-wise  RoIAlign  10%~50%  mask   </p>\n<p> binary mask  binary mask  RoI  FCN </p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN Mask R-CNN  binary mask Mask R-CNN </p>\n<p><strong>Faster R-CNN:</strong>  Faster R-CNN two-stage  stage  RPN proposals stage  Fast R-CNN RoIPooling  proposal  bbox </p>\n<p><strong>Mask R-CNN:</strong>  Faster R-CNN  stage  RoI  binary mask</p>\n<p> RoI  $L=L_{cls}+L_{box}+L_{mask}$ $L_{cls}$  $L_{box}$  Fast/Faster R-CNN <br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br> log lossproposal  gt  u$p_u$  proposal  u <br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br> smooth L1 loss$t_u$  u  bbox v  gt box  proposal  target<br>mask  RoI  $Km^2$  pixel-wise sigmoid K  binary mask mask  $m \\times m$ K  $L_{mark}$  RoI  gt  k$L_{mark}$  k  binary mask  K-1  binary mask  $L_{mark}$ <br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br> $f(\\cdot)$  sigmoid</p>\n<p><strong>Mask Representation:</strong>  RoI  bbox  fc  mask  pixel-to-pixel  RoI  FCN  RoI  $m \\times m$  maskpixel-to-pixel  RoI  RoIAlign </p>\n<p><strong>RoIAlign:</strong> RoIPool  RoI  7x7 RoI  feature map RoI  bins bin  bin  bin </p>\n<p> RoI  x  image  Faster R-CNN  stride  16 RoI  $[x/16]$ $[\\cdot]$  bin  RoI  $(x_1,y_1,x_2,y_2)$ RPN  anchor  RoI RoI  7x7  bins RoI <br>$$x_1=[x_1/16] \\quad y_1=[y_1/16]<br>\\quad x_2=[x_2/16]<br>\\quad y_2=[y_2/16]$$<br>RoI    bin <br>$$w=x_2-x_1+1<br>\\quad h=y_2-y_1+1<br>\\\\ w^b=w/7 \\quad h^b=h/7$$<br> (i,j)  bin<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p> $0 \\le i&lt;7, \\ 0\\le j&lt;7$ bin </p>\n<p> RoI  pixel-to-pixel  mask  RoIAlign  RoIAlign  RoIPool 3<br><img src=\"/images/mask-rcnn_fig3.png\" alt></p>\n<p> RoI  $x/16$ bin  4  bin max  average </p>\n<p><strong>Network Architecture:</strong> Mask R-CNN  1.  backbone2. network head bbox  mask </p>\n<p>Backbone  ResNet  ResNeXt 50 101Faster R-CNN  ResNet  4-th stage  conv  C4 ResNet  ResNet-50  backbone  ResNet-50-C4</p>\n<p> backbone  FPNFPN  top-down  feature pyramid ResNet-FPN  backbone Mask R-CNN </p>\n<p> Network head 4<br><img src=\"/images/mask-rcnn_fig4.png\" alt></p>\n<p>ResNet-C4  backbone  head  ResNet  5-th stage 9  conv  res5ResNet-FPN  backbone  backbone  res5 head <br> 4 res5  ResNet  5-th stage 7x7  RoI feature maps  conv  stride  1 ResNet  conv conv4_x 14x14 feature maps  conv  stride  2</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p> mask  4mask  $(R,K,m,m)$ bbox  box  $(R,4)$ R  box K mxm  mask  i  $0 \\le i &lt; R$ box  $(x_1,y_1,x_2,y_2)$ k  mask map  $M_i^k$</p>\n<ol>\n<li><p> i  box <br>$w=x_2-x_1, \\ h=y_2-y_1$</p>\n</li>\n<li><p> mask map resize  box   </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> mask map  mask  pixel-wise sigmoid  (0,1)   </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> binary mask  image  image  (W,H) mask  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p> Mask R-CNN   </p>\n<p>Mask R-CNN  Faster R-CNN  RoI segmentation masks 1mask  RoI  pixel-to-pixel  segmentation mask mask  FCN mask <br><img src=\"/images/mask-rcnn_fig1.png\" alt></p>\n<p>Faster R-CNN  RoIPool RoI  image  feature map  1/16 mask  pixel-wise  RoIAlign  10%~50%  mask   </p>\n<p> binary mask  binary mask  RoI  FCN </p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN Mask R-CNN  binary mask Mask R-CNN </p>\n<p><strong>Faster R-CNN:</strong>  Faster R-CNN two-stage  stage  RPN proposals stage  Fast R-CNN RoIPooling  proposal  bbox </p>\n<p><strong>Mask R-CNN:</strong>  Faster R-CNN  stage  RoI  binary mask</p>\n<p> RoI  $L=L_{cls}+L_{box}+L_{mask}$ $L_{cls}$  $L_{box}$  Fast/Faster R-CNN <br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br> log lossproposal  gt  u$p_u$  proposal  u <br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br> smooth L1 loss$t_u$  u  bbox v  gt box  proposal  target<br>mask  RoI  $Km^2$  pixel-wise sigmoid K  binary mask mask  $m \\times m$ K  $L_{mark}$  RoI  gt  k$L_{mark}$  k  binary mask  K-1  binary mask  $L_{mark}$ <br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br> $f(\\cdot)$  sigmoid</p>\n<p><strong>Mask Representation:</strong>  RoI  bbox  fc  mask  pixel-to-pixel  RoI  FCN  RoI  $m \\times m$  maskpixel-to-pixel  RoI  RoIAlign </p>\n<p><strong>RoIAlign:</strong> RoIPool  RoI  7x7 RoI  feature map RoI  bins bin  bin  bin </p>\n<p> RoI  x  image  Faster R-CNN  stride  16 RoI  $[x/16]$ $[\\cdot]$  bin  RoI  $(x_1,y_1,x_2,y_2)$ RPN  anchor  RoI RoI  7x7  bins RoI <br>$$x_1=[x_1/16] \\quad y_1=[y_1/16]<br>\\quad x_2=[x_2/16]<br>\\quad y_2=[y_2/16]$$<br>RoI    bin <br>$$w=x_2-x_1+1<br>\\quad h=y_2-y_1+1<br>\\\\ w^b=w/7 \\quad h^b=h/7$$<br> (i,j)  bin<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p> $0 \\le i&lt;7, \\ 0\\le j&lt;7$ bin </p>\n<p> RoI  pixel-to-pixel  mask  RoIAlign  RoIAlign  RoIPool 3<br><img src=\"/images/mask-rcnn_fig3.png\" alt></p>\n<p> RoI  $x/16$ bin  4  bin max  average </p>\n<p><strong>Network Architecture:</strong> Mask R-CNN  1.  backbone2. network head bbox  mask </p>\n<p>Backbone  ResNet  ResNeXt 50 101Faster R-CNN  ResNet  4-th stage  conv  C4 ResNet  ResNet-50  backbone  ResNet-50-C4</p>\n<p> backbone  FPNFPN  top-down  feature pyramid ResNet-FPN  backbone Mask R-CNN </p>\n<p> Network head 4<br><img src=\"/images/mask-rcnn_fig4.png\" alt></p>\n<p>ResNet-C4  backbone  head  ResNet  5-th stage 9  conv  res5ResNet-FPN  backbone  backbone  res5 head <br> 4 res5  ResNet  5-th stage 7x7  RoI feature maps  conv  stride  1 ResNet  conv conv4_x 14x14 feature maps  conv  stride  2</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p></p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p> mask  4mask  $(R,K,m,m)$ bbox  box  $(R,4)$ R  box K mxm  mask  i  $0 \\le i &lt; R$ box  $(x_1,y_1,x_2,y_2)$ k  mask map  $M_i^k$</p>\n<ol>\n<li><p> i  box <br>$w=x_2-x_1, \\ h=y_2-y_1$</p>\n</li>\n<li><p> mask map resize  box   </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> mask map  mask  pixel-wise sigmoid  (0,1)   </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> binary mask  image  image  (W,H) mask  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n"},{"title":"TridentNet","date":"2019-06-21T08:24:19.000Z","mathjax":true,"_content":"[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n\n[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# \n\n1. one stage YOLO, SSD\n2. two stage Faster R-CNN, R-FCN\n\n\n1.  image pyramids  1(a)\n2.  feature maps feature maps  SSD\n3. 2  low level  feature  high level  feature  RF  low level  feature FPN  bottom-up  top-down pathway   1(b) features  layers feature pyramids  image pyramids \n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n 1(c) trident  feature mapstrident  RF inference  TridentNet \n\n# \nbackbone  RF  backbone \n\n $d_s$ 3x3  RF  kernel size  $3+2(d_s-1)$  RF  feature map  image  s $d_s$  RF  $2(d_s-1)s$ n  RF  $2(d_s-1)sn$ n  feature map  image  s\n\n COCO benchmark  Faster R-CNNbackbone  ResNet-50  ResNet-101 _conv4_ stage  residual block  3x3  1-3  AP  a. b. c. d.  1\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO  RF  Faster R-CNN </font>\n\n RF ResNet-50  ResNet-101 \n1.  RF  RF \n2.  ResNet-101  RF  96x96 RF  RF \n\n# Trident \nTridentNet  trident  scale-aware \n## \n 2\n![](/images/TridentNet_fig2.png)\n\n image feature maps\n\n____  backbone  trident trident  1 \n\n ResNet  backbonebottleneck ResNet-50, ResNet=101  residual 1x13x31x1trident  residual  residual  residual  3x3  trident  RF backbone  stage  residual  trident  stage  stride  RF \n\n____  RPN  Fast R-CNN head\n\n1. TridentNet \n2.  feature maps feature pyramid \n3.  RF \n\n## scale-aware \ntrident  feature maps 1  scale-aware  or \n\n $[l_i,u_i]$ proposal  gt box  ROI  `(w,h)` $l_i \\le \\sqrt{wh} \\le u_i$ ROI  i \n\nscale-aware  RPN  Fast R-CNN  RPN  anchors /  box  scale-aware  gt box  anchor  Fast R-CNN head  proposal\n\n## Inference \nInference  NMS  soft-NMS \n\n____  inference  TridentNet [0,&infin;]  2  TridentNet  Faster R-CNN  TridentNet \n\n# \n COCO  80k  35k _trainval35k_ 5k _minival_\n\n## \n Faster R-CNN  MXNet  baseline backbone  ImageNet resize  image 800 Baseline  TridentNet  end-to-end  8  GPU batch size 16 12 epochs 0.02 8    10  epoch  10% ResNet  conv4 stage  backbone  feature maps conv5 stage  baseline  TridentNet  rcnn head TridentNet   image  128  ROIs TridentNet  123. scale-aware  [0,90][30,160][90,&infin;]\n\n COCO  AP $AP_{50}/AP_{75}$ $AP_s, AP_m, AP_l$  32x32, 32x32 ~ 96x96,  96x96\n\n## \n\n__TridentNet __ Baseline (Table 2(a))  ResNet-101  ResNet-101-Deformable  backbone Baseline   scale-aware \n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n    Table 2(b) baseline  RF\n2. __Scale-aware__\n   Table 2(d)  Table 2(b)  scale-aware  $AP_s$ scale-sware \n3. __Weight-sharing__\n   Table 2(c)   Table 2(b) Table 2(e) TridentNet  Baseline  scale-aware \n\n____ Table 3  1-4  scale-aware Table 3  TridentNet baseline TridentNet\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ ResNet-50</font>\n\n conv stage  trident  trident  TridentNet  SOTA \n\n# \n TridentNet  scale  feature maps scale-aware  inference  TridentNet baseline","source":"_posts/TridentNet.md","raw":"---\ntitle: TridentNet\ndate: 2019-06-21 16:24:19\ntags: object detection\nmathjax: true\n---\n[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n\n[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# \n\n1. one stage YOLO, SSD\n2. two stage Faster R-CNN, R-FCN\n\n\n1.  image pyramids  1(a)\n2.  feature maps feature maps  SSD\n3. 2  low level  feature  high level  feature  RF  low level  feature FPN  bottom-up  top-down pathway   1(b) features  layers feature pyramids  image pyramids \n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n 1(c) trident  feature mapstrident  RF inference  TridentNet \n\n# \nbackbone  RF  backbone \n\n $d_s$ 3x3  RF  kernel size  $3+2(d_s-1)$  RF  feature map  image  s $d_s$  RF  $2(d_s-1)s$ n  RF  $2(d_s-1)sn$ n  feature map  image  s\n\n COCO benchmark  Faster R-CNNbackbone  ResNet-50  ResNet-101 _conv4_ stage  residual block  3x3  1-3  AP  a. b. c. d.  1\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO  RF  Faster R-CNN </font>\n\n RF ResNet-50  ResNet-101 \n1.  RF  RF \n2.  ResNet-101  RF  96x96 RF  RF \n\n# Trident \nTridentNet  trident  scale-aware \n## \n 2\n![](/images/TridentNet_fig2.png)\n\n image feature maps\n\n____  backbone  trident trident  1 \n\n ResNet  backbonebottleneck ResNet-50, ResNet=101  residual 1x13x31x1trident  residual  residual  residual  3x3  trident  RF backbone  stage  residual  trident  stage  stride  RF \n\n____  RPN  Fast R-CNN head\n\n1. TridentNet \n2.  feature maps feature pyramid \n3.  RF \n\n## scale-aware \ntrident  feature maps 1  scale-aware  or \n\n $[l_i,u_i]$ proposal  gt box  ROI  `(w,h)` $l_i \\le \\sqrt{wh} \\le u_i$ ROI  i \n\nscale-aware  RPN  Fast R-CNN  RPN  anchors /  box  scale-aware  gt box  anchor  Fast R-CNN head  proposal\n\n## Inference \nInference  NMS  soft-NMS \n\n____  inference  TridentNet [0,&infin;]  2  TridentNet  Faster R-CNN  TridentNet \n\n# \n COCO  80k  35k _trainval35k_ 5k _minival_\n\n## \n Faster R-CNN  MXNet  baseline backbone  ImageNet resize  image 800 Baseline  TridentNet  end-to-end  8  GPU batch size 16 12 epochs 0.02 8    10  epoch  10% ResNet  conv4 stage  backbone  feature maps conv5 stage  baseline  TridentNet  rcnn head TridentNet   image  128  ROIs TridentNet  123. scale-aware  [0,90][30,160][90,&infin;]\n\n COCO  AP $AP_{50}/AP_{75}$ $AP_s, AP_m, AP_l$  32x32, 32x32 ~ 96x96,  96x96\n\n## \n\n__TridentNet __ Baseline (Table 2(a))  ResNet-101  ResNet-101-Deformable  backbone Baseline   scale-aware \n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n    Table 2(b) baseline  RF\n2. __Scale-aware__\n   Table 2(d)  Table 2(b)  scale-aware  $AP_s$ scale-sware \n3. __Weight-sharing__\n   Table 2(c)   Table 2(b) Table 2(e) TridentNet  Baseline  scale-aware \n\n____ Table 3  1-4  scale-aware Table 3  TridentNet baseline TridentNet\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ ResNet-50</font>\n\n conv stage  trident  trident  TridentNet  SOTA \n\n# \n TridentNet  scale  feature maps scale-aware  inference  TridentNet baseline","slug":"TridentNet","published":1,"updated":"2019-06-27T12:17:29.335Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379cc001kdgvclw2pjtqb","content":"<p><a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>\n<p><a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ol>\n<li>one stage YOLO, SSD</li>\n<li>two stage Faster R-CNN, R-FCN</li>\n</ol>\n<p></p>\n<ol>\n<li> image pyramids  1(a)</li>\n<li> feature maps feature maps  SSD</li>\n<li>2  low level  feature  high level  feature  RF  low level  feature FPN  bottom-up  top-down pathway   1(b) features  layers feature pyramids  image pyramids </li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p> 1(c) trident  feature mapstrident  RF inference  TridentNet </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>backbone  RF  backbone </p>\n<p> $d_s$ 3x3  RF  kernel size  $3+2(d_s-1)$  RF  feature map  image  s $d_s$  RF  $2(d_s-1)s$ n  RF  $2(d_s-1)sn$ n  feature map  image  s</p>\n<p> COCO benchmark  Faster R-CNNbackbone  ResNet-50  ResNet-101 <em>conv4</em> stage  residual block  3x3  1-3  AP  a. b. c. d.  1</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 1 COCO  RF  Faster R-CNN </font></p>\n<p> RF ResNet-50  ResNet-101 </p>\n<ol>\n<li> RF  RF </li>\n<li> ResNet-101  RF  96x96 RF  RF </li>\n</ol>\n<h1 id=\"Trident-\"><a href=\"#Trident-\" class=\"headerlink\" title=\"Trident \"></a>Trident </h1><p>TridentNet  trident  scale-aware </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 2<br><img src=\"/images/TridentNet_fig2.png\" alt></p>\n<p> image feature maps</p>\n<p><strong></strong>  backbone  trident trident  1 </p>\n<p> ResNet  backbonebottleneck ResNet-50, ResNet=101  residual 1x13x31x1trident  residual  residual  residual  3x3  trident  RF backbone  stage  residual  trident  stage  stride  RF </p>\n<p><strong></strong>  RPN  Fast R-CNN head<br></p>\n<ol>\n<li>TridentNet </li>\n<li> feature maps feature pyramid </li>\n<li> RF </li>\n</ol>\n<h2 id=\"scale-aware-\"><a href=\"#scale-aware-\" class=\"headerlink\" title=\"scale-aware \"></a>scale-aware </h2><p>trident  feature maps 1  scale-aware  or </p>\n<p> $[l_i,u_i]$ proposal  gt box  ROI  <code>(w,h)</code> $l_i \\le \\sqrt{wh} \\le u_i$ ROI  i </p>\n<p>scale-aware  RPN  Fast R-CNN  RPN  anchors /  box  scale-aware  gt box  anchor  Fast R-CNN head  proposal</p>\n<h2 id=\"Inference-\"><a href=\"#Inference-\" class=\"headerlink\" title=\"Inference \"></a>Inference </h2><p>Inference  NMS  soft-NMS </p>\n<p><strong></strong>  inference  TridentNet [0,&infin;]  2  TridentNet  Faster R-CNN  TridentNet </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> COCO  80k  35k _trainval35k_ 5k _minival_</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> Faster R-CNN  MXNet  baseline backbone  ImageNet resize  image 800 Baseline  TridentNet  end-to-end  8  GPU batch size 16 12 epochs 0.02 8    10  epoch  10% ResNet  conv4 stage  backbone  feature maps conv5 stage  baseline  TridentNet  rcnn head TridentNet   image  128  ROIs TridentNet  123. scale-aware  [0,90][30,160][90,&infin;]</p>\n<p> COCO  AP $AP_{50}/AP_{75}$ $AP_s, AP_m, AP_l$  32x32, 32x32 ~ 96x96,  96x96</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong>TridentNet </strong> Baseline (Table 2(a))  ResNet-101  ResNet-101-Deformable  backbone Baseline   scale-aware <br><img src=\"/images/TridentNet_fig3.png\" alt></p>\n<ol>\n<li><strong>Multi-branch</strong><br> Table 2(b) baseline  RF</li>\n<li><strong>Scale-aware</strong><br>Table 2(d)  Table 2(b)  scale-aware  $AP_s$ scale-sware </li>\n<li><strong>Weight-sharing</strong><br>Table 2(c)   Table 2(b) Table 2(e) TridentNet  Baseline  scale-aware </li>\n</ol>\n<p><strong></strong> Table 3  1-4  scale-aware Table 3  TridentNet baseline TridentNet</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 3 COCO <em>minival</em> ResNet-50</font></p>\n<p> conv stage  trident  trident  TridentNet  SOTA </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> TridentNet  scale  feature maps scale-aware  inference  TridentNet baseline</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>\n<p><a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ol>\n<li>one stage YOLO, SSD</li>\n<li>two stage Faster R-CNN, R-FCN</li>\n</ol>\n<p></p>\n<ol>\n<li> image pyramids  1(a)</li>\n<li> feature maps feature maps  SSD</li>\n<li>2  low level  feature  high level  feature  RF  low level  feature FPN  bottom-up  top-down pathway   1(b) features  layers feature pyramids  image pyramids </li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p> 1(c) trident  feature mapstrident  RF inference  TridentNet </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>backbone  RF  backbone </p>\n<p> $d_s$ 3x3  RF  kernel size  $3+2(d_s-1)$  RF  feature map  image  s $d_s$  RF  $2(d_s-1)s$ n  RF  $2(d_s-1)sn$ n  feature map  image  s</p>\n<p> COCO benchmark  Faster R-CNNbackbone  ResNet-50  ResNet-101 <em>conv4</em> stage  residual block  3x3  1-3  AP  a. b. c. d.  1</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 1 COCO  RF  Faster R-CNN </font></p>\n<p> RF ResNet-50  ResNet-101 </p>\n<ol>\n<li> RF  RF </li>\n<li> ResNet-101  RF  96x96 RF  RF </li>\n</ol>\n<h1 id=\"Trident-\"><a href=\"#Trident-\" class=\"headerlink\" title=\"Trident \"></a>Trident </h1><p>TridentNet  trident  scale-aware </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 2<br><img src=\"/images/TridentNet_fig2.png\" alt></p>\n<p> image feature maps</p>\n<p><strong></strong>  backbone  trident trident  1 </p>\n<p> ResNet  backbonebottleneck ResNet-50, ResNet=101  residual 1x13x31x1trident  residual  residual  residual  3x3  trident  RF backbone  stage  residual  trident  stage  stride  RF </p>\n<p><strong></strong>  RPN  Fast R-CNN head<br></p>\n<ol>\n<li>TridentNet </li>\n<li> feature maps feature pyramid </li>\n<li> RF </li>\n</ol>\n<h2 id=\"scale-aware-\"><a href=\"#scale-aware-\" class=\"headerlink\" title=\"scale-aware \"></a>scale-aware </h2><p>trident  feature maps 1  scale-aware  or </p>\n<p> $[l_i,u_i]$ proposal  gt box  ROI  <code>(w,h)</code> $l_i \\le \\sqrt{wh} \\le u_i$ ROI  i </p>\n<p>scale-aware  RPN  Fast R-CNN  RPN  anchors /  box  scale-aware  gt box  anchor  Fast R-CNN head  proposal</p>\n<h2 id=\"Inference-\"><a href=\"#Inference-\" class=\"headerlink\" title=\"Inference \"></a>Inference </h2><p>Inference  NMS  soft-NMS </p>\n<p><strong></strong>  inference  TridentNet [0,&infin;]  2  TridentNet  Faster R-CNN  TridentNet </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> COCO  80k  35k _trainval35k_ 5k _minival_</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> Faster R-CNN  MXNet  baseline backbone  ImageNet resize  image 800 Baseline  TridentNet  end-to-end  8  GPU batch size 16 12 epochs 0.02 8    10  epoch  10% ResNet  conv4 stage  backbone  feature maps conv5 stage  baseline  TridentNet  rcnn head TridentNet   image  128  ROIs TridentNet  123. scale-aware  [0,90][30,160][90,&infin;]</p>\n<p> COCO  AP $AP_{50}/AP_{75}$ $AP_s, AP_m, AP_l$  32x32, 32x32 ~ 96x96,  96x96</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong>TridentNet </strong> Baseline (Table 2(a))  ResNet-101  ResNet-101-Deformable  backbone Baseline   scale-aware <br><img src=\"/images/TridentNet_fig3.png\" alt></p>\n<ol>\n<li><strong>Multi-branch</strong><br> Table 2(b) baseline  RF</li>\n<li><strong>Scale-aware</strong><br>Table 2(d)  Table 2(b)  scale-aware  $AP_s$ scale-sware </li>\n<li><strong>Weight-sharing</strong><br>Table 2(c)   Table 2(b) Table 2(e) TridentNet  Baseline  scale-aware </li>\n</ol>\n<p><strong></strong> Table 3  1-4  scale-aware Table 3  TridentNet baseline TridentNet</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 3 COCO <em>minival</em> ResNet-50</font></p>\n<p> conv stage  trident  trident  TridentNet  SOTA </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> TridentNet  scale  feature maps scale-aware  inference  TridentNet baseline</p>\n"},{"title":"mAP","date":"2019-06-16T03:43:57.000Z","mathjax":true,"_content":"# mAP\n PASCAL VOC \n- mAPmean Average Precision mAP  AP  AP  AP  mAP\n## \n0. Positive \n1. True Positive (TP): IoU  box\n2. False Positive (FP): IoU  box\n3. Precision = TP/(TP+FP) = TP/()\n4. Recall = TP/(TP+FN) = TP/(gt)\n\n TP,FP,TN,FN 4[](https://github.io/shajian/shajian.github.io) issue\n1. TP\n   \n   P (Positive) gt box  IoU $Threshold_{VOC}=0.5$ TP\n2. FP\n   \n   P (Positive) gt box  IoU  FP gt box  IoU  gt box match confidence  FP\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n    gt box  gt box  IoU 0 gt box  FN\n4. TN\n   \n   TN = 0 Positive  NegativeTN  Negative Positive\n\nVOC  `0.5`\n## \n### PR \n box  score  confidence confidence  box confidence  PRPrecision x Recall confidence  1  rank=1  PR  PR  R' >= R  P  PR [stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)\n\n \"Aeroplane\",\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\nBB  \"match\"  GT box\n\n confidence GT=1  TPGT=0  FP BBox FN=2TP=5 (BB1,BB2,BB5,BB8,BB9)FP=5 BB1 confidence  0.9  FP rank=3  case PASCAL VOC  Detection Task  [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) GT box  TP+FN=5+2=7 PR \n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n\n\n1. rank=1 1TP  BB1  FP P=1R=1/7=0.14\n2. rank=2 2TP  BB1,BB2 FP P=1R=2/7=0.29\n3. rank=3 3TP  BB1,BB2FP  BB1 P=2/3=0.66R=2/7=0.29\n4. ...\n\n### AP\nVOC  2010  11  R   R={0,0.1,...,1} R' >= R  P  PR  AP  R average precisionVOC 2010  R' >= R  P  R  [0,1]  __PR __  AP  PR  AUC area under the curve\n\n#### 11-\n11 R  [0,1] \n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n$\\rho(\\tilde r)$ \n[](https://github.com/rafaelpadilla/Object-Detection-Metrics)\n![](/images/mAP_fig1.png)\n\n PR 11 R  PR  R=0.2  (2) 0.2  $\\tilde r$  {0.2,0.2666,0.3333,0.4,0.4666} $\\tilde r=0.4$  P  0.4285 11- AP\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### \nAP \n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n$\\rho (\\tilde r)$  Recall $\\tilde r$  AP  PR  AUC\n\n![](/images/mAP_fig2.png)\n\n PR  RP  AUC  4 \n![](/images/mAP_fig3.png)\n\n AP \n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC \n## \n1. TPR (true positive rate) (sensitivity) (recall)TPR = TP/(TP+FN)\n2. TNR (true negative rate) (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate): FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate): FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC \n\nROC \n\nROC  [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n\n TPR-FPR  ROC \n\n![](/images/mAP_fig4.png)\n\n (0,0)  (1,1) \n1.  1  Negative TP=FP=0 TPR=FPR=0\n2.  0  Positive TN=FN=0 TPR=FPR=1\n\n $(-\\infty,0) \\cup (1,+\\infty)$ $(-\\infty,0)$  2  $(1,+\\infty)$  1 \n\n ROC  y=x  y=x TPR  1FPR  0 ROC  (0,1) y=x\n\n## ROC \n X score T X>T X  $f_1(x)$ X  $f_0(x)$,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n T \n\n1. TPR(T) \n2. FPR(T) \n\n\n![ 5](/images/mAP_fig5.png)\n\n X  score score \n## AUC\n ROC  AUC \n\nAUC  Score  Score ROC  TPR  FPR  T \n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n$X_1$ $X_0$\n\n $X_1$  $X_0$ :\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n $f_1,f_0$\n\n$X_1, X_0$  $(X_1,X_0)$  $f(x_1,x_0)=f_1(x_1) f_0(x_0)$ $X_0 < X_1$ \n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n","source":"_posts/mAP.md","raw":"---\ntitle: mAP\ndate: 2019-06-16 11:43:57\ntags: object detection\nmathjax: true\n---\n# mAP\n PASCAL VOC \n- mAPmean Average Precision mAP  AP  AP  AP  mAP\n## \n0. Positive \n1. True Positive (TP): IoU  box\n2. False Positive (FP): IoU  box\n3. Precision = TP/(TP+FP) = TP/()\n4. Recall = TP/(TP+FN) = TP/(gt)\n\n TP,FP,TN,FN 4[](https://github.io/shajian/shajian.github.io) issue\n1. TP\n   \n   P (Positive) gt box  IoU $Threshold_{VOC}=0.5$ TP\n2. FP\n   \n   P (Positive) gt box  IoU  FP gt box  IoU  gt box match confidence  FP\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n    gt box  gt box  IoU 0 gt box  FN\n4. TN\n   \n   TN = 0 Positive  NegativeTN  Negative Positive\n\nVOC  `0.5`\n## \n### PR \n box  score  confidence confidence  box confidence  PRPrecision x Recall confidence  1  rank=1  PR  PR  R' >= R  P  PR [stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)\n\n \"Aeroplane\",\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\nBB  \"match\"  GT box\n\n confidence GT=1  TPGT=0  FP BBox FN=2TP=5 (BB1,BB2,BB5,BB8,BB9)FP=5 BB1 confidence  0.9  FP rank=3  case PASCAL VOC  Detection Task  [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) GT box  TP+FN=5+2=7 PR \n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n\n\n1. rank=1 1TP  BB1  FP P=1R=1/7=0.14\n2. rank=2 2TP  BB1,BB2 FP P=1R=2/7=0.29\n3. rank=3 3TP  BB1,BB2FP  BB1 P=2/3=0.66R=2/7=0.29\n4. ...\n\n### AP\nVOC  2010  11  R   R={0,0.1,...,1} R' >= R  P  PR  AP  R average precisionVOC 2010  R' >= R  P  R  [0,1]  __PR __  AP  PR  AUC area under the curve\n\n#### 11-\n11 R  [0,1] \n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n$\\rho(\\tilde r)$ \n[](https://github.com/rafaelpadilla/Object-Detection-Metrics)\n![](/images/mAP_fig1.png)\n\n PR 11 R  PR  R=0.2  (2) 0.2  $\\tilde r$  {0.2,0.2666,0.3333,0.4,0.4666} $\\tilde r=0.4$  P  0.4285 11- AP\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### \nAP \n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n$\\rho (\\tilde r)$  Recall $\\tilde r$  AP  PR  AUC\n\n![](/images/mAP_fig2.png)\n\n PR  RP  AUC  4 \n![](/images/mAP_fig3.png)\n\n AP \n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC \n## \n1. TPR (true positive rate) (sensitivity) (recall)TPR = TP/(TP+FN)\n2. TNR (true negative rate) (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate): FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate): FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC \n\nROC \n\nROC  [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n\n TPR-FPR  ROC \n\n![](/images/mAP_fig4.png)\n\n (0,0)  (1,1) \n1.  1  Negative TP=FP=0 TPR=FPR=0\n2.  0  Positive TN=FN=0 TPR=FPR=1\n\n $(-\\infty,0) \\cup (1,+\\infty)$ $(-\\infty,0)$  2  $(1,+\\infty)$  1 \n\n ROC  y=x  y=x TPR  1FPR  0 ROC  (0,1) y=x\n\n## ROC \n X score T X>T X  $f_1(x)$ X  $f_0(x)$,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n T \n\n1. TPR(T) \n2. FPR(T) \n\n\n![ 5](/images/mAP_fig5.png)\n\n X  score score \n## AUC\n ROC  AUC \n\nAUC  Score  Score ROC  TPR  FPR  T \n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n$X_1$ $X_0$\n\n $X_1$  $X_0$ :\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n $f_1,f_0$\n\n$X_1, X_0$  $(X_1,X_0)$  $f(x_1,x_0)=f_1(x_1) f_0(x_0)$ $X_0 < X_1$ \n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n","slug":"mAP","published":1,"updated":"2019-06-21T07:02:21.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379ce001ndgvcm8w0miq4","content":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p> PASCAL VOC <br>- mAPmean Average Precision mAP  AP  AP  AP  mAP</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ol start=\"0\">\n<li>Positive </li>\n<li>True Positive (TP): IoU  box</li>\n<li>False Positive (FP): IoU  box</li>\n<li>Precision = TP/(TP+FP) = TP/()</li>\n<li>Recall = TP/(TP+FN) = TP/(gt)</li>\n</ol>\n<p> TP,FP,TN,FN 4<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\"></a> issue</p>\n<ol>\n<li><p>TP</p>\n<p>P (Positive) gt box  IoU $Threshold_{VOC}=0.5$ TP</p>\n</li>\n<li><p>FP</p>\n<p>P (Positive) gt box  IoU  FP gt box  IoU  gt box match confidence  FP</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p> gt box  gt box  IoU 0 gt box  FN</p>\n</li>\n<li><p>TN</p>\n<p>TN = 0 Positive  NegativeTN  Negative Positive</p>\n</li>\n</ol>\n<p>VOC  <code>0.5</code></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"PR-\"><a href=\"#PR-\" class=\"headerlink\" title=\"PR \"></a>PR </h3><p> box  score  confidence confidence  box confidence  PRPrecision x Recall confidence  1  rank=1  PR  PR  R &gt;= R  P  PR <a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a></p>\n<p> Aeroplane,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n\n<p>BB  match  GT box</p>\n<p> confidence GT=1  TPGT=0  FP BBox FN=2TP=5 (BB1,BB2,BB5,BB8,BB9)FP=5 BB1 confidence  0.9  FP rank=3  case PASCAL VOC  Detection Task  <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> GT box  TP+FN=5+2=7 PR </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank=1  precision=1.00 and recall=0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=2  precision=1.00 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=3  precision=0.66 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=4  precision=0.50 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=5  precision=0.40 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=6  precision=0.50 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=7  precision=0.43 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=8  precision=0.38 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=9  precision=0.44 and recall=0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=10 precision=0.50 and recall=0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<ol>\n<li>rank=1 1TP  BB1  FP P=1R=1/7=0.14</li>\n<li>rank=2 2TP  BB1,BB2 FP P=1R=2/7=0.29</li>\n<li>rank=3 3TP  BB1,BB2FP  BB1 P=2/3=0.66R=2/7=0.29</li>\n<li></li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC  2010  11  R   R={0,0.1,,1} R &gt;= R  P  PR  AP  R average precisionVOC 2010  R &gt;= R  P  R  [0,1]  <strong>PR </strong>  AP  PR  AUC area under the curve</p>\n<h4 id=\"11-\"><a href=\"#11-\" class=\"headerlink\" title=\"11-\"></a>11-</h4><p>11 R  [0,1] <br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>$\\rho(\\tilde r)$ <br><a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\"></a><br><img src=\"/images/mAP_fig1.png\" alt></p>\n<p> PR 11 R  PR  R=0.2  (2) 0.2  $\\tilde r$  {0.2,0.2666,0.3333,0.4,0.4666} $\\tilde r=0.4$  P  0.4285 11- AP</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>AP <br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>$\\rho (\\tilde r)$  Recall $\\tilde r$  AP  PR  AUC<br><br><img src=\"/images/mAP_fig2.png\" alt></p>\n<p> PR  RP  AUC  4 <br><img src=\"/images/mAP_fig3.png\" alt></p>\n<p> AP </p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-\"><a href=\"#ROC-\" class=\"headerlink\" title=\"ROC \"></a>ROC </h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><ol>\n<li><p>TPR (true positive rate) (sensitivity) (recall)TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate) (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate): FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate): FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC--1\"><a href=\"#ROC--1\" class=\"headerlink\" title=\"ROC \"></a>ROC </h2><p>ROC </p>\n<p>ROC  <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a></p>\n<p> TPR-FPR  ROC <br><br><img src=\"/images/mAP_fig4.png\" alt></p>\n<p> (0,0)  (1,1) </p>\n<ol>\n<li> 1  Negative TP=FP=0 TPR=FPR=0</li>\n<li> 0  Positive TN=FN=0 TPR=FPR=1</li>\n</ol>\n<p> $(-\\infty,0) \\cup (1,+\\infty)$ $(-\\infty,0)$  2  $(1,+\\infty)$  1 </p>\n<p> ROC  y=x  y=x TPR  1FPR  0 ROC  (0,1) y=x</p>\n<h2 id=\"ROC-\"><a href=\"#ROC-\" class=\"headerlink\" title=\"ROC \"></a>ROC </h2><p> X score T X&gt;T X  $f_1(x)$ X  $f_0(x)$,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br> T </p>\n<ol>\n<li>TPR(T) </li>\n<li>FPR(T) </li>\n</ol>\n<p><br><img src=\"/images/mAP_fig5.png\" alt=\" 5\"></p>\n<p> X  score score </p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p> ROC  AUC </p>\n<p>AUC  Score  Score ROC  TPR  FPR  T <br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br><br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ (T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T) \\ dT \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T)f_0(T) \\ dT dT \\\\ = P(X_1&gt;X_0)<br>$$<br>$X_1$ $X_0$</p>\n<p> $X_1$  $X_0$ :<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br> $f_1,f_0$</p>\n<p>$X_1, X_0$  $(X_1,X_0)$  $f(x_1,x_0)=f_1(x_1) f_0(x_0)$ $X_0 &lt; X_1$ <br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p> PASCAL VOC <br>- mAPmean Average Precision mAP  AP  AP  AP  mAP</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ol start=\"0\">\n<li>Positive </li>\n<li>True Positive (TP): IoU  box</li>\n<li>False Positive (FP): IoU  box</li>\n<li>Precision = TP/(TP+FP) = TP/()</li>\n<li>Recall = TP/(TP+FN) = TP/(gt)</li>\n</ol>\n<p> TP,FP,TN,FN 4<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\"></a> issue</p>\n<ol>\n<li><p>TP</p>\n<p>P (Positive) gt box  IoU $Threshold_{VOC}=0.5$ TP</p>\n</li>\n<li><p>FP</p>\n<p>P (Positive) gt box  IoU  FP gt box  IoU  gt box match confidence  FP</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p> gt box  gt box  IoU 0 gt box  FN</p>\n</li>\n<li><p>TN</p>\n<p>TN = 0 Positive  NegativeTN  Negative Positive</p>\n</li>\n</ol>\n<p>VOC  <code>0.5</code></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"PR-\"><a href=\"#PR-\" class=\"headerlink\" title=\"PR \"></a>PR </h3><p> box  score  confidence confidence  box confidence  PRPrecision x Recall confidence  1  rank=1  PR  PR  R &gt;= R  P  PR <a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a></p>\n<p> Aeroplane,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n\n<p>BB  match  GT box</p>\n<p> confidence GT=1  TPGT=0  FP BBox FN=2TP=5 (BB1,BB2,BB5,BB8,BB9)FP=5 BB1 confidence  0.9  FP rank=3  case PASCAL VOC  Detection Task  <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> GT box  TP+FN=5+2=7 PR </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank=1  precision=1.00 and recall=0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=2  precision=1.00 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=3  precision=0.66 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=4  precision=0.50 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=5  precision=0.40 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=6  precision=0.50 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=7  precision=0.43 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=8  precision=0.38 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=9  precision=0.44 and recall=0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=10 precision=0.50 and recall=0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<ol>\n<li>rank=1 1TP  BB1  FP P=1R=1/7=0.14</li>\n<li>rank=2 2TP  BB1,BB2 FP P=1R=2/7=0.29</li>\n<li>rank=3 3TP  BB1,BB2FP  BB1 P=2/3=0.66R=2/7=0.29</li>\n<li></li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC  2010  11  R   R={0,0.1,,1} R &gt;= R  P  PR  AP  R average precisionVOC 2010  R &gt;= R  P  R  [0,1]  <strong>PR </strong>  AP  PR  AUC area under the curve</p>\n<h4 id=\"11-\"><a href=\"#11-\" class=\"headerlink\" title=\"11-\"></a>11-</h4><p>11 R  [0,1] <br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>$\\rho(\\tilde r)$ <br><a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\"></a><br><img src=\"/images/mAP_fig1.png\" alt></p>\n<p> PR 11 R  PR  R=0.2  (2) 0.2  $\\tilde r$  {0.2,0.2666,0.3333,0.4,0.4666} $\\tilde r=0.4$  P  0.4285 11- AP</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>AP <br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>$\\rho (\\tilde r)$  Recall $\\tilde r$  AP  PR  AUC<br><br><img src=\"/images/mAP_fig2.png\" alt></p>\n<p> PR  RP  AUC  4 <br><img src=\"/images/mAP_fig3.png\" alt></p>\n<p> AP </p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-\"><a href=\"#ROC-\" class=\"headerlink\" title=\"ROC \"></a>ROC </h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><ol>\n<li><p>TPR (true positive rate) (sensitivity) (recall)TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate) (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate): FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate): FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC--1\"><a href=\"#ROC--1\" class=\"headerlink\" title=\"ROC \"></a>ROC </h2><p>ROC </p>\n<p>ROC  <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a></p>\n<p> TPR-FPR  ROC <br><br><img src=\"/images/mAP_fig4.png\" alt></p>\n<p> (0,0)  (1,1) </p>\n<ol>\n<li> 1  Negative TP=FP=0 TPR=FPR=0</li>\n<li> 0  Positive TN=FN=0 TPR=FPR=1</li>\n</ol>\n<p> $(-\\infty,0) \\cup (1,+\\infty)$ $(-\\infty,0)$  2  $(1,+\\infty)$  1 </p>\n<p> ROC  y=x  y=x TPR  1FPR  0 ROC  (0,1) y=x</p>\n<h2 id=\"ROC-\"><a href=\"#ROC-\" class=\"headerlink\" title=\"ROC \"></a>ROC </h2><p> X score T X&gt;T X  $f_1(x)$ X  $f_0(x)$,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br> T </p>\n<ol>\n<li>TPR(T) </li>\n<li>FPR(T) </li>\n</ol>\n<p><br><img src=\"/images/mAP_fig5.png\" alt=\" 5\"></p>\n<p> X  score score </p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p> ROC  AUC </p>\n<p>AUC  Score  Score ROC  TPR  FPR  T <br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br><br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ (T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T) \\ dT \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T)f_0(T) \\ dT dT \\\\ = P(X_1&gt;X_0)<br>$$<br>$X_1$ $X_0$</p>\n<p> $X_1$  $X_0$ :<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br> $f_1,f_0$</p>\n<p>$X_1, X_0$  $(X_1,X_0)$  $f(x_1,x_0)=f_1(x_1) f_0(x_0)$ $X_0 &lt; X_1$ <br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br></p>\n"},{"title":"GAN","date":"2019-07-23T02:15:08.000Z","mathjax":true,"_content":" [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n\n# GAN\n## \n GAN G  DG D \n\n z $p_z(z)$G  z  $G(z;\\theta_g)$ G  MLP  $\\theta_g$D  MLP $D(x;\\theta_d)$  x  D  x  x $G(x)$  x  G  $D(G(z))$  G  $D(G(x))$  $\\log(1-D(G(z)))$ \n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$\n\n D  log D  G  D  log \n\n 1 \n![](/images/GAN_fig1.png)\n\n 1  D  x  $p_x$ G  $p_g$  x  z  z  x  x=G(z) G   \n(a)  $p_g,\\ p_{data}$ D   \n(b)  D  $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$  \n(c) D  G(z) G   \n(d) G  D  $p_g=p_{data}$D $D(x)=1/2$\n\n\n![](/images/GAN_alg1.png)\n\nk  D  G  G  D \n\n(1)  GG  D $\\log (1-D(G(z)))$ log  G  $\\log D(G(z))$ G  D log \n\n## \n z  $p_z$  G  1 G G  $p_{data}$ $p_g=p_{data}$  (1) \n### \n__Proposition 1.__  GD \n$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$\n____   GD  V(G,D)  \n$$\\begin{aligned} V(G,D)&=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz\n\\\\\\\\ &=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$\n$\\forall (a,b) \\in \\Bbb R^2 \\setminus \\{0,0\\}$ $y \\rightarrow a \\log y+b \\log(1-y)$  (0,1)  $y=\\frac a {a+b}$  0  V(G,D)  x  D(x)  (2) \n\nD  $P(Y=y|x)$  log  binary cross-entropy x  $p_{data}$  y=1 x  $p_g$  y=0 D  $D_G^{\\ast}$  (1)   \n\n$$\\begin{aligned} C(G)&=\\max_D V(G,D)\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$\n\n__Theorem 1.__  $p_g=p_{data}$  C(G)  -log4  \n\n____ \n\n1.   \n $p_g=p_{data}$ (2)  $D_G^{\\ast}(x)=1/2$ (4) \n$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$\n2.   \n   $$\\begin{aligned}C(G)&=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\\\\\ &=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\\\\\ &=-\\log4+KL \\left(p_{data} \\| \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g \\| \\frac {p_{data}+p_g} 2 \\right) \\\\\\\\ &=-\\log4 + 2\\cdot JSD(p_{data} \\| p_g) \\end{aligned}$$\n    KL  Kullback-Leibler JSD  Jensen-Shannon  JSD  $p_g=p_{data}$  0 C(G)=-log4 $p_g=p_{data}$  \n\n\n\n###  1 \n $p_g=p_{data}$__Proposition 2__  1 \n\n__Proposition 2.__  G  D  1  G  G  C(G)  $p_g$ \n$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$\n$p_g$  $p_{data}$\n\n____\n\n $V(G,D)=U(p_g,D)$  $p_g$ $p_g$  (5)  $U(p_g,D)$  $p_g$  D  ____  D  $p_g$D  $p_g$ \n\n-  $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ $f_{\\alpha}(x)$  $\\alpha$  x  $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$  $\\partial f_{\\beta}(x) \\in \\partial f(x)$\n\n$V(G,D)=U(p_g,D)$  $p_g$   $p_g$  D  D  D*  V(G,D)  (5) / $p_g$ $p_g$  $p_{data}$ Theorem 1\n\n (5)  1  SGD  $p_g$  $p_g$ D  $p_g$ \n\n $G(z;\\theta_g)$  $p_g$  $\\theta_g$  G  G  MLP $\\theta_g$  $p_g$  $p_g$   $\\theta_g$  MLP  G  (1)  $p_g$  batch $p_{data}$  batch  (1)  batch  log-likelihood function log \n\n## \n [adversarial](http://www.github.com/goodfeli/adversarial)\n\n>  Theano  Pylearn2github  GAN  [generative-models](https://github.com/wiseodd/generative-models)\n\n github  clone  adversarial  mnist \n\n mnist.yaml \n```yaml\n!obj:pylearn2.train.Train {         # \n    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {    #  mnist \n        which_set: 'train',                                 #  train  50000 \n        start: 0,\n        stop: 50000\n    },\n    model: !obj:adversarial.AdversaryPair {                 # GANG & D\n        generator: !obj:adversarial.Generator {             # G\n            noise: 'uniform',                               # noise \n            monitor_ll: 1,\n            mlp: !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     !obj:pylearn2.models.mlp.RectifiedLinear { #  ReLu  FC \n                         layer_name: 'h0',\n                         dim: 1200,                             #  output units \n                         irange: .05,\n                     },\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {     # FC  sigmoid\n                         init_bias: !obj:pylearn2.models.dbm.init_sigmoid_bias_from_marginals { dataset: *train},\n                         layer_name: 'y',\n                         irange: .05,\n                         dim: 784                               # 784=28x28 mnist \n                     }\n                    ],\n            nvis: 100,                                          # G \n        }},\n        discriminator:                                          # D\n            !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {\n                         layer_name: 'y',\n                         dim: 1,                                # \n                         irange: .005\n                     }\n                    ],\n            nvis: 784,                                          # \n        },\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {      # \n        ...\n        cost: !obj:adversarial.AdversaryCost2 {                 # \n            scale_grads: 0,\n            #target_scale: 1.,\n            discriminator_default_input_include_prob: .5,\n            discriminator_input_include_probs: {\n                'h0': .8\n            },\n            discriminator_default_input_scale: 2.,\n            discriminator_input_scales: {\n                'h0': 1.25   \n            }\n            },\n        ...\n    },\n    ...\n}\n```\n mnist  `train`  50000  adversarial.AdversaryPair adversarial.Generator MLP MLP adversarial.AdversaryCost2 `__init__.py`  AdversaryCost2\n\n `get_samples_and_objectives`\n```python\ng=model.generator       # model is an instance of AdversaryPair\nd=model.discriminator\nX=data                  #  batch\nm=data.shape[space.get_batch_axis()]    #  batch \ny1=T.alloc(1,m,1)       #  m  1  label\ny0=T.alloc(0,m,1)       #  m  0  label\n# 1.  m  G  z\n# 2. G  m  S\nS,z,other_layers=g.sample_and_noise(m,\n    default_input_include_prob=self.generator_default_input_include_prob,   # 1\n    default_input_scale=self.generator_default_input_scale,                 # 1\n    all_g_layers=(self.infer_layer is not None)                         # False\n)\nif self.noise_both !=0:     # \n    ...\n# D  label  label\ny_hat1 = d.dropout_fprop(...)       # \ny_hat0 = d.dropout_fprop(...)\n# D d.layers[-1]  Sigmoid  KL \nd_obj = 0.5*(d.layers[-1].cost(y1,y_hat1)+d.layers[-1].cost(y0,y_hat0))\n# G G  D  y_hat0  label y1   \ng_obj = d.layers[-1].cost(y1,y_hat0)\nif model.inferer is not None:       # \n    ...\nelse:\n    i_obj = 0\nreturn S, d_obj, g_obj, i_obj       # D  G \n```\n `get_gradients` \n```python\ng=model.generator\nd=model.generator\nS,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   # \ng_params = g.get_params()\nd_params = d.get_params()\n# \nd_grads = T.grad(d_obj,d_params)\ng_grads = T.grad(g_obj,g_params)\nif self.scale_grads:    #  g_grads\n    S_grad = T.grad(g_obj, S)   # G  G \n    # S_grad \n    scale = T.maximum(1.,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))\n    #  g_grads\n    g_grads = [g_grad * scale for g_grad in g_grads]\n\n# \nrval = OrderDict()\nrval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg for dg in d_grads])))\nrval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg for gg in g_grads])))\n\nupdates = OrderDict()\nif self.alternate_g:\n    updates[self.now_train_generator]=1. - self.now_train_generator\nreturn rval, updates\n```\n Pylearn2/Theano \n\n log  1  `g_grads`  scale \n\n $S=\\theta_g \\cdot z$ $\\theta_g$ \n$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$\n\n S  D  y_0$y_0=\\theta_d \\cdot S$\n$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$\n G $\\nabla_S L$  D \n1.  L2  1 G  $\\nabla_{\\theta_g}L$   scale  $\\nabla_S L$  L2  `self.target_scale`1\n    G \n   ![](/images/GAN_fig2.png)<center>fig 2.  $G_0$  V(G,D)  $G_1$  V(G,D)  $D_1^{\\ast}$  G ~</center>\n\n    $D_0^{\\ast}$  $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$ $G_0$  $G_1$ G  V(G,D) $V(G_1,D_0^{\\ast}) < V(G_0,D_0^{\\ast})$ D  V(G,D) $V(G_1,D_1^{\\ast}) < V(G_0,D_0^{\\ast})$ $D_1^{\\ast}$  $D_0^{\\ast}$  G  D  G  G  G  D  G \n2.  L2 1 scale \n\n\n\n\n\n## \n zG  z  G(z)D  G(z)  (1)  D  G G  D  G D ","source":"_posts/GAN.md","raw":"---\ntitle: GAN\ndate: 2019-07-23 10:15:08\ntags: GAN\nmathjax: true\n---\n [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n\n# GAN\n## \n GAN G  DG D \n\n z $p_z(z)$G  z  $G(z;\\theta_g)$ G  MLP  $\\theta_g$D  MLP $D(x;\\theta_d)$  x  D  x  x $G(x)$  x  G  $D(G(z))$  G  $D(G(x))$  $\\log(1-D(G(z)))$ \n\n$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$\n\n D  log D  G  D  log \n\n 1 \n![](/images/GAN_fig1.png)\n\n 1  D  x  $p_x$ G  $p_g$  x  z  z  x  x=G(z) G   \n(a)  $p_g,\\ p_{data}$ D   \n(b)  D  $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$  \n(c) D  G(z) G   \n(d) G  D  $p_g=p_{data}$D $D(x)=1/2$\n\n\n![](/images/GAN_alg1.png)\n\nk  D  G  G  D \n\n(1)  GG  D $\\log (1-D(G(z)))$ log  G  $\\log D(G(z))$ G  D log \n\n## \n z  $p_z$  G  1 G G  $p_{data}$ $p_g=p_{data}$  (1) \n### \n__Proposition 1.__  GD \n$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$\n____   GD  V(G,D)  \n$$\\begin{aligned} V(G,D)&=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz\n\\\\\\\\ &=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$\n$\\forall (a,b) \\in \\Bbb R^2 \\setminus \\{0,0\\}$ $y \\rightarrow a \\log y+b \\log(1-y)$  (0,1)  $y=\\frac a {a+b}$  0  V(G,D)  x  D(x)  (2) \n\nD  $P(Y=y|x)$  log  binary cross-entropy x  $p_{data}$  y=1 x  $p_g$  y=0 D  $D_G^{\\ast}$  (1)   \n\n$$\\begin{aligned} C(G)&=\\max_D V(G,D)\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]\n\\\\\\\\ &=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$\n\n__Theorem 1.__  $p_g=p_{data}$  C(G)  -log4  \n\n____ \n\n1.   \n $p_g=p_{data}$ (2)  $D_G^{\\ast}(x)=1/2$ (4) \n$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$\n2.   \n   $$\\begin{aligned}C(G)&=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\\\\\ &=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\\\\\ &=-\\log4+KL \\left(p_{data} \\| \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g \\| \\frac {p_{data}+p_g} 2 \\right) \\\\\\\\ &=-\\log4 + 2\\cdot JSD(p_{data} \\| p_g) \\end{aligned}$$\n    KL  Kullback-Leibler JSD  Jensen-Shannon  JSD  $p_g=p_{data}$  0 C(G)=-log4 $p_g=p_{data}$  \n\n\n\n###  1 \n $p_g=p_{data}$__Proposition 2__  1 \n\n__Proposition 2.__  G  D  1  G  G  C(G)  $p_g$ \n$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$\n$p_g$  $p_{data}$\n\n____\n\n $V(G,D)=U(p_g,D)$  $p_g$ $p_g$  (5)  $U(p_g,D)$  $p_g$  D  ____  D  $p_g$D  $p_g$ \n\n-  $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ $f_{\\alpha}(x)$  $\\alpha$  x  $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$  $\\partial f_{\\beta}(x) \\in \\partial f(x)$\n\n$V(G,D)=U(p_g,D)$  $p_g$   $p_g$  D  D  D*  V(G,D)  (5) / $p_g$ $p_g$  $p_{data}$ Theorem 1\n\n (5)  1  SGD  $p_g$  $p_g$ D  $p_g$ \n\n $G(z;\\theta_g)$  $p_g$  $\\theta_g$  G  G  MLP $\\theta_g$  $p_g$  $p_g$   $\\theta_g$  MLP  G  (1)  $p_g$  batch $p_{data}$  batch  (1)  batch  log-likelihood function log \n\n## \n [adversarial](http://www.github.com/goodfeli/adversarial)\n\n>  Theano  Pylearn2github  GAN  [generative-models](https://github.com/wiseodd/generative-models)\n\n github  clone  adversarial  mnist \n\n mnist.yaml \n```yaml\n!obj:pylearn2.train.Train {         # \n    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {    #  mnist \n        which_set: 'train',                                 #  train  50000 \n        start: 0,\n        stop: 50000\n    },\n    model: !obj:adversarial.AdversaryPair {                 # GANG & D\n        generator: !obj:adversarial.Generator {             # G\n            noise: 'uniform',                               # noise \n            monitor_ll: 1,\n            mlp: !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     !obj:pylearn2.models.mlp.RectifiedLinear { #  ReLu  FC \n                         layer_name: 'h0',\n                         dim: 1200,                             #  output units \n                         irange: .05,\n                     },\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {     # FC  sigmoid\n                         init_bias: !obj:pylearn2.models.dbm.init_sigmoid_bias_from_marginals { dataset: *train},\n                         layer_name: 'y',\n                         irange: .05,\n                         dim: 784                               # 784=28x28 mnist \n                     }\n                    ],\n            nvis: 100,                                          # G \n        }},\n        discriminator:                                          # D\n            !obj:pylearn2.models.mlp.MLP {\n            layers: [\n                     ...\n                     !obj:pylearn2.models.mlp.Sigmoid {\n                         layer_name: 'y',\n                         dim: 1,                                # \n                         irange: .005\n                     }\n                    ],\n            nvis: 784,                                          # \n        },\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {      # \n        ...\n        cost: !obj:adversarial.AdversaryCost2 {                 # \n            scale_grads: 0,\n            #target_scale: 1.,\n            discriminator_default_input_include_prob: .5,\n            discriminator_input_include_probs: {\n                'h0': .8\n            },\n            discriminator_default_input_scale: 2.,\n            discriminator_input_scales: {\n                'h0': 1.25   \n            }\n            },\n        ...\n    },\n    ...\n}\n```\n mnist  `train`  50000  adversarial.AdversaryPair adversarial.Generator MLP MLP adversarial.AdversaryCost2 `__init__.py`  AdversaryCost2\n\n `get_samples_and_objectives`\n```python\ng=model.generator       # model is an instance of AdversaryPair\nd=model.discriminator\nX=data                  #  batch\nm=data.shape[space.get_batch_axis()]    #  batch \ny1=T.alloc(1,m,1)       #  m  1  label\ny0=T.alloc(0,m,1)       #  m  0  label\n# 1.  m  G  z\n# 2. G  m  S\nS,z,other_layers=g.sample_and_noise(m,\n    default_input_include_prob=self.generator_default_input_include_prob,   # 1\n    default_input_scale=self.generator_default_input_scale,                 # 1\n    all_g_layers=(self.infer_layer is not None)                         # False\n)\nif self.noise_both !=0:     # \n    ...\n# D  label  label\ny_hat1 = d.dropout_fprop(...)       # \ny_hat0 = d.dropout_fprop(...)\n# D d.layers[-1]  Sigmoid  KL \nd_obj = 0.5*(d.layers[-1].cost(y1,y_hat1)+d.layers[-1].cost(y0,y_hat0))\n# G G  D  y_hat0  label y1   \ng_obj = d.layers[-1].cost(y1,y_hat0)\nif model.inferer is not None:       # \n    ...\nelse:\n    i_obj = 0\nreturn S, d_obj, g_obj, i_obj       # D  G \n```\n `get_gradients` \n```python\ng=model.generator\nd=model.generator\nS,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   # \ng_params = g.get_params()\nd_params = d.get_params()\n# \nd_grads = T.grad(d_obj,d_params)\ng_grads = T.grad(g_obj,g_params)\nif self.scale_grads:    #  g_grads\n    S_grad = T.grad(g_obj, S)   # G  G \n    # S_grad \n    scale = T.maximum(1.,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))\n    #  g_grads\n    g_grads = [g_grad * scale for g_grad in g_grads]\n\n# \nrval = OrderDict()\nrval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg for dg in d_grads])))\nrval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg for gg in g_grads])))\n\nupdates = OrderDict()\nif self.alternate_g:\n    updates[self.now_train_generator]=1. - self.now_train_generator\nreturn rval, updates\n```\n Pylearn2/Theano \n\n log  1  `g_grads`  scale \n\n $S=\\theta_g \\cdot z$ $\\theta_g$ \n$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$\n\n S  D  y_0$y_0=\\theta_d \\cdot S$\n$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$\n G $\\nabla_S L$  D \n1.  L2  1 G  $\\nabla_{\\theta_g}L$   scale  $\\nabla_S L$  L2  `self.target_scale`1\n    G \n   ![](/images/GAN_fig2.png)<center>fig 2.  $G_0$  V(G,D)  $G_1$  V(G,D)  $D_1^{\\ast}$  G ~</center>\n\n    $D_0^{\\ast}$  $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$ $G_0$  $G_1$ G  V(G,D) $V(G_1,D_0^{\\ast}) < V(G_0,D_0^{\\ast})$ D  V(G,D) $V(G_1,D_1^{\\ast}) < V(G_0,D_0^{\\ast})$ $D_1^{\\ast}$  $D_0^{\\ast}$  G  D  G  G  G  D  G \n2.  L2 1 scale \n\n\n\n\n\n## \n zG  z  G(z)D  G(z)  (1)  D  G G  D  G D ","slug":"GAN","published":1,"updated":"2019-07-30T08:17:01.780Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379e3001qdgvczx4srz2a","content":"<p> <a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\" rel=\"noopener\">Generative Adversarial Nets</a></p>\n<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> GAN G  DG D </p>\n<p> z $p_z(z)$G  z  $G(z;\\theta_g)$ G  MLP  $\\theta_g$D  MLP $D(x;\\theta_d)$  x  D  x  x $G(x)$  x  G  $D(G(z))$  G  $D(G(x))$  $\\log(1-D(G(z)))$ </p>\n<p>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$</p>\n<p> D  log D  G  D  log </p>\n<p> 1 <br><img src=\"/images/GAN_fig1.png\" alt></p>\n<p> 1  D  x  $p_x$ G  $p_g$  x  z  z  x  x=G(z) G <br>(a)  $p_g,\\ p_{data}$ D <br>(b)  D  $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$<br>(c) D  G(z) G <br>(d) G  D  $p_g=p_{data}$D $D(x)=1/2$</p>\n<p><br><img src=\"/images/GAN_alg1.png\" alt></p>\n<p>k  D  G  G  D </p>\n<p>(1)  GG  D $\\log (1-D(G(z)))$ log  G  $\\log D(G(z))$ G  D log </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> z  $p_z$  G  1 G G  $p_{data}$ $p_g=p_{data}$  (1) </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong>Proposition 1.</strong>  GD <br>$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$<br><strong></strong>   GD  V(G,D)<br>$$\\begin{aligned} V(G,D)&amp;=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz<br>\\\\ &amp;=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$<br>$\\forall (a,b) \\in \\Bbb R^2 \\setminus {0,0}$ $y \\rightarrow a \\log y+b \\log(1-y)$  (0,1)  $y=\\frac a {a+b}$  0  V(G,D)  x  D(x)  (2) </p>\n<p>D  $P(Y=y|x)$  log  binary cross-entropy x  $p_{data}$  y=1 x  $p_g$  y=0 D  $D_G^{\\ast}$  (1)   </p>\n<p>$$\\begin{aligned} C(G)&amp;=\\max_D V(G,D)<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$</p>\n<p><strong>Theorem 1.</strong>  $p_g=p_{data}$  C(G)  -log4  </p>\n<p><strong></strong> </p>\n<ol>\n<li><br> $p_g=p_{data}$ (2)  $D_G^{\\ast}(x)=1/2$ (4) <br>$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$</li>\n<li><br>$$\\begin{aligned}C(G)&amp;=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\ &amp;=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\ &amp;=-\\log4+KL \\left(p_{data} | \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g | \\frac {p_{data}+p_g} 2 \\right) \\\\ &amp;=-\\log4 + 2\\cdot JSD(p_{data} | p_g) \\end{aligned}$$<br> KL  Kullback-Leibler JSD  Jensen-Shannon  JSD  $p_g=p_{data}$  0 C(G)=-log4 $p_g=p_{data}$  </li>\n</ol>\n<p></p>\n<h3 id=\"-1-\"><a href=\"#-1-\" class=\"headerlink\" title=\" 1 \"></a> 1 </h3><p> $p_g=p_{data}$<strong>Proposition 2</strong>  1 </p>\n<p><strong>Proposition 2.</strong>  G  D  1  G  G  C(G)  $p_g$<br>$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$<br>$p_g$  $p_{data}$</p>\n<p><strong></strong></p>\n<p> $V(G,D)=U(p_g,D)$  $p_g$ $p_g$  (5)  $U(p_g,D)$  $p_g$  D  <strong></strong>  D  $p_g$D  $p_g$ </p>\n<ul>\n<li> $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ $f_{\\alpha}(x)$  $\\alpha$  x  $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$  $\\partial f_{\\beta}(x) \\in \\partial f(x)$</li>\n</ul>\n<p>$V(G,D)=U(p_g,D)$  $p_g$   $p_g$  D  D  D*  V(G,D)  (5) / $p_g$ $p_g$  $p_{data}$ Theorem 1</p>\n<p> (5)  1  SGD  $p_g$  $p_g$ D  $p_g$ </p>\n<p> $G(z;\\theta_g)$  $p_g$  $\\theta_g$  G  G  MLP $\\theta_g$  $p_g$  $p_g$   $\\theta_g$  MLP  G  (1)  $p_g$  batch $p_{data}$  batch  (1)  batch  log-likelihood function log </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> <a href=\"http://www.github.com/goodfeli/adversarial\" target=\"_blank\" rel=\"noopener\">adversarial</a></p>\n<blockquote>\n<p> Theano  Pylearn2github  GAN  <a href=\"https://github.com/wiseodd/generative-models\" target=\"_blank\" rel=\"noopener\">generative-models</a></p>\n</blockquote>\n<p> github  clone  adversarial  mnist </p>\n<p> mnist.yaml </p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">!obj</span><span class=\"string\">:pylearn2.train.Train</span> <span class=\"string\">&#123;</span>         <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">    dataset:</span> <span class=\"meta\">&amp;train</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.datasets.mnist.MNIST</span> <span class=\"string\">&#123;</span>    <span class=\"comment\">#  mnist </span></span><br><span class=\"line\"><span class=\"attr\">        which_set:</span> <span class=\"string\">'train'</span><span class=\"string\">,</span>                                 <span class=\"comment\">#  train  50000 </span></span><br><span class=\"line\"><span class=\"attr\">        start:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">        stop:</span> <span class=\"number\">50000</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">    model:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryPair</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># GANG &amp; D</span></span><br><span class=\"line\"><span class=\"attr\">        generator:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.Generator</span> <span class=\"string\">&#123;</span>             <span class=\"comment\"># G</span></span><br><span class=\"line\"><span class=\"attr\">            noise:</span> <span class=\"string\">'uniform'</span><span class=\"string\">,</span>                               <span class=\"comment\"># noise </span></span><br><span class=\"line\"><span class=\"attr\">            monitor_ll:</span> <span class=\"number\">1</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            mlp:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">            layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.RectifiedLinear</span> <span class=\"string\">&#123;</span> <span class=\"comment\">#  ReLu  FC </span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'h0'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">1200</span><span class=\"string\">,</span>                             <span class=\"comment\">#  output units </span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;,</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span>     <span class=\"comment\"># FC  sigmoid</span></span><br><span class=\"line\"><span class=\"attr\">                         init_bias:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.dbm.init_sigmoid_bias_from_marginals</span> <span class=\"string\">&#123;</span> <span class=\"attr\">dataset:</span> <span class=\"string\">*train&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">784</span>                               <span class=\"comment\"># 784=28x28 mnist </span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\"><span class=\"attr\">            nvis:</span> <span class=\"number\">100</span><span class=\"string\">,</span>                                          <span class=\"comment\"># G </span></span><br><span class=\"line\">        <span class=\"string\">&#125;&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">        discriminator:</span>                                          <span class=\"comment\"># D</span></span><br><span class=\"line\">            <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">            layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">1</span><span class=\"string\">,</span>                                <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.005</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\"><span class=\"attr\">            nvis:</span> <span class=\"number\">784</span><span class=\"string\">,</span>                                          <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">    algorithm:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.training_algorithms.sgd.SGD</span> <span class=\"string\">&#123;</span>      <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"attr\">        cost:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryCost2</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">            scale_grads:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"comment\">#target_scale: 1.,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_default_input_include_prob:</span> <span class=\"number\">.5</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_input_include_probs:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                'h0':</span> <span class=\"number\">.8</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_default_input_scale:</span> <span class=\"number\">2.</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_input_scales:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                'h0':</span> <span class=\"number\">1.25</span>   </span><br><span class=\"line\">            <span class=\"string\">&#125;</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p> mnist  <code>train</code>  50000  adversarial.AdversaryPair adversarial.Generator MLP MLP adversarial.AdversaryCost2 <code>__init__.py</code>  AdversaryCost2</p>\n<p> <code>get_samples_and_objectives</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator       <span class=\"comment\"># model is an instance of AdversaryPair</span></span><br><span class=\"line\">d=model.discriminator</span><br><span class=\"line\">X=data                  <span class=\"comment\">#  batch</span></span><br><span class=\"line\">m=data.shape[space.get_batch_axis()]    <span class=\"comment\">#  batch </span></span><br><span class=\"line\">y1=T.alloc(<span class=\"number\">1</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\">#  m  1  label</span></span><br><span class=\"line\">y0=T.alloc(<span class=\"number\">0</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\">#  m  0  label</span></span><br><span class=\"line\"><span class=\"comment\"># 1.  m  G  z</span></span><br><span class=\"line\"><span class=\"comment\"># 2. G  m  S</span></span><br><span class=\"line\">S,z,other_layers=g.sample_and_noise(m,</span><br><span class=\"line\">    default_input_include_prob=self.generator_default_input_include_prob,   <span class=\"comment\"># 1</span></span><br><span class=\"line\">    default_input_scale=self.generator_default_input_scale,                 <span class=\"comment\"># 1</span></span><br><span class=\"line\">    all_g_layers=(self.infer_layer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>)                         <span class=\"comment\"># False</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.noise_both !=<span class=\"number\">0</span>:     <span class=\"comment\"># </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"comment\"># D  label  label</span></span><br><span class=\"line\">y_hat1 = d.dropout_fprop(...)       <span class=\"comment\"># </span></span><br><span class=\"line\">y_hat0 = d.dropout_fprop(...)</span><br><span class=\"line\"><span class=\"comment\"># D d.layers[-1]  Sigmoid  KL </span></span><br><span class=\"line\">d_obj = <span class=\"number\">0.5</span>*(d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat1)+d.layers[<span class=\"number\">-1</span>].cost(y0,y_hat0))</span><br><span class=\"line\"><span class=\"comment\"># G G  D  y_hat0  label y1   </span></span><br><span class=\"line\">g_obj = d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat0)</span><br><span class=\"line\"><span class=\"keyword\">if</span> model.inferer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:       <span class=\"comment\"># </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    i_obj = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> S, d_obj, g_obj, i_obj       <span class=\"comment\"># D  G </span></span><br></pre></td></tr></table></figure>\n\n<p> <code>get_gradients</code> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator</span><br><span class=\"line\">d=model.generator</span><br><span class=\"line\">S,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   <span class=\"comment\"># </span></span><br><span class=\"line\">g_params = g.get_params()</span><br><span class=\"line\">d_params = d.get_params()</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">d_grads = T.grad(d_obj,d_params)</span><br><span class=\"line\">g_grads = T.grad(g_obj,g_params)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.scale_grads:    <span class=\"comment\">#  g_grads</span></span><br><span class=\"line\">    S_grad = T.grad(g_obj, S)   <span class=\"comment\"># G  G </span></span><br><span class=\"line\">    <span class=\"comment\"># S_grad </span></span><br><span class=\"line\">    scale = T.maximum(<span class=\"number\">1.</span>,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))</span><br><span class=\"line\">    <span class=\"comment\">#  g_grads</span></span><br><span class=\"line\">    g_grads = [g_grad * scale <span class=\"keyword\">for</span> g_grad <span class=\"keyword\">in</span> g_grads]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">rval = OrderDict()</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg <span class=\"keyword\">for</span> dg <span class=\"keyword\">in</span> d_grads])))</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg <span class=\"keyword\">for</span> gg <span class=\"keyword\">in</span> g_grads])))</span><br><span class=\"line\"></span><br><span class=\"line\">updates = OrderDict()</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.alternate_g:</span><br><span class=\"line\">    updates[self.now_train_generator]=<span class=\"number\">1.</span> - self.now_train_generator</span><br><span class=\"line\"><span class=\"keyword\">return</span> rval, updates</span><br></pre></td></tr></table></figure>\n\n<p> Pylearn2/Theano </p>\n<p> log  1  <code>g_grads</code>  scale </p>\n<p> $S=\\theta_g \\cdot z$ $\\theta_g$ <br>$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$</p>\n<p> S  D  y_0$y_0=\\theta_d \\cdot S$<br>$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$<br> G $\\nabla_S L$  D </p>\n<ol>\n<li><p> L2  1 G  $\\nabla_{\\theta_g}L$   scale  $\\nabla_S L$  L2  <code>self.target_scale</code>1<br> G <br><img src=\"/images/GAN_fig2.png\" alt><center>fig 2.  $G_0$  V(G,D)  $G_1$  V(G,D)  $D_1^{\\ast}$  G ~</center></p>\n<p> $D_0^{\\ast}$  $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$ $G_0$  $G_1$ G  V(G,D) $V(G_1,D_0^{\\ast}) &lt; V(G_0,D_0^{\\ast})$ D  V(G,D) $V(G_1,D_1^{\\ast}) &lt; V(G_0,D_0^{\\ast})$ $D_1^{\\ast}$  $D_0^{\\ast}$  G  D  G  G  G  D  G </p>\n</li>\n<li><p> L2 1 scale </p>\n</li>\n</ol>\n<p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> zG  z  G(z)D  G(z)  (1)  D  G G  D  G D </p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\" rel=\"noopener\">Generative Adversarial Nets</a></p>\n<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> GAN G  DG D </p>\n<p> z $p_z(z)$G  z  $G(z;\\theta_g)$ G  MLP  $\\theta_g$D  MLP $D(x;\\theta_d)$  x  D  x  x $G(x)$  x  G  $D(G(z))$  G  $D(G(x))$  $\\log(1-D(G(z)))$ </p>\n<p>$$\\min_G \\max_D V(D,G)=\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_z(z)}[\\log(1-D(G(z)))] \\qquad (1)$$</p>\n<p> D  log D  G  D  log </p>\n<p> 1 <br><img src=\"/images/GAN_fig1.png\" alt></p>\n<p> 1  D  x  $p_x$ G  $p_g$  x  z  z  x  x=G(z) G <br>(a)  $p_g,\\ p_{data}$ D <br>(b)  D  $D^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)}$<br>(c) D  G(z) G <br>(d) G  D  $p_g=p_{data}$D $D(x)=1/2$</p>\n<p><br><img src=\"/images/GAN_alg1.png\" alt></p>\n<p>k  D  G  G  D </p>\n<p>(1)  GG  D $\\log (1-D(G(z)))$ log  G  $\\log D(G(z))$ G  D log </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> z  $p_z$  G  1 G G  $p_{data}$ $p_g=p_{data}$  (1) </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong>Proposition 1.</strong>  GD <br>$$D_G^{\\ast}(x)=\\frac {p_{data}(x)}{p_{data}(x)+p_g(x)} \\qquad (2)$$<br><strong></strong>   GD  V(G,D)<br>$$\\begin{aligned} V(G,D)&amp;=\\int_x p_{data}(x) \\log D(x) dx+\\int_z p_z(z) \\log (1-D(g(z))) dz<br>\\\\ &amp;=\\int_x p_{data}(x) \\log D(x)+p_g(x) \\log(1-D(x))dx \\end{aligned}$$<br>$\\forall (a,b) \\in \\Bbb R^2 \\setminus {0,0}$ $y \\rightarrow a \\log y+b \\log(1-y)$  (0,1)  $y=\\frac a {a+b}$  0  V(G,D)  x  D(x)  (2) </p>\n<p>D  $P(Y=y|x)$  log  binary cross-entropy x  $p_{data}$  y=1 x  $p_g$  y=0 D  $D_G^{\\ast}$  (1)   </p>\n<p>$$\\begin{aligned} C(G)&amp;=\\max_D V(G,D)<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{z \\sim p_z} [\\log(1-D_G^{\\ast}(G(z)))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))]<br>\\\\ &amp;=\\Bbb E_{x \\sim p_{data}} \\left[\\log \\frac {P_{data}(x)} {p_{data}(x)+p_g(x)} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {p_{data}(x)+p_g(x)}\\right] \\qquad(4) \\end{aligned}$$</p>\n<p><strong>Theorem 1.</strong>  $p_g=p_{data}$  C(G)  -log4  </p>\n<p><strong></strong> </p>\n<ol>\n<li><br> $p_g=p_{data}$ (2)  $D_G^{\\ast}(x)=1/2$ (4) <br>$$C(G)=\\Bbb E_{x \\sim p_{data}}[-\\log 2]+\\Bbb E_{x \\sim p_g}[-\\log 2] \\equiv -\\log 4$$</li>\n<li><br>$$\\begin{aligned}C(G)&amp;=C(G)+\\Bbb E_{x \\sim p_{data}}[\\log 2]+\\Bbb E_{x \\sim p_g}[\\log 2]  -\\log 4 \\\\ &amp;=-\\log4 +\\Bbb E_{x \\sim p_{data}}\\left[\\log \\frac {P_{data}(x)} {\\frac {p_{data}(x)+p_g(x)} 2} \\right]+\\Bbb E_{x \\sim p_g} \\left[\\log \\frac {p_g(x)} {\\frac {p_{data}(x)+p_g(x)} 2}\\right] \\\\ &amp;=-\\log4+KL \\left(p_{data} | \\frac {p_{data}+p_g} 2 \\right)+KL \\left(p_g | \\frac {p_{data}+p_g} 2 \\right) \\\\ &amp;=-\\log4 + 2\\cdot JSD(p_{data} | p_g) \\end{aligned}$$<br> KL  Kullback-Leibler JSD  Jensen-Shannon  JSD  $p_g=p_{data}$  0 C(G)=-log4 $p_g=p_{data}$  </li>\n</ol>\n<p></p>\n<h3 id=\"-1-\"><a href=\"#-1-\" class=\"headerlink\" title=\" 1 \"></a> 1 </h3><p> $p_g=p_{data}$<strong>Proposition 2</strong>  1 </p>\n<p><strong>Proposition 2.</strong>  G  D  1  G  G  C(G)  $p_g$<br>$$\\Bbb E_{x \\sim p_{data}}[\\log D_G^{\\ast}(x)] + \\Bbb E_{x \\sim p_g} [\\log(1-D_G^{\\ast}(x))] \\qquad(5)$$<br>$p_g$  $p_{data}$</p>\n<p><strong></strong></p>\n<p> $V(G,D)=U(p_g,D)$  $p_g$ $p_g$  (5)  $U(p_g,D)$  $p_g$  D  <strong></strong>  D  $p_g$D  $p_g$ </p>\n<ul>\n<li> $f(x)=\\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$ $f_{\\alpha}(x)$  $\\alpha$  x  $\\beta=\\arg \\sup_{\\alpha \\in \\mathcal A} f_{\\alpha}(x)$  $\\partial f_{\\beta}(x) \\in \\partial f(x)$</li>\n</ul>\n<p>$V(G,D)=U(p_g,D)$  $p_g$   $p_g$  D  D  D*  V(G,D)  (5) / $p_g$ $p_g$  $p_{data}$ Theorem 1</p>\n<p> (5)  1  SGD  $p_g$  $p_g$ D  $p_g$ </p>\n<p> $G(z;\\theta_g)$  $p_g$  $\\theta_g$  G  G  MLP $\\theta_g$  $p_g$  $p_g$   $\\theta_g$  MLP  G  (1)  $p_g$  batch $p_{data}$  batch  (1)  batch  log-likelihood function log </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> <a href=\"http://www.github.com/goodfeli/adversarial\" target=\"_blank\" rel=\"noopener\">adversarial</a></p>\n<blockquote>\n<p> Theano  Pylearn2github  GAN  <a href=\"https://github.com/wiseodd/generative-models\" target=\"_blank\" rel=\"noopener\">generative-models</a></p>\n</blockquote>\n<p> github  clone  adversarial  mnist </p>\n<p> mnist.yaml </p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">!obj</span><span class=\"string\">:pylearn2.train.Train</span> <span class=\"string\">&#123;</span>         <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">    dataset:</span> <span class=\"meta\">&amp;train</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.datasets.mnist.MNIST</span> <span class=\"string\">&#123;</span>    <span class=\"comment\">#  mnist </span></span><br><span class=\"line\"><span class=\"attr\">        which_set:</span> <span class=\"string\">'train'</span><span class=\"string\">,</span>                                 <span class=\"comment\">#  train  50000 </span></span><br><span class=\"line\"><span class=\"attr\">        start:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">        stop:</span> <span class=\"number\">50000</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">    model:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryPair</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># GANG &amp; D</span></span><br><span class=\"line\"><span class=\"attr\">        generator:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.Generator</span> <span class=\"string\">&#123;</span>             <span class=\"comment\"># G</span></span><br><span class=\"line\"><span class=\"attr\">            noise:</span> <span class=\"string\">'uniform'</span><span class=\"string\">,</span>                               <span class=\"comment\"># noise </span></span><br><span class=\"line\"><span class=\"attr\">            monitor_ll:</span> <span class=\"number\">1</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            mlp:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">            layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.RectifiedLinear</span> <span class=\"string\">&#123;</span> <span class=\"comment\">#  ReLu  FC </span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'h0'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">1200</span><span class=\"string\">,</span>                             <span class=\"comment\">#  output units </span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;,</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span>     <span class=\"comment\"># FC  sigmoid</span></span><br><span class=\"line\"><span class=\"attr\">                         init_bias:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.dbm.init_sigmoid_bias_from_marginals</span> <span class=\"string\">&#123;</span> <span class=\"attr\">dataset:</span> <span class=\"string\">*train&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.05</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">784</span>                               <span class=\"comment\"># 784=28x28 mnist </span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\"><span class=\"attr\">            nvis:</span> <span class=\"number\">100</span><span class=\"string\">,</span>                                          <span class=\"comment\"># G </span></span><br><span class=\"line\">        <span class=\"string\">&#125;&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">        discriminator:</span>                                          <span class=\"comment\"># D</span></span><br><span class=\"line\">            <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.MLP</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">            layers:</span> <span class=\"string\">[</span></span><br><span class=\"line\">                     <span class=\"string\">...</span></span><br><span class=\"line\">                     <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.models.mlp.Sigmoid</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                         layer_name:</span> <span class=\"string\">'y'</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">                         dim:</span> <span class=\"number\">1</span><span class=\"string\">,</span>                                <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">                         irange:</span> <span class=\"number\">.005</span></span><br><span class=\"line\">                     <span class=\"string\">&#125;</span></span><br><span class=\"line\">                    <span class=\"string\">],</span></span><br><span class=\"line\"><span class=\"attr\">            nvis:</span> <span class=\"number\">784</span><span class=\"string\">,</span>                                          <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">    algorithm:</span> <span class=\"type\">!obj</span><span class=\"string\">:pylearn2.training_algorithms.sgd.SGD</span> <span class=\"string\">&#123;</span>      <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"attr\">        cost:</span> <span class=\"type\">!obj</span><span class=\"string\">:adversarial.AdversaryCost2</span> <span class=\"string\">&#123;</span>                 <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"attr\">            scale_grads:</span> <span class=\"number\">0</span><span class=\"string\">,</span></span><br><span class=\"line\">            <span class=\"comment\">#target_scale: 1.,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_default_input_include_prob:</span> <span class=\"number\">.5</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_input_include_probs:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                'h0':</span> <span class=\"number\">.8</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_default_input_scale:</span> <span class=\"number\">2.</span><span class=\"string\">,</span></span><br><span class=\"line\"><span class=\"attr\">            discriminator_input_scales:</span> <span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">                'h0':</span> <span class=\"number\">1.25</span>   </span><br><span class=\"line\">            <span class=\"string\">&#125;</span></span><br><span class=\"line\">            <span class=\"string\">&#125;,</span></span><br><span class=\"line\">        <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"string\">&#125;,</span></span><br><span class=\"line\">    <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p> mnist  <code>train</code>  50000  adversarial.AdversaryPair adversarial.Generator MLP MLP adversarial.AdversaryCost2 <code>__init__.py</code>  AdversaryCost2</p>\n<p> <code>get_samples_and_objectives</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator       <span class=\"comment\"># model is an instance of AdversaryPair</span></span><br><span class=\"line\">d=model.discriminator</span><br><span class=\"line\">X=data                  <span class=\"comment\">#  batch</span></span><br><span class=\"line\">m=data.shape[space.get_batch_axis()]    <span class=\"comment\">#  batch </span></span><br><span class=\"line\">y1=T.alloc(<span class=\"number\">1</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\">#  m  1  label</span></span><br><span class=\"line\">y0=T.alloc(<span class=\"number\">0</span>,m,<span class=\"number\">1</span>)       <span class=\"comment\">#  m  0  label</span></span><br><span class=\"line\"><span class=\"comment\"># 1.  m  G  z</span></span><br><span class=\"line\"><span class=\"comment\"># 2. G  m  S</span></span><br><span class=\"line\">S,z,other_layers=g.sample_and_noise(m,</span><br><span class=\"line\">    default_input_include_prob=self.generator_default_input_include_prob,   <span class=\"comment\"># 1</span></span><br><span class=\"line\">    default_input_scale=self.generator_default_input_scale,                 <span class=\"comment\"># 1</span></span><br><span class=\"line\">    all_g_layers=(self.infer_layer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>)                         <span class=\"comment\"># False</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.noise_both !=<span class=\"number\">0</span>:     <span class=\"comment\"># </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"comment\"># D  label  label</span></span><br><span class=\"line\">y_hat1 = d.dropout_fprop(...)       <span class=\"comment\"># </span></span><br><span class=\"line\">y_hat0 = d.dropout_fprop(...)</span><br><span class=\"line\"><span class=\"comment\"># D d.layers[-1]  Sigmoid  KL </span></span><br><span class=\"line\">d_obj = <span class=\"number\">0.5</span>*(d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat1)+d.layers[<span class=\"number\">-1</span>].cost(y0,y_hat0))</span><br><span class=\"line\"><span class=\"comment\"># G G  D  y_hat0  label y1   </span></span><br><span class=\"line\">g_obj = d.layers[<span class=\"number\">-1</span>].cost(y1,y_hat0)</span><br><span class=\"line\"><span class=\"keyword\">if</span> model.inferer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:       <span class=\"comment\"># </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    i_obj = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> S, d_obj, g_obj, i_obj       <span class=\"comment\"># D  G </span></span><br></pre></td></tr></table></figure>\n\n<p> <code>get_gradients</code> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g=model.generator</span><br><span class=\"line\">d=model.generator</span><br><span class=\"line\">S,d_obj,g_obj,i_obj = self.get_samples_and_objectives(model,data)   <span class=\"comment\"># </span></span><br><span class=\"line\">g_params = g.get_params()</span><br><span class=\"line\">d_params = d.get_params()</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">d_grads = T.grad(d_obj,d_params)</span><br><span class=\"line\">g_grads = T.grad(g_obj,g_params)</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.scale_grads:    <span class=\"comment\">#  g_grads</span></span><br><span class=\"line\">    S_grad = T.grad(g_obj, S)   <span class=\"comment\"># G  G </span></span><br><span class=\"line\">    <span class=\"comment\"># S_grad </span></span><br><span class=\"line\">    scale = T.maximum(<span class=\"number\">1.</span>,self.target_scale/T.sqrt(T.sqr(S_grad).sum()))</span><br><span class=\"line\">    <span class=\"comment\">#  g_grads</span></span><br><span class=\"line\">    g_grads = [g_grad * scale <span class=\"keyword\">for</span> g_grad <span class=\"keyword\">in</span> g_grads]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">rval = OrderDict()</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(d_params, [self.now_train_discriminator * dg <span class=\"keyword\">for</span> dg <span class=\"keyword\">in</span> d_grads])))</span><br><span class=\"line\">rval.update(OrderedDict(safe_zip(g_params, [self.now_train_generator * gg <span class=\"keyword\">for</span> gg <span class=\"keyword\">in</span> g_grads])))</span><br><span class=\"line\"></span><br><span class=\"line\">updates = OrderDict()</span><br><span class=\"line\"><span class=\"keyword\">if</span> self.alternate_g:</span><br><span class=\"line\">    updates[self.now_train_generator]=<span class=\"number\">1.</span> - self.now_train_generator</span><br><span class=\"line\"><span class=\"keyword\">return</span> rval, updates</span><br></pre></td></tr></table></figure>\n\n<p> Pylearn2/Theano </p>\n<p> log  1  <code>g_grads</code>  scale </p>\n<p> $S=\\theta_g \\cdot z$ $\\theta_g$ <br>$$\\nabla_{\\theta_g}L=\\nabla_S L \\cdot \\frac {\\partial S}{\\partial \\theta_g}$$</p>\n<p> S  D  y_0$y_0=\\theta_d \\cdot S$<br>$$\\nabla_S L=\\frac {dL}{dy_0}\\cdot \\theta_d$$<br> G $\\nabla_S L$  D </p>\n<ol>\n<li><p> L2  1 G  $\\nabla_{\\theta_g}L$   scale  $\\nabla_S L$  L2  <code>self.target_scale</code>1<br> G <br><img src=\"/images/GAN_fig2.png\" alt><center>fig 2.  $G_0$  V(G,D)  $G_1$  V(G,D)  $D_1^{\\ast}$  G ~</center></p>\n<p> $D_0^{\\ast}$  $\\max_D V(G_0,D_0)=V(G_0,D_0^{\\ast})$ $G_0$  $G_1$ G  V(G,D) $V(G_1,D_0^{\\ast}) &lt; V(G_0,D_0^{\\ast})$ D  V(G,D) $V(G_1,D_1^{\\ast}) &lt; V(G_0,D_0^{\\ast})$ $D_1^{\\ast}$  $D_0^{\\ast}$  G  D  G  G  G  D  G </p>\n</li>\n<li><p> L2 1 scale </p>\n</li>\n</ol>\n<p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> zG  z  G(z)D  G(z)  (1)  D  G G  D  G D </p>\n"},{"title":"GIoU","date":"2019-06-13T07:00:48.000Z","mathjax":true,"_content":" [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n# \nIoU benchmarksbboxIoU2D bboxIoU  IoU bboxIou GIou GIoU  sota benchmarks PASCAL VOC  MS COCO IoU  GIoU \n# \n2D/3D bbox bbox IoU  $l_1, l_2$ \n\nIoU  bbox IoU\n\n$l_n$IoU2D1(a)\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\nboxgt box$(x_1,y_1,x_2,y_2)$boxcorner$l_2$ gt box  corner  box  corner  $l_2$ IoUbbox1(b)\n\n IoU  IoU $l_n$  bbox bbox  center+size $(x_c,y_c)$ $(w,h)$  box size\n\nsota  anchor faster-rcnnIoU\n\nIoU$ndim \\ge 2$IoUIoU IoU IouIoUIouIoU0IoU00\n\nIoU(a)  IoU (b)  IoU (c) IoUIoUGIoUGIoU sota benchmarksIoUGIoU\n\n\n\n- GIoU\n- GIoU\n- GIoUsota Faster R-CNNMask R-CNNYOLO v3benchmark\n\n# \n____ IoUboxIoUPASCAL VOCmAP IoU=0.5IoUmAPIoUmAPIoUMS COCO benchmark IoUmAP\n\n__bbox__ 2Dbboxbbox\n1. YOLO v1\n   \n   YOLO v1  bbox $(x_c,y_c,w,h)$scale(w,h)$(w-\\hat w)^2+(h-\\hat h)^2$  $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$\n2. R-CNN\n   \n   R-CNNselective searchboxesbboxsizescalesize log$l_2$MSE\n3. Fast R-CNN\n   \n   Fast R-CNN $l_1$-smooth  $l_2$ \n4. Faster R-CNN\n   \n   Faster R-CNNanchor boxessizeanchor boxes1:3batch 128proposalsFast R-CNNRetinaNet  focal loss\n\nbboxGIoU  bbox IoU \n\n__IoU__ IoUIoUbboxIoUGIoUIoU\n\n# IoU\n $A,B \\subseteq S \\in \\mathbb{R}^n$  IoU \n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n2D/3D\n- IoU\n  \n  IoU $\\mathcal L_{IoU}=1-IoU$ $\\mathcal L_{IoU}$  IoU \n- IoUA B S \n   \nIoU\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$IoUA B\n\nIoU GIoU\n\n $A, B \\subseteq S \\in \\mathbb S^n$ S  A  B  CC C  C  A  B  C  A  B  IoU  GIoU/ 2D/3D\n\n1\n___\n1GIoU\n___\n  $A,B \\subseteq S \\in \\mathbb S^n$\n\n GIoU\n-  S  A B  C\n-  IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n-  GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\nGIoU \n\n-  IoU GIoU \n  \n  IoU  $\\mathcal L_{GIoU} = 1-GIoU$\n-  IoU GIoU \n- GIoU  IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$ A B GIoU  IoU $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$\n- IoU  GIoU \n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$ GIoU $-1 \\le GIoU(A,B) \\le 1$\n\n  \n   *  IoU  A B $|A \\cup B|=|A \\cap B|$$GIoU =IoU=1$\n   *  A B  C  0GIoU  -1$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$\n\nGIoU  IoU  IoU 2D/3D GIoU  IoU 2D  GIoU GIoU  3D  GIoU \n\n## GIoUBBox\n GIoU  IoU \n\n2D bbox  GIoU  A B  A B  A B  min  max  A B  A B  x  x  $x^{tl} < x^{br}$ $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$ $x_B^{tl} \\le x_A^{tl}<x_B^{br}$  $x_A^{tl} \\le x_B^{tl}<x_A^{br}$\n\nminmax ReLU 2  IoU  GIoU  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ \n_________\n2IoUGIoUBBox\n_________\n $B^p$  GT  $B^g$ $B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1.   box \n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2.  GT box \n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3.  box \n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. \n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5.  c\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6.  c \n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7.  IoU\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$ $\\mathcal U = A^p+A^g-\\mathcal I$\n8.  GIoU\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9.  GIoU \n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\nbboxIoU=00GIoU 3GIoU  IoU  IoU 2 \n\n![](/images/GIoU_fig2.png)\n\n21 2D pair IoU  GIoU $IoU \\le 0.2, \\ GIoU \\le 0.2$GIoU  IoU GIoU  GIoU $\\mathcal L_{GIoU}$ IoU $\\mathcal L_{IoU}$IoU\n\n### \n0\n\n GT box 0$A^g > 0$214bbox$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$$\\mathcal U \\ge A^g$ $\\mathcal U > 0$ IoU  $\\mathcal U \\ge \\mathcal I$ $0 \\le IoU \\le 1$ IoU  $0 \\le \\mathcal L_{IoU} \\le 1$\n\n GIoU  $\\frac {A^c-\\mathcal U} {A^c}$ A B  A B  $A^c \\ge \\mathcal U > 0$ $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$$\\frac {A^c-\\mathcal U} {A^c} <1$ A B  A B  size  A B  $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$ $-1 < GIoU \\le 1$ $-1 \\le GIoU \\le 1$\n\n### IoU=0$\\mathcal L_{GIoU}$\nGIoU  $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$ $B^p$  $B^g$  $\\mathcal I=0, IoU=0$ GIoU  $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$ GIoU  $\\frac {\\mathcal U}{A^c}$ $0\\le \\frac {\\mathcal U}{A^c} \\le 1$ $A^c$ $\\mathcal U$ $\\mathcal I=0$ $\\mathcal U=A^p+A^g$ $A^g$  $A^p$ $A^c$  $A^p$ $B^p$  $B^g$ \n\n# \n bbox  $\\mathcal L_{GIoU}$ 2D Faster R-CNNMask R-CNN  YOLO v3 Faster R-CNN/Mask R-CNN  $l_1$-smooth  YOLO v3  MSE  $\\mathcal L_{GIoU}$ baseline  $\\mathcal L_{IoU}$  baseline\n\n____ PASCAL VOC  MS COCO \n\n____  MS COCO  mAP IoU  mAP IoU mAP $IoU=\\{.5,.55,...,.95\\}$ IoU  mAP  __AP__ GIoU  IoU  $GIoU=\\{.5,.55,...,.95\\}$ mAP __AP__ IoU  GIoU  0.75  mAP __AP75__\n\n## YOLO v3\n____  YOLO v3  Darknet  Baseline  MSE  DarkNet-608  backbone YOLO v3  IoU  GIoU  YOLO v3  MSE  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ MSE  YOLO v3  bbox \n\n## Faster R-CNN  Mask R-CNN\n____  Faster R-CNN/Mask R-CNN  PyTorch baseline$l_1$-smooth ResNet-50  backbone IoU  GIoU RPN $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ YOLO v3  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$  10\n\n# \n GIoU GIoU  IoU  IoU  IoU  2D/3D GIoU \n\n GIoU  GIoU / GIoU  bbox  GIoU  sota GIoU  bbox  2D bbox \n\n# \n","source":"_posts/GIoU.md","raw":"---\ntitle: GIoU\ndate: 2019-06-13 15:00:48\ntags: object detection\nmathjax: true\n---\n [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n# \nIoU benchmarksbboxIoU2D bboxIoU  IoU bboxIou GIou GIoU  sota benchmarks PASCAL VOC  MS COCO IoU  GIoU \n# \n2D/3D bbox bbox IoU  $l_1, l_2$ \n\nIoU  bbox IoU\n\n$l_n$IoU2D1(a)\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\nboxgt box$(x_1,y_1,x_2,y_2)$boxcorner$l_2$ gt box  corner  box  corner  $l_2$ IoUbbox1(b)\n\n IoU  IoU $l_n$  bbox bbox  center+size $(x_c,y_c)$ $(w,h)$  box size\n\nsota  anchor faster-rcnnIoU\n\nIoU$ndim \\ge 2$IoUIoU IoU IouIoUIouIoU0IoU00\n\nIoU(a)  IoU (b)  IoU (c) IoUIoUGIoUGIoU sota benchmarksIoUGIoU\n\n\n\n- GIoU\n- GIoU\n- GIoUsota Faster R-CNNMask R-CNNYOLO v3benchmark\n\n# \n____ IoUboxIoUPASCAL VOCmAP IoU=0.5IoUmAPIoUmAPIoUMS COCO benchmark IoUmAP\n\n__bbox__ 2Dbboxbbox\n1. YOLO v1\n   \n   YOLO v1  bbox $(x_c,y_c,w,h)$scale(w,h)$(w-\\hat w)^2+(h-\\hat h)^2$  $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$\n2. R-CNN\n   \n   R-CNNselective searchboxesbboxsizescalesize log$l_2$MSE\n3. Fast R-CNN\n   \n   Fast R-CNN $l_1$-smooth  $l_2$ \n4. Faster R-CNN\n   \n   Faster R-CNNanchor boxessizeanchor boxes1:3batch 128proposalsFast R-CNNRetinaNet  focal loss\n\nbboxGIoU  bbox IoU \n\n__IoU__ IoUIoUbboxIoUGIoUIoU\n\n# IoU\n $A,B \\subseteq S \\in \\mathbb{R}^n$  IoU \n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n2D/3D\n- IoU\n  \n  IoU $\\mathcal L_{IoU}=1-IoU$ $\\mathcal L_{IoU}$  IoU \n- IoUA B S \n   \nIoU\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$IoUA B\n\nIoU GIoU\n\n $A, B \\subseteq S \\in \\mathbb S^n$ S  A  B  CC C  C  A  B  C  A  B  IoU  GIoU/ 2D/3D\n\n1\n___\n1GIoU\n___\n  $A,B \\subseteq S \\in \\mathbb S^n$\n\n GIoU\n-  S  A B  C\n-  IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n-  GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\nGIoU \n\n-  IoU GIoU \n  \n  IoU  $\\mathcal L_{GIoU} = 1-GIoU$\n-  IoU GIoU \n- GIoU  IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$ A B GIoU  IoU $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$\n- IoU  GIoU \n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$ GIoU $-1 \\le GIoU(A,B) \\le 1$\n\n  \n   *  IoU  A B $|A \\cup B|=|A \\cap B|$$GIoU =IoU=1$\n   *  A B  C  0GIoU  -1$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$\n\nGIoU  IoU  IoU 2D/3D GIoU  IoU 2D  GIoU GIoU  3D  GIoU \n\n## GIoUBBox\n GIoU  IoU \n\n2D bbox  GIoU  A B  A B  A B  min  max  A B  A B  x  x  $x^{tl} < x^{br}$ $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$ $x_B^{tl} \\le x_A^{tl}<x_B^{br}$  $x_A^{tl} \\le x_B^{tl}<x_A^{br}$\n\nminmax ReLU 2  IoU  GIoU  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ \n_________\n2IoUGIoUBBox\n_________\n $B^p$  GT  $B^g$ $B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1.   box \n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2.  GT box \n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3.  box \n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. \n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5.  c\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6.  c \n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7.  IoU\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$ $\\mathcal U = A^p+A^g-\\mathcal I$\n8.  GIoU\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9.  GIoU \n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\nbboxIoU=00GIoU 3GIoU  IoU  IoU 2 \n\n![](/images/GIoU_fig2.png)\n\n21 2D pair IoU  GIoU $IoU \\le 0.2, \\ GIoU \\le 0.2$GIoU  IoU GIoU  GIoU $\\mathcal L_{GIoU}$ IoU $\\mathcal L_{IoU}$IoU\n\n### \n0\n\n GT box 0$A^g > 0$214bbox$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$$\\mathcal U \\ge A^g$ $\\mathcal U > 0$ IoU  $\\mathcal U \\ge \\mathcal I$ $0 \\le IoU \\le 1$ IoU  $0 \\le \\mathcal L_{IoU} \\le 1$\n\n GIoU  $\\frac {A^c-\\mathcal U} {A^c}$ A B  A B  $A^c \\ge \\mathcal U > 0$ $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$$\\frac {A^c-\\mathcal U} {A^c} <1$ A B  A B  size  A B  $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$ $-1 < GIoU \\le 1$ $-1 \\le GIoU \\le 1$\n\n### IoU=0$\\mathcal L_{GIoU}$\nGIoU  $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$ $B^p$  $B^g$  $\\mathcal I=0, IoU=0$ GIoU  $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$ GIoU  $\\frac {\\mathcal U}{A^c}$ $0\\le \\frac {\\mathcal U}{A^c} \\le 1$ $A^c$ $\\mathcal U$ $\\mathcal I=0$ $\\mathcal U=A^p+A^g$ $A^g$  $A^p$ $A^c$  $A^p$ $B^p$  $B^g$ \n\n# \n bbox  $\\mathcal L_{GIoU}$ 2D Faster R-CNNMask R-CNN  YOLO v3 Faster R-CNN/Mask R-CNN  $l_1$-smooth  YOLO v3  MSE  $\\mathcal L_{GIoU}$ baseline  $\\mathcal L_{IoU}$  baseline\n\n____ PASCAL VOC  MS COCO \n\n____  MS COCO  mAP IoU  mAP IoU mAP $IoU=\\{.5,.55,...,.95\\}$ IoU  mAP  __AP__ GIoU  IoU  $GIoU=\\{.5,.55,...,.95\\}$ mAP __AP__ IoU  GIoU  0.75  mAP __AP75__\n\n## YOLO v3\n____  YOLO v3  Darknet  Baseline  MSE  DarkNet-608  backbone YOLO v3  IoU  GIoU  YOLO v3  MSE  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ MSE  YOLO v3  bbox \n\n## Faster R-CNN  Mask R-CNN\n____  Faster R-CNN/Mask R-CNN  PyTorch baseline$l_1$-smooth ResNet-50  backbone IoU  GIoU RPN $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ YOLO v3  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$  10\n\n# \n GIoU GIoU  IoU  IoU  IoU  2D/3D GIoU \n\n GIoU  GIoU / GIoU  bbox  GIoU  sota GIoU  bbox  2D bbox \n\n# \n","slug":"GIoU","published":1,"updated":"2019-08-01T06:24:28.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379e4001rdgvccnao3c28","content":"<p> <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>IoU benchmarksbboxIoU2D bboxIoU  IoU bboxIou GIou GIoU  sota benchmarks PASCAL VOC  MS COCO IoU  GIoU </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>2D/3D bbox bbox IoU  $l_1, l_2$ </p>\n<p>IoU  bbox IoU</p>\n<p>$l_n$IoU2D1(a)<br><img src=\"/images/GIoU_fig1.png\" alt></p>\n<!--  -->\n\n<p>boxgt box$(x_1,y_1,x_2,y_2)$boxcorner$l_2$ gt box  corner  box  corner  $l_2$ IoUbbox1(b)</p>\n<p> IoU  IoU $l_n$  bbox bbox  center+size $(x_c,y_c)$ $(w,h)$  box size</p>\n<p>sota  anchor faster-rcnnIoU</p>\n<p>IoU$ndim \\ge 2$IoUIoU IoU IouIoUIouIoU0IoU00</p>\n<p>IoU(a)  IoU (b)  IoU (c) IoUIoUGIoUGIoU sota benchmarksIoUGIoU</p>\n<p></p>\n<ul>\n<li>GIoU</li>\n<li>GIoU</li>\n<li>GIoUsota Faster R-CNNMask R-CNNYOLO v3benchmark</li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong></strong> IoUboxIoUPASCAL VOCmAP IoU=0.5IoUmAPIoUmAPIoUMS COCO benchmark IoUmAP</p>\n<p><strong>bbox</strong> 2Dbboxbbox</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1  bbox $(x_c,y_c,w,h)$scale(w,h)$(w-\\hat w)^2+(h-\\hat h)^2$  $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNNselective searchboxesbboxsizescalesize log$l_2$MSE</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN $l_1$-smooth  $l_2$ </p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNNanchor boxessizeanchor boxes1:3batch 128proposalsFast R-CNNRetinaNet  focal loss</p>\n</li>\n</ol>\n<p>bboxGIoU  bbox IoU </p>\n<p><strong>IoU</strong> IoUIoUbboxIoUGIoUIoU</p>\n<h1 id=\"IoU\"><a href=\"#IoU\" class=\"headerlink\" title=\"IoU\"></a>IoU</h1><p> $A,B \\subseteq S \\in \\mathbb{R}^n$  IoU <br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>2D/3D</p>\n<ul>\n<li><p>IoU</p>\n<p>IoU $\\mathcal L_{IoU}=1-IoU$ $\\mathcal L_{IoU}$  IoU </p>\n</li>\n<li><p>IoUA B S </p>\n</li>\n</ul>\n<p>IoU</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$IoUA B</li>\n</ul>\n<p>IoU GIoU</p>\n<p> $A, B \\subseteq S \\in \\mathbb S^n$ S  A  B  CC C  C  A  B  C  A  B  IoU  GIoU/ 2D/3D</p>\n<p>1</p>\n<hr>\n<p>1GIoU</p>\n<hr>\n<p>  $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p> GIoU</p>\n<ul>\n<li><p> S  A B  C</p>\n</li>\n<li><p> IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p> GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>GIoU </p>\n<ul>\n<li><p> IoU GIoU </p>\n<p>IoU  $\\mathcal L_{GIoU} = 1-GIoU$</p>\n</li>\n<li><p> IoU GIoU </p>\n</li>\n<li><p>GIoU  IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$ A B GIoU  IoU $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$</p>\n</li>\n<li><p>IoU  GIoU </p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$ GIoU $-1 \\le GIoU(A,B) \\le 1$</p>\n<p></p>\n<ul>\n<li> IoU  A B $|A \\cup B|=|A \\cap B|$$GIoU =IoU=1$</li>\n<li> A B  C  0GIoU  -1$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>GIoU  IoU  IoU 2D/3D GIoU  IoU 2D  GIoU GIoU  3D  GIoU </p>\n<h2 id=\"GIoUBBox\"><a href=\"#GIoUBBox\" class=\"headerlink\" title=\"GIoUBBox\"></a>GIoUBBox</h2><p> GIoU  IoU </p>\n<p>2D bbox  GIoU  A B  A B  A B  min  max  A B  A B  x  x  $x^{tl} &lt; x^{br}$ $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$ $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$  $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$</p>\n<p>minmax ReLU 2  IoU  GIoU  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ </p>\n<hr>\n<p>2IoUGIoUBBox</p>\n<hr>\n<p> $B^p$  GT  $B^g$ $B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>  box </p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p> GT box </p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p> box </p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p></p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p> c</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p> c </p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p> IoU</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$ $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p> GIoU</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p> GIoU </p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>bboxIoU=00GIoU 3GIoU  IoU  IoU 2 </p>\n<p><img src=\"/images/GIoU_fig2.png\" alt></p>\n<p>21 2D pair IoU  GIoU $IoU \\le 0.2, \\ GIoU \\le 0.2$GIoU  IoU GIoU  GIoU $\\mathcal L_{GIoU}$ IoU $\\mathcal L_{IoU}$IoU</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>0</p>\n<p> GT box 0$A^g &gt; 0$214bbox$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$$\\mathcal U \\ge A^g$ $\\mathcal U &gt; 0$ IoU  $\\mathcal U \\ge \\mathcal I$ $0 \\le IoU \\le 1$ IoU  $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p> GIoU  $\\frac {A^c-\\mathcal U} {A^c}$ A B  A B  $A^c \\ge \\mathcal U &gt; 0$ $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$$\\frac {A^c-\\mathcal U} {A^c} &lt;1$ A B  A B  size  A B  $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$ $-1 &lt; GIoU \\le 1$ $-1 \\le GIoU \\le 1$</p>\n<h3 id=\"IoU-0-mathcal-L-GIoU-\"><a href=\"#IoU-0-mathcal-L-GIoU-\" class=\"headerlink\" title=\"IoU=0$\\mathcal L_{GIoU}$\"></a>IoU=0$\\mathcal L_{GIoU}$</h3><p>GIoU  $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$ $B^p$  $B^g$  $\\mathcal I=0, IoU=0$ GIoU  $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$ GIoU  $\\frac {\\mathcal U}{A^c}$ $0\\le \\frac {\\mathcal U}{A^c} \\le 1$ $A^c$ $\\mathcal U$ $\\mathcal I=0$ $\\mathcal U=A^p+A^g$ $A^g$  $A^p$ $A^c$  $A^p$ $B^p$  $B^g$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> bbox  $\\mathcal L_{GIoU}$ 2D Faster R-CNNMask R-CNN  YOLO v3 Faster R-CNN/Mask R-CNN  $l_1$-smooth  YOLO v3  MSE  $\\mathcal L_{GIoU}$ baseline  $\\mathcal L_{IoU}$  baseline</p>\n<p><strong></strong> PASCAL VOC  MS COCO </p>\n<p><strong></strong>  MS COCO  mAP IoU  mAP IoU mAP $IoU={.5,.55,,.95}$ IoU  mAP  <strong>AP</strong> GIoU  IoU  $GIoU={.5,.55,,.95}$ mAP <strong>AP</strong> IoU  GIoU  0.75  mAP <strong>AP75</strong></p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong></strong>  YOLO v3  Darknet  Baseline  MSE  DarkNet-608  backbone YOLO v3  IoU  GIoU  YOLO v3  MSE  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ MSE  YOLO v3  bbox </p>\n<h2 id=\"Faster-R-CNN--Mask-R-CNN\"><a href=\"#Faster-R-CNN--Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN  Mask R-CNN\"></a>Faster R-CNN  Mask R-CNN</h2><p><strong></strong>  Faster R-CNN/Mask R-CNN  PyTorch baseline$l_1$-smooth ResNet-50  backbone IoU  GIoU RPN $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ YOLO v3  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$  10</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GIoU GIoU  IoU  IoU  IoU  2D/3D GIoU </p>\n<p> GIoU  GIoU / GIoU  bbox  GIoU  sota GIoU  bbox  2D bbox </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>IoU benchmarksbboxIoU2D bboxIoU  IoU bboxIou GIou GIoU  sota benchmarks PASCAL VOC  MS COCO IoU  GIoU </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>2D/3D bbox bbox IoU  $l_1, l_2$ </p>\n<p>IoU  bbox IoU</p>\n<p>$l_n$IoU2D1(a)<br><img src=\"/images/GIoU_fig1.png\" alt></p>\n<!--  -->\n\n<p>boxgt box$(x_1,y_1,x_2,y_2)$boxcorner$l_2$ gt box  corner  box  corner  $l_2$ IoUbbox1(b)</p>\n<p> IoU  IoU $l_n$  bbox bbox  center+size $(x_c,y_c)$ $(w,h)$  box size</p>\n<p>sota  anchor faster-rcnnIoU</p>\n<p>IoU$ndim \\ge 2$IoUIoU IoU IouIoUIouIoU0IoU00</p>\n<p>IoU(a)  IoU (b)  IoU (c) IoUIoUGIoUGIoU sota benchmarksIoUGIoU</p>\n<p></p>\n<ul>\n<li>GIoU</li>\n<li>GIoU</li>\n<li>GIoUsota Faster R-CNNMask R-CNNYOLO v3benchmark</li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong></strong> IoUboxIoUPASCAL VOCmAP IoU=0.5IoUmAPIoUmAPIoUMS COCO benchmark IoUmAP</p>\n<p><strong>bbox</strong> 2Dbboxbbox</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1  bbox $(x_c,y_c,w,h)$scale(w,h)$(w-\\hat w)^2+(h-\\hat h)^2$  $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNNselective searchboxesbboxsizescalesize log$l_2$MSE</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN $l_1$-smooth  $l_2$ </p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNNanchor boxessizeanchor boxes1:3batch 128proposalsFast R-CNNRetinaNet  focal loss</p>\n</li>\n</ol>\n<p>bboxGIoU  bbox IoU </p>\n<p><strong>IoU</strong> IoUIoUbboxIoUGIoUIoU</p>\n<h1 id=\"IoU\"><a href=\"#IoU\" class=\"headerlink\" title=\"IoU\"></a>IoU</h1><p> $A,B \\subseteq S \\in \\mathbb{R}^n$  IoU <br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>2D/3D</p>\n<ul>\n<li><p>IoU</p>\n<p>IoU $\\mathcal L_{IoU}=1-IoU$ $\\mathcal L_{IoU}$  IoU </p>\n</li>\n<li><p>IoUA B S </p>\n</li>\n</ul>\n<p>IoU</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$IoUA B</li>\n</ul>\n<p>IoU GIoU</p>\n<p> $A, B \\subseteq S \\in \\mathbb S^n$ S  A  B  CC C  C  A  B  C  A  B  IoU  GIoU/ 2D/3D</p>\n<p>1</p>\n<hr>\n<p>1GIoU</p>\n<hr>\n<p>  $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p> GIoU</p>\n<ul>\n<li><p> S  A B  C</p>\n</li>\n<li><p> IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p> GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>GIoU </p>\n<ul>\n<li><p> IoU GIoU </p>\n<p>IoU  $\\mathcal L_{GIoU} = 1-GIoU$</p>\n</li>\n<li><p> IoU GIoU </p>\n</li>\n<li><p>GIoU  IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$ A B GIoU  IoU $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$</p>\n</li>\n<li><p>IoU  GIoU </p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$ GIoU $-1 \\le GIoU(A,B) \\le 1$</p>\n<p></p>\n<ul>\n<li> IoU  A B $|A \\cup B|=|A \\cap B|$$GIoU =IoU=1$</li>\n<li> A B  C  0GIoU  -1$\\lim_{\\frac{|A \\cup B|}{|C|}\\rightarrow 0} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>GIoU  IoU  IoU 2D/3D GIoU  IoU 2D  GIoU GIoU  3D  GIoU </p>\n<h2 id=\"GIoUBBox\"><a href=\"#GIoUBBox\" class=\"headerlink\" title=\"GIoUBBox\"></a>GIoUBBox</h2><p> GIoU  IoU </p>\n<p>2D bbox  GIoU  A B  A B  A B  min  max  A B  A B  x  x  $x^{tl} &lt; x^{br}$ $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$ $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$  $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$</p>\n<p>minmax ReLU 2  IoU  GIoU  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ </p>\n<hr>\n<p>2IoUGIoUBBox</p>\n<hr>\n<p> $B^p$  GT  $B^g$ $B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>  box </p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p> GT box </p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p> box </p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p></p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p> c</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p> c </p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p> IoU</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$ $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p> GIoU</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p> GIoU </p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>bboxIoU=00GIoU 3GIoU  IoU  IoU 2 </p>\n<p><img src=\"/images/GIoU_fig2.png\" alt></p>\n<p>21 2D pair IoU  GIoU $IoU \\le 0.2, \\ GIoU \\le 0.2$GIoU  IoU GIoU  GIoU $\\mathcal L_{GIoU}$ IoU $\\mathcal L_{IoU}$IoU</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>0</p>\n<p> GT box 0$A^g &gt; 0$214bbox$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$$\\mathcal U \\ge A^g$ $\\mathcal U &gt; 0$ IoU  $\\mathcal U \\ge \\mathcal I$ $0 \\le IoU \\le 1$ IoU  $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p> GIoU  $\\frac {A^c-\\mathcal U} {A^c}$ A B  A B  $A^c \\ge \\mathcal U &gt; 0$ $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$$\\frac {A^c-\\mathcal U} {A^c} &lt;1$ A B  A B  size  A B  $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$ $-1 &lt; GIoU \\le 1$ $-1 \\le GIoU \\le 1$</p>\n<h3 id=\"IoU-0-mathcal-L-GIoU-\"><a href=\"#IoU-0-mathcal-L-GIoU-\" class=\"headerlink\" title=\"IoU=0$\\mathcal L_{GIoU}$\"></a>IoU=0$\\mathcal L_{GIoU}$</h3><p>GIoU  $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$ $B^p$  $B^g$  $\\mathcal I=0, IoU=0$ GIoU  $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$ GIoU  $\\frac {\\mathcal U}{A^c}$ $0\\le \\frac {\\mathcal U}{A^c} \\le 1$ $A^c$ $\\mathcal U$ $\\mathcal I=0$ $\\mathcal U=A^p+A^g$ $A^g$  $A^p$ $A^c$  $A^p$ $B^p$  $B^g$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> bbox  $\\mathcal L_{GIoU}$ 2D Faster R-CNNMask R-CNN  YOLO v3 Faster R-CNN/Mask R-CNN  $l_1$-smooth  YOLO v3  MSE  $\\mathcal L_{GIoU}$ baseline  $\\mathcal L_{IoU}$  baseline</p>\n<p><strong></strong> PASCAL VOC  MS COCO </p>\n<p><strong></strong>  MS COCO  mAP IoU  mAP IoU mAP $IoU={.5,.55,,.95}$ IoU  mAP  <strong>AP</strong> GIoU  IoU  $GIoU={.5,.55,,.95}$ mAP <strong>AP</strong> IoU  GIoU  0.75  mAP <strong>AP75</strong></p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong></strong>  YOLO v3  Darknet  Baseline  MSE  DarkNet-608  backbone YOLO v3  IoU  GIoU  YOLO v3  MSE  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ MSE  YOLO v3  bbox </p>\n<h2 id=\"Faster-R-CNN--Mask-R-CNN\"><a href=\"#Faster-R-CNN--Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN  Mask R-CNN\"></a>Faster R-CNN  Mask R-CNN</h2><p><strong></strong>  Faster R-CNN/Mask R-CNN  PyTorch baseline$l_1$-smooth ResNet-50  backbone IoU  GIoU RPN $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ YOLO v3  $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$  10</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GIoU GIoU  IoU  IoU  IoU  2D/3D GIoU </p>\n<p> GIoU  GIoU / GIoU  bbox  GIoU  sota GIoU  bbox  2D bbox </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n"},{"title":"PyTorch-1","date":"2019-06-12T11:17:11.000Z","_content":"# \nC++pythonCUDA\n\ntensorflowpytorchtensorflowpytorch\n\n\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n--recursivepytorchroot dir$ROOT_DIR\n\nLinux\n```\ncd pytorch\npython setup.py install\n```\npytorchC++pythonsetuptoolspython$ROOT_DIR/setup.pysetup()build_deps() caffe2 \n\n### build_deps()\n\n```\nbuild_caffe2(...)\n```\nbuild_caffe2\n1. run_cmakecmakecmake`$ROOD_DIR/build` cmakeSource Tree$ROOD_DIR top levelCMakeLists.txt\n2. $ROOT_DIR/buildmake install ninja installcmakeMakefileinstalltargetbuild\n3. build/caffe2/proto.py caffe2/proto/.pycaffe2/proto/.proto\n\nrun_cmakecmake-D optioncmakesource treecmake top levelCMakeLists.txt1)2)include dirlib dir3.cmakecmake4C++5/\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\nc10,caffe2,modulesbuild treeCMakeLists.txt CMakeLists.txt\n\ntop level CMakeLists.txt\n```\ninclude(cmake/Dependencies.cmake)\n```\nDependencies.cmakeCaffe2`$ROOT_DIR/third_party`$ROOT_DIR/caffe2\n1. BLASBLAS=OpenBLASsetup.py, OpenBLASDependencies.cmake\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\nfind_package`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`.cmakeOpenBLAScblas.hopenblas OpenBLAS_INCLUDE_DIROpenBLAS_LIBCaffe2_PUBLIC_DEPENDENCY_LIBS$ROOT_DIR/caffe2/CMakeLists.txt\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n-lopenblas\n\ncaffe2_pybind11_state$ROOT_DIR/caffe2/CMakeLists.txt\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\nCaffe2_CPU_PYTHON_SRCS$ROOT_DIR/caffe2/python/CMakeLists.txt CUDAROCM , caffe2_pybind11_state_gpucaffe2_pybind11_state_hipinstallpythonsite-packagescaffe2/python\n\nbuild_dep()$ROOT_DIR/setup.pysetup\n\n### setup()\nsetup[setup()](https://docs.python.org/3/distutils/apiref.html)\n1. ext_modules 5\n- torch._C C++/dir\n- torch._dl WINDOWSC\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\ncaffe2.pythonpackage$ROOT_DIR/build/caffe2/pythonext_modulesbuild_ext\n\n2. cmdclassbuild_ext, clean, installactionactionpython setup.py <action> install cleanpattern.gitignorebuild_extbuild_ext\n\n- create_compile_commandscompile_commands.jsongccg++gccsinclude c++ compile_commands.json`$ROOT_DIR/CMakeLists.txtset(CMAKE_EXPORT_COMPILE_COMMAND ON)`$ROOT_DIR/buildjsonworking directorycommand\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n{...} .o gccg++$ROOT_DIR/compile_commands.json\n- runlibrary CUDA, CUDNN, NUMPY\n- build_extensions ext_modulespython\n\next_modules5build_deps()caffe2_pybind11_state_gpucaffe2_pybind11_state_hipCUDAROCMbuild_deps()ext_modulesbuild_extensionstorch._C, torch._dl\n\nbuild_deps()ext_modules `$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`build_deps()ext_modules `$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`caffe2.python.caffe2_pybind11_state`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`caffe2_pybind11_statepython.cpython-37m-x86_64-linux-gnu.so$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so build_extensions() pythonsite-packages.../miniconda3/lib/python3.7/site-packages/caffe2/python/\n\n3. packages python site-packages\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\nPyTorchtoolscaffe2torch__init__.pycaffe2torchsite-packages\n\next_modules5torch._C, torch._dlsite-packages/torch_C, _dlextcaffe2.site-packages/caffe2/python\n\n### \n\npytorch:\n\n1. CMakec++build_deps()\n2. pythonsetupbuild_ext\n\n\n```\ntop-levelCMakeLists.txt\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\ncaffe2CMakeLists.txt \n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\ninstall\n```\nadd_subdirectory(../torch torch)\n```\ncaffe2CMakeLists.txtadd_subdirectorytorch\n\ntorchCMakeLists.txt \n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\ntorch/lib/libshmCMakeLists.txt\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\npytorchCMakecmake/Dependencies.cmake\n- Caffe2_PUBLIC_DEPENDENCY_LIBSOpenBLAS g++flag `-I<include dir>flag`\n- \n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # tbb\n```\n(2) qnnpack\n```\n#  qnnpack \n# source directory${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\nCMakeQNNPACKthird_partyqnnpackDependencies.cmakeCaffe2_DEPENDENCY_LIBS\n(3) nnpack\n```\n#  nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\nnnpack.cmake\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\nNNPACKthird_partyNNPACKCMakeLists.txtCMake nnpackDependencies.cmakennpackCaffe2_DEPENDENCY_LIBS\n\n(4)  cpuinfogflagglog::gloggoogletestfbgemmfp16\n\n(5) LMDB\n```\nfind_package(LMDB)\n```\ncmake/ModulesFindLMDB.cmake .cmakelmdblmdb.hlinux/usr/lib/x86_64-linux-gnu/usr/include, LMDB_LIBRARIESLMDB_INCLUDE_DIRDependencies.cmake\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\nOPENCLLEVELDBNUMAZMQREDISOPENCVFFMPEGPythonMPI\n\n(6) pybind11Dependencies.cmakepybind11\n```\nfind_package(pybind11 CONFIG)# ${pybind11_DIR}pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # module\nendif()\n```\ncmake/Modules/Findpybind11.cmakefind_pathpybind11/pybind11.hpybind11CMakeDependencies.cmake\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\nOpenMP${OpenMP_CXX_FLAGS}  ${OpenMP_CXX_LIBRARIES}flagcaffe2OpenMPcaffe2/CMakeLists.txt\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDADependencies.cmake\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\ncuda.cmake find_librarycudaIMPORTED target\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\ncudacaffe2::cudartcaffe2::cudnncaffe2::curandcaffe2::cufftcaffe2::tensorrt caffe2::cublascaffe2::nvrtcDependencies.cmake\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\nCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txt\n\n(9) NCCLCUBGLOO\n\nDependencies.cmakecaffe2QNNPACKCaffe2_DEPENDENCY_LIBSCaffe2_PUBLIC_DEPENDENCY_LIBSCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txtflag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. pythoncaffe2\n\n- torch._C \n```\nmain_libraries=['shm', 'torch_python']\n```\n\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dltorch/csrc/dl.c <dlfcn.h>torch._dl\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\ntorch._Cdlopen()python os flagpythonDLFCNtorch._dlflagtorch._dl\n\n### ...\nPyTorch","source":"_posts/PyTorch-1.md","raw":"---\ntitle: PyTorch-1\ndate: 2019-06-12 19:17:11\ntags: PyTorch\ncategories: DL Framework\n---\n# \nC++pythonCUDA\n\ntensorflowpytorchtensorflowpytorch\n\n\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n--recursivepytorchroot dir$ROOT_DIR\n\nLinux\n```\ncd pytorch\npython setup.py install\n```\npytorchC++pythonsetuptoolspython$ROOT_DIR/setup.pysetup()build_deps() caffe2 \n\n### build_deps()\n\n```\nbuild_caffe2(...)\n```\nbuild_caffe2\n1. run_cmakecmakecmake`$ROOD_DIR/build` cmakeSource Tree$ROOD_DIR top levelCMakeLists.txt\n2. $ROOT_DIR/buildmake install ninja installcmakeMakefileinstalltargetbuild\n3. build/caffe2/proto.py caffe2/proto/.pycaffe2/proto/.proto\n\nrun_cmakecmake-D optioncmakesource treecmake top levelCMakeLists.txt1)2)include dirlib dir3.cmakecmake4C++5/\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\nc10,caffe2,modulesbuild treeCMakeLists.txt CMakeLists.txt\n\ntop level CMakeLists.txt\n```\ninclude(cmake/Dependencies.cmake)\n```\nDependencies.cmakeCaffe2`$ROOT_DIR/third_party`$ROOT_DIR/caffe2\n1. BLASBLAS=OpenBLASsetup.py, OpenBLASDependencies.cmake\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\nfind_package`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`.cmakeOpenBLAScblas.hopenblas OpenBLAS_INCLUDE_DIROpenBLAS_LIBCaffe2_PUBLIC_DEPENDENCY_LIBS$ROOT_DIR/caffe2/CMakeLists.txt\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n-lopenblas\n\ncaffe2_pybind11_state$ROOT_DIR/caffe2/CMakeLists.txt\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\nCaffe2_CPU_PYTHON_SRCS$ROOT_DIR/caffe2/python/CMakeLists.txt CUDAROCM , caffe2_pybind11_state_gpucaffe2_pybind11_state_hipinstallpythonsite-packagescaffe2/python\n\nbuild_dep()$ROOT_DIR/setup.pysetup\n\n### setup()\nsetup[setup()](https://docs.python.org/3/distutils/apiref.html)\n1. ext_modules 5\n- torch._C C++/dir\n- torch._dl WINDOWSC\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\ncaffe2.pythonpackage$ROOT_DIR/build/caffe2/pythonext_modulesbuild_ext\n\n2. cmdclassbuild_ext, clean, installactionactionpython setup.py <action> install cleanpattern.gitignorebuild_extbuild_ext\n\n- create_compile_commandscompile_commands.jsongccg++gccsinclude c++ compile_commands.json`$ROOT_DIR/CMakeLists.txtset(CMAKE_EXPORT_COMPILE_COMMAND ON)`$ROOT_DIR/buildjsonworking directorycommand\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n{...} .o gccg++$ROOT_DIR/compile_commands.json\n- runlibrary CUDA, CUDNN, NUMPY\n- build_extensions ext_modulespython\n\next_modules5build_deps()caffe2_pybind11_state_gpucaffe2_pybind11_state_hipCUDAROCMbuild_deps()ext_modulesbuild_extensionstorch._C, torch._dl\n\nbuild_deps()ext_modules `$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`build_deps()ext_modules `$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`caffe2.python.caffe2_pybind11_state`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`caffe2_pybind11_statepython.cpython-37m-x86_64-linux-gnu.so$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so build_extensions() pythonsite-packages.../miniconda3/lib/python3.7/site-packages/caffe2/python/\n\n3. packages python site-packages\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\nPyTorchtoolscaffe2torch__init__.pycaffe2torchsite-packages\n\next_modules5torch._C, torch._dlsite-packages/torch_C, _dlextcaffe2.site-packages/caffe2/python\n\n### \n\npytorch:\n\n1. CMakec++build_deps()\n2. pythonsetupbuild_ext\n\n\n```\ntop-levelCMakeLists.txt\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\ncaffe2CMakeLists.txt \n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\ninstall\n```\nadd_subdirectory(../torch torch)\n```\ncaffe2CMakeLists.txtadd_subdirectorytorch\n\ntorchCMakeLists.txt \n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\ntorch/lib/libshmCMakeLists.txt\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\npytorchCMakecmake/Dependencies.cmake\n- Caffe2_PUBLIC_DEPENDENCY_LIBSOpenBLAS g++flag `-I<include dir>flag`\n- \n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # tbb\n```\n(2) qnnpack\n```\n#  qnnpack \n# source directory${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\nCMakeQNNPACKthird_partyqnnpackDependencies.cmakeCaffe2_DEPENDENCY_LIBS\n(3) nnpack\n```\n#  nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\nnnpack.cmake\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\nNNPACKthird_partyNNPACKCMakeLists.txtCMake nnpackDependencies.cmakennpackCaffe2_DEPENDENCY_LIBS\n\n(4)  cpuinfogflagglog::gloggoogletestfbgemmfp16\n\n(5) LMDB\n```\nfind_package(LMDB)\n```\ncmake/ModulesFindLMDB.cmake .cmakelmdblmdb.hlinux/usr/lib/x86_64-linux-gnu/usr/include, LMDB_LIBRARIESLMDB_INCLUDE_DIRDependencies.cmake\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\nOPENCLLEVELDBNUMAZMQREDISOPENCVFFMPEGPythonMPI\n\n(6) pybind11Dependencies.cmakepybind11\n```\nfind_package(pybind11 CONFIG)# ${pybind11_DIR}pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # module\nendif()\n```\ncmake/Modules/Findpybind11.cmakefind_pathpybind11/pybind11.hpybind11CMakeDependencies.cmake\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\nOpenMP${OpenMP_CXX_FLAGS}  ${OpenMP_CXX_LIBRARIES}flagcaffe2OpenMPcaffe2/CMakeLists.txt\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDADependencies.cmake\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\ncuda.cmake find_librarycudaIMPORTED target\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\ncudacaffe2::cudartcaffe2::cudnncaffe2::curandcaffe2::cufftcaffe2::tensorrt caffe2::cublascaffe2::nvrtcDependencies.cmake\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\nCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txt\n\n(9) NCCLCUBGLOO\n\nDependencies.cmakecaffe2QNNPACKCaffe2_DEPENDENCY_LIBSCaffe2_PUBLIC_DEPENDENCY_LIBSCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txtflag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. pythoncaffe2\n\n- torch._C \n```\nmain_libraries=['shm', 'torch_python']\n```\n\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dltorch/csrc/dl.c <dlfcn.h>torch._dl\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\ntorch._Cdlopen()python os flagpythonDLFCNtorch._dlflagtorch._dl\n\n### ...\nPyTorch","slug":"PyTorch-1","published":1,"updated":"2019-06-20T10:07:49.801Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379e6001tdgvcvw7kygjd","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>C++pythonCUDA</p>\n<p>tensorflowpytorchtensorflowpytorch</p>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/pytorch/pytorch</span><br></pre></td></tr></table></figure>\n\n<p>recursivepytorchroot dir$ROOT_DIR</p>\n<p>Linux</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n\n<p>pytorchC++pythonsetuptoolspython$ROOT_DIR/setup.pysetup()build_deps() caffe2 </p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n\n<p>build_caffe2</p>\n<ol>\n<li>run_cmakecmakecmake<code>$ROOD_DIR/build</code> cmakeSource Tree$ROOD_DIR top levelCMakeLists.txt</li>\n<li>$ROOT_DIR/buildmake install ninja installcmakeMakefileinstalltargetbuild</li>\n<li>build/caffe2/proto.py caffe2/proto/.pycaffe2/proto/.proto</li>\n</ol>\n<p>run_cmakecmake-D optioncmakesource treecmake top levelCMakeLists.txt1)2)include dirlib dir3.cmakecmake4C++5/</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n\n<p>c10,caffe2,modulesbuild treeCMakeLists.txt CMakeLists.txt</p>\n<p>top level CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake/Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>Dependencies.cmakeCaffe2<code>$ROOT_DIR/third_party</code>$ROOT_DIR/caffe2</p>\n<ol>\n<li>BLASBLAS=OpenBLASsetup.py, OpenBLASDependencies.cmake<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>find_package<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>.cmakeOpenBLAScblas.hopenblas OpenBLAS_INCLUDE_DIROpenBLAS_LIBCaffe2_PUBLIC_DEPENDENCY_LIBS$ROOT_DIR/caffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>-lopenblas</p>\n<p>caffe2_pybind11_state$ROOT_DIR/caffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;/caffe2/python&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>Caffe2_CPU_PYTHON_SRCS$ROOT_DIR/caffe2/python/CMakeLists.txt CUDAROCM , caffe2_pybind11_state_gpucaffe2_pybind11_state_hipinstallpythonsite-packagescaffe2/python</p>\n<p>build_dep()$ROOT_DIR/setup.pysetup</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a></p>\n<ol>\n<li>ext_modules 5</li>\n</ol>\n<ul>\n<li>torch._C C++/dir</li>\n<li>torch._dl WINDOWSC</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>caffe2.pythonpackage$ROOT_DIR/build/caffe2/pythonext_modulesbuild_ext</p>\n<ol start=\"2\">\n<li>cmdclassbuild_ext, clean, installactionactionpython setup.py <action> install cleanpattern.gitignorebuild_extbuild_ext</action></li>\n</ol>\n<ul>\n<li>create_compile_commandscompile_commands.jsongccg++gccsinclude c++ compile_commands.json<code>$ROOT_DIR/CMakeLists.txtset(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>$ROOT_DIR/buildjsonworking directorycommand<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path/to/root&gt;build/third_party/protobuf/cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;/usr/bin/c++ ... -I&lt;path/to/root&gt;/third_party/protobuf/src ... </span><br><span class=\"line\">                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path/to/root&gt;/third_party/protobuf/src/google/protobuf/arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>{} .o gccg++$ROOT_DIR/compile_commands.json</p>\n<ul>\n<li>runlibrary CUDA, CUDNN, NUMPY</li>\n<li>build_extensions ext_modulespython</li>\n</ul>\n<p>ext_modules5build_deps()caffe2_pybind11_state_gpucaffe2_pybind11_state_hipCUDAROCMbuild_deps()ext_modulesbuild_extensionstorch._C, torch._dl</p>\n<p>build_deps()ext_modules <code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>build_deps()ext_modules <code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>caffe2.python.caffe2_pybind11_state<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>caffe2_pybind11_statepython.cpython-37m-x86_64-linux-gnu.so$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so build_extensions() pythonsite-packages/miniconda3/lib/python3.7/site-packages/caffe2/python/</p>\n<ol start=\"3\">\n<li>packages python site-packages<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages = find_packages(exclude=[&apos;tools&apos;, &apos;tools.*&apos;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>PyTorchtoolscaffe2torch<strong>init</strong>.pycaffe2torchsite-packages</p>\n<p>ext_modules5torch._C, torch._dlsite-packages/torch_C, _dlextcaffe2.site-packages/caffe2/python</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>pytorch:</p>\n<ol>\n<li>CMakec++build_deps()</li>\n<li>pythonsetupbuild_ext</li>\n</ol>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-levelCMakeLists.txt</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n\n<p>caffe2CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;/csrc/jit/fuser/cuda/thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(../torch torch)</span><br></pre></td></tr></table></figure>\n\n<p>caffe2CMakeLists.txtadd_subdirectorytorch</p>\n<p>torchCMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>torch/lib/libshmCMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n\n<p>pytorchCMakecmake/Dependencies.cmake</p>\n<ul>\n<li>Caffe2_PUBLIC_DEPENDENCY_LIBSOpenBLAS g++flag <code>-I&lt;include dir&gt;flag</code></li>\n<li><br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;/aten/src/ATen/cpu/tbb)    # tbb</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>(2) qnnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#  qnnpack </span><br><span class=\"line\"># source directory$&#123;PROJECT_SOURCE_DIR&#125;/third_party/QNNPACK</span><br><span class=\"line\"># output directory$&#123;PROJECT_BINARY_DIR&#125;/confu-deps/QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n\n<p>CMakeQNNPACKthird_partyqnnpackDependencies.cmakeCaffe2_DEPENDENCY_LIBS<br>(3) nnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#  nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/External/nnpack.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>nnpack.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/NNPACK)</span><br></pre></td></tr></table></figure>\n\n<p>NNPACKthird_partyNNPACKCMakeLists.txtCMake nnpackDependencies.cmakennpackCaffe2_DEPENDENCY_LIBS</p>\n<p>(4)  cpuinfogflagglog::gloggoogletestfbgemmfp16</p>\n<p>(5) LMDB</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n\n<p>cmake/ModulesFindLMDB.cmake .cmakelmdblmdb.hlinux/usr/lib/x86_64-linux-gnu/usr/include, LMDB_LIBRARIESLMDB_INCLUDE_DIRDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>OPENCLLEVELDBNUMAZMQREDISOPENCVFFMPEGPythonMPI</p>\n<p>(6) pybind11Dependencies.cmakepybind11</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# $&#123;pybind11_DIR&#125;pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # module</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n\n<p>cmake/Modules/Findpybind11.cmakefind_pathpybind11/pybind11.hpybind11CMakeDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../third_party/pybind11/include)</span><br></pre></td></tr></table></figure>\n\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n\n<p>OpenMP${OpenMP_CXX_FLAGS}  ${OpenMP_CXX_LIBRARIES}flagcaffe2OpenMPcaffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>(8) CUDADependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/public/cuda.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>cuda.cmake find_librarycudaIMPORTED target</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n\n<p>cudacaffe2::cudartcaffe2::cudnncaffe2::curandcaffe2::cufftcaffe2::tensorrt caffe2::cublascaffe2::nvrtcDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n\n<p>Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txt</p>\n<p>(9) NCCLCUBGLOO</p>\n<p>Dependencies.cmakecaffe2QNNPACKCaffe2_DEPENDENCY_LIBSCaffe2_PUBLIC_DEPENDENCY_LIBSCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txtflag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li>pythoncaffe2</li>\n</ol>\n<ul>\n<li>torch._C <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries=[&apos;shm&apos;, &apos;torch_python&apos;]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources=[&quot;torch/csrc/stub.cpp&quot;]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>torch._dltorch/csrc/dl.c &lt;dlfcn.h&gt;torch._dl<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL=0x100</span><br><span class=\"line\">RTLD_NOW   =0x2</span><br><span class=\"line\">RTLD_LAZY  =0x1</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>torch._Cdlopen()python os flagpythonDLFCNtorch._dlflagtorch._dl</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>PyTorch</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>C++pythonCUDA</p>\n<p>tensorflowpytorchtensorflowpytorch</p>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/pytorch/pytorch</span><br></pre></td></tr></table></figure>\n\n<p>recursivepytorchroot dir$ROOT_DIR</p>\n<p>Linux</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n\n<p>pytorchC++pythonsetuptoolspython$ROOT_DIR/setup.pysetup()build_deps() caffe2 </p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n\n<p>build_caffe2</p>\n<ol>\n<li>run_cmakecmakecmake<code>$ROOD_DIR/build</code> cmakeSource Tree$ROOD_DIR top levelCMakeLists.txt</li>\n<li>$ROOT_DIR/buildmake install ninja installcmakeMakefileinstalltargetbuild</li>\n<li>build/caffe2/proto.py caffe2/proto/.pycaffe2/proto/.proto</li>\n</ol>\n<p>run_cmakecmake-D optioncmakesource treecmake top levelCMakeLists.txt1)2)include dirlib dir3.cmakecmake4C++5/</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n\n<p>c10,caffe2,modulesbuild treeCMakeLists.txt CMakeLists.txt</p>\n<p>top level CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake/Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>Dependencies.cmakeCaffe2<code>$ROOT_DIR/third_party</code>$ROOT_DIR/caffe2</p>\n<ol>\n<li>BLASBLAS=OpenBLASsetup.py, OpenBLASDependencies.cmake<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>find_package<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>.cmakeOpenBLAScblas.hopenblas OpenBLAS_INCLUDE_DIROpenBLAS_LIBCaffe2_PUBLIC_DEPENDENCY_LIBS$ROOT_DIR/caffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>-lopenblas</p>\n<p>caffe2_pybind11_state$ROOT_DIR/caffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;/caffe2/python&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>Caffe2_CPU_PYTHON_SRCS$ROOT_DIR/caffe2/python/CMakeLists.txt CUDAROCM , caffe2_pybind11_state_gpucaffe2_pybind11_state_hipinstallpythonsite-packagescaffe2/python</p>\n<p>build_dep()$ROOT_DIR/setup.pysetup</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a></p>\n<ol>\n<li>ext_modules 5</li>\n</ol>\n<ul>\n<li>torch._C C++/dir</li>\n<li>torch._dl WINDOWSC</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>caffe2.pythonpackage$ROOT_DIR/build/caffe2/pythonext_modulesbuild_ext</p>\n<ol start=\"2\">\n<li>cmdclassbuild_ext, clean, installactionactionpython setup.py <action> install cleanpattern.gitignorebuild_extbuild_ext</action></li>\n</ol>\n<ul>\n<li>create_compile_commandscompile_commands.jsongccg++gccsinclude c++ compile_commands.json<code>$ROOT_DIR/CMakeLists.txtset(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>$ROOT_DIR/buildjsonworking directorycommand<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path/to/root&gt;build/third_party/protobuf/cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;/usr/bin/c++ ... -I&lt;path/to/root&gt;/third_party/protobuf/src ... </span><br><span class=\"line\">                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path/to/root&gt;/third_party/protobuf/src/google/protobuf/arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>{} .o gccg++$ROOT_DIR/compile_commands.json</p>\n<ul>\n<li>runlibrary CUDA, CUDNN, NUMPY</li>\n<li>build_extensions ext_modulespython</li>\n</ul>\n<p>ext_modules5build_deps()caffe2_pybind11_state_gpucaffe2_pybind11_state_hipCUDAROCMbuild_deps()ext_modulesbuild_extensionstorch._C, torch._dl</p>\n<p>build_deps()ext_modules <code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>build_deps()ext_modules <code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>caffe2.python.caffe2_pybind11_state<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>caffe2_pybind11_statepython.cpython-37m-x86_64-linux-gnu.so$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so build_extensions() pythonsite-packages/miniconda3/lib/python3.7/site-packages/caffe2/python/</p>\n<ol start=\"3\">\n<li>packages python site-packages<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages = find_packages(exclude=[&apos;tools&apos;, &apos;tools.*&apos;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>PyTorchtoolscaffe2torch<strong>init</strong>.pycaffe2torchsite-packages</p>\n<p>ext_modules5torch._C, torch._dlsite-packages/torch_C, _dlextcaffe2.site-packages/caffe2/python</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>pytorch:</p>\n<ol>\n<li>CMakec++build_deps()</li>\n<li>pythonsetupbuild_ext</li>\n</ol>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-levelCMakeLists.txt</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n\n<p>caffe2CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;/csrc/jit/fuser/cuda/thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(../torch torch)</span><br></pre></td></tr></table></figure>\n\n<p>caffe2CMakeLists.txtadd_subdirectorytorch</p>\n<p>torchCMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>torch/lib/libshmCMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n\n<p>pytorchCMakecmake/Dependencies.cmake</p>\n<ul>\n<li>Caffe2_PUBLIC_DEPENDENCY_LIBSOpenBLAS g++flag <code>-I&lt;include dir&gt;flag</code></li>\n<li><br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;/aten/src/ATen/cpu/tbb)    # tbb</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>(2) qnnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#  qnnpack </span><br><span class=\"line\"># source directory$&#123;PROJECT_SOURCE_DIR&#125;/third_party/QNNPACK</span><br><span class=\"line\"># output directory$&#123;PROJECT_BINARY_DIR&#125;/confu-deps/QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n\n<p>CMakeQNNPACKthird_partyqnnpackDependencies.cmakeCaffe2_DEPENDENCY_LIBS<br>(3) nnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#  nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/External/nnpack.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>nnpack.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/NNPACK)</span><br></pre></td></tr></table></figure>\n\n<p>NNPACKthird_partyNNPACKCMakeLists.txtCMake nnpackDependencies.cmakennpackCaffe2_DEPENDENCY_LIBS</p>\n<p>(4)  cpuinfogflagglog::gloggoogletestfbgemmfp16</p>\n<p>(5) LMDB</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n\n<p>cmake/ModulesFindLMDB.cmake .cmakelmdblmdb.hlinux/usr/lib/x86_64-linux-gnu/usr/include, LMDB_LIBRARIESLMDB_INCLUDE_DIRDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>OPENCLLEVELDBNUMAZMQREDISOPENCVFFMPEGPythonMPI</p>\n<p>(6) pybind11Dependencies.cmakepybind11</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# $&#123;pybind11_DIR&#125;pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # module</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n\n<p>cmake/Modules/Findpybind11.cmakefind_pathpybind11/pybind11.hpybind11CMakeDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../third_party/pybind11/include)</span><br></pre></td></tr></table></figure>\n\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n\n<p>OpenMP${OpenMP_CXX_FLAGS}  ${OpenMP_CXX_LIBRARIES}flagcaffe2OpenMPcaffe2/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>(8) CUDADependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/public/cuda.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>cuda.cmake find_librarycudaIMPORTED target</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n\n<p>cudacaffe2::cudartcaffe2::cudnncaffe2::curandcaffe2::cufftcaffe2::tensorrt caffe2::cublascaffe2::nvrtcDependencies.cmake</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n\n<p>Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txt</p>\n<p>(9) NCCLCUBGLOO</p>\n<p>Dependencies.cmakecaffe2QNNPACKCaffe2_DEPENDENCY_LIBSCaffe2_PUBLIC_DEPENDENCY_LIBSCaffe2_PUBLIC_CUDA_DEPENDENCY_LIBScaffe2/CMakeLists.txtflag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li>pythoncaffe2</li>\n</ol>\n<ul>\n<li>torch._C <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries=[&apos;shm&apos;, &apos;torch_python&apos;]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources=[&quot;torch/csrc/stub.cpp&quot;]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>torch._dltorch/csrc/dl.c &lt;dlfcn.h&gt;torch._dl<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL=0x100</span><br><span class=\"line\">RTLD_NOW   =0x2</span><br><span class=\"line\">RTLD_LAZY  =0x1</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>torch._Cdlopen()python os flagpythonDLFCNtorch._dlflagtorch._dl</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>PyTorch</p>\n"},{"title":"Dynamic Programming (1)","date":"2019-08-07T09:29:56.000Z","mathjax":true,"_content":" Dynamic Programming  A Computational Tool\n\n# \n\n / ``\n\n## \n\n> \n\n~~~~\n\n### \n$opt_{d \\in \\Delta} \\{H(d)\\}$ d  $\\Delta$H H(d)  $d^{\\ast}$$d^{\\ast}=\\arg opt_d \\{H(d)\\}$ $\\{d_1,...,d_n\\}$ $h(d_1,...,h_n)$  $H^{\\ast}$\n\n $\\{d_1,...,d_n\\}$    $d_1,...,d_n$\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2}\\{...\\{opt_{d_n \\in D_n}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.1)\\end{aligned}$$\n\n $(d_1,...,d_n) \\in \\Delta=D_1 \\times ... \\times D_n$ i $d_i \\in D_i(d_1,...,d_{i-1})$ (1.1) \n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.2)\\end{aligned}$$\n\n (1.2)  $d_n$ $d_n^{\\ast}(d_1,...,d_{n-1})$ $d_n^{\\ast}$  $opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast},...,d_n^{\\ast} \\}$  $d_1^{\\ast}$\n\n\n$$\\begin{aligned} &opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)} \\{... \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})} \\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_n \\in D_n} \\{opt_{d_{n-1} \\in D_{n-1}(d_n)} \\{... \\{opt_{d_1 \\in D_n(d_2,...,d_n)} \\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.3) \\end{aligned}$$\n\n $D_i$  $D_i$  $(d_{i+1},...,d_n)$\n\n (1.2) $d_1$ $d_1$ \n$$\\begin{aligned}H^{\\ast}&=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1}\\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1)\\} \\qquad \\qquad(1.4) \\end{aligned}$$\n\n $d_i^{\\ast}(d_1), \\ i>1$  $d_1$ partial function $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1))\\}$$d_1$ \n\n#### \n $d_1$  $d_2,...,d_n$  $opt_{d_1 \\in D_1}\\{H'(d_1)\\}$  $d_1$  $opt_{d_1}\\{H'(d_1)\\}$  $H^{\\ast}$  h \n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ ... \\circ C_n(d_n) \\qquad (1.5)$$\n $C_i$  $d_i$ $\\circ$     \n$$opt_d\\{a \\circ C(d)\\}=a \\circ opt_d\\{C(d)\\}$$\n a  d $C_n$  $d_n$ $(d_1,d_2,...,d_{n-1})$\n$$h(d_1,...,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\qquad(1.6)$$\n\n h \n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n) \\qquad(1.7)$$\n\n$$\\begin{aligned} & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}\\{C_2(d_1,d_2) \\circ ... \\circ \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_n(d_1,...,d_n)\\}...\\}\\} \\qquad(1.8) \\end{aligned}$$\n $\\circ$ \n\n $f(d_1,...,d_n)$  $d_1,...,d_{i-1}$ \n$$f(d_1,...,d_{i-1})=opt_{d_i}\\{opt_{d_{i+1}}\\{... \\{opt_{d_n} \\{C_i(d_i|d_1,...,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,...,d_i) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\}\\}...\\}\\} \\ (1.9)$$\n $D_i$\n$$\\begin{aligned}f(\\emptyset)&=opt_{d_1}\\{opt_{d_2}\\{...\\{opt_{d_n}\\{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1})\\}\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ opt_{d_2}\\{C(d_2|d_1) \\circ...\\circ opt_{d_n}\\{C_n(d_n|d_1,...,d_{n-1})\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ f(d_1)\\}  \\qquad \\quad (1.10) \\end{aligned}$$\n\n$$f(d_1,...,d_{i-1})=opt_{d_i \\in D_i(d_1,...,d_{i-1})}\\{C_i(d_i|d_1,...,d_{i-1})\\circ f(d_1,...,d_i)\\} \\qquad(1.11)$$\n\nDPFE\n\n### DPFE\n DPFE  $f(d_1,...,d_{i-1})$ $S=(d_1,...,d_{i-1})$ $i=|S|+1=|\\{d_1,...,d_{i-1}\\}|+1$ DPFE \n$$f(S)=opt_{d_i \\in D_i(S)}\\{C_i(d_i|S) \\circ f(S')\\} \\qquad (1.12)$$\n $S'=(d_1,...,d_i)$ $\\emptyset$  $\\mathcal S$ DPFE  $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$ $S_0$$f(S_0)$  DPFE  b\n\n n  S  d  D(S)  S  d  $d \\in S$ DPFE \n$$f(S)=opt_{d \\in S} \\{C(d|S) \\circ f(S')\\} \\qquad (1.13)$$\n\n S  d  S  S'D(S)  S  DPFE  b(S,S') d  C(d|S) $S'=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$T  DPFE \n$$f(S)=opt_S\\{b(S,S') \\circ f(S')\\} \\qquad(1.14) $$\n\nDPFE \n$$f'(S)=opt_{S'}\\{f'(S') \\circ b(S',S)\\} \\qquad(1.15) $$\n f'(S)  $S_0$  S  f(S)  S  $S_0$  (1.14) backward (1.15) forward\n\n### \n\n$$f(S)=opt_{d \\in D(S)} \\{R(S,d) \\circ f(T(S,d))\\}  \\quad (1.16)$$\nS  $\\mathcal S$ d  D(S) R(S,d)  C(d|S)T(S,d) $\\circ$ \n\n### \n N  A  x  $p_x$ A={a,b,c} $p_a=0.2,p_b=0.5,p_c=0.3$ 6  abc,acb,bac,bca,cab,cba bca  1.7\n1. Strong separable S  \n   $1p_b+2p_c+3p_a$\n2. Weak separable W  \n   $(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$\n\n A  A'  S  x  $ip_x$ i  x  A'  W  A'  bca  {a,b,c} b  A'  {a,c}...  $\\sum_{x\\in D_i} p_x$\n\n i=1,2,3 $D_1=A,D_2=A-\\{d_1\\},D_3=A-\\{d_1,d_2\\}$ S  $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{1p_{d_1}+2p_{d_2}+3p_{d_3}\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{1p_{d_1}+\\min_{d_2 \\in A-\\{d_1\\}}\\{2p_{d_2}+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{3p_{d_3}\\}\\}\\} \\end{aligned}$$\n\n W  $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x$\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-\\{d_1\\}}\\{\\sum_{x\\in A-\\{d_1\\}}p_x+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\} \\end{aligned}$$\n\n____ bca \n\n### \n DP  S  A' DPFE \n$$f(S)=\\min_{x \\in S} \\{C(x|S)+f(S-\\{x\\})\\}     \\quad(1.17)$$\n $S \\in 2^A$$2^A$  A  $f(\\emptyset)=0$ $f(A)$\n\n DPFE DPFE (1.15) \n$$f(S)=\\min_{S'} \\{C(x|S')+f(S')\\}     \\quad(1.18)$$\n $S \\in 2^A$ $f(\\emptyset)$ $f(A)=0$S'  S  S'  x  S\n\n W\n$$C_W(x|S)=\\sum_{y\\in S}p_y$$\n x  S  S \n\n S\n$$C_S(x|S)=(N+1-|S|)p_x$$\n x  A'  A' ... A'  $C_S'(x|S)=|S|p_x$ S  $C(x|S)+f(S-\\{x\\})$ DP \n\n DP  (1.17)  DPFE \n$$\\begin{aligned} f(\\{a,b,c\\}) &= \\min\\{C(a|\\{a,b,c\\})+f(\\{b,c\\}), C(b|\\{a,b,c\\})+f(\\{a,c\\}), C(c|\\{a,b,c\\})+f(\\{a,b\\})\\}\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(b|\\{b,c\\}+f(\\{c\\}),C(c|\\{b,c\\}+f(\\{b\\})\\}\n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(a|\\{a,c\\}+f(\\{c\\}),C(c|\\{a,c\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(a|\\{a,b\\}+f(\\{b\\}),C(c|\\{a,b\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(c|\\{c\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(b|\\{b\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(a|\\{a\\})+f(\\emptyset)\\} \n\\\\\\\\ f(\\emptyset) &= 0 \\end{aligned}$$\n S  W \n\n (1.18)  DPFE \n$$\\begin{aligned} f(\\{a,b,c\\}) &= 0\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(a|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0 \n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(b|\\{a,a,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(c|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(a|\\{a,c\\})+f(\\{a,c\\}), C(b|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.5+1.0,0.8+1.0\\}=1.5\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(a|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.8+1.0\\}=1.7\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(b|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{a,c\\})+f(\\{a,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.5+1.0\\}=1.5\n\\\\\\\\ f(\\emptyset) &= \\min \\{C(a|a)+f(\\{a\\}), C(b|b)+f(\\{b\\}), C(c|c)+f(\\{c\\})\\}\\stackrel W = \\min \\{0.2+1.5,0.5+1.7,0.3+1.5\\}=1.7 \\end{aligned}$$\n $\\stackrel W=$  W  S \n\n### \n 1 2...  S DPFE  (1.17) \n$$f(k,S)=\\min_{x \\in S} \\{C(x|k,S)+f(k+1,S-\\{x\\})\\} \\quad(1.19)$$\n\n### Path-States\n S  $(d_1,...,d_{i-1})$ S  $\\emptyset$  S  $(d_1,...,d_{i-1})$ S  S={a,b} a  b a  b a  b  Path-States S=(a,b) a  b DPFE \n$$f(S)=\\min_{x \\notin S} \\{C(x|S)+f(S + (x))\\} \\qquad(1.20)$$\n\n\n$$\\begin{aligned} f(\\emptyset) &= \\min \\{C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)\\}\\stackrel S= \\min\\{0.2+1.9,0.5+1.2,0.3+1.6\\}=1.7\n\\\\\\\\ f(a) &= \\min \\{C(b|a)+f(ab), C(c|a)+f(ac)\\}\\stackrel S= \\min\\{2*0.5+0.9,2*0.3+1.5\\}=1.9\n\\\\\\\\ f(b) &= \\min \\{C(a|b)+f(ba), C(c|b)+f(bc)\\}\\stackrel S= \\min\\{2*0.2+0.9,2*0.3+0.6\\}=1.2\n\\\\\\\\ f(c) &= \\min \\{C(a|c)+f(ca), C(b|c)+f(cb)\\}\\stackrel S= \\min\\{2*0.2+1.5,2*0.5+0.6\\}=1.6\n\\\\\\\\ f(ab) &= \\min \\{C(c|ab)+f(abc)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(ac) &= \\min \\{C(b|ac)+f(acb)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(ba) &= \\min \\{C(c|ba)+f(bac)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(bc) &= \\min \\{C(a|bc)+f(bca)\\}\\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(ca) &= \\min \\{C(b|ca)+f(cab)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(cb) &= \\min \\{C(a|cb)+f(cba)\\} \\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(abc) &= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$\n $N!$  S  W  S $C(c|ab)$  c S  $C(c|ab)=3p_c=3*0.3=0.9$ $\\stackrel S=$  S  W \n\n###  Relaxation\n $\\{a_1,...,a_N\\}$\n$$x^{\\ast}=\\min\\{\\min\\{...\\{\\min\\{a_1,a_2\\},a_3\\},...\\},a_N\\}$$\n $x_1=a_1, x_2=\\min\\{x_1,a_2\\},...$ $x_1=\\min\\{x_0,a_1\\}, x_0=\\infty$ $x_1,x_2,...$  $x^{\\ast}$ DPFE \n$$\\begin{aligned} f(S)&=\\min_{x \\in S} \\{C(x|S)+f(S_x')\\}\n\\\\\\\\ &=\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}'),...,C(x_m|S)+f(S_{x_m}')\\} \\end{aligned}$$\n $S=\\{x_1,x_2,...,x_m\\}$$S_x'$  x  $C(x|S)+f(S_x')$ \n$$f(S)=\\min\\{\\min\\{...\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}')\\},...\\},C(x_m|S)+f(S_{x_m}')\\} \\quad (1.21)$$\n $C(x_i|S)$  S $f(S_{x_i}')$ \n\n DPFE \n$$f(k,S)=\\min_x \\{C(x|k,S) + f(k-1,S_x')\\} \\qquad(1.22)$$\n k  k  S  k  T (1.22) $f(0,S),f(1,S),...$  $f(S)$ $f(S)$ $f(S)=\\min_k \\{f(k,S)\\}$ $f(k,S)$  $f(k-1,S)$  $f(k-1,S_x')$  $f(k,S)$  $F(k,S)$ \n$$F(k,S)=\\min\\{F(k-1,S), \\min_x \\{C(x|k,S)+F(k-1,S_x')\\}\\} \\quad(1.23)$$\n $F(k,S)$  S  T  k  (1.22)  k  \n\n### \n\n\n s  t  DPFE \n$$f(p)=\\min_q \\{b(p,q)+f(q)\\} \\qquad(1.24)$$\n b(p,q)  p  q q  p f(p)  p  t  f(t)=0 q  p  $b(p,q)=\\infty$ f(p)  f(q)\n\np  q f(p)  f(q)  p  p \n1. b(p,p) > 0\n2. b(p,p) < 0\n3. b(p,p) = 0\n\nDPFE \n$$f(p) = \\min_q \\{b(p,q)+f(q)\\} \\qquad(1.25)$$\n f(q)  f(p) $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$ (1.25)  (1.24) \n\n 1.2\n![](/images/DP1_fig1.png)\n\n (1.25) \n$$\\begin{aligned}f(s)&=\\min \\{b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)\\}=\\min \\{3+f(x),5+f(y),\\infty+f(t)\\}\n\\\\\\\\f(x)&=\\min \\{b(x,y)+f(y),b(x,t)+f(t)\\}=\\min \\{1+f(y),8+f(t)\\}\n\\\\\\\\f(y)&=\\min \\{b(y,x)+f(x),b(y,t)+f(t)\\}=\\min \\{2+f(x),5+f(t)\\}\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n f(x)  f(y) \n\n $f(s)=f(x)=f(y)=\\infty$\n$$\\begin{aligned}f(s)&=\\min \\{3+\\infty,5+\\infty,\\infty+f(t)\\}=\\infty\n\\\\\\\\f(x)&=\\min \\{1+\\infty,8+0\\}=8\n\\\\\\\\f(y)&=\\min \\{2+\\infty,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n\n$$\\begin{aligned}f(s)&=\\min \\{3+8,5+5,\\infty+0\\}=10\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+8,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n$$\\begin{aligned}f(s)&=\\min \\{3+6,5+5,\\infty+0\\}=9\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+6,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n $f(x),f(y),f(t)$  f(s)  f(s)\n\n Relaxation  (1.22) DPFE \n$$f(k,p)=\\min_q \\{b(p,q)+f(k-1,q)\\} \\qquad(1.26)$$\n f(k,p)  p  t k  p  t  k  $f(0,t)=0;f(k,t)=\\infty, k>0;f(0,p)=\\infty, \\forall p \\ne t$ t  t  0  0 0  $\\infty$ t p  t  0  $\\infty$ p  (1.26) \n$$\\begin{aligned}f(k,s)&=\\min \\{b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)\\}\n\\\\\\\\f(k,x)&=\\min \\{b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)\\}\n\\\\\\\\f(k,y)&=\\min \\{b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)\\}\n\\\\\\\\f(k,t) \\end{aligned}$$\n k=0 \n$$\\begin{aligned}f(1,s)&=\\min \\{3+f(0,x),5+f(0,y),\\infty+f(0,t)\\}=\\infty\n\\\\\\\\f(1,x)&=\\min \\{1+f(0,y),8+f(0,t)\\}=8\n\\\\\\\\f(1,y)&=\\min \\{2+f(0,x),5+f(0,t)\\}=5\n\\\\\\\\f(1,t)&=\\infty \\end{aligned}$$\n\n$$\\begin{aligned}f(2,s)&=\\min \\{3+f(1,x),5+f(1,y),\\infty+f(1,t)\\}=10\n\\\\\\\\f(2,x)&=\\min \\{1+f(1,y),8+f(1,t)\\}=6\n\\\\\\\\f(2,y)&=\\min \\{2+f(1,x),5+f(1,t)\\}=10\n\\\\\\\\f(2,t)&=\\infty \\end{aligned}$$\n\n$$\\begin{aligned}f(3,s)&=\\min \\{3+f(2,x),5+f(2,y),\\infty+f(2,t)\\}=9\n\\\\\\\\f(3,x)&=\\min \\{1+f(2,y),8+f(2,t)\\}=11\n\\\\\\\\f(3,y)&=\\min \\{2+f(1,x),5+f(2,t)\\}=8\n\\\\\\\\f(3,t)&=\\infty \\end{aligned}$$\n N  p  t  k k  {0,1,...,N-1}k  N circle 0 N=4 k  3 f  $f(p)=\\min_k \\{f(k,p)\\}$ \n$$\\begin{aligned}f(s)&=\\min \\{\\infty,\\infty,10,9\\}=9\n\\\\\\\\f(x)&=\\min \\{\\infty,8,6,11\\}=6\n\\\\\\\\f(y)&=\\min \\{\\infty,5,10,8\\}=5\n\\\\\\\\f(t)&=\\min \\{0,\\infty,\\infty,\\infty\\}=0 \\end{aligned}$$\n (1.22)  $f(k,S)$ \n\n $f(k,S)$  (1.23)  DPFE\n$$F(k,p)=\\min \\{F(k-1,p), \\ \\min_q \\{b(p,q)+F(k-1,q)\\}\\} \\qquad(1.27)$$\n\n $F(k,p)$  p  t  k  p  t  $N-1$  $F(N-1,s)$k  $N-1$ $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$\n\n\n DPFE","source":"_posts/DP1.md","raw":"---\ntitle: Dynamic Programming (1)\ndate: 2019-08-07 17:29:56\ntags:\n    - math\n    - DP\ncategory: math\nmathjax: true\n---\n Dynamic Programming  A Computational Tool\n\n# \n\n / ``\n\n## \n\n> \n\n~~~~\n\n### \n$opt_{d \\in \\Delta} \\{H(d)\\}$ d  $\\Delta$H H(d)  $d^{\\ast}$$d^{\\ast}=\\arg opt_d \\{H(d)\\}$ $\\{d_1,...,d_n\\}$ $h(d_1,...,h_n)$  $H^{\\ast}$\n\n $\\{d_1,...,d_n\\}$    $d_1,...,d_n$\n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2}\\{...\\{opt_{d_n \\in D_n}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.1)\\end{aligned}$$\n\n $(d_1,...,d_n) \\in \\Delta=D_1 \\times ... \\times D_n$ i $d_i \\in D_i(d_1,...,d_{i-1})$ (1.1) \n$$\\begin{aligned}H^{\\ast}&=opt_{(d_1,...,d_n)\\in \\Delta} \\{h(d_1,...,d_n)\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.2)\\end{aligned}$$\n\n (1.2)  $d_n$ $d_n^{\\ast}(d_1,...,d_{n-1})$ $d_n^{\\ast}$  $opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast},...,d_n^{\\ast} \\}$  $d_1^{\\ast}$\n\n\n$$\\begin{aligned} &opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)} \\{... \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})} \\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_n \\in D_n} \\{opt_{d_{n-1} \\in D_{n-1}(d_n)} \\{... \\{opt_{d_1 \\in D_n(d_2,...,d_n)} \\{h(d_1,...,d_n)\\}\\}...\\}\\} \\quad(1.3) \\end{aligned}$$\n\n $D_i$  $D_i$  $(d_{i+1},...,d_n)$\n\n (1.2) $d_1$ $d_1$ \n$$\\begin{aligned}H^{\\ast}&=opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ &=opt_{d_1 \\in D_1}\\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1)\\} \\qquad \\qquad(1.4) \\end{aligned}$$\n\n $d_i^{\\ast}(d_1), \\ i>1$  $d_1$ partial function $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} \\{h(d_1,d_2^{\\ast}(d_1),...,d_n^{\\ast}(d_1))\\}$$d_1$ \n\n#### \n $d_1$  $d_2,...,d_n$  $opt_{d_1 \\in D_1}\\{H'(d_1)\\}$  $d_1$  $opt_{d_1}\\{H'(d_1)\\}$  $H^{\\ast}$  h \n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ ... \\circ C_n(d_n) \\qquad (1.5)$$\n $C_i$  $d_i$ $\\circ$     \n$$opt_d\\{a \\circ C(d)\\}=a \\circ opt_d\\{C(d)\\}$$\n a  d $C_n$  $d_n$ $(d_1,d_2,...,d_{n-1})$\n$$h(d_1,...,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\qquad(1.6)$$\n\n h \n$$h(d_1,...,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n) \\qquad(1.7)$$\n\n$$\\begin{aligned} & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{h(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{opt_{d_2 \\in D_2(d_1)}\\{...\\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_1(d_1) \\circ C_2(d_1,d_2) \\circ ... \\circ C_n(d_1,...,d_n)\\}\\}...\\}\\}\n\\\\\\\\ = \\ & opt_{d_1 \\in D_1} \\{C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}\\{C_2(d_1,d_2) \\circ ... \\circ \\{opt_{d_n \\in D_n(d_1,...,d_{n-1})}\\{C_n(d_1,...,d_n)\\}...\\}\\} \\qquad(1.8) \\end{aligned}$$\n $\\circ$ \n\n $f(d_1,...,d_n)$  $d_1,...,d_{i-1}$ \n$$f(d_1,...,d_{i-1})=opt_{d_i}\\{opt_{d_{i+1}}\\{... \\{opt_{d_n} \\{C_i(d_i|d_1,...,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,...,d_i) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1}) \\}\\}...\\}\\} \\ (1.9)$$\n $D_i$\n$$\\begin{aligned}f(\\emptyset)&=opt_{d_1}\\{opt_{d_2}\\{...\\{opt_{d_n}\\{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ ... \\circ C_n(d_n|d_1,...,d_{n-1})\\}\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ opt_{d_2}\\{C(d_2|d_1) \\circ...\\circ opt_{d_n}\\{C_n(d_n|d_1,...,d_{n-1})\\}...\\}\\}\n\\\\\\\\ &opt_{d_1}\\{C_1(d_1|\\emptyset) \\circ f(d_1)\\}  \\qquad \\quad (1.10) \\end{aligned}$$\n\n$$f(d_1,...,d_{i-1})=opt_{d_i \\in D_i(d_1,...,d_{i-1})}\\{C_i(d_i|d_1,...,d_{i-1})\\circ f(d_1,...,d_i)\\} \\qquad(1.11)$$\n\nDPFE\n\n### DPFE\n DPFE  $f(d_1,...,d_{i-1})$ $S=(d_1,...,d_{i-1})$ $i=|S|+1=|\\{d_1,...,d_{i-1}\\}|+1$ DPFE \n$$f(S)=opt_{d_i \\in D_i(S)}\\{C_i(d_i|S) \\circ f(S')\\} \\qquad (1.12)$$\n $S'=(d_1,...,d_i)$ $\\emptyset$  $\\mathcal S$ DPFE  $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$ $S_0$$f(S_0)$  DPFE  b\n\n n  S  d  D(S)  S  d  $d \\in S$ DPFE \n$$f(S)=opt_{d \\in S} \\{C(d|S) \\circ f(S')\\} \\qquad (1.13)$$\n\n S  d  S  S'D(S)  S  DPFE  b(S,S') d  C(d|S) $S'=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$T  DPFE \n$$f(S)=opt_S\\{b(S,S') \\circ f(S')\\} \\qquad(1.14) $$\n\nDPFE \n$$f'(S)=opt_{S'}\\{f'(S') \\circ b(S',S)\\} \\qquad(1.15) $$\n f'(S)  $S_0$  S  f(S)  S  $S_0$  (1.14) backward (1.15) forward\n\n### \n\n$$f(S)=opt_{d \\in D(S)} \\{R(S,d) \\circ f(T(S,d))\\}  \\quad (1.16)$$\nS  $\\mathcal S$ d  D(S) R(S,d)  C(d|S)T(S,d) $\\circ$ \n\n### \n N  A  x  $p_x$ A={a,b,c} $p_a=0.2,p_b=0.5,p_c=0.3$ 6  abc,acb,bac,bca,cab,cba bca  1.7\n1. Strong separable S  \n   $1p_b+2p_c+3p_a$\n2. Weak separable W  \n   $(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$\n\n A  A'  S  x  $ip_x$ i  x  A'  W  A'  bca  {a,b,c} b  A'  {a,c}...  $\\sum_{x\\in D_i} p_x$\n\n i=1,2,3 $D_1=A,D_2=A-\\{d_1\\},D_3=A-\\{d_1,d_2\\}$ S  $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{1p_{d_1}+2p_{d_2}+3p_{d_3}\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{1p_{d_1}+\\min_{d_2 \\in A-\\{d_1\\}}\\{2p_{d_2}+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{3p_{d_3}\\}\\}\\} \\end{aligned}$$\n\n W  $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x$\n$$\\begin{aligned}f(\\emptyset)&=\\min_{d_1\\in A}\\{\\min_{d_2\\in A-\\{d_1\\}}\\{\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A}p_x+\\sum_{x\\in A-\\{d_1\\}}p_x+\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\}\n\\\\\\\\ &=\\min_{d_1\\in A}\\{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-\\{d_1\\}}\\{\\sum_{x\\in A-\\{d_1\\}}p_x+\\min_{d_3\\in A-\\{d_1,d_2\\}}\\{\\sum_{x \\in A-\\{d_1,d_2\\}}p_x\\}\\}\\} \\end{aligned}$$\n\n____ bca \n\n### \n DP  S  A' DPFE \n$$f(S)=\\min_{x \\in S} \\{C(x|S)+f(S-\\{x\\})\\}     \\quad(1.17)$$\n $S \\in 2^A$$2^A$  A  $f(\\emptyset)=0$ $f(A)$\n\n DPFE DPFE (1.15) \n$$f(S)=\\min_{S'} \\{C(x|S')+f(S')\\}     \\quad(1.18)$$\n $S \\in 2^A$ $f(\\emptyset)$ $f(A)=0$S'  S  S'  x  S\n\n W\n$$C_W(x|S)=\\sum_{y\\in S}p_y$$\n x  S  S \n\n S\n$$C_S(x|S)=(N+1-|S|)p_x$$\n x  A'  A' ... A'  $C_S'(x|S)=|S|p_x$ S  $C(x|S)+f(S-\\{x\\})$ DP \n\n DP  (1.17)  DPFE \n$$\\begin{aligned} f(\\{a,b,c\\}) &= \\min\\{C(a|\\{a,b,c\\})+f(\\{b,c\\}), C(b|\\{a,b,c\\})+f(\\{a,c\\}), C(c|\\{a,b,c\\})+f(\\{a,b\\})\\}\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(b|\\{b,c\\}+f(\\{c\\}),C(c|\\{b,c\\}+f(\\{b\\})\\}\n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(a|\\{a,c\\}+f(\\{c\\}),C(c|\\{a,c\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(a|\\{a,b\\}+f(\\{b\\}),C(c|\\{a,b\\}+f(\\{a\\})\\}\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(c|\\{c\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(b|\\{b\\})+f(\\emptyset)\\}\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(a|\\{a\\})+f(\\emptyset)\\} \n\\\\\\\\ f(\\emptyset) &= 0 \\end{aligned}$$\n S  W \n\n (1.18)  DPFE \n$$\\begin{aligned} f(\\{a,b,c\\}) &= 0\n\\\\\\\\ f(\\{b,c\\}) &= \\min\\{C(a|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0 \n\\\\\\\\ f(\\{a,c\\}) &= \\min\\{C(b|\\{a,a,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{a,b\\}) &= \\min\\{C(c|\\{a,b,c\\}+f(\\{a,b,c\\})\\}\\stackrel W=\\min \\{1.0+0\\}=1.0\n\\\\\\\\ f(\\{c\\}) &= \\min \\{C(a|\\{a,c\\})+f(\\{a,c\\}), C(b|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.5+1.0,0.8+1.0\\}=1.5\n\\\\\\\\ f(\\{b\\}) &= \\min \\{C(a|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{b,c\\})+f(\\{b,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.8+1.0\\}=1.7\n\\\\\\\\ f(\\{a\\}) &= \\min \\{C(b|\\{a,b\\})+f(\\{a,b\\}), C(c|\\{a,c\\})+f(\\{a,c\\})\\}\\stackrel W = \\min \\{0.7+1.0,0.5+1.0\\}=1.5\n\\\\\\\\ f(\\emptyset) &= \\min \\{C(a|a)+f(\\{a\\}), C(b|b)+f(\\{b\\}), C(c|c)+f(\\{c\\})\\}\\stackrel W = \\min \\{0.2+1.5,0.5+1.7,0.3+1.5\\}=1.7 \\end{aligned}$$\n $\\stackrel W=$  W  S \n\n### \n 1 2...  S DPFE  (1.17) \n$$f(k,S)=\\min_{x \\in S} \\{C(x|k,S)+f(k+1,S-\\{x\\})\\} \\quad(1.19)$$\n\n### Path-States\n S  $(d_1,...,d_{i-1})$ S  $\\emptyset$  S  $(d_1,...,d_{i-1})$ S  S={a,b} a  b a  b a  b  Path-States S=(a,b) a  b DPFE \n$$f(S)=\\min_{x \\notin S} \\{C(x|S)+f(S + (x))\\} \\qquad(1.20)$$\n\n\n$$\\begin{aligned} f(\\emptyset) &= \\min \\{C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)\\}\\stackrel S= \\min\\{0.2+1.9,0.5+1.2,0.3+1.6\\}=1.7\n\\\\\\\\ f(a) &= \\min \\{C(b|a)+f(ab), C(c|a)+f(ac)\\}\\stackrel S= \\min\\{2*0.5+0.9,2*0.3+1.5\\}=1.9\n\\\\\\\\ f(b) &= \\min \\{C(a|b)+f(ba), C(c|b)+f(bc)\\}\\stackrel S= \\min\\{2*0.2+0.9,2*0.3+0.6\\}=1.2\n\\\\\\\\ f(c) &= \\min \\{C(a|c)+f(ca), C(b|c)+f(cb)\\}\\stackrel S= \\min\\{2*0.2+1.5,2*0.5+0.6\\}=1.6\n\\\\\\\\ f(ab) &= \\min \\{C(c|ab)+f(abc)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(ac) &= \\min \\{C(b|ac)+f(acb)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(ba) &= \\min \\{C(c|ba)+f(bac)\\}\\stackrel S= 3*0.3=0.9\n\\\\\\\\ f(bc) &= \\min \\{C(a|bc)+f(bca)\\}\\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(ca) &= \\min \\{C(b|ca)+f(cab)\\}\\stackrel S= 3*0.5=1.5\n\\\\\\\\ f(cb) &= \\min \\{C(a|cb)+f(cba)\\} \\stackrel S= 3*0.2=0.6\n\\\\\\\\ f(abc) &= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$\n $N!$  S  W  S $C(c|ab)$  c S  $C(c|ab)=3p_c=3*0.3=0.9$ $\\stackrel S=$  S  W \n\n###  Relaxation\n $\\{a_1,...,a_N\\}$\n$$x^{\\ast}=\\min\\{\\min\\{...\\{\\min\\{a_1,a_2\\},a_3\\},...\\},a_N\\}$$\n $x_1=a_1, x_2=\\min\\{x_1,a_2\\},...$ $x_1=\\min\\{x_0,a_1\\}, x_0=\\infty$ $x_1,x_2,...$  $x^{\\ast}$ DPFE \n$$\\begin{aligned} f(S)&=\\min_{x \\in S} \\{C(x|S)+f(S_x')\\}\n\\\\\\\\ &=\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}'),...,C(x_m|S)+f(S_{x_m}')\\} \\end{aligned}$$\n $S=\\{x_1,x_2,...,x_m\\}$$S_x'$  x  $C(x|S)+f(S_x')$ \n$$f(S)=\\min\\{\\min\\{...\\min\\{C(x_1|S)+f(S_{x_1}'), C(x_2|S)+f(S_{x_2}')\\},...\\},C(x_m|S)+f(S_{x_m}')\\} \\quad (1.21)$$\n $C(x_i|S)$  S $f(S_{x_i}')$ \n\n DPFE \n$$f(k,S)=\\min_x \\{C(x|k,S) + f(k-1,S_x')\\} \\qquad(1.22)$$\n k  k  S  k  T (1.22) $f(0,S),f(1,S),...$  $f(S)$ $f(S)$ $f(S)=\\min_k \\{f(k,S)\\}$ $f(k,S)$  $f(k-1,S)$  $f(k-1,S_x')$  $f(k,S)$  $F(k,S)$ \n$$F(k,S)=\\min\\{F(k-1,S), \\min_x \\{C(x|k,S)+F(k-1,S_x')\\}\\} \\quad(1.23)$$\n $F(k,S)$  S  T  k  (1.22)  k  \n\n### \n\n\n s  t  DPFE \n$$f(p)=\\min_q \\{b(p,q)+f(q)\\} \\qquad(1.24)$$\n b(p,q)  p  q q  p f(p)  p  t  f(t)=0 q  p  $b(p,q)=\\infty$ f(p)  f(q)\n\np  q f(p)  f(q)  p  p \n1. b(p,p) > 0\n2. b(p,p) < 0\n3. b(p,p) = 0\n\nDPFE \n$$f(p) = \\min_q \\{b(p,q)+f(q)\\} \\qquad(1.25)$$\n f(q)  f(p) $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$ (1.25)  (1.24) \n\n 1.2\n![](/images/DP1_fig1.png)\n\n (1.25) \n$$\\begin{aligned}f(s)&=\\min \\{b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)\\}=\\min \\{3+f(x),5+f(y),\\infty+f(t)\\}\n\\\\\\\\f(x)&=\\min \\{b(x,y)+f(y),b(x,t)+f(t)\\}=\\min \\{1+f(y),8+f(t)\\}\n\\\\\\\\f(y)&=\\min \\{b(y,x)+f(x),b(y,t)+f(t)\\}=\\min \\{2+f(x),5+f(t)\\}\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n f(x)  f(y) \n\n $f(s)=f(x)=f(y)=\\infty$\n$$\\begin{aligned}f(s)&=\\min \\{3+\\infty,5+\\infty,\\infty+f(t)\\}=\\infty\n\\\\\\\\f(x)&=\\min \\{1+\\infty,8+0\\}=8\n\\\\\\\\f(y)&=\\min \\{2+\\infty,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n\n$$\\begin{aligned}f(s)&=\\min \\{3+8,5+5,\\infty+0\\}=10\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+8,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n\n$$\\begin{aligned}f(s)&=\\min \\{3+6,5+5,\\infty+0\\}=9\n\\\\\\\\f(x)&=\\min \\{1+5,8+0\\}=6\n\\\\\\\\f(y)&=\\min \\{2+6,5+0\\}=5\n\\\\\\\\f(t)&=0 \\end{aligned}$$\n $f(x),f(y),f(t)$  f(s)  f(s)\n\n Relaxation  (1.22) DPFE \n$$f(k,p)=\\min_q \\{b(p,q)+f(k-1,q)\\} \\qquad(1.26)$$\n f(k,p)  p  t k  p  t  k  $f(0,t)=0;f(k,t)=\\infty, k>0;f(0,p)=\\infty, \\forall p \\ne t$ t  t  0  0 0  $\\infty$ t p  t  0  $\\infty$ p  (1.26) \n$$\\begin{aligned}f(k,s)&=\\min \\{b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)\\}\n\\\\\\\\f(k,x)&=\\min \\{b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)\\}\n\\\\\\\\f(k,y)&=\\min \\{b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)\\}\n\\\\\\\\f(k,t) \\end{aligned}$$\n k=0 \n$$\\begin{aligned}f(1,s)&=\\min \\{3+f(0,x),5+f(0,y),\\infty+f(0,t)\\}=\\infty\n\\\\\\\\f(1,x)&=\\min \\{1+f(0,y),8+f(0,t)\\}=8\n\\\\\\\\f(1,y)&=\\min \\{2+f(0,x),5+f(0,t)\\}=5\n\\\\\\\\f(1,t)&=\\infty \\end{aligned}$$\n\n$$\\begin{aligned}f(2,s)&=\\min \\{3+f(1,x),5+f(1,y),\\infty+f(1,t)\\}=10\n\\\\\\\\f(2,x)&=\\min \\{1+f(1,y),8+f(1,t)\\}=6\n\\\\\\\\f(2,y)&=\\min \\{2+f(1,x),5+f(1,t)\\}=10\n\\\\\\\\f(2,t)&=\\infty \\end{aligned}$$\n\n$$\\begin{aligned}f(3,s)&=\\min \\{3+f(2,x),5+f(2,y),\\infty+f(2,t)\\}=9\n\\\\\\\\f(3,x)&=\\min \\{1+f(2,y),8+f(2,t)\\}=11\n\\\\\\\\f(3,y)&=\\min \\{2+f(1,x),5+f(2,t)\\}=8\n\\\\\\\\f(3,t)&=\\infty \\end{aligned}$$\n N  p  t  k k  {0,1,...,N-1}k  N circle 0 N=4 k  3 f  $f(p)=\\min_k \\{f(k,p)\\}$ \n$$\\begin{aligned}f(s)&=\\min \\{\\infty,\\infty,10,9\\}=9\n\\\\\\\\f(x)&=\\min \\{\\infty,8,6,11\\}=6\n\\\\\\\\f(y)&=\\min \\{\\infty,5,10,8\\}=5\n\\\\\\\\f(t)&=\\min \\{0,\\infty,\\infty,\\infty\\}=0 \\end{aligned}$$\n (1.22)  $f(k,S)$ \n\n $f(k,S)$  (1.23)  DPFE\n$$F(k,p)=\\min \\{F(k-1,p), \\ \\min_q \\{b(p,q)+F(k-1,q)\\}\\} \\qquad(1.27)$$\n\n $F(k,p)$  p  t  k  p  t  $N-1$  $F(N-1,s)$k  $N-1$ $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$\n\n\n DPFE","slug":"DP1","published":1,"updated":"2019-08-16T11:02:54.849Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379f2001xdgvckxdxe07t","content":"<p> Dynamic Programming  A Computational Tool</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> / <code></code></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><blockquote>\n<p></p>\n</blockquote>\n<p><del></del></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$opt_{d \\in \\Delta} {H(d)}$ d  $\\Delta$H H(d)  $d^{\\ast}$$d^{\\ast}=\\arg opt_d {H(d)}$ ${d_1,,d_n}$ $h(d_1,,h_n)$  $H^{\\ast}$</p>\n<p> ${d_1,,d_n}$    $d_1,,d_n$<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,,d_n)\\in \\Delta} {h(d_1,,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2}{{opt_{d_n \\in D_n}{h(d_1,,d_n)}}}} \\quad(1.1)\\end{aligned}$$</p>\n<p> $(d_1,,d_n) \\in \\Delta=D_1 \\times  \\times D_n$ i $d_i \\in D_i(d_1,,d_{i-1})$ (1.1) <br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,,d_n)\\in \\Delta} {h(d_1,,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}} \\quad(1.2)\\end{aligned}$$</p>\n<p> (1.2)  $d_n$ $d_n^{\\ast}(d_1,,d_{n-1})$ $d_n^{\\ast}$  $opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast},,d_n^{\\ast} }$  $d_1^{\\ast}$</p>\n<p><br>$$\\begin{aligned} &amp;opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)} { {opt_{d_n \\in D_n(d_1,,d_{n-1})} {h(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_n \\in D_n} {opt_{d_{n-1} \\in D_{n-1}(d_n)} { {opt_{d_1 \\in D_n(d_2,,d_n)} {h(d_1,,d_n)}}}} \\quad(1.3) \\end{aligned}$$</p>\n<p> $D_i$  $D_i$  $(d_{i+1},,d_n)$</p>\n<p> (1.2) $d_1$ $d_1$ <br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}}<br>\\\\ &amp;=opt_{d_1 \\in D_1}{h(d_1,d_2^{\\ast}(d_1),,d_n^{\\ast}(d_1)} \\qquad \\qquad(1.4) \\end{aligned}$$</p>\n<p> $d_i^{\\ast}(d_1), \\ i&gt;1$  $d_1$ partial function $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast}(d_1),,d_n^{\\ast}(d_1))}$$d_1$ </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p> $d_1$  $d_2,,d_n$  $opt_{d_1 \\in D_1}{H(d_1)}$  $d_1$  $opt_{d_1}{H(d_1)}$  $H^{\\ast}$  h <br>$$h(d_1,,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ  \\circ C_n(d_n) \\qquad (1.5)$$<br> $C_i$  $d_i$ $\\circ$    <br>$$opt_d{a \\circ C(d)}=a \\circ opt_d{C(d)}$$<br> a  d $C_n$  $d_n$ $(d_1,d_2,,d_{n-1})$<br>$$h(d_1,,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ  \\circ C_n(d_n|d_1,,d_{n-1}) \\qquad(1.6)$$</p>\n<p> h <br>$$h(d_1,,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ  \\circ C_n(d_1,,d_n) \\qquad(1.7)$$<br><br>$$\\begin{aligned} &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{C_1(d_1) \\circ C_2(d_1,d_2) \\circ  \\circ C_n(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}{C_2(d_1,d_2) \\circ  \\circ {opt_{d_n \\in D_n(d_1,,d_{n-1})}{C_n(d_1,,d_n)}}} \\qquad(1.8) \\end{aligned}$$<br> $\\circ$ </p>\n<p> $f(d_1,,d_n)$  $d_1,,d_{i-1}$ <br>$$f(d_1,,d_{i-1})=opt_{d_i}{opt_{d_{i+1}}{ {opt_{d_n} {C_i(d_i|d_1,,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,,d_i) \\circ  \\circ C_n(d_n|d_1,,d_{n-1}) }}}} \\ (1.9)$$<br> $D_i$<br>$$\\begin{aligned}f(\\emptyset)&amp;=opt_{d_1}{opt_{d_2}{{opt_{d_n}{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ  \\circ C_n(d_n|d_1,,d_{n-1})}}}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ opt_{d_2}{C(d_2|d_1) \\circ\\circ opt_{d_n}{C_n(d_n|d_1,,d_{n-1})}}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ f(d_1)}  \\qquad \\quad (1.10) \\end{aligned}$$<br><br>$$f(d_1,,d_{i-1})=opt_{d_i \\in D_i(d_1,,d_{i-1})}{C_i(d_i|d_1,,d_{i-1})\\circ f(d_1,,d_i)} \\qquad(1.11)$$</p>\n<p>DPFE</p>\n<h3 id=\"DPFE\"><a href=\"#DPFE\" class=\"headerlink\" title=\"DPFE\"></a>DPFE</h3><p> DPFE  $f(d_1,,d_{i-1})$ $S=(d_1,,d_{i-1})$ $i=|S|+1=|{d_1,,d_{i-1}}|+1$ DPFE <br>$$f(S)=opt_{d_i \\in D_i(S)}{C_i(d_i|S) \\circ f(S)} \\qquad (1.12)$$<br> $S=(d_1,,d_i)$ $\\emptyset$  $\\mathcal S$ DPFE  $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$ $S_0$$f(S_0)$  DPFE  b</p>\n<p> n  S  d  D(S)  S  d  $d \\in S$ DPFE <br>$$f(S)=opt_{d \\in S} {C(d|S) \\circ f(S)} \\qquad (1.13)$$</p>\n<p> S  d  S  SD(S)  S  DPFE  b(S,S) d  C(d|S) $S=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$T  DPFE <br>$$f(S)=opt_S{b(S,S) \\circ f(S)} \\qquad(1.14) $$</p>\n<p>DPFE <br>$$f(S)=opt_{S}{f(S) \\circ b(S,S)} \\qquad(1.15) $$<br> f(S)  $S_0$  S  f(S)  S  $S_0$  (1.14) backward (1.15) forward</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br>$$f(S)=opt_{d \\in D(S)} {R(S,d) \\circ f(T(S,d))}  \\quad (1.16)$$<br>S  $\\mathcal S$ d  D(S) R(S,d)  C(d|S)T(S,d) $\\circ$ </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> N  A  x  $p_x$ A={a,b,c} $p_a=0.2,p_b=0.5,p_c=0.3$ 6  abc,acb,bac,bca,cab,cba bca  1.7</p>\n<ol>\n<li>Strong separable S<br>$1p_b+2p_c+3p_a$</li>\n<li>Weak separable W<br>$(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$</li>\n</ol>\n<p> A  A  S  x  $ip_x$ i  x  A  W  A  bca  {a,b,c} b  A  {a,c}  $\\sum_{x\\in D_i} p_x$</p>\n<p> i=1,2,3 $D_1=A,D_2=A-{d_1},D_3=A-{d_1,d_2}$ S  $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{1p_{d_1}+2p_{d_2}+3p_{d_3}}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{1p_{d_1}+\\min_{d_2 \\in A-{d_1}}{2p_{d_2}+\\min_{d_3\\in A-{d_1,d_2}}{3p_{d_3}}}} \\end{aligned}$$</p>\n<p> W  $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x$<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-{d_1}}{\\sum_{x\\in A-{d_1}}p_x+\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A-{d_1,d_2}}p_x}}} \\end{aligned}$$</p>\n<p><strong></strong> bca </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> DP  S  A DPFE <br>$$f(S)=\\min_{x \\in S} {C(x|S)+f(S-{x})}     \\quad(1.17)$$<br> $S \\in 2^A$$2^A$  A  $f(\\emptyset)=0$ $f(A)$</p>\n<p> DPFE DPFE (1.15) <br>$$f(S)=\\min_{S} {C(x|S)+f(S)}     \\quad(1.18)$$<br> $S \\in 2^A$ $f(\\emptyset)$ $f(A)=0$S  S  S  x  S</p>\n<p> W<br>$$C_W(x|S)=\\sum_{y\\in S}p_y$$<br> x  S  S </p>\n<p> S<br>$$C_S(x|S)=(N+1-|S|)p_x$$<br> x  A  A  A  $C_S(x|S)=|S|p_x$ S  $C(x|S)+f(S-{x})$ DP </p>\n<p> DP  (1.17)  DPFE <br>$$\\begin{aligned} f({a,b,c}) &amp;= \\min{C(a|{a,b,c})+f({b,c}), C(b|{a,b,c})+f({a,c}), C(c|{a,b,c})+f({a,b})}<br>\\\\ f({b,c}) &amp;= \\min{C(b|{b,c}+f({c}),C(c|{b,c}+f({b})}<br>\\\\ f({a,c}) &amp;= \\min{C(a|{a,c}+f({c}),C(c|{a,c}+f({a})}<br>\\\\ f({a,b}) &amp;= \\min{C(a|{a,b}+f({b}),C(c|{a,b}+f({a})}<br>\\\\ f({c}) &amp;= \\min {C(c|{c})+f(\\emptyset)}<br>\\\\ f({b}) &amp;= \\min {C(b|{b})+f(\\emptyset)}<br>\\\\ f({a}) &amp;= \\min {C(a|{a})+f(\\emptyset)}<br>\\\\ f(\\emptyset) &amp;= 0 \\end{aligned}$$<br> S  W </p>\n<p> (1.18)  DPFE <br>$$\\begin{aligned} f({a,b,c}) &amp;= 0<br>\\\\ f({b,c}) &amp;= \\min{C(a|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,c}) &amp;= \\min{C(b|{a,a,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,b}) &amp;= \\min{C(c|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({c}) &amp;= \\min {C(a|{a,c})+f({a,c}), C(b|{b,c})+f({b,c})}\\stackrel W = \\min {0.5+1.0,0.8+1.0}=1.5<br>\\\\ f({b}) &amp;= \\min {C(a|{a,b})+f({a,b}), C(c|{b,c})+f({b,c})}\\stackrel W = \\min {0.7+1.0,0.8+1.0}=1.7<br>\\\\ f({a}) &amp;= \\min {C(b|{a,b})+f({a,b}), C(c|{a,c})+f({a,c})}\\stackrel W = \\min {0.7+1.0,0.5+1.0}=1.5<br>\\\\ f(\\emptyset) &amp;= \\min {C(a|a)+f({a}), C(b|b)+f({b}), C(c|c)+f({c})}\\stackrel W = \\min {0.2+1.5,0.5+1.7,0.3+1.5}=1.7 \\end{aligned}$$<br> $\\stackrel W=$  W  S </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 1 2  S DPFE  (1.17) <br>$$f(k,S)=\\min_{x \\in S} {C(x|k,S)+f(k+1,S-{x})} \\quad(1.19)$$</p>\n<h3 id=\"Path-States\"><a href=\"#Path-States\" class=\"headerlink\" title=\"Path-States\"></a>Path-States</h3><p> S  $(d_1,,d_{i-1})$ S  $\\emptyset$  S  $(d_1,,d_{i-1})$ S  S={a,b} a  b a  b a  b  Path-States S=(a,b) a  b DPFE <br>$$f(S)=\\min_{x \\notin S} {C(x|S)+f(S + (x))} \\qquad(1.20)$$</p>\n<p><br>$$\\begin{aligned} f(\\emptyset) &amp;= \\min {C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)}\\stackrel S= \\min{0.2+1.9,0.5+1.2,0.3+1.6}=1.7<br>\\\\ f(a) &amp;= \\min {C(b|a)+f(ab), C(c|a)+f(ac)}\\stackrel S= \\min{2<em>0.5+0.9,2</em>0.3+1.5}=1.9<br>\\\\ f(b) &amp;= \\min {C(a|b)+f(ba), C(c|b)+f(bc)}\\stackrel S= \\min{2<em>0.2+0.9,2</em>0.3+0.6}=1.2<br>\\\\ f(c) &amp;= \\min {C(a|c)+f(ca), C(b|c)+f(cb)}\\stackrel S= \\min{2<em>0.2+1.5,2</em>0.5+0.6}=1.6<br>\\\\ f(ab) &amp;= \\min {C(c|ab)+f(abc)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(ac) &amp;= \\min {C(b|ac)+f(acb)}\\stackrel S= 3</em>0.5=1.5<br>\\\\ f(ba) &amp;= \\min {C(c|ba)+f(bac)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(bc) &amp;= \\min {C(a|bc)+f(bca)}\\stackrel S= 3</em>0.2=0.6<br>\\\\ f(ca) &amp;= \\min {C(b|ca)+f(cab)}\\stackrel S= 3<em>0.5=1.5<br>\\\\ f(cb) &amp;= \\min {C(a|cb)+f(cba)} \\stackrel S= 3</em>0.2=0.6<br>\\\\ f(abc) &amp;= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$<br> $N!$  S  W  S $C(c|ab)$  c S  $C(c|ab)=3p_c=3*0.3=0.9$ $\\stackrel S=$  S  W </p>\n<h3 id=\"-Relaxation\"><a href=\"#-Relaxation\" class=\"headerlink\" title=\" Relaxation\"></a> Relaxation</h3><p> ${a_1,,a_N}$<br>$$x^{\\ast}=\\min{\\min{{\\min{a_1,a_2},a_3},},a_N}$$<br> $x_1=a_1, x_2=\\min{x_1,a_2},$ $x_1=\\min{x_0,a_1}, x_0=\\infty$ $x_1,x_2,$  $x^{\\ast}$ DPFE <br>$$\\begin{aligned} f(S)&amp;=\\min_{x \\in S} {C(x|S)+f(S_x)}<br>\\\\ &amp;=\\min{C(x_1|S)+f(S_{x_1}), C(x_2|S)+f(S_{x_2}),,C(x_m|S)+f(S_{x_m})} \\end{aligned}$$<br> $S={x_1,x_2,,x_m}$$S_x$  x  $C(x|S)+f(S_x)$ <br>$$f(S)=\\min{\\min{\\min{C(x_1|S)+f(S_{x_1}), C(x_2|S)+f(S_{x_2})},},C(x_m|S)+f(S_{x_m})} \\quad (1.21)$$<br> $C(x_i|S)$  S $f(S_{x_i})$ </p>\n<p> DPFE <br>$$f(k,S)=\\min_x {C(x|k,S) + f(k-1,S_x)} \\qquad(1.22)$$<br> k  k  S  k  T (1.22) $f(0,S),f(1,S),$  $f(S)$ $f(S)$ $f(S)=\\min_k {f(k,S)}$ $f(k,S)$  $f(k-1,S)$  $f(k-1,S_x)$  $f(k,S)$  $F(k,S)$ <br>$$F(k,S)=\\min{F(k-1,S), \\min_x {C(x|k,S)+F(k-1,S_x)}} \\quad(1.23)$$<br> $F(k,S)$  S  T  k  (1.22)  k  </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<p> s  t  DPFE <br>$$f(p)=\\min_q {b(p,q)+f(q)} \\qquad(1.24)$$<br> b(p,q)  p  q q  p f(p)  p  t  f(t)=0 q  p  $b(p,q)=\\infty$ f(p)  f(q)</p>\n<p>p  q f(p)  f(q)  p  p </p>\n<ol>\n<li>b(p,p) &gt; 0</li>\n<li>b(p,p) &lt; 0</li>\n<li>b(p,p) = 0</li>\n</ol>\n<p>DPFE <br>$$f(p) = \\min_q {b(p,q)+f(q)} \\qquad(1.25)$$<br> f(q)  f(p) $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$ (1.25)  (1.24) </p>\n<p> 1.2<br><img src=\"/images/DP1_fig1.png\" alt></p>\n<p> (1.25) <br>$$\\begin{aligned}f(s)&amp;=\\min {b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)}=\\min {3+f(x),5+f(y),\\infty+f(t)}<br>\\\\f(x)&amp;=\\min {b(x,y)+f(y),b(x,t)+f(t)}=\\min {1+f(y),8+f(t)}<br>\\\\f(y)&amp;=\\min {b(y,x)+f(x),b(y,t)+f(t)}=\\min {2+f(x),5+f(t)}<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br> f(x)  f(y) </p>\n<p> $f(s)=f(x)=f(y)=\\infty$<br>$$\\begin{aligned}f(s)&amp;=\\min {3+\\infty,5+\\infty,\\infty+f(t)}=\\infty<br>\\\\f(x)&amp;=\\min {1+\\infty,8+0}=8<br>\\\\f(y)&amp;=\\min {2+\\infty,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$</p>\n<p><br>$$\\begin{aligned}f(s)&amp;=\\min {3+8,5+5,\\infty+0}=10<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+8,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br><br>$$\\begin{aligned}f(s)&amp;=\\min {3+6,5+5,\\infty+0}=9<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+6,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br> $f(x),f(y),f(t)$  f(s)  f(s)</p>\n<p> Relaxation  (1.22) DPFE <br>$$f(k,p)=\\min_q {b(p,q)+f(k-1,q)} \\qquad(1.26)$$<br> f(k,p)  p  t k  p  t  k  $f(0,t)=0;f(k,t)=\\infty, k&gt;0;f(0,p)=\\infty, \\forall p \\ne t$ t  t  0  0 0  $\\infty$ t p  t  0  $\\infty$ p  (1.26) <br>$$\\begin{aligned}f(k,s)&amp;=\\min {b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)}<br>\\\\f(k,x)&amp;=\\min {b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)}<br>\\\\f(k,y)&amp;=\\min {b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)}<br>\\\\f(k,t) \\end{aligned}$$<br> k=0 <br>$$\\begin{aligned}f(1,s)&amp;=\\min {3+f(0,x),5+f(0,y),\\infty+f(0,t)}=\\infty<br>\\\\f(1,x)&amp;=\\min {1+f(0,y),8+f(0,t)}=8<br>\\\\f(1,y)&amp;=\\min {2+f(0,x),5+f(0,t)}=5<br>\\\\f(1,t)&amp;=\\infty \\end{aligned}$$<br><br>$$\\begin{aligned}f(2,s)&amp;=\\min {3+f(1,x),5+f(1,y),\\infty+f(1,t)}=10<br>\\\\f(2,x)&amp;=\\min {1+f(1,y),8+f(1,t)}=6<br>\\\\f(2,y)&amp;=\\min {2+f(1,x),5+f(1,t)}=10<br>\\\\f(2,t)&amp;=\\infty \\end{aligned}$$<br><br>$$\\begin{aligned}f(3,s)&amp;=\\min {3+f(2,x),5+f(2,y),\\infty+f(2,t)}=9<br>\\\\f(3,x)&amp;=\\min {1+f(2,y),8+f(2,t)}=11<br>\\\\f(3,y)&amp;=\\min {2+f(1,x),5+f(2,t)}=8<br>\\\\f(3,t)&amp;=\\infty \\end{aligned}$$<br> N  p  t  k k  {0,1,,N-1}k  N circle 0 N=4 k  3 f  $f(p)=\\min_k {f(k,p)}$ <br>$$\\begin{aligned}f(s)&amp;=\\min {\\infty,\\infty,10,9}=9<br>\\\\f(x)&amp;=\\min {\\infty,8,6,11}=6<br>\\\\f(y)&amp;=\\min {\\infty,5,10,8}=5<br>\\\\f(t)&amp;=\\min {0,\\infty,\\infty,\\infty}=0 \\end{aligned}$$<br> (1.22)  $f(k,S)$ </p>\n<p> $f(k,S)$  (1.23)  DPFE<br>$$F(k,p)=\\min {F(k-1,p), \\ \\min_q {b(p,q)+F(k-1,q)}} \\qquad(1.27)$$</p>\n<p> $F(k,p)$  p  t  k  p  t  $N-1$  $F(N-1,s)$k  $N-1$ $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$</p>\n<p> DPFE</p>\n","site":{"data":{}},"excerpt":"","more":"<p> Dynamic Programming  A Computational Tool</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> / <code></code></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><blockquote>\n<p></p>\n</blockquote>\n<p><del></del></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$opt_{d \\in \\Delta} {H(d)}$ d  $\\Delta$H H(d)  $d^{\\ast}$$d^{\\ast}=\\arg opt_d {H(d)}$ ${d_1,,d_n}$ $h(d_1,,h_n)$  $H^{\\ast}$</p>\n<p> ${d_1,,d_n}$    $d_1,,d_n$<br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,,d_n)\\in \\Delta} {h(d_1,,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2}{{opt_{d_n \\in D_n}{h(d_1,,d_n)}}}} \\quad(1.1)\\end{aligned}$$</p>\n<p> $(d_1,,d_n) \\in \\Delta=D_1 \\times  \\times D_n$ i $d_i \\in D_i(d_1,,d_{i-1})$ (1.1) <br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{(d_1,,d_n)\\in \\Delta} {h(d_1,,d_n)}<br>\\\\ &amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}} \\quad(1.2)\\end{aligned}$$</p>\n<p> (1.2)  $d_n$ $d_n^{\\ast}(d_1,,d_{n-1})$ $d_n^{\\ast}$  $opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast},,d_n^{\\ast} }$  $d_1^{\\ast}$</p>\n<p><br>$$\\begin{aligned} &amp;opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)} { {opt_{d_n \\in D_n(d_1,,d_{n-1})} {h(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_n \\in D_n} {opt_{d_{n-1} \\in D_{n-1}(d_n)} { {opt_{d_1 \\in D_n(d_2,,d_n)} {h(d_1,,d_n)}}}} \\quad(1.3) \\end{aligned}$$</p>\n<p> $D_i$  $D_i$  $(d_{i+1},,d_n)$</p>\n<p> (1.2) $d_1$ $d_1$ <br>$$\\begin{aligned}H^{\\ast}&amp;=opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}}<br>\\\\ &amp;=opt_{d_1 \\in D_1}{h(d_1,d_2^{\\ast}(d_1),,d_n^{\\ast}(d_1)} \\qquad \\qquad(1.4) \\end{aligned}$$</p>\n<p> $d_i^{\\ast}(d_1), \\ i&gt;1$  $d_1$ partial function $d_1^{\\ast}=\\arg opt_{d_1 \\in D_1} {h(d_1,d_2^{\\ast}(d_1),,d_n^{\\ast}(d_1))}$$d_1$ </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p> $d_1$  $d_2,,d_n$  $opt_{d_1 \\in D_1}{H(d_1)}$  $d_1$  $opt_{d_1}{H(d_1)}$  $H^{\\ast}$  h <br>$$h(d_1,,d_n)=C_1(d_1) \\circ C_2(d_2) \\circ  \\circ C_n(d_n) \\qquad (1.5)$$<br> $C_i$  $d_i$ $\\circ$    <br>$$opt_d{a \\circ C(d)}=a \\circ opt_d{C(d)}$$<br> a  d $C_n$  $d_n$ $(d_1,d_2,,d_{n-1})$<br>$$h(d_1,,d_n)=C_1(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ  \\circ C_n(d_n|d_1,,d_{n-1}) \\qquad(1.6)$$</p>\n<p> h <br>$$h(d_1,,d_n)=C_1(d_1) \\circ C_2(d_1,d_2) \\circ  \\circ C_n(d_1,,d_n) \\qquad(1.7)$$<br><br>$$\\begin{aligned} &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{h(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {opt_{d_2 \\in D_2(d_1)}{{opt_{d_n \\in D_n(d_1,,d_{n-1})}{C_1(d_1) \\circ C_2(d_1,d_2) \\circ  \\circ C_n(d_1,,d_n)}}}}<br>\\\\ = \\ &amp; opt_{d_1 \\in D_1} {C_1(d_1|\\emptyset) \\circ opt_{d_2 \\in D_2(d_1)}{C_2(d_1,d_2) \\circ  \\circ {opt_{d_n \\in D_n(d_1,,d_{n-1})}{C_n(d_1,,d_n)}}} \\qquad(1.8) \\end{aligned}$$<br> $\\circ$ </p>\n<p> $f(d_1,,d_n)$  $d_1,,d_{i-1}$ <br>$$f(d_1,,d_{i-1})=opt_{d_i}{opt_{d_{i+1}}{ {opt_{d_n} {C_i(d_i|d_1,,d_{i-1}) \\circ C_{i+1}(d_{i+1}|d_1,,d_i) \\circ  \\circ C_n(d_n|d_1,,d_{n-1}) }}}} \\ (1.9)$$<br> $D_i$<br>$$\\begin{aligned}f(\\emptyset)&amp;=opt_{d_1}{opt_{d_2}{{opt_{d_n}{C(d_1|\\emptyset) \\circ C_2(d_2|d_1) \\circ  \\circ C_n(d_n|d_1,,d_{n-1})}}}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ opt_{d_2}{C(d_2|d_1) \\circ\\circ opt_{d_n}{C_n(d_n|d_1,,d_{n-1})}}}<br>\\\\ &amp;opt_{d_1}{C_1(d_1|\\emptyset) \\circ f(d_1)}  \\qquad \\quad (1.10) \\end{aligned}$$<br><br>$$f(d_1,,d_{i-1})=opt_{d_i \\in D_i(d_1,,d_{i-1})}{C_i(d_i|d_1,,d_{i-1})\\circ f(d_1,,d_i)} \\qquad(1.11)$$</p>\n<p>DPFE</p>\n<h3 id=\"DPFE\"><a href=\"#DPFE\" class=\"headerlink\" title=\"DPFE\"></a>DPFE</h3><p> DPFE  $f(d_1,,d_{i-1})$ $S=(d_1,,d_{i-1})$ $i=|S|+1=|{d_1,,d_{i-1}}|+1$ DPFE <br>$$f(S)=opt_{d_i \\in D_i(S)}{C_i(d_i|S) \\circ f(S)} \\qquad (1.12)$$<br> $S=(d_1,,d_i)$ $\\emptyset$  $\\mathcal S$ DPFE  $f(S_0)=b, \\ S_0 \\in \\mathcal S_{base}$ $S_0$$f(S_0)$  DPFE  b</p>\n<p> n  S  d  D(S)  S  d  $d \\in S$ DPFE <br>$$f(S)=opt_{d \\in S} {C(d|S) \\circ f(S)} \\qquad (1.13)$$</p>\n<p> S  d  S  SD(S)  S  DPFE  b(S,S) d  C(d|S) $S=T(S,d), \\ T: \\mathcal S \\times D \\rightarrow \\mathcal S$T  DPFE <br>$$f(S)=opt_S{b(S,S) \\circ f(S)} \\qquad(1.14) $$</p>\n<p>DPFE <br>$$f(S)=opt_{S}{f(S) \\circ b(S,S)} \\qquad(1.15) $$<br> f(S)  $S_0$  S  f(S)  S  $S_0$  (1.14) backward (1.15) forward</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br>$$f(S)=opt_{d \\in D(S)} {R(S,d) \\circ f(T(S,d))}  \\quad (1.16)$$<br>S  $\\mathcal S$ d  D(S) R(S,d)  C(d|S)T(S,d) $\\circ$ </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> N  A  x  $p_x$ A={a,b,c} $p_a=0.2,p_b=0.5,p_c=0.3$ 6  abc,acb,bac,bca,cab,cba bca  1.7</p>\n<ol>\n<li>Strong separable S<br>$1p_b+2p_c+3p_a$</li>\n<li>Weak separable W<br>$(p_a+p_b+p_c)+(p_a+p_c)+(p_a)$</li>\n</ol>\n<p> A  A  S  x  $ip_x$ i  x  A  W  A  bca  {a,b,c} b  A  {a,c}  $\\sum_{x\\in D_i} p_x$</p>\n<p> i=1,2,3 $D_1=A,D_2=A-{d_1},D_3=A-{d_1,d_2}$ S  $h(d_1,d_2,d_3)=1p_{d_1}+2p_{d_2}+3p_{d_3}$<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{1p_{d_1}+2p_{d_2}+3p_{d_3}}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{1p_{d_1}+\\min_{d_2 \\in A-{d_1}}{2p_{d_2}+\\min_{d_3\\in A-{d_1,d_2}}{3p_{d_3}}}} \\end{aligned}$$</p>\n<p> W  $h(d_1,d_2,d_3)=\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x$<br>$$\\begin{aligned}f(\\emptyset)&amp;=\\min_{d_1\\in A}{\\min_{d_2\\in A-{d_1}}{\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A}p_x+\\sum_{x\\in A-{d_1}}p_x+\\sum_{x \\in A-{d_1,d_2}}p_x}}}<br>\\\\ &amp;=\\min_{d_1\\in A}{\\sum_{x \\in A}p_x+\\min_{d_2 \\in A-{d_1}}{\\sum_{x\\in A-{d_1}}p_x+\\min_{d_3\\in A-{d_1,d_2}}{\\sum_{x \\in A-{d_1,d_2}}p_x}}} \\end{aligned}$$</p>\n<p><strong></strong> bca </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> DP  S  A DPFE <br>$$f(S)=\\min_{x \\in S} {C(x|S)+f(S-{x})}     \\quad(1.17)$$<br> $S \\in 2^A$$2^A$  A  $f(\\emptyset)=0$ $f(A)$</p>\n<p> DPFE DPFE (1.15) <br>$$f(S)=\\min_{S} {C(x|S)+f(S)}     \\quad(1.18)$$<br> $S \\in 2^A$ $f(\\emptyset)$ $f(A)=0$S  S  S  x  S</p>\n<p> W<br>$$C_W(x|S)=\\sum_{y\\in S}p_y$$<br> x  S  S </p>\n<p> S<br>$$C_S(x|S)=(N+1-|S|)p_x$$<br> x  A  A  A  $C_S(x|S)=|S|p_x$ S  $C(x|S)+f(S-{x})$ DP </p>\n<p> DP  (1.17)  DPFE <br>$$\\begin{aligned} f({a,b,c}) &amp;= \\min{C(a|{a,b,c})+f({b,c}), C(b|{a,b,c})+f({a,c}), C(c|{a,b,c})+f({a,b})}<br>\\\\ f({b,c}) &amp;= \\min{C(b|{b,c}+f({c}),C(c|{b,c}+f({b})}<br>\\\\ f({a,c}) &amp;= \\min{C(a|{a,c}+f({c}),C(c|{a,c}+f({a})}<br>\\\\ f({a,b}) &amp;= \\min{C(a|{a,b}+f({b}),C(c|{a,b}+f({a})}<br>\\\\ f({c}) &amp;= \\min {C(c|{c})+f(\\emptyset)}<br>\\\\ f({b}) &amp;= \\min {C(b|{b})+f(\\emptyset)}<br>\\\\ f({a}) &amp;= \\min {C(a|{a})+f(\\emptyset)}<br>\\\\ f(\\emptyset) &amp;= 0 \\end{aligned}$$<br> S  W </p>\n<p> (1.18)  DPFE <br>$$\\begin{aligned} f({a,b,c}) &amp;= 0<br>\\\\ f({b,c}) &amp;= \\min{C(a|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,c}) &amp;= \\min{C(b|{a,a,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({a,b}) &amp;= \\min{C(c|{a,b,c}+f({a,b,c})}\\stackrel W=\\min {1.0+0}=1.0<br>\\\\ f({c}) &amp;= \\min {C(a|{a,c})+f({a,c}), C(b|{b,c})+f({b,c})}\\stackrel W = \\min {0.5+1.0,0.8+1.0}=1.5<br>\\\\ f({b}) &amp;= \\min {C(a|{a,b})+f({a,b}), C(c|{b,c})+f({b,c})}\\stackrel W = \\min {0.7+1.0,0.8+1.0}=1.7<br>\\\\ f({a}) &amp;= \\min {C(b|{a,b})+f({a,b}), C(c|{a,c})+f({a,c})}\\stackrel W = \\min {0.7+1.0,0.5+1.0}=1.5<br>\\\\ f(\\emptyset) &amp;= \\min {C(a|a)+f({a}), C(b|b)+f({b}), C(c|c)+f({c})}\\stackrel W = \\min {0.2+1.5,0.5+1.7,0.3+1.5}=1.7 \\end{aligned}$$<br> $\\stackrel W=$  W  S </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 1 2  S DPFE  (1.17) <br>$$f(k,S)=\\min_{x \\in S} {C(x|k,S)+f(k+1,S-{x})} \\quad(1.19)$$</p>\n<h3 id=\"Path-States\"><a href=\"#Path-States\" class=\"headerlink\" title=\"Path-States\"></a>Path-States</h3><p> S  $(d_1,,d_{i-1})$ S  $\\emptyset$  S  $(d_1,,d_{i-1})$ S  S={a,b} a  b a  b a  b  Path-States S=(a,b) a  b DPFE <br>$$f(S)=\\min_{x \\notin S} {C(x|S)+f(S + (x))} \\qquad(1.20)$$</p>\n<p><br>$$\\begin{aligned} f(\\emptyset) &amp;= \\min {C(a|\\emptyset)+f(a),C(b|\\emptyset)+f(b),C(c|\\emptyset)+f(c)}\\stackrel S= \\min{0.2+1.9,0.5+1.2,0.3+1.6}=1.7<br>\\\\ f(a) &amp;= \\min {C(b|a)+f(ab), C(c|a)+f(ac)}\\stackrel S= \\min{2<em>0.5+0.9,2</em>0.3+1.5}=1.9<br>\\\\ f(b) &amp;= \\min {C(a|b)+f(ba), C(c|b)+f(bc)}\\stackrel S= \\min{2<em>0.2+0.9,2</em>0.3+0.6}=1.2<br>\\\\ f(c) &amp;= \\min {C(a|c)+f(ca), C(b|c)+f(cb)}\\stackrel S= \\min{2<em>0.2+1.5,2</em>0.5+0.6}=1.6<br>\\\\ f(ab) &amp;= \\min {C(c|ab)+f(abc)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(ac) &amp;= \\min {C(b|ac)+f(acb)}\\stackrel S= 3</em>0.5=1.5<br>\\\\ f(ba) &amp;= \\min {C(c|ba)+f(bac)}\\stackrel S= 3<em>0.3=0.9<br>\\\\ f(bc) &amp;= \\min {C(a|bc)+f(bca)}\\stackrel S= 3</em>0.2=0.6<br>\\\\ f(ca) &amp;= \\min {C(b|ca)+f(cab)}\\stackrel S= 3<em>0.5=1.5<br>\\\\ f(cb) &amp;= \\min {C(a|cb)+f(cba)} \\stackrel S= 3</em>0.2=0.6<br>\\\\ f(abc) &amp;= f(acb)=f(bac)=f(bca)=f(cab)=f(cba)=0 \\end{aligned}$$<br> $N!$  S  W  S $C(c|ab)$  c S  $C(c|ab)=3p_c=3*0.3=0.9$ $\\stackrel S=$  S  W </p>\n<h3 id=\"-Relaxation\"><a href=\"#-Relaxation\" class=\"headerlink\" title=\" Relaxation\"></a> Relaxation</h3><p> ${a_1,,a_N}$<br>$$x^{\\ast}=\\min{\\min{{\\min{a_1,a_2},a_3},},a_N}$$<br> $x_1=a_1, x_2=\\min{x_1,a_2},$ $x_1=\\min{x_0,a_1}, x_0=\\infty$ $x_1,x_2,$  $x^{\\ast}$ DPFE <br>$$\\begin{aligned} f(S)&amp;=\\min_{x \\in S} {C(x|S)+f(S_x)}<br>\\\\ &amp;=\\min{C(x_1|S)+f(S_{x_1}), C(x_2|S)+f(S_{x_2}),,C(x_m|S)+f(S_{x_m})} \\end{aligned}$$<br> $S={x_1,x_2,,x_m}$$S_x$  x  $C(x|S)+f(S_x)$ <br>$$f(S)=\\min{\\min{\\min{C(x_1|S)+f(S_{x_1}), C(x_2|S)+f(S_{x_2})},},C(x_m|S)+f(S_{x_m})} \\quad (1.21)$$<br> $C(x_i|S)$  S $f(S_{x_i})$ </p>\n<p> DPFE <br>$$f(k,S)=\\min_x {C(x|k,S) + f(k-1,S_x)} \\qquad(1.22)$$<br> k  k  S  k  T (1.22) $f(0,S),f(1,S),$  $f(S)$ $f(S)$ $f(S)=\\min_k {f(k,S)}$ $f(k,S)$  $f(k-1,S)$  $f(k-1,S_x)$  $f(k,S)$  $F(k,S)$ <br>$$F(k,S)=\\min{F(k-1,S), \\min_x {C(x|k,S)+F(k-1,S_x)}} \\quad(1.23)$$<br> $F(k,S)$  S  T  k  (1.22)  k  </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<p> s  t  DPFE <br>$$f(p)=\\min_q {b(p,q)+f(q)} \\qquad(1.24)$$<br> b(p,q)  p  q q  p f(p)  p  t  f(t)=0 q  p  $b(p,q)=\\infty$ f(p)  f(q)</p>\n<p>p  q f(p)  f(q)  p  p </p>\n<ol>\n<li>b(p,p) &gt; 0</li>\n<li>b(p,p) &lt; 0</li>\n<li>b(p,p) = 0</li>\n</ol>\n<p>DPFE <br>$$f(p) = \\min_q {b(p,q)+f(q)} \\qquad(1.25)$$<br> f(q)  f(p) $f(p)=\\infty, \\forall p \\ne t; \\ f(t)=0$ (1.25)  (1.24) </p>\n<p> 1.2<br><img src=\"/images/DP1_fig1.png\" alt></p>\n<p> (1.25) <br>$$\\begin{aligned}f(s)&amp;=\\min {b(s,x)+f(x),b(s,y)+f(y),b(s,t)+f(t)}=\\min {3+f(x),5+f(y),\\infty+f(t)}<br>\\\\f(x)&amp;=\\min {b(x,y)+f(y),b(x,t)+f(t)}=\\min {1+f(y),8+f(t)}<br>\\\\f(y)&amp;=\\min {b(y,x)+f(x),b(y,t)+f(t)}=\\min {2+f(x),5+f(t)}<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br> f(x)  f(y) </p>\n<p> $f(s)=f(x)=f(y)=\\infty$<br>$$\\begin{aligned}f(s)&amp;=\\min {3+\\infty,5+\\infty,\\infty+f(t)}=\\infty<br>\\\\f(x)&amp;=\\min {1+\\infty,8+0}=8<br>\\\\f(y)&amp;=\\min {2+\\infty,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$</p>\n<p><br>$$\\begin{aligned}f(s)&amp;=\\min {3+8,5+5,\\infty+0}=10<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+8,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br><br>$$\\begin{aligned}f(s)&amp;=\\min {3+6,5+5,\\infty+0}=9<br>\\\\f(x)&amp;=\\min {1+5,8+0}=6<br>\\\\f(y)&amp;=\\min {2+6,5+0}=5<br>\\\\f(t)&amp;=0 \\end{aligned}$$<br> $f(x),f(y),f(t)$  f(s)  f(s)</p>\n<p> Relaxation  (1.22) DPFE <br>$$f(k,p)=\\min_q {b(p,q)+f(k-1,q)} \\qquad(1.26)$$<br> f(k,p)  p  t k  p  t  k  $f(0,t)=0;f(k,t)=\\infty, k&gt;0;f(0,p)=\\infty, \\forall p \\ne t$ t  t  0  0 0  $\\infty$ t p  t  0  $\\infty$ p  (1.26) <br>$$\\begin{aligned}f(k,s)&amp;=\\min {b(s,x)+f(k-1,x),b(s,y)+f(k-1,y),b(s,t)+f(k-1,t)}<br>\\\\f(k,x)&amp;=\\min {b(x,y)+f(k-1,y),b(x,t)+f(k-1,t)}<br>\\\\f(k,y)&amp;=\\min {b(y,x)+f(k-1,x),b(y,t)+f(k-1,t)}<br>\\\\f(k,t) \\end{aligned}$$<br> k=0 <br>$$\\begin{aligned}f(1,s)&amp;=\\min {3+f(0,x),5+f(0,y),\\infty+f(0,t)}=\\infty<br>\\\\f(1,x)&amp;=\\min {1+f(0,y),8+f(0,t)}=8<br>\\\\f(1,y)&amp;=\\min {2+f(0,x),5+f(0,t)}=5<br>\\\\f(1,t)&amp;=\\infty \\end{aligned}$$<br><br>$$\\begin{aligned}f(2,s)&amp;=\\min {3+f(1,x),5+f(1,y),\\infty+f(1,t)}=10<br>\\\\f(2,x)&amp;=\\min {1+f(1,y),8+f(1,t)}=6<br>\\\\f(2,y)&amp;=\\min {2+f(1,x),5+f(1,t)}=10<br>\\\\f(2,t)&amp;=\\infty \\end{aligned}$$<br><br>$$\\begin{aligned}f(3,s)&amp;=\\min {3+f(2,x),5+f(2,y),\\infty+f(2,t)}=9<br>\\\\f(3,x)&amp;=\\min {1+f(2,y),8+f(2,t)}=11<br>\\\\f(3,y)&amp;=\\min {2+f(1,x),5+f(2,t)}=8<br>\\\\f(3,t)&amp;=\\infty \\end{aligned}$$<br> N  p  t  k k  {0,1,,N-1}k  N circle 0 N=4 k  3 f  $f(p)=\\min_k {f(k,p)}$ <br>$$\\begin{aligned}f(s)&amp;=\\min {\\infty,\\infty,10,9}=9<br>\\\\f(x)&amp;=\\min {\\infty,8,6,11}=6<br>\\\\f(y)&amp;=\\min {\\infty,5,10,8}=5<br>\\\\f(t)&amp;=\\min {0,\\infty,\\infty,\\infty}=0 \\end{aligned}$$<br> (1.22)  $f(k,S)$ </p>\n<p> $f(k,S)$  (1.23)  DPFE<br>$$F(k,p)=\\min {F(k-1,p), \\ \\min_q {b(p,q)+F(k-1,q)}} \\qquad(1.27)$$</p>\n<p> $F(k,p)$  p  t  k  p  t  $N-1$  $F(N-1,s)$k  $N-1$ $F(k,t)=0,k\\ge 0; F(0,p)=\\infty, p \\ne t$</p>\n<p> DPFE</p>\n"},{"title":"Dynamic Programming (2)","date":"2019-08-14T06:36:05.000Z","mathjax":true,"_content":" [Dynamic Programming (1)](2019/08/07/DP1)  DPFE\n\n##  ALLOT\n ALLOTALLOT  KSINT \n\n M  N  C(k,d)  d  k  $d_1$  1 $d_2$  2 (k,m)  k  m  k  m k  C(k,d) (k+1,m-d) [Dynamic Programming (1)](2019/08/07/DP1)  (1.19)  DPFE \n$$f(k,m)=\\min_{d \\in \\{0,...,m\\}} \\{C(k,d)+f(k+1,m-d)\\} \\quad (2.1)$$\n f(1,M) f(N+1,m)=0 $m \\ge 0$\n\n M=4N=3\n$$(C_{k,d})_{k\\in \\{1,2,3\\};d\\in \\{0,...,4\\}}=\\begin{pmatrix}\\infty & 1.0 & 0.8& 0.4 & 0.0 \\\\\\\\ \\infty & 1.0& 0.5 & 0.0 & 0.0 \\\\\\\\ \\infty & 1.0 & 0.6 & 0.3 & 0.0 \\end{pmatrix}$$\nf(1,M)=1.0+0.5+1.0=2.5 $d_1=1,d_2=2,d_3=1$ (2.1) \n```python\ndef allot(cache=True):\n    M=4\n    N=3\n    max_float=1e8\n    cost=[[max_float, 1.0, 0.8, 0.4, 0.0],\n          [max_float, 1.0, 0.5, 0.0, 0.0],\n          [max_float, 1.0, 0.6, 0.3, 0.0]]\n    \n    if cache:\n        cache_dict = {}\n    def allot_inner(k,m):\n        if cache and (k,m) in cache_dict:\n            return cache_dict[(k,m)]\n        if k>= N: return [], 0\n\n        min_f=max_float\n        min_ds = []\n        for d in range(m+1):\n            ds,f=allot_inner(k+1,m-d)\n            temp=cost[k][d]+f\n            if min_f > temp:\n                min_f = temp\n                min_ds = [d]+ds\n        if cache and k > 1:\n            cache_dict[(k,m)]=(min_ds,min_f)\n        return min_ds, min_f\n    ds, f=allot_inner(0,M)\n    print(\"min cost:\",f,\"opt allotments:\", ds)\n```\n\n\n\n##  APSP\n\n s  t  Nself-loop\n\n [Dynamic Programming (1)](2019/08/07/DP1)  (s,t) \n\n__Relaxation__ \n\n F(k,p,q)  p  q k  p  q  k  [Dynamic Programming (1)](2019/08/07/DP1)  (1.27)DPFE \n$$F(k,p,q)=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\quad(2.2)$$\n r  p :\n1. $F(k,p,q)=0, k \\ge 0, p=q$ p  q  k  0\n2. $F(0,p,q)=\\infty, p \\ne q$  p  q  0 \n\n $b(p,p)=0$ (2.2) \n$$\\begin{aligned}F(k,p,q)&=\\min \\{F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\\\\\\\ &=\\min_{r \\in succ(p)\\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.3) \\end{aligned}$$\n\n\n__Floyd-Warshall__ \n\n (2.2)  DPFE  p  q  k  p  r  r  k-1  q  p  r  r  q p  r  1 p  q  k r  k  k  $\\{1,2,...,k\\}$ p  q  $\\{1,2,...,N\\}$  p  q DPFE \n$$F(k,p,q)=\\min \\{F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)\\} \\qquad(2.4)$$\n\n\n1.  N  $V=\\{1,2,...,N\\}$$p,q \\in V$\n2. $F(k,p,q)$  p  q  $\\{1,2,...,k\\}$ \n3.  $F(N,p,q)$\n4.  $(2.4)$\n   - p  q  k $\\{1,2,...,k-1\\}$ \n   -  p  q  k p  k  k  q $\\{1,2,...,k-1\\}$ \n5.  $(2.4)$  k>0k=0  $F(0,p,q)=0, p=q$ $F(0,p,q)=b(p,q), p\\ne q$ p q  0 p q  $b(p,q)$ q  p  $F(0,p,q)=\\infty, p \\notin succ(p) \\cup \\{p\\}$\n6.  $\\{1,2,...,k\\}$  p  q $b(p,p)=0$\n\n (2.2)  r  p  $N-1$  p  $N-1$k  $N-1$ p  q  N  (2.2)  $O(N^4)$ (2.4)  $O(N^3)$\n\n APSP  (2.2)  $\\{F^{(1)},F^{(2)},...,F^{(N-1)}\\}$ $F^{(k)}$  $N \\times N$$F_{p,q}^{k}$  p  q  k  $\\min_{p,q} F_{p,q}^{(N-1)}$  APSP \n\n____\n\n (2.2) \n$$\\begin{aligned} F(k,p,q)&=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\}\n\\\\\\\\ &= \\min_{r \\in succ(p)} \\{b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in succ(p) \\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in \\{1,2,...,N\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.5) \\end{aligned}$$\n$b(p,r)$  edge  $\\infty$ $W_{N \\times N}$ (2.2)  $F^{(0)}$  0 $\\infty$\n\n$$F^{(0)}=\\begin{bmatrix}0 & \\infty & \\cdots & \\infty\n\\\\\\\\                    \\infty & 0  & \\cdots & \\infty\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    \\infty & \\infty & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n$$W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n (2.5) $F^{(k-1)}$  $F^{(k)}$ \n```python\nimport sys\nF_k=[[None]*N]*N\nfor p in range(0,N):\n  for q in range(0,N):\n    F_k[p][q]=sys.info.float_max\n    # F_k[p][q]=0\n    for r in range(0,N):\n      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])\n      # F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])\n```\n $F^{(N-1)}$ $F^{(k)}$ \n$$\\begin{aligned} F^{(1)}&=W \\circ F^{(0)}=W\n\\\\\\\\ F^{(2)}&=W \\circ F^{(1)}=W^2\n\\\\\\\\ F^{(3)}&=W \\circ F^{(2)}=W^3\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{(N-1)}&=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$\n\n$\\circ$ \n$$\\begin{aligned} F^{(1)}&=W\n\\\\\\\\ F^{(2)}&=W^2=W \\circ W\n\\\\\\\\ F^{(4)}&=W^4=W^2 \\circ W^2\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$\n $2^{\\lceil log(N-1) \\rceil}$  $\\lceil \\cdot \\rceil$  $2^{\\lceil log(N-1) \\rceil} \\ge N-1$ $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ element-wise comparison\n\n (2.6)  (2.7)\n$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$\n $\\circ$  $\\min$ $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$ $\\min$  element-wise operator (2.6)  (2.7) \n\n (2.7)  $F^{(M)}, M \\ge N-1$ $F^{(k)}$  $F^{(N-1)}$ $F^{(M)}$ \n\n\n```python\nimport sys\n\ndef fast_apsp():\n  k=1\n  F_prev=W\n  while k<N-1:\n    F_next=[[sys.info.float_max]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        for r in range(0,N):\n          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])\n    F_prev=F_next\n    k*=2\n  return F_prev\n```\n\n__Floyd-Warshall__ \n\n (2.4) $F^{(k)}$  k  $\\{1,2,...,k\\}$ $F^{(0)}$ \n$$F^{(0)}=W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n (2.4) __Floyd-Warshall__ \n```python\nF_prev=F_0\ndef floyd_warshall():\n  for k in range(0,N):\n    F_next=[[None]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])\n    F_prev=F_next\n  return F_prev\n```\n\n##  ARC\n\nARC *\n\nARCinternal nodes root $S=(w_0,w_1,...,w_{n-1})$  $(i,j)$  $(w_i,...,w_j)$\n$$f(i,j)=\\min_{}\\{c(i,j,d)+f(i,d)+f(d+1,j)\\}, \\quad i<j \\qquad(2.8)$$\n $f(i,j)$  $(i,...,j)$  $c(i,j,d)=\\sum_{k=i}^j w_k$  (2.8)d  $(i,...,d)$ $(d+1,...,j)$  d  $(i,...,j)$  $c(i,j,d)$ \n\n $f(0,n-1)$ $f(i,i)=0, \\ \\forall i \\in \\{0,1,...,n-1\\}$\n\n ARC  Huffman $S=(1,2,3,4)$ $(((1,2),3),4)$$f(S)=3+6+10=19$ $S=(2,3,3,4)$ $((2,3),(3,4))$$f(S)=5+7+12=24$\n\n  \n##  ASMBAL\n/ stage  k i  k+1  j  c(k,i,j) c(k,i,i)=0 0 s  t  0  c(0,0,j) c(N,j,0)j  \n\n 0~13  14  0  13  14 \n$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$\n\n![](/images/DP2_fig1.png)\n\n 14x14 \n\n k i  j  $R(k,i,j)=v(k,i)+c(k,i,j)$DPFE \n$$f(k,i)=\\min_j \\{R(k,i,j)+f(k+1,j)\\}$$\n $f(k,i)$  k  i  $f(0,0)$ $f(k,i)=0, k > N$N  N=6\n$$\\begin{aligned} f(0,0)=\\min \\{R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)\\}\n\\\\\\\\ f(1,0)=\\min \\{R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)\\}\n\\\\\\\\ f(1,1)=\\min \\{R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)\\}\n\\\\\\\\ \\cdots \\ (omitted)\n\\end{aligned}$$\n\n##  ASSIGN\n B  A  A  A  $\\{1,2,3\\}$  $\\{3,2,1\\}$ 3!  $A=(a_0,a_1,...,a_{n-1})$  $B=(b_0,b_1,...,b_{n-1})$  i $a_j$  $b_i$ $c(i,j)$ A  $(k,S)$ k  A  S k  d $C(k,S,d)$ $(k+1,S-\\{d\\})$DPFE \n$$f(k,S)=\\min_{d \\in S} \\{C(k,S,d)+f(k+1,S-\\{d\\})\\}$$\n $f(1,S^{\\ast})$ $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$\n\n##  BST\n n  $X=\\{x_0,...,x_{n-1}\\}$ ____  $x_i$  $p(x_i)$ $p_i$ $\\sum_{i=0}^{n-1}p_i=1$\n$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$\n$\\text{level}(x_i)$  $x_i$  DP \n\n### \n S  DPFE \n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & S \\ne \\emptyset\n\\\\\\\\ 0 & S=\\emptyset \\end{cases}$$\n $S_l = \\{x \\in S: x < \\alpha\\}, \\ S_r = \\{x \\in S: x > \\alpha\\}$ $r(\\alpha, S)=\\sum_{x \\in S} p(x)$ 1 DPFE \n\n $S_l,\\ S_r$  ____ \n\n\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & |S|>1\n\\\\\\\\ p(x) & S=\\{x\\} \\end{cases}$$\n\n### \n $(i,j)$ $X=\\{x_0,...,x_{n-1}\\}$ DPFE \n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i \\le j\n\\\\\\\\ 0 & i > j \\end{cases}$$\n$(i,j)$  k  $(i,j)$, $(i,k-1)$ $(k+1,j)$  k$(i,j)$  level  level level  $f(i,j)$ $(i,j)$  level level  $\\sum_{l=i}^j p_l$\n\n\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i < j\n\\\\\\\\ p_i & i = j \\end{cases}$$\n\n##  COV\n k k  0  i  $c_i$ n  $n \\le k$ n \n\n DP  $0,1,...,k-1$ $l$  $s_l$ $c_{s_l}$ $c_l$ $j$ $l$  DPFE \n$$f(j,l)=\\begin{cases} \\min_{d \\in \\{j-2,...,l-1\\}} \\{(l-d)c_l+f(j-1,d)\\} & j>1\n\\\\\\\\ (l+1)c_l & j=1 \\end{cases}$$\nd exclusive $s_l$  $\\{d+1,...,l\\}$  d  $l-1$ $l$ d  $j-2$ $\\{j-1,...,l\\}$ $\\{0,1,...,j-2\\}$  $j-1$  $j-1$ \n\n$f(j,l)=(l+1)c_l, j=1$ $l$ $0,...,l$ \n\n##  DEADLINE\n 0 ____\n\n $S^{\\ast}=\\{0,1,2,3,4\\}$ $p=\\{10,15,20,1,5\\}$ $t=\\{1,2,2,3,3\\}$\n```python\nimport numpy as np\nt=np.array([1,2,2,3,3])\np=np.array([10,15,20,1,5])\nm=0   # \nn=t.shape[0]  # \n\nfor i in range(n):\n  idx=np.where(t>0)[0]\n  if idx.shape[0]==0:\n    break   # \n  c=np.max(p[idx])  # \n  if c < 0:\n    break\n  m+=c\n  idx=np.argmax(p[idx])+idx[0]  # \n  p[idx]=-1e8                   # \n  t-=1                          # \nprint('', sep=' ')\nprint(m)    # 40\n```\n\n DP  $(k,S)$ k S  d  S  $(k+1,S-\\{d\\})$DPFE \n$$f(k,S)=\\max_{d \\in S}\\{c(d|S)+f(k+1,S-\\{d\\})\\}$$\n d  k k  1 $c(d|S)=w_d$  $c(d|S)=0$ $f(1,S^{\\ast})$ $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$ n \n```python\nt=[1,2,2,3,3]\np=[10,15,20,1,5]\nS=[0,1,2,3,4]\nn=len(t)\ndef profit(k,d):\n  return p[d] if t[d]>=k else 0\n\ndef deadline(k,S):\n  return 0 if len(S)==0 or k==n+1 else \\\n    max([profit(k,S[i])+deadline(k+1,S[:i]+S[i+1:]) for i in range(len(S))])\n\nprint(deadline(1, S))   # 40\n```\n\n##  DPP\n $b_1$  t  $b_t$ t  $x_t$  $r(x_t)$ $c(x_t,b_t)$ s s  $1,...,T$ T  y $x_t$   t  $(t,b)$ t b  t DPFE \n$$f(t,b)=\\begin{cases} \\max_{x_t \\in \\{0,...,b\\}} \\{r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)\\} & t \\le T\n\\\\\\\\ 0 & t=T+1 \\end{cases}$$\n\n\n##  EDP\n $\\Sigma$  $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$ $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$ x  y\n-  D $c(D)$\n-  I $c(I)$\n-  R $c(R)$ $c(R)=0$\n  \n x  yDPFE \n$$f(i,j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min \\{f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)\\} & i>0,j>0 \\end{cases}$$\n $f(i,j)$  $X_i$  $Y_j$ $X_i$  x  i $Y_j$  y  j  i=0  $X_i$  j  $Y_j$ j=0  $X_i$  i  $Y_j$ i>0  j>0  $X_i$  $Y_j$/\n-  $X_i$  i  $X_{i-1}$  $Y_j$ $c(D)$  $f(i-1,j)$\n-  $X_i$  $Y_{j-1}$ $Y_j$ $f(i,j-1)$  $c(I)$\n- $x_i \\rightarrow y_j$ $X_{i-1}$  $Y_{j-1}$  $X_i$  $Y_j$  $f(i-1,j-1)$  $c(R)$\n\n DPFE\n$$f(X_i,Y_j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min_{d \\in \\{D,I,R\\}} \\{f(t(X_i,Y_j,d))+c(d)\\} & i>0,j>0\n\\end{cases}$$\n\n$$t(X_i,Y_j,D)=(X_{i-1},Y_j)\n\\\\\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})\n\\\\\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$\n","source":"_posts/DP2.md","raw":"---\ntitle: Dynamic Programming (2)\ndate: 2019-08-14 14:36:05\ntags: \n    - math\n    - DP\nmathjax: true\n---\n [Dynamic Programming (1)](2019/08/07/DP1)  DPFE\n\n##  ALLOT\n ALLOTALLOT  KSINT \n\n M  N  C(k,d)  d  k  $d_1$  1 $d_2$  2 (k,m)  k  m  k  m k  C(k,d) (k+1,m-d) [Dynamic Programming (1)](2019/08/07/DP1)  (1.19)  DPFE \n$$f(k,m)=\\min_{d \\in \\{0,...,m\\}} \\{C(k,d)+f(k+1,m-d)\\} \\quad (2.1)$$\n f(1,M) f(N+1,m)=0 $m \\ge 0$\n\n M=4N=3\n$$(C_{k,d})_{k\\in \\{1,2,3\\};d\\in \\{0,...,4\\}}=\\begin{pmatrix}\\infty & 1.0 & 0.8& 0.4 & 0.0 \\\\\\\\ \\infty & 1.0& 0.5 & 0.0 & 0.0 \\\\\\\\ \\infty & 1.0 & 0.6 & 0.3 & 0.0 \\end{pmatrix}$$\nf(1,M)=1.0+0.5+1.0=2.5 $d_1=1,d_2=2,d_3=1$ (2.1) \n```python\ndef allot(cache=True):\n    M=4\n    N=3\n    max_float=1e8\n    cost=[[max_float, 1.0, 0.8, 0.4, 0.0],\n          [max_float, 1.0, 0.5, 0.0, 0.0],\n          [max_float, 1.0, 0.6, 0.3, 0.0]]\n    \n    if cache:\n        cache_dict = {}\n    def allot_inner(k,m):\n        if cache and (k,m) in cache_dict:\n            return cache_dict[(k,m)]\n        if k>= N: return [], 0\n\n        min_f=max_float\n        min_ds = []\n        for d in range(m+1):\n            ds,f=allot_inner(k+1,m-d)\n            temp=cost[k][d]+f\n            if min_f > temp:\n                min_f = temp\n                min_ds = [d]+ds\n        if cache and k > 1:\n            cache_dict[(k,m)]=(min_ds,min_f)\n        return min_ds, min_f\n    ds, f=allot_inner(0,M)\n    print(\"min cost:\",f,\"opt allotments:\", ds)\n```\n\n\n\n##  APSP\n\n s  t  Nself-loop\n\n [Dynamic Programming (1)](2019/08/07/DP1)  (s,t) \n\n__Relaxation__ \n\n F(k,p,q)  p  q k  p  q  k  [Dynamic Programming (1)](2019/08/07/DP1)  (1.27)DPFE \n$$F(k,p,q)=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\quad(2.2)$$\n r  p :\n1. $F(k,p,q)=0, k \\ge 0, p=q$ p  q  k  0\n2. $F(0,p,q)=\\infty, p \\ne q$  p  q  0 \n\n $b(p,p)=0$ (2.2) \n$$\\begin{aligned}F(k,p,q)&=\\min \\{F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\} \\\\\\\\ &=\\min_{r \\in succ(p)\\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.3) \\end{aligned}$$\n\n\n__Floyd-Warshall__ \n\n (2.2)  DPFE  p  q  k  p  r  r  k-1  q  p  r  r  q p  r  1 p  q  k r  k  k  $\\{1,2,...,k\\}$ p  q  $\\{1,2,...,N\\}$  p  q DPFE \n$$F(k,p,q)=\\min \\{F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)\\} \\qquad(2.4)$$\n\n\n1.  N  $V=\\{1,2,...,N\\}$$p,q \\in V$\n2. $F(k,p,q)$  p  q  $\\{1,2,...,k\\}$ \n3.  $F(N,p,q)$\n4.  $(2.4)$\n   - p  q  k $\\{1,2,...,k-1\\}$ \n   -  p  q  k p  k  k  q $\\{1,2,...,k-1\\}$ \n5.  $(2.4)$  k>0k=0  $F(0,p,q)=0, p=q$ $F(0,p,q)=b(p,q), p\\ne q$ p q  0 p q  $b(p,q)$ q  p  $F(0,p,q)=\\infty, p \\notin succ(p) \\cup \\{p\\}$\n6.  $\\{1,2,...,k\\}$  p  q $b(p,p)=0$\n\n (2.2)  r  p  $N-1$  p  $N-1$k  $N-1$ p  q  N  (2.2)  $O(N^4)$ (2.4)  $O(N^3)$\n\n APSP  (2.2)  $\\{F^{(1)},F^{(2)},...,F^{(N-1)}\\}$ $F^{(k)}$  $N \\times N$$F_{p,q}^{k}$  p  q  k  $\\min_{p,q} F_{p,q}^{(N-1)}$  APSP \n\n____\n\n (2.2) \n$$\\begin{aligned} F(k,p,q)&=\\min \\{F(k-1,p,q), \\min_{r \\in succ(p)} \\{b(p,r)+F(k-1,r,q)\\}\\}\n\\\\\\\\ &= \\min_{r \\in succ(p)} \\{b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in succ(p) \\cup \\{p\\}} \\{b(p,r)+F(k-1,r,q)\\}\n\\\\\\\\ &= \\min_{r \\in \\{1,2,...,N\\}} \\{b(p,r)+F(k-1,r,q)\\} \\qquad(2.5) \\end{aligned}$$\n$b(p,r)$  edge  $\\infty$ $W_{N \\times N}$ (2.2)  $F^{(0)}$  0 $\\infty$\n\n$$F^{(0)}=\\begin{bmatrix}0 & \\infty & \\cdots & \\infty\n\\\\\\\\                    \\infty & 0  & \\cdots & \\infty\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    \\infty & \\infty & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n$$W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n (2.5) $F^{(k-1)}$  $F^{(k)}$ \n```python\nimport sys\nF_k=[[None]*N]*N\nfor p in range(0,N):\n  for q in range(0,N):\n    F_k[p][q]=sys.info.float_max\n    # F_k[p][q]=0\n    for r in range(0,N):\n      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])\n      # F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])\n```\n $F^{(N-1)}$ $F^{(k)}$ \n$$\\begin{aligned} F^{(1)}&=W \\circ F^{(0)}=W\n\\\\\\\\ F^{(2)}&=W \\circ F^{(1)}=W^2\n\\\\\\\\ F^{(3)}&=W \\circ F^{(2)}=W^3\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{(N-1)}&=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$\n\n$\\circ$ \n$$\\begin{aligned} F^{(1)}&=W\n\\\\\\\\ F^{(2)}&=W^2=W \\circ W\n\\\\\\\\ F^{(4)}&=W^4=W^2 \\circ W^2\n\\\\\\\\ &\\vdots\n\\\\\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$\n $2^{\\lceil log(N-1) \\rceil}$  $\\lceil \\cdot \\rceil$  $2^{\\lceil log(N-1) \\rceil} \\ge N-1$ $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ element-wise comparison\n\n (2.6)  (2.7)\n$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$\n $\\circ$  $\\min$ $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$ $\\min$  element-wise operator (2.6)  (2.7) \n\n (2.7)  $F^{(M)}, M \\ge N-1$ $F^{(k)}$  $F^{(N-1)}$ $F^{(M)}$ \n\n\n```python\nimport sys\n\ndef fast_apsp():\n  k=1\n  F_prev=W\n  while k<N-1:\n    F_next=[[sys.info.float_max]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        for r in range(0,N):\n          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])\n    F_prev=F_next\n    k*=2\n  return F_prev\n```\n\n__Floyd-Warshall__ \n\n (2.4) $F^{(k)}$  k  $\\{1,2,...,k\\}$ $F^{(0)}$ \n$$F^{(0)}=W=\\begin{bmatrix}0 & w_{12} & \\cdots & w_{1N}\n\\\\\\\\                    w_{21} & 0  & \\cdots & w_{2N}\n\\\\\\\\                    \\vdots & \\vdots & \\ddots & \\vdots\n\\\\\\\\                    w_{N1} & w_{N2} & \\cdots & 0 \\end{bmatrix}_{N \\times N}$$\n\n (2.4) __Floyd-Warshall__ \n```python\nF_prev=F_0\ndef floyd_warshall():\n  for k in range(0,N):\n    F_next=[[None]*N]*N\n    for p in range(0,N):\n      for q in range(0,N):\n        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])\n    F_prev=F_next\n  return F_prev\n```\n\n##  ARC\n\nARC *\n\nARCinternal nodes root $S=(w_0,w_1,...,w_{n-1})$  $(i,j)$  $(w_i,...,w_j)$\n$$f(i,j)=\\min_{}\\{c(i,j,d)+f(i,d)+f(d+1,j)\\}, \\quad i<j \\qquad(2.8)$$\n $f(i,j)$  $(i,...,j)$  $c(i,j,d)=\\sum_{k=i}^j w_k$  (2.8)d  $(i,...,d)$ $(d+1,...,j)$  d  $(i,...,j)$  $c(i,j,d)$ \n\n $f(0,n-1)$ $f(i,i)=0, \\ \\forall i \\in \\{0,1,...,n-1\\}$\n\n ARC  Huffman $S=(1,2,3,4)$ $(((1,2),3),4)$$f(S)=3+6+10=19$ $S=(2,3,3,4)$ $((2,3),(3,4))$$f(S)=5+7+12=24$\n\n  \n##  ASMBAL\n/ stage  k i  k+1  j  c(k,i,j) c(k,i,i)=0 0 s  t  0  c(0,0,j) c(N,j,0)j  \n\n 0~13  14  0  13  14 \n$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$\n\n![](/images/DP2_fig1.png)\n\n 14x14 \n\n k i  j  $R(k,i,j)=v(k,i)+c(k,i,j)$DPFE \n$$f(k,i)=\\min_j \\{R(k,i,j)+f(k+1,j)\\}$$\n $f(k,i)$  k  i  $f(0,0)$ $f(k,i)=0, k > N$N  N=6\n$$\\begin{aligned} f(0,0)=\\min \\{R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)\\}\n\\\\\\\\ f(1,0)=\\min \\{R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)\\}\n\\\\\\\\ f(1,1)=\\min \\{R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)\\}\n\\\\\\\\ \\cdots \\ (omitted)\n\\end{aligned}$$\n\n##  ASSIGN\n B  A  A  A  $\\{1,2,3\\}$  $\\{3,2,1\\}$ 3!  $A=(a_0,a_1,...,a_{n-1})$  $B=(b_0,b_1,...,b_{n-1})$  i $a_j$  $b_i$ $c(i,j)$ A  $(k,S)$ k  A  S k  d $C(k,S,d)$ $(k+1,S-\\{d\\})$DPFE \n$$f(k,S)=\\min_{d \\in S} \\{C(k,S,d)+f(k+1,S-\\{d\\})\\}$$\n $f(1,S^{\\ast})$ $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$\n\n##  BST\n n  $X=\\{x_0,...,x_{n-1}\\}$ ____  $x_i$  $p(x_i)$ $p_i$ $\\sum_{i=0}^{n-1}p_i=1$\n$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$\n$\\text{level}(x_i)$  $x_i$  DP \n\n### \n S  DPFE \n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & S \\ne \\emptyset\n\\\\\\\\ 0 & S=\\emptyset \\end{cases}$$\n $S_l = \\{x \\in S: x < \\alpha\\}, \\ S_r = \\{x \\in S: x > \\alpha\\}$ $r(\\alpha, S)=\\sum_{x \\in S} p(x)$ 1 DPFE \n\n $S_l,\\ S_r$  ____ \n\n\n$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} \\{f(S_l)+f(S_r)+r(\\alpha, S)\\} & |S|>1\n\\\\\\\\ p(x) & S=\\{x\\} \\end{cases}$$\n\n### \n $(i,j)$ $X=\\{x_0,...,x_{n-1}\\}$ DPFE \n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i \\le j\n\\\\\\\\ 0 & i > j \\end{cases}$$\n$(i,j)$  k  $(i,j)$, $(i,k-1)$ $(k+1,j)$  k$(i,j)$  level  level level  $f(i,j)$ $(i,j)$  level level  $\\sum_{l=i}^j p_l$\n\n\n$$f(i,j)=\\begin{cases} \\min_{k \\in \\{i,...,j\\}} \\{f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l\\} & i < j\n\\\\\\\\ p_i & i = j \\end{cases}$$\n\n##  COV\n k k  0  i  $c_i$ n  $n \\le k$ n \n\n DP  $0,1,...,k-1$ $l$  $s_l$ $c_{s_l}$ $c_l$ $j$ $l$  DPFE \n$$f(j,l)=\\begin{cases} \\min_{d \\in \\{j-2,...,l-1\\}} \\{(l-d)c_l+f(j-1,d)\\} & j>1\n\\\\\\\\ (l+1)c_l & j=1 \\end{cases}$$\nd exclusive $s_l$  $\\{d+1,...,l\\}$  d  $l-1$ $l$ d  $j-2$ $\\{j-1,...,l\\}$ $\\{0,1,...,j-2\\}$  $j-1$  $j-1$ \n\n$f(j,l)=(l+1)c_l, j=1$ $l$ $0,...,l$ \n\n##  DEADLINE\n 0 ____\n\n $S^{\\ast}=\\{0,1,2,3,4\\}$ $p=\\{10,15,20,1,5\\}$ $t=\\{1,2,2,3,3\\}$\n```python\nimport numpy as np\nt=np.array([1,2,2,3,3])\np=np.array([10,15,20,1,5])\nm=0   # \nn=t.shape[0]  # \n\nfor i in range(n):\n  idx=np.where(t>0)[0]\n  if idx.shape[0]==0:\n    break   # \n  c=np.max(p[idx])  # \n  if c < 0:\n    break\n  m+=c\n  idx=np.argmax(p[idx])+idx[0]  # \n  p[idx]=-1e8                   # \n  t-=1                          # \nprint('', sep=' ')\nprint(m)    # 40\n```\n\n DP  $(k,S)$ k S  d  S  $(k+1,S-\\{d\\})$DPFE \n$$f(k,S)=\\max_{d \\in S}\\{c(d|S)+f(k+1,S-\\{d\\})\\}$$\n d  k k  1 $c(d|S)=w_d$  $c(d|S)=0$ $f(1,S^{\\ast})$ $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$ n \n```python\nt=[1,2,2,3,3]\np=[10,15,20,1,5]\nS=[0,1,2,3,4]\nn=len(t)\ndef profit(k,d):\n  return p[d] if t[d]>=k else 0\n\ndef deadline(k,S):\n  return 0 if len(S)==0 or k==n+1 else \\\n    max([profit(k,S[i])+deadline(k+1,S[:i]+S[i+1:]) for i in range(len(S))])\n\nprint(deadline(1, S))   # 40\n```\n\n##  DPP\n $b_1$  t  $b_t$ t  $x_t$  $r(x_t)$ $c(x_t,b_t)$ s s  $1,...,T$ T  y $x_t$   t  $(t,b)$ t b  t DPFE \n$$f(t,b)=\\begin{cases} \\max_{x_t \\in \\{0,...,b\\}} \\{r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)\\} & t \\le T\n\\\\\\\\ 0 & t=T+1 \\end{cases}$$\n\n\n##  EDP\n $\\Sigma$  $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$ $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$ x  y\n-  D $c(D)$\n-  I $c(I)$\n-  R $c(R)$ $c(R)=0$\n  \n x  yDPFE \n$$f(i,j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min \\{f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)\\} & i>0,j>0 \\end{cases}$$\n $f(i,j)$  $X_i$  $Y_j$ $X_i$  x  i $Y_j$  y  j  i=0  $X_i$  j  $Y_j$ j=0  $X_i$  i  $Y_j$ i>0  j>0  $X_i$  $Y_j$/\n-  $X_i$  i  $X_{i-1}$  $Y_j$ $c(D)$  $f(i-1,j)$\n-  $X_i$  $Y_{j-1}$ $Y_j$ $f(i,j-1)$  $c(I)$\n- $x_i \\rightarrow y_j$ $X_{i-1}$  $Y_{j-1}$  $X_i$  $Y_j$  $f(i-1,j-1)$  $c(R)$\n\n DPFE\n$$f(X_i,Y_j)=\\begin{cases} jI & i=0\n\\\\\\\\ iD & j=0\n\\\\\\\\ \\min_{d \\in \\{D,I,R\\}} \\{f(t(X_i,Y_j,d))+c(d)\\} & i>0,j>0\n\\end{cases}$$\n\n$$t(X_i,Y_j,D)=(X_{i-1},Y_j)\n\\\\\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})\n\\\\\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$\n","slug":"DP2","published":1,"updated":"2019-08-27T10:52:07.614Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379f3001ydgvcgv0y6avu","content":"<p> <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  DPFE</p>\n<h2 id=\"-ALLOT\"><a href=\"#-ALLOT\" class=\"headerlink\" title=\" ALLOT\"></a> ALLOT</h2><p> ALLOTALLOT  KSINT </p>\n<p> M  N  C(k,d)  d  k  $d_1$  1 $d_2$  2 (k,m)  k  m  k  m k  C(k,d) (k+1,m-d) <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (1.19)  DPFE <br>$$f(k,m)=\\min_{d \\in {0,,m}} {C(k,d)+f(k+1,m-d)} \\quad (2.1)$$<br> f(1,M) f(N+1,m)=0 $m \\ge 0$</p>\n<p> M=4N=3<br>$$(C_{k,d})_{k\\in {1,2,3};d\\in {0,,4}}=\\begin{pmatrix}\\infty &amp; 1.0 &amp; 0.8&amp; 0.4 &amp; 0.0 \\\\ \\infty &amp; 1.0&amp; 0.5 &amp; 0.0 &amp; 0.0 \\\\ \\infty &amp; 1.0 &amp; 0.6 &amp; 0.3 &amp; 0.0 \\end{pmatrix}$$<br>f(1,M)=1.0+0.5+1.0=2.5 $d_1=1,d_2=2,d_3=1$ (2.1) </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot</span><span class=\"params\">(cache=True)</span>:</span></span><br><span class=\"line\">    M=<span class=\"number\">4</span></span><br><span class=\"line\">    N=<span class=\"number\">3</span></span><br><span class=\"line\">    max_float=<span class=\"number\">1e8</span></span><br><span class=\"line\">    cost=[[max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.0</span>]]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> cache:</span><br><span class=\"line\">        cache_dict = &#123;&#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot_inner</span><span class=\"params\">(k,m)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> (k,m) <span class=\"keyword\">in</span> cache_dict:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> cache_dict[(k,m)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;= N: <span class=\"keyword\">return</span> [], <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        min_f=max_float</span><br><span class=\"line\">        min_ds = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> range(m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ds,f=allot_inner(k+<span class=\"number\">1</span>,m-d)</span><br><span class=\"line\">            temp=cost[k][d]+f</span><br><span class=\"line\">            <span class=\"keyword\">if</span> min_f &gt; temp:</span><br><span class=\"line\">                min_f = temp</span><br><span class=\"line\">                min_ds = [d]+ds</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> k &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            cache_dict[(k,m)]=(min_ds,min_f)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> min_ds, min_f</span><br><span class=\"line\">    ds, f=allot_inner(<span class=\"number\">0</span>,M)</span><br><span class=\"line\">    print(<span class=\"string\">\"min cost:\"</span>,f,<span class=\"string\">\"opt allotments:\"</span>, ds)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-APSP\"><a href=\"#-APSP\" class=\"headerlink\" title=\" APSP\"></a> APSP</h2><p> s  t  Nself-loop</p>\n<p> <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (s,t) </p>\n<p><strong>Relaxation</strong> </p>\n<p> F(k,p,q)  p  q k  p  q  k  <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (1.27)DPFE <br>$$F(k,p,q)=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\quad(2.2)$$<br> r  p :</p>\n<ol>\n<li>$F(k,p,q)=0, k \\ge 0, p=q$ p  q  k  0</li>\n<li>$F(0,p,q)=\\infty, p \\ne q$  p  q  0 </li>\n</ol>\n<p> $b(p,p)=0$ (2.2) <br>$$\\begin{aligned}F(k,p,q)&amp;=\\min {F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\\\ &amp;=\\min_{r \\in succ(p)\\cup {p}} {b(p,r)+F(k-1,r,q)} \\qquad(2.3) \\end{aligned}$$</p>\n<p><strong>Floyd-Warshall</strong> </p>\n<p> (2.2)  DPFE  p  q  k  p  r  r  k-1  q  p  r  r  q p  r  1 p  q  k r  k  k  ${1,2,,k}$ p  q  ${1,2,,N}$  p  q DPFE <br>$$F(k,p,q)=\\min {F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)} \\qquad(2.4)$$</p>\n<p></p>\n<ol>\n<li> N  $V={1,2,,N}$$p,q \\in V$</li>\n<li>$F(k,p,q)$  p  q  ${1,2,,k}$ </li>\n<li> $F(N,p,q)$</li>\n<li> $(2.4)$<ul>\n<li>p  q  k ${1,2,,k-1}$ </li>\n<li> p  q  k p  k  k  q ${1,2,,k-1}$ </li>\n</ul>\n</li>\n<li> $(2.4)$  k&gt;0k=0  $F(0,p,q)=0, p=q$ $F(0,p,q)=b(p,q), p\\ne q$ p q  0 p q  $b(p,q)$ q  p  $F(0,p,q)=\\infty, p \\notin succ(p) \\cup {p}$</li>\n<li> ${1,2,,k}$  p  q $b(p,p)=0$</li>\n</ol>\n<p> (2.2)  r  p  $N-1$  p  $N-1$k  $N-1$ p  q  N  (2.2)  $O(N^4)$ (2.4)  $O(N^3)$</p>\n<p> APSP  (2.2)  ${F^{(1)},F^{(2)},,F^{(N-1)}}$ $F^{(k)}$  $N \\times N$$F_{p,q}^{k}$  p  q  k  $\\min_{p,q} F_{p,q}^{(N-1)}$  APSP </p>\n<p><strong></strong></p>\n<p> (2.2) <br>$$\\begin{aligned} F(k,p,q)&amp;=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}}<br>\\\\ &amp;= \\min_{r \\in succ(p)} {b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in succ(p) \\cup {p}} {b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in {1,2,,N}} {b(p,r)+F(k-1,r,q)} \\qquad(2.5) \\end{aligned}$$<br>$b(p,r)$  edge  $\\infty$ $W_{N \\times N}$ (2.2)  $F^{(0)}$  0 $\\infty$</p>\n<p>$$F^{(0)}=\\begin{bmatrix}0 &amp; \\infty &amp; \\cdots &amp; \\infty<br>\\\\                    \\infty &amp; 0  &amp; \\cdots &amp; \\infty<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    \\infty &amp; \\infty &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>$$W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p> (2.5) $F^{(k-1)}$  $F^{(k)}$ </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">F_k=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">  <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_k[p][q]=sys.info.float_max</span><br><span class=\"line\">    <span class=\"comment\"># F_k[p][q]=0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])</span><br><span class=\"line\">      <span class=\"comment\"># F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])</span></span><br></pre></td></tr></table></figure>\n\n<p> $F^{(N-1)}$ $F^{(k)}$ <br>$$\\begin{aligned} F^{(1)}&amp;=W \\circ F^{(0)}=W<br>\\\\ F^{(2)}&amp;=W \\circ F^{(1)}=W^2<br>\\\\ F^{(3)}&amp;=W \\circ F^{(2)}=W^3<br>\\\\ &amp;\\vdots<br>\\\\ F^{(N-1)}&amp;=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$</p>\n<p>$\\circ$ <br>$$\\begin{aligned} F^{(1)}&amp;=W<br>\\\\ F^{(2)}&amp;=W^2=W \\circ W<br>\\\\ F^{(4)}&amp;=W^4=W^2 \\circ W^2<br>\\\\ &amp;\\vdots<br>\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&amp;=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$<br> $2^{\\lceil log(N-1) \\rceil}$  $\\lceil \\cdot \\rceil$  $2^{\\lceil log(N-1) \\rceil} \\ge N-1$ $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ element-wise comparison</p>\n<p> (2.6)  (2.7)<br>$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$<br> $\\circ$  $\\min$ $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$ $\\min$  element-wise operator (2.6)  (2.7) </p>\n<p> (2.7)  $F^{(M)}, M \\ge N-1$ $F^{(k)}$  $F^{(N-1)}$ $F^{(M)}$ </p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fast_apsp</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  k=<span class=\"number\">1</span></span><br><span class=\"line\">  F_prev=W</span><br><span class=\"line\">  <span class=\"keyword\">while</span> k&lt;N<span class=\"number\">-1</span>:</span><br><span class=\"line\">    F_next=[[sys.info.float_max]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">    k*=<span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<p><strong>Floyd-Warshall</strong> </p>\n<p> (2.4) $F^{(k)}$  k  ${1,2,,k}$ $F^{(0)}$ <br>$$F^{(0)}=W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p> (2.4) <strong>Floyd-Warshall</strong> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F_prev=F_0</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">floyd_warshall</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_next=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-ARC\"><a href=\"#-ARC\" class=\"headerlink\" title=\" ARC\"></a> ARC</h2><p>ARC *</p>\n<p>ARCinternal nodes root $S=(w_0,w_1,,w_{n-1})$  $(i,j)$  $(w_i,,w_j)$<br>$$f(i,j)=\\min_{}{c(i,j,d)+f(i,d)+f(d+1,j)}, \\quad i&lt;j \\qquad(2.8)$$<br> $f(i,j)$  $(i,,j)$  $c(i,j,d)=\\sum_{k=i}^j w_k$  (2.8)d  $(i,,d)$ $(d+1,,j)$  d  $(i,,j)$  $c(i,j,d)$ </p>\n<p> $f(0,n-1)$ $f(i,i)=0, \\ \\forall i \\in {0,1,,n-1}$</p>\n<p> ARC  Huffman $S=(1,2,3,4)$ $(((1,2),3),4)$$f(S)=3+6+10=19$ $S=(2,3,3,4)$ $((2,3),(3,4))$$f(S)=5+7+12=24$</p>\n<h2 id=\"-ASMBAL\"><a href=\"#-ASMBAL\" class=\"headerlink\" title=\" ASMBAL\"></a> ASMBAL</h2><p>/ stage  k i  k+1  j  c(k,i,j) c(k,i,i)=0 0 s  t  0  c(0,0,j) c(N,j,0)j  </p>\n<p> 0~13  14  0  13  14 <br>$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$<br><br><img src=\"/images/DP2_fig1.png\" alt></p>\n<p> 14x14 </p>\n<p> k i  j  $R(k,i,j)=v(k,i)+c(k,i,j)$DPFE <br>$$f(k,i)=\\min_j {R(k,i,j)+f(k+1,j)}$$<br> $f(k,i)$  k  i  $f(0,0)$ $f(k,i)=0, k &gt; N$N  N=6<br>$$\\begin{aligned} f(0,0)=\\min {R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)}<br>\\\\ f(1,0)=\\min {R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)}<br>\\\\ f(1,1)=\\min {R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)}<br>\\\\ \\cdots \\ (omitted)<br>\\end{aligned}$$</p>\n<h2 id=\"-ASSIGN\"><a href=\"#-ASSIGN\" class=\"headerlink\" title=\" ASSIGN\"></a> ASSIGN</h2><p> B  A  A  A  ${1,2,3}$  ${3,2,1}$ 3!  $A=(a_0,a_1,,a_{n-1})$  $B=(b_0,b_1,,b_{n-1})$  i $a_j$  $b_i$ $c(i,j)$ A  $(k,S)$ k  A  S k  d $C(k,S,d)$ $(k+1,S-{d})$DPFE <br>$$f(k,S)=\\min_{d \\in S} {C(k,S,d)+f(k+1,S-{d})}$$<br> $f(1,S^{\\ast})$ $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$</p>\n<h2 id=\"-BST\"><a href=\"#-BST\" class=\"headerlink\" title=\" BST\"></a> BST</h2><p> n  $X={x_0,,x_{n-1}}$ <strong></strong>  $x_i$  $p(x_i)$ $p_i$ $\\sum_{i=0}^{n-1}p_i=1$<br>$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$<br>$\\text{level}(x_i)$  $x_i$  DP </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> S  DPFE <br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; S \\ne \\emptyset<br>\\\\ 0 &amp; S=\\emptyset \\end{cases}$$<br> $S_l = {x \\in S: x &lt; \\alpha}, \\ S_r = {x \\in S: x &gt; \\alpha}$ $r(\\alpha, S)=\\sum_{x \\in S} p(x)$ 1 DPFE </p>\n<p> $S_l,\\ S_r$  <strong></strong> </p>\n<p><br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; |S|&gt;1<br>\\\\ p(x) &amp; S={x} \\end{cases}$$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> $(i,j)$ $X={x_0,,x_{n-1}}$ DPFE <br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i \\le j<br>\\\\ 0 &amp; i &gt; j \\end{cases}$$<br>$(i,j)$  k  $(i,j)$, $(i,k-1)$ $(k+1,j)$  k$(i,j)$  level  level level  $f(i,j)$ $(i,j)$  level level  $\\sum_{l=i}^j p_l$</p>\n<p><br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i &lt; j<br>\\\\ p_i &amp; i = j \\end{cases}$$</p>\n<h2 id=\"-COV\"><a href=\"#-COV\" class=\"headerlink\" title=\" COV\"></a> COV</h2><p> k k  0  i  $c_i$ n  $n \\le k$ n </p>\n<p> DP  $0,1,,k-1$ $l$  $s_l$ $c_{s_l}$ $c_l$ $j$ $l$  DPFE <br>$$f(j,l)=\\begin{cases} \\min_{d \\in {j-2,,l-1}} {(l-d)c_l+f(j-1,d)} &amp; j&gt;1<br>\\\\ (l+1)c_l &amp; j=1 \\end{cases}$$<br>d exclusive $s_l$  ${d+1,,l}$  d  $l-1$ $l$ d  $j-2$ ${j-1,,l}$ ${0,1,,j-2}$  $j-1$  $j-1$ </p>\n<p>$f(j,l)=(l+1)c_l, j=1$ $l$ $0,,l$ </p>\n<h2 id=\"-DEADLINE\"><a href=\"#-DEADLINE\" class=\"headerlink\" title=\" DEADLINE\"></a> DEADLINE</h2><p> 0 <strong></strong></p>\n<p> $S^{\\ast}={0,1,2,3,4}$ $p={10,15,20,1,5}$ $t={1,2,2,3,3}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">t=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">p=np.array([<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">m=<span class=\"number\">0</span>   <span class=\"comment\"># </span></span><br><span class=\"line\">n=t.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">  idx=np.where(t&gt;<span class=\"number\">0</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> idx.shape[<span class=\"number\">0</span>]==<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span>   <span class=\"comment\"># </span></span><br><span class=\"line\">  c=np.max(p[idx])  <span class=\"comment\"># </span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> c &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br><span class=\"line\">  m+=c</span><br><span class=\"line\">  idx=np.argmax(p[idx])+idx[<span class=\"number\">0</span>]  <span class=\"comment\"># </span></span><br><span class=\"line\">  p[idx]=<span class=\"number\">-1e8</span>                   <span class=\"comment\"># </span></span><br><span class=\"line\">  t-=<span class=\"number\">1</span>                          <span class=\"comment\"># </span></span><br><span class=\"line\">print(<span class=\"string\">''</span>, sep=<span class=\"string\">' '</span>)</span><br><span class=\"line\">print(m)    <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<p> DP  $(k,S)$ k S  d  S  $(k+1,S-{d})$DPFE <br>$$f(k,S)=\\max_{d \\in S}{c(d|S)+f(k+1,S-{d})}$$<br> d  k k  1 $c(d|S)=w_d$  $c(d|S)=0$ $f(1,S^{\\ast})$ $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$ n </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">S=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">n=len(t)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">profit</span><span class=\"params\">(k,d)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> p[d] <span class=\"keyword\">if</span> t[d]&gt;=k <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deadline</span><span class=\"params\">(k,S)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span> <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> k==n+<span class=\"number\">1</span> <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    max([profit(k,S[i])+deadline(k+<span class=\"number\">1</span>,S[:i]+S[i+<span class=\"number\">1</span>:]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(S))])</span><br><span class=\"line\"></span><br><span class=\"line\">print(deadline(<span class=\"number\">1</span>, S))   <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-DPP\"><a href=\"#-DPP\" class=\"headerlink\" title=\" DPP\"></a> DPP</h2><p> $b_1$  t  $b_t$ t  $x_t$  $r(x_t)$ $c(x_t,b_t)$ s s  $1,,T$ T  y $x_t$   t  $(t,b)$ t b  t DPFE <br>$$f(t,b)=\\begin{cases} \\max_{x_t \\in {0,,b}} {r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)} &amp; t \\le T<br>\\\\ 0 &amp; t=T+1 \\end{cases}$$</p>\n<h2 id=\"-EDP\"><a href=\"#-EDP\" class=\"headerlink\" title=\" EDP\"></a> EDP</h2><p> $\\Sigma$  $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$ $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$ x  y</p>\n<ul>\n<li> D $c(D)$</li>\n<li> I $c(I)$</li>\n<li> R $c(R)$ $c(R)=0$</li>\n</ul>\n<p> x  yDPFE <br>$$f(i,j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min {f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)} &amp; i&gt;0,j&gt;0 \\end{cases}$$<br> $f(i,j)$  $X_i$  $Y_j$ $X_i$  x  i $Y_j$  y  j  i=0  $X_i$  j  $Y_j$ j=0  $X_i$  i  $Y_j$ i&gt;0  j&gt;0  $X_i$  $Y_j$/</p>\n<ul>\n<li> $X_i$  i  $X_{i-1}$  $Y_j$ $c(D)$  $f(i-1,j)$</li>\n<li> $X_i$  $Y_{j-1}$ $Y_j$ $f(i,j-1)$  $c(I)$</li>\n<li>$x_i \\rightarrow y_j$ $X_{i-1}$  $Y_{j-1}$  $X_i$  $Y_j$  $f(i-1,j-1)$  $c(R)$</li>\n</ul>\n<p> DPFE<br>$$f(X_i,Y_j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min_{d \\in {D,I,R}} {f(t(X_i,Y_j,d))+c(d)} &amp; i&gt;0,j&gt;0<br>\\end{cases}$$<br><br>$$t(X_i,Y_j,D)=(X_{i-1},Y_j)<br>\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})<br>\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$</p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  DPFE</p>\n<h2 id=\"-ALLOT\"><a href=\"#-ALLOT\" class=\"headerlink\" title=\" ALLOT\"></a> ALLOT</h2><p> ALLOTALLOT  KSINT </p>\n<p> M  N  C(k,d)  d  k  $d_1$  1 $d_2$  2 (k,m)  k  m  k  m k  C(k,d) (k+1,m-d) <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (1.19)  DPFE <br>$$f(k,m)=\\min_{d \\in {0,,m}} {C(k,d)+f(k+1,m-d)} \\quad (2.1)$$<br> f(1,M) f(N+1,m)=0 $m \\ge 0$</p>\n<p> M=4N=3<br>$$(C_{k,d})_{k\\in {1,2,3};d\\in {0,,4}}=\\begin{pmatrix}\\infty &amp; 1.0 &amp; 0.8&amp; 0.4 &amp; 0.0 \\\\ \\infty &amp; 1.0&amp; 0.5 &amp; 0.0 &amp; 0.0 \\\\ \\infty &amp; 1.0 &amp; 0.6 &amp; 0.3 &amp; 0.0 \\end{pmatrix}$$<br>f(1,M)=1.0+0.5+1.0=2.5 $d_1=1,d_2=2,d_3=1$ (2.1) </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot</span><span class=\"params\">(cache=True)</span>:</span></span><br><span class=\"line\">    M=<span class=\"number\">4</span></span><br><span class=\"line\">    N=<span class=\"number\">3</span></span><br><span class=\"line\">    max_float=<span class=\"number\">1e8</span></span><br><span class=\"line\">    cost=[[max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>],</span><br><span class=\"line\">          [max_float, <span class=\"number\">1.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.0</span>]]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> cache:</span><br><span class=\"line\">        cache_dict = &#123;&#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allot_inner</span><span class=\"params\">(k,m)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> (k,m) <span class=\"keyword\">in</span> cache_dict:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> cache_dict[(k,m)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;= N: <span class=\"keyword\">return</span> [], <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        min_f=max_float</span><br><span class=\"line\">        min_ds = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> range(m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ds,f=allot_inner(k+<span class=\"number\">1</span>,m-d)</span><br><span class=\"line\">            temp=cost[k][d]+f</span><br><span class=\"line\">            <span class=\"keyword\">if</span> min_f &gt; temp:</span><br><span class=\"line\">                min_f = temp</span><br><span class=\"line\">                min_ds = [d]+ds</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cache <span class=\"keyword\">and</span> k &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            cache_dict[(k,m)]=(min_ds,min_f)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> min_ds, min_f</span><br><span class=\"line\">    ds, f=allot_inner(<span class=\"number\">0</span>,M)</span><br><span class=\"line\">    print(<span class=\"string\">\"min cost:\"</span>,f,<span class=\"string\">\"opt allotments:\"</span>, ds)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-APSP\"><a href=\"#-APSP\" class=\"headerlink\" title=\" APSP\"></a> APSP</h2><p> s  t  Nself-loop</p>\n<p> <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (s,t) </p>\n<p><strong>Relaxation</strong> </p>\n<p> F(k,p,q)  p  q k  p  q  k  <a href=\"2019/08/07/DP1\">Dynamic Programming (1)</a>  (1.27)DPFE <br>$$F(k,p,q)=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\quad(2.2)$$<br> r  p :</p>\n<ol>\n<li>$F(k,p,q)=0, k \\ge 0, p=q$ p  q  k  0</li>\n<li>$F(0,p,q)=\\infty, p \\ne q$  p  q  0 </li>\n</ol>\n<p> $b(p,p)=0$ (2.2) <br>$$\\begin{aligned}F(k,p,q)&amp;=\\min {F(k-1,p,q)+b(p,p), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}} \\\\ &amp;=\\min_{r \\in succ(p)\\cup {p}} {b(p,r)+F(k-1,r,q)} \\qquad(2.3) \\end{aligned}$$</p>\n<p><strong>Floyd-Warshall</strong> </p>\n<p> (2.2)  DPFE  p  q  k  p  r  r  k-1  q  p  r  r  q p  r  1 p  q  k r  k  k  ${1,2,,k}$ p  q  ${1,2,,N}$  p  q DPFE <br>$$F(k,p,q)=\\min {F(k-1,p,q), F(k-1,p,k)+F(k-1,k,q)} \\qquad(2.4)$$</p>\n<p></p>\n<ol>\n<li> N  $V={1,2,,N}$$p,q \\in V$</li>\n<li>$F(k,p,q)$  p  q  ${1,2,,k}$ </li>\n<li> $F(N,p,q)$</li>\n<li> $(2.4)$<ul>\n<li>p  q  k ${1,2,,k-1}$ </li>\n<li> p  q  k p  k  k  q ${1,2,,k-1}$ </li>\n</ul>\n</li>\n<li> $(2.4)$  k&gt;0k=0  $F(0,p,q)=0, p=q$ $F(0,p,q)=b(p,q), p\\ne q$ p q  0 p q  $b(p,q)$ q  p  $F(0,p,q)=\\infty, p \\notin succ(p) \\cup {p}$</li>\n<li> ${1,2,,k}$  p  q $b(p,p)=0$</li>\n</ol>\n<p> (2.2)  r  p  $N-1$  p  $N-1$k  $N-1$ p  q  N  (2.2)  $O(N^4)$ (2.4)  $O(N^3)$</p>\n<p> APSP  (2.2)  ${F^{(1)},F^{(2)},,F^{(N-1)}}$ $F^{(k)}$  $N \\times N$$F_{p,q}^{k}$  p  q  k  $\\min_{p,q} F_{p,q}^{(N-1)}$  APSP </p>\n<p><strong></strong></p>\n<p> (2.2) <br>$$\\begin{aligned} F(k,p,q)&amp;=\\min {F(k-1,p,q), \\min_{r \\in succ(p)} {b(p,r)+F(k-1,r,q)}}<br>\\\\ &amp;= \\min_{r \\in succ(p)} {b(p,p)+F(k-1,p,q), b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in succ(p) \\cup {p}} {b(p,r)+F(k-1,r,q)}<br>\\\\ &amp;= \\min_{r \\in {1,2,,N}} {b(p,r)+F(k-1,r,q)} \\qquad(2.5) \\end{aligned}$$<br>$b(p,r)$  edge  $\\infty$ $W_{N \\times N}$ (2.2)  $F^{(0)}$  0 $\\infty$</p>\n<p>$$F^{(0)}=\\begin{bmatrix}0 &amp; \\infty &amp; \\cdots &amp; \\infty<br>\\\\                    \\infty &amp; 0  &amp; \\cdots &amp; \\infty<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    \\infty &amp; \\infty &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p>$$W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p> (2.5) $F^{(k-1)}$  $F^{(k)}$ </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">F_k=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">  <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_k[p][q]=sys.info.float_max</span><br><span class=\"line\">    <span class=\"comment\"># F_k[p][q]=0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      F_k[p][q]=min(F_k[p][q],W[p][r]+F_k_1[r][q])</span><br><span class=\"line\">      <span class=\"comment\"># F_k[p][q]=F_k[p][q]+W[p][r]*F_k_1[r][q])</span></span><br></pre></td></tr></table></figure>\n\n<p> $F^{(N-1)}$ $F^{(k)}$ <br>$$\\begin{aligned} F^{(1)}&amp;=W \\circ F^{(0)}=W<br>\\\\ F^{(2)}&amp;=W \\circ F^{(1)}=W^2<br>\\\\ F^{(3)}&amp;=W \\circ F^{(2)}=W^3<br>\\\\ &amp;\\vdots<br>\\\\ F^{(N-1)}&amp;=W \\circ F^{(N-2)}=W^{(N-1)} \\end{aligned} \\quad(2.6)$$</p>\n<p>$\\circ$ <br>$$\\begin{aligned} F^{(1)}&amp;=W<br>\\\\ F^{(2)}&amp;=W^2=W \\circ W<br>\\\\ F^{(4)}&amp;=W^4=W^2 \\circ W^2<br>\\\\ &amp;\\vdots<br>\\\\ F^{2^{\\lceil log(N-1) \\rceil}}&amp;=W^{2^{\\lceil log(N-1) \\rceil}} =W^{2^{\\lceil log(N-1) \\rceil-1}} \\circ W^{2^{\\lceil log(N-1) \\rceil-1}} \\end{aligned} \\quad(2.7)$$<br> $2^{\\lceil log(N-1) \\rceil}$  $\\lceil \\cdot \\rceil$  $2^{\\lceil log(N-1) \\rceil} \\ge N-1$ $F^{2^{\\lceil log(N-1) \\rceil}} \\le F^{(N-1)}$ element-wise comparison</p>\n<p> (2.6)  (2.7)<br>$$F^{(4)}=W \\circ F^{(3)}=W \\circ (W \\circ F^{(2)})=\\cdots =W \\circ(W \\circ (W \\circ W)) \\stackrel{*}=(W \\circ W) \\circ (W \\circ W)=W^2 \\circ W^2$$<br> $\\circ$  $\\min$ $\\min (W, \\min(W, \\min(W,W)))=\\min(\\min(W,W), \\min(W,W))$ $\\min$  element-wise operator (2.6)  (2.7) </p>\n<p> (2.7)  $F^{(M)}, M \\ge N-1$ $F^{(k)}$  $F^{(N-1)}$ $F^{(M)}$ </p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fast_apsp</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  k=<span class=\"number\">1</span></span><br><span class=\"line\">  F_prev=W</span><br><span class=\"line\">  <span class=\"keyword\">while</span> k&lt;N<span class=\"number\">-1</span>:</span><br><span class=\"line\">    F_next=[[sys.info.float_max]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">          F_next[p][q]=min(F_next[p][q], F_prev[p][r]+F_prev[r][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">    k*=<span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<p><strong>Floyd-Warshall</strong> </p>\n<p> (2.4) $F^{(k)}$  k  ${1,2,,k}$ $F^{(0)}$ <br>$$F^{(0)}=W=\\begin{bmatrix}0 &amp; w_{12} &amp; \\cdots &amp; w_{1N}<br>\\\\                    w_{21} &amp; 0  &amp; \\cdots &amp; w_{2N}<br>\\\\                    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots<br>\\\\                    w_{N1} &amp; w_{N2} &amp; \\cdots &amp; 0 \\end{bmatrix}_{N \\times N}$$</p>\n<p> (2.4) <strong>Floyd-Warshall</strong> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F_prev=F_0</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">floyd_warshall</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">    F_next=[[<span class=\"literal\">None</span>]*N]*N</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">      <span class=\"keyword\">for</span> q <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,N):</span><br><span class=\"line\">        F_next[p][q]=min(F_prev[p][q], F_prev[p][k]+F_prev[k][q])</span><br><span class=\"line\">    F_prev=F_next</span><br><span class=\"line\">  <span class=\"keyword\">return</span> F_prev</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-ARC\"><a href=\"#-ARC\" class=\"headerlink\" title=\" ARC\"></a> ARC</h2><p>ARC *</p>\n<p>ARCinternal nodes root $S=(w_0,w_1,,w_{n-1})$  $(i,j)$  $(w_i,,w_j)$<br>$$f(i,j)=\\min_{}{c(i,j,d)+f(i,d)+f(d+1,j)}, \\quad i&lt;j \\qquad(2.8)$$<br> $f(i,j)$  $(i,,j)$  $c(i,j,d)=\\sum_{k=i}^j w_k$  (2.8)d  $(i,,d)$ $(d+1,,j)$  d  $(i,,j)$  $c(i,j,d)$ </p>\n<p> $f(0,n-1)$ $f(i,i)=0, \\ \\forall i \\in {0,1,,n-1}$</p>\n<p> ARC  Huffman $S=(1,2,3,4)$ $(((1,2),3),4)$$f(S)=3+6+10=19$ $S=(2,3,3,4)$ $((2,3),(3,4))$$f(S)=5+7+12=24$</p>\n<h2 id=\"-ASMBAL\"><a href=\"#-ASMBAL\" class=\"headerlink\" title=\" ASMBAL\"></a> ASMBAL</h2><p>/ stage  k i  k+1  j  c(k,i,j) c(k,i,i)=0 0 s  t  0  c(0,0,j) c(N,j,0)j  </p>\n<p> 0~13  14  0  13  14 <br>$$v=(0,7,8,9,5,3,6,4,8,5,4,7,0)$$<br><br><img src=\"/images/DP2_fig1.png\" alt></p>\n<p> 14x14 </p>\n<p> k i  j  $R(k,i,j)=v(k,i)+c(k,i,j)$DPFE <br>$$f(k,i)=\\min_j {R(k,i,j)+f(k+1,j)}$$<br> $f(k,i)$  k  i  $f(0,0)$ $f(k,i)=0, k &gt; N$N  N=6<br>$$\\begin{aligned} f(0,0)=\\min {R(0,0,0)+f(1,0), R(0,0,1)+f(1,1)}<br>\\\\ f(1,0)=\\min {R(1,0,0)+f(2,0), R(1,0,1)+f(2,1)}<br>\\\\ f(1,1)=\\min {R(1,1,0)+f(2,0), R(1,1,1)+f(2,1)}<br>\\\\ \\cdots \\ (omitted)<br>\\end{aligned}$$</p>\n<h2 id=\"-ASSIGN\"><a href=\"#-ASSIGN\" class=\"headerlink\" title=\" ASSIGN\"></a> ASSIGN</h2><p> B  A  A  A  ${1,2,3}$  ${3,2,1}$ 3!  $A=(a_0,a_1,,a_{n-1})$  $B=(b_0,b_1,,b_{n-1})$  i $a_j$  $b_i$ $c(i,j)$ A  $(k,S)$ k  A  S k  d $C(k,S,d)$ $(k+1,S-{d})$DPFE <br>$$f(k,S)=\\min_{d \\in S} {C(k,S,d)+f(k+1,S-{d})}$$<br> $f(1,S^{\\ast})$ $f(k,S)=0, \\ k=n+1 \\ or \\ S=\\emptyset$</p>\n<h2 id=\"-BST\"><a href=\"#-BST\" class=\"headerlink\" title=\" BST\"></a> BST</h2><p> n  $X={x_0,,x_{n-1}}$ <strong></strong>  $x_i$  $p(x_i)$ $p_i$ $\\sum_{i=0}^{n-1}p_i=1$<br>$$\\sum_{i=0}^{n-1}(p_i \\text{level}(x_i))$$<br>$\\text{level}(x_i)$  $x_i$  DP </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> S  DPFE <br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; S \\ne \\emptyset<br>\\\\ 0 &amp; S=\\emptyset \\end{cases}$$<br> $S_l = {x \\in S: x &lt; \\alpha}, \\ S_r = {x \\in S: x &gt; \\alpha}$ $r(\\alpha, S)=\\sum_{x \\in S} p(x)$ 1 DPFE </p>\n<p> $S_l,\\ S_r$  <strong></strong> </p>\n<p><br>$$f(S)=\\begin{cases} \\min_{\\alpha \\in S} {f(S_l)+f(S_r)+r(\\alpha, S)} &amp; |S|&gt;1<br>\\\\ p(x) &amp; S={x} \\end{cases}$$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> $(i,j)$ $X={x_0,,x_{n-1}}$ DPFE <br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i \\le j<br>\\\\ 0 &amp; i &gt; j \\end{cases}$$<br>$(i,j)$  k  $(i,j)$, $(i,k-1)$ $(k+1,j)$  k$(i,j)$  level  level level  $f(i,j)$ $(i,j)$  level level  $\\sum_{l=i}^j p_l$</p>\n<p><br>$$f(i,j)=\\begin{cases} \\min_{k \\in {i,,j}} {f(i,k-1)+f(k+1,j)+\\sum_{l=i}^j p_l} &amp; i &lt; j<br>\\\\ p_i &amp; i = j \\end{cases}$$</p>\n<h2 id=\"-COV\"><a href=\"#-COV\" class=\"headerlink\" title=\" COV\"></a> COV</h2><p> k k  0  i  $c_i$ n  $n \\le k$ n </p>\n<p> DP  $0,1,,k-1$ $l$  $s_l$ $c_{s_l}$ $c_l$ $j$ $l$  DPFE <br>$$f(j,l)=\\begin{cases} \\min_{d \\in {j-2,,l-1}} {(l-d)c_l+f(j-1,d)} &amp; j&gt;1<br>\\\\ (l+1)c_l &amp; j=1 \\end{cases}$$<br>d exclusive $s_l$  ${d+1,,l}$  d  $l-1$ $l$ d  $j-2$ ${j-1,,l}$ ${0,1,,j-2}$  $j-1$  $j-1$ </p>\n<p>$f(j,l)=(l+1)c_l, j=1$ $l$ $0,,l$ </p>\n<h2 id=\"-DEADLINE\"><a href=\"#-DEADLINE\" class=\"headerlink\" title=\" DEADLINE\"></a> DEADLINE</h2><p> 0 <strong></strong></p>\n<p> $S^{\\ast}={0,1,2,3,4}$ $p={10,15,20,1,5}$ $t={1,2,2,3,3}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">t=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">p=np.array([<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">m=<span class=\"number\">0</span>   <span class=\"comment\"># </span></span><br><span class=\"line\">n=t.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">  idx=np.where(t&gt;<span class=\"number\">0</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> idx.shape[<span class=\"number\">0</span>]==<span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span>   <span class=\"comment\"># </span></span><br><span class=\"line\">  c=np.max(p[idx])  <span class=\"comment\"># </span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> c &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br><span class=\"line\">  m+=c</span><br><span class=\"line\">  idx=np.argmax(p[idx])+idx[<span class=\"number\">0</span>]  <span class=\"comment\"># </span></span><br><span class=\"line\">  p[idx]=<span class=\"number\">-1e8</span>                   <span class=\"comment\"># </span></span><br><span class=\"line\">  t-=<span class=\"number\">1</span>                          <span class=\"comment\"># </span></span><br><span class=\"line\">print(<span class=\"string\">''</span>, sep=<span class=\"string\">' '</span>)</span><br><span class=\"line\">print(m)    <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<p> DP  $(k,S)$ k S  d  S  $(k+1,S-{d})$DPFE <br>$$f(k,S)=\\max_{d \\in S}{c(d|S)+f(k+1,S-{d})}$$<br> d  k k  1 $c(d|S)=w_d$  $c(d|S)=0$ $f(1,S^{\\ast})$ $f(k,S)=0, k=n+1 \\ or \\ S=\\emptyset$ n </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\">p=[<span class=\"number\">10</span>,<span class=\"number\">15</span>,<span class=\"number\">20</span>,<span class=\"number\">1</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">S=[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">n=len(t)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">profit</span><span class=\"params\">(k,d)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> p[d] <span class=\"keyword\">if</span> t[d]&gt;=k <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deadline</span><span class=\"params\">(k,S)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span> <span class=\"keyword\">if</span> len(S)==<span class=\"number\">0</span> <span class=\"keyword\">or</span> k==n+<span class=\"number\">1</span> <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    max([profit(k,S[i])+deadline(k+<span class=\"number\">1</span>,S[:i]+S[i+<span class=\"number\">1</span>:]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(S))])</span><br><span class=\"line\"></span><br><span class=\"line\">print(deadline(<span class=\"number\">1</span>, S))   <span class=\"comment\"># 40</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"-DPP\"><a href=\"#-DPP\" class=\"headerlink\" title=\" DPP\"></a> DPP</h2><p> $b_1$  t  $b_t$ t  $x_t$  $r(x_t)$ $c(x_t,b_t)$ s s  $1,,T$ T  y $x_t$   t  $(t,b)$ t b  t DPFE <br>$$f(t,b)=\\begin{cases} \\max_{x_t \\in {0,,b}} {r(x_t)-c(x_t,b)+\\frac 1 {1+y} f(t+1, \\lfloor s(b-x_t) \\rfloor)} &amp; t \\le T<br>\\\\ 0 &amp; t=T+1 \\end{cases}$$</p>\n<h2 id=\"-EDP\"><a href=\"#-EDP\" class=\"headerlink\" title=\" EDP\"></a> EDP</h2><p> $\\Sigma$  $x\\in \\Sigma^m, \\ y \\in \\Sigma^n$ $x=x_1\\cdots x_m, \\ y=y_1 \\cdots y_n$ x  y</p>\n<ul>\n<li> D $c(D)$</li>\n<li> I $c(I)$</li>\n<li> R $c(R)$ $c(R)=0$</li>\n</ul>\n<p> x  yDPFE <br>$$f(i,j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min {f(i-1,j)+c(D),f(i,j-1)+c(I),f(i-1,j-1)+c(R)} &amp; i&gt;0,j&gt;0 \\end{cases}$$<br> $f(i,j)$  $X_i$  $Y_j$ $X_i$  x  i $Y_j$  y  j  i=0  $X_i$  j  $Y_j$ j=0  $X_i$  i  $Y_j$ i&gt;0  j&gt;0  $X_i$  $Y_j$/</p>\n<ul>\n<li> $X_i$  i  $X_{i-1}$  $Y_j$ $c(D)$  $f(i-1,j)$</li>\n<li> $X_i$  $Y_{j-1}$ $Y_j$ $f(i,j-1)$  $c(I)$</li>\n<li>$x_i \\rightarrow y_j$ $X_{i-1}$  $Y_{j-1}$  $X_i$  $Y_j$  $f(i-1,j-1)$  $c(R)$</li>\n</ul>\n<p> DPFE<br>$$f(X_i,Y_j)=\\begin{cases} jI &amp; i=0<br>\\\\ iD &amp; j=0<br>\\\\ \\min_{d \\in {D,I,R}} {f(t(X_i,Y_j,d))+c(d)} &amp; i&gt;0,j&gt;0<br>\\end{cases}$$<br><br>$$t(X_i,Y_j,D)=(X_{i-1},Y_j)<br>\\\\ t(X_i,Y_j,I)=(X_i,Y_{j-1})<br>\\\\ t(X_i,Y_j,R)=(X_{i-1},Y_{j-1})$$</p>\n"},{"title":"PyTorch-2","date":"2019-06-13T02:19:52.000Z","_content":"# torch installization\nPyTorchpythonPyTorch\n```\nimport torch\n```\n `torch/__init__.py`torch._CRTLD_GLOBAL|RTLD_LAZYRTLD_GLOBAL|RTLD_LAZYtorch._C\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__torch._C_Base__\n\n`__init__.py`import torch._Cimportmodulepackagetorch._C torch._Ctorch/csrc/stub.cppshmtorch_pythonstub.cpp\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // pythonimport _C \n{\n  return initModule();\n}\n```\npython3`import torch._C` PyInit__CPyInit_&lt;package>initModuleinitModuleexterninitModuleshmtorch_python\n\nshmDomain Sockettorch/CMakeLists.txtshm\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\nshmtorch/lib/libshmtorch._Ctorch_pythoninitModuletorch/CMakeLists.txt\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCStorch_pythontorch_pythontorch/CMakeLists.txt\n\ninitModuletorch/csrc/Module.cpp\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // cuda\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // \n  // methods\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // \n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // \n    nullptr,                           \n    -1,\n    methods.data()                       // \n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // \n  // \n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // cuda\n#endif\n  ...\n  // setter\n  // namevincref\n  // 10\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // \n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // \n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // \n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  //  _THNN \n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\ntorch._Cattr\n# methods/members in torch._C\n-  THPUtils_addPyMethodDefs torch._C \n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  \n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- torch._C \n\n    - torch._C_PtrWrapperGeneratorFatalErrorSizedtypeiinfolayoutmemory_formatdevice_LegacyVariableBase_TensorBase_VariableFunctions_FunctionBase_EngineBaseJITExceptionIODescriptor_THNN_THCUNN\n\n        torch._C._TensorBase\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        \n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        torch._C._FunctionBase \n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n         torch._C._VariableFunctions \n        ```python\n        arange\n        as_tensor\n        ...\n        empty       #  torch.empty\n        empty_like\n        ...\n        ```\n\n        _TensorBaseTensorTensor_FunctionBase\n\n    - torch._C _wrap_tensor_impl_tensor_impl_raw_handle_demangle_log_api_usage_once_jit\n\n    - torch._C _nncpp_onnx\n\n    - torch._C has_cudnnhas_openmphas_mklhas_lapackhas_cudahas_mkldnn_GLIBCXX_USE_CXX11_APIdefault_generator\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._CTensor THPxxxStorage_init  THCPxxxStorage_init \n\nModule.cpp\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\nATenc10torchATen [A Tensor Library] Tensorc10 [caffe2ATen] Tensor\n\n TH/TH.h #include <TH/THGeneral.h>aten/src/THCMakeLists.txt\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\nTHGeneral.h\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)\n```\ntorch/csrc/THP.h #include <torch/src/Storage.h>Storage.h\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n4include/include torch/csrc/generic/Storage.htorch/csrc/generic/Storage.h \n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\nTH/THGenerateAllType.h\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4includeinclude#undef TH_GENERIC_FILEincludeinclude torch/csrc/generic/Storage.h (0) (1)TH/THGenerateFloatTypes.h\n```\n//  TH_GENERIC_FILE\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // TH_GENERIC_FILE \n```\nTH/THGenerateFloatType.h\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n (2) include torch/csrc/generic/Storate.hTH_GENERIC_FILE  (1) \n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n#include <TH/THGenerateDoubleType.h>THPDoubleStorage_init\n\n#include <TH/THGenerateIntTypes.h> \n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n4include\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\ntorch/csrc/Storage.cpp\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init \n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n4include torch/csrc/generic/Storage.cpp\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\nmoduleTHPStorageBaseStr torch/csrc/Storage.h\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\nTH/THGeneral.h\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\nStorageBaseReal Int, Float, Double, Short, CharTHPxxxStorage_initReal=FloatTHPStorageBaseStr\"FloatStorageBase\"torch._C FloatStorageBase python class torch._C.FloatStorageBase\n\n4includeinclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (11)include TH/THGenerateAllTypes.hTH/THGenerateFloatType.h\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\ninclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (12) THPFloatStorage_inittorch._C  FloatStorageBase\n\nIntCharByteDoubleHalfQUInt8\n\ntorch/csrc/Module.cppinitModule THCPxxxStorage_init  THPxxxStorage_init  torch/csrc/cuda/Storage.h  torch/csrc/cuda/Storage.cpp \n\ntorch._CFloatStorageBasetorch/csrc/generic/Storage.cpp THPStorageType\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\npythonFloatStorageBaseC++THPStorage torch/csrc/StorageDef.hTHPStorage\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\ntorch/csrc/generic/Storage.cpp  THPStoragetorch/csrc/Storage.cppinclude torch/csrc/THP.htorch/csrc/generic/Storage.cpp torch/csrc/THP.h include torch/csrc/Storage.htorch/csrc/Storage.hincludetorch/csrc/generic/Storage.hgeneric/Storage.hinclude torch/csrc/StorageDef.h\n\n THPStorage_(pynew) \n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // \n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // self\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // allocator\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      //  c10::Allocator \n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // cdata\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // THPStorage\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // cdataTHWStorage\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     //  self->cdata\n}   \n```\nFloatStorageBase THPStorage.cdataTHWStoragetorch/csrc/THP.h\n```\n#define THWStorage THStorage\n```\n THStorage torch/csrc/Storage.cppinclude\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n TH/generic/THStorage.h \n```\n#define THStorage at::StorageImpl\n```\n c10/core/StorageImpl.h \n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // \n  DataPtr data_ptr_;             // \n  int64_t numel_;                // \n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // \n};\n}\n```\nTHWStorage at::StorageImpl THPStorage_(pynew)  cdata THWStorage THWStorage_(NAME)NAME\n```\nnew                // THStorage sizesize=0Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // THStorage sizeAllocator\nnewWithAllocator   // THStorage size  Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\nTHStorage_(NAME)  TH/generic/THStorage.hTH/generic/THStorageCopy.h cpp\n\ncuda#define THWStorage_(NAME) THCStorage_(NAME)THC/generic/THCStorage.hTHC/generic/THCStorageCopy.h\n\n THStorage_(newWithSize) TH/generic/THStorage.cpp\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // scalar_t \n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\nStorageImplintrusive_ptrStorageImplintrusive_ptr  THStorage  at::StorageImplStorageImplc10::make_instrusiveStorageImpl\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\nStorageImpl\n\nFloatStorageBaseTH/THGenerateFloatType.h  4include\n```\n#define scalar_t float\n```\n\n```\ncaffe2::TypeMeta::Make<scalar_t>()    //  THQUANTIZED \n```\ncaffe2::TypeMeta::Make caffe2::TypeMeta detail::TypeMetaData* data_new TypeMetaData\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n _typeMetaDataInstance\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// \nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n float\n```\n// \nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\nc10/util/typeid.cpp\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstanceTypeMetaDataTypeMetaData floatid\n```\nstruct TypeMetaData final {\n// \nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // new\nusing Copy = void(const void*, void*, size_t);  // \nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //\n\nsize_t itemsize_;  // \nNew* new_;\nPlacementNew* placementNew_;   //  new\nCopy* copy_;        // \nDelete* delete_;    // \nTypeIdentifier id_; // id\nconst char* name_;  // \n};\n```\nfloatdetail::_makeTypeMetaDataInstance \n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // T\n          _PickNew<T>(),             //  new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // id\n          typeName};                 // float\"float\"\n```\nstructstruct{}id\n```\nTypeIdentifier::Get<T>()\n```\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE TypeIdentifer(PreallocatedId)floatPreallocatedId6\n\n intdoubleint64_t\n\nPyTorchidid_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCETypeIdentifier::createTypeId()PyTorchid32_CaffeHighestPreallocatedTypeIdid1\n\nTypeMetaDataTypeMetaTypeMetaDataStorageImplTHStorage_(newWithSize)(ptrdiff_t size)StorageImpl\n```\nsize,             // StorageImplTypeMetafloat\ngetTHDefaultAllocator(),  // posix_memalign\ntrue                      // StorageImplresize\n```\nStorageImplTHPStorageStorageImplTHPStorage torch._C FloatStorageBase\n\nfloatTHPStorageIntStorageBase\n\nFloatStorageBasemethods, members, properties generic/Storage.cppTHPStorage_(int)(PyObject* module)\n\n _THNN  _THCUNN  torch._C\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\ntorch/csrc/nnTHNN.cppTHCUNN.cpp torch_python TARGET tools/setup_helpers/generate_code.py torch/CMakeLists.txt\n\n`torch._C` `torch/__init__.py` import torch\n\n1.  typenameis_tensoris_storage\n2. torch\n3. _C._init_nametorch/csrc/Module.cpp torchDoubleStorage torch.DoubleStorageFloatStorageHalfStorage\n4. _C._initExtensiontorch/csrc/Module.cpp \n    - layouttorchstridedsparse_coo_mkldnn\n    - torchany_formatpreserve_formatcontiguous_formatchannels_last\n    - torchuint8int8float64float32int32int64int16float16complex32complex64complex128boolqint8quint8qint32torch\n    - python1PyTensorType PyTensorTypeBackendScalarType2torch.tensortypetorch.FloatTensorTensormetaclass3pythonTensortorch.FloatTensor4Tensor torch 5FloatTensorTensor\n    - \n    -  THPxxxStorage_postInit(module)xxxTHPxxxStorage_Init moduletorchtorch._CPython storageFloattorch.FloatStorage\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n         TH_CONCAT_2(at::k, Real)at::kRealReal=Floatat::ScalarType::Float\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        THPStorageClassback+at::kReal\n\nimport torch \n\n# \nPyTorch","source":"_posts/PyTorch-2.md","raw":"---\ntitle: PyTorch-2\ndate: 2019-06-13 10:19:52\ntags: PyTorch\ncategories: DL Framework\n---\n# torch installization\nPyTorchpythonPyTorch\n```\nimport torch\n```\n `torch/__init__.py`torch._CRTLD_GLOBAL|RTLD_LAZYRTLD_GLOBAL|RTLD_LAZYtorch._C\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__torch._C_Base__\n\n`__init__.py`import torch._Cimportmodulepackagetorch._C torch._Ctorch/csrc/stub.cppshmtorch_pythonstub.cpp\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // pythonimport _C \n{\n  return initModule();\n}\n```\npython3`import torch._C` PyInit__CPyInit_&lt;package>initModuleinitModuleexterninitModuleshmtorch_python\n\nshmDomain Sockettorch/CMakeLists.txtshm\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\nshmtorch/lib/libshmtorch._Ctorch_pythoninitModuletorch/CMakeLists.txt\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCStorch_pythontorch_pythontorch/CMakeLists.txt\n\ninitModuletorch/csrc/Module.cpp\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // cuda\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // \n  // methods\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // \n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // \n    nullptr,                           \n    -1,\n    methods.data()                       // \n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // \n  // \n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // cuda\n#endif\n  ...\n  // setter\n  // namevincref\n  // 10\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // \n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // \n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // \n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  //  _THNN \n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\ntorch._Cattr\n# methods/members in torch._C\n-  THPUtils_addPyMethodDefs torch._C \n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  \n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- torch._C \n\n    - torch._C_PtrWrapperGeneratorFatalErrorSizedtypeiinfolayoutmemory_formatdevice_LegacyVariableBase_TensorBase_VariableFunctions_FunctionBase_EngineBaseJITExceptionIODescriptor_THNN_THCUNN\n\n        torch._C._TensorBase\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        \n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        torch._C._FunctionBase \n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n         torch._C._VariableFunctions \n        ```python\n        arange\n        as_tensor\n        ...\n        empty       #  torch.empty\n        empty_like\n        ...\n        ```\n\n        _TensorBaseTensorTensor_FunctionBase\n\n    - torch._C _wrap_tensor_impl_tensor_impl_raw_handle_demangle_log_api_usage_once_jit\n\n    - torch._C _nncpp_onnx\n\n    - torch._C has_cudnnhas_openmphas_mklhas_lapackhas_cudahas_mkldnn_GLIBCXX_USE_CXX11_APIdefault_generator\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._CTensor THPxxxStorage_init  THCPxxxStorage_init \n\nModule.cpp\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\nATenc10torchATen [A Tensor Library] Tensorc10 [caffe2ATen] Tensor\n\n TH/TH.h #include <TH/THGeneral.h>aten/src/THCMakeLists.txt\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\nTHGeneral.h\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)\n```\ntorch/csrc/THP.h #include <torch/src/Storage.h>Storage.h\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n4include/include torch/csrc/generic/Storage.htorch/csrc/generic/Storage.h \n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\nTH/THGenerateAllType.h\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4includeinclude#undef TH_GENERIC_FILEincludeinclude torch/csrc/generic/Storage.h (0) (1)TH/THGenerateFloatTypes.h\n```\n//  TH_GENERIC_FILE\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // TH_GENERIC_FILE \n```\nTH/THGenerateFloatType.h\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n (2) include torch/csrc/generic/Storate.hTH_GENERIC_FILE  (1) \n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n#include <TH/THGenerateDoubleType.h>THPDoubleStorage_init\n\n#include <TH/THGenerateIntTypes.h> \n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n4include\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\ntorch/csrc/Storage.cpp\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init \n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n4include torch/csrc/generic/Storage.cpp\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\nmoduleTHPStorageBaseStr torch/csrc/Storage.h\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\nTH/THGeneral.h\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\nStorageBaseReal Int, Float, Double, Short, CharTHPxxxStorage_initReal=FloatTHPStorageBaseStr\"FloatStorageBase\"torch._C FloatStorageBase python class torch._C.FloatStorageBase\n\n4includeinclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (11)include TH/THGenerateAllTypes.hTH/THGenerateFloatType.h\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\ninclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (12) THPFloatStorage_inittorch._C  FloatStorageBase\n\nIntCharByteDoubleHalfQUInt8\n\ntorch/csrc/Module.cppinitModule THCPxxxStorage_init  THPxxxStorage_init  torch/csrc/cuda/Storage.h  torch/csrc/cuda/Storage.cpp \n\ntorch._CFloatStorageBasetorch/csrc/generic/Storage.cpp THPStorageType\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\npythonFloatStorageBaseC++THPStorage torch/csrc/StorageDef.hTHPStorage\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\ntorch/csrc/generic/Storage.cpp  THPStoragetorch/csrc/Storage.cppinclude torch/csrc/THP.htorch/csrc/generic/Storage.cpp torch/csrc/THP.h include torch/csrc/Storage.htorch/csrc/Storage.hincludetorch/csrc/generic/Storage.hgeneric/Storage.hinclude torch/csrc/StorageDef.h\n\n THPStorage_(pynew) \n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // \n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // self\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // allocator\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      //  c10::Allocator \n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // cdata\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // THPStorage\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // cdataTHWStorage\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     //  self->cdata\n}   \n```\nFloatStorageBase THPStorage.cdataTHWStoragetorch/csrc/THP.h\n```\n#define THWStorage THStorage\n```\n THStorage torch/csrc/Storage.cppinclude\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n TH/generic/THStorage.h \n```\n#define THStorage at::StorageImpl\n```\n c10/core/StorageImpl.h \n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // \n  DataPtr data_ptr_;             // \n  int64_t numel_;                // \n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // \n};\n}\n```\nTHWStorage at::StorageImpl THPStorage_(pynew)  cdata THWStorage THWStorage_(NAME)NAME\n```\nnew                // THStorage sizesize=0Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // THStorage sizeAllocator\nnewWithAllocator   // THStorage size  Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\nTHStorage_(NAME)  TH/generic/THStorage.hTH/generic/THStorageCopy.h cpp\n\ncuda#define THWStorage_(NAME) THCStorage_(NAME)THC/generic/THCStorage.hTHC/generic/THCStorageCopy.h\n\n THStorage_(newWithSize) TH/generic/THStorage.cpp\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // scalar_t \n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\nStorageImplintrusive_ptrStorageImplintrusive_ptr  THStorage  at::StorageImplStorageImplc10::make_instrusiveStorageImpl\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\nStorageImpl\n\nFloatStorageBaseTH/THGenerateFloatType.h  4include\n```\n#define scalar_t float\n```\n\n```\ncaffe2::TypeMeta::Make<scalar_t>()    //  THQUANTIZED \n```\ncaffe2::TypeMeta::Make caffe2::TypeMeta detail::TypeMetaData* data_new TypeMetaData\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n _typeMetaDataInstance\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// \nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n float\n```\n// \nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\nc10/util/typeid.cpp\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstanceTypeMetaDataTypeMetaData floatid\n```\nstruct TypeMetaData final {\n// \nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // new\nusing Copy = void(const void*, void*, size_t);  // \nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //\n\nsize_t itemsize_;  // \nNew* new_;\nPlacementNew* placementNew_;   //  new\nCopy* copy_;        // \nDelete* delete_;    // \nTypeIdentifier id_; // id\nconst char* name_;  // \n};\n```\nfloatdetail::_makeTypeMetaDataInstance \n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // T\n          _PickNew<T>(),             //  new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // id\n          typeName};                 // float\"float\"\n```\nstructstruct{}id\n```\nTypeIdentifier::Get<T>()\n```\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE TypeIdentifer(PreallocatedId)floatPreallocatedId6\n\n intdoubleint64_t\n\nPyTorchidid_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCETypeIdentifier::createTypeId()PyTorchid32_CaffeHighestPreallocatedTypeIdid1\n\nTypeMetaDataTypeMetaTypeMetaDataStorageImplTHStorage_(newWithSize)(ptrdiff_t size)StorageImpl\n```\nsize,             // StorageImplTypeMetafloat\ngetTHDefaultAllocator(),  // posix_memalign\ntrue                      // StorageImplresize\n```\nStorageImplTHPStorageStorageImplTHPStorage torch._C FloatStorageBase\n\nfloatTHPStorageIntStorageBase\n\nFloatStorageBasemethods, members, properties generic/Storage.cppTHPStorage_(int)(PyObject* module)\n\n _THNN  _THCUNN  torch._C\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\ntorch/csrc/nnTHNN.cppTHCUNN.cpp torch_python TARGET tools/setup_helpers/generate_code.py torch/CMakeLists.txt\n\n`torch._C` `torch/__init__.py` import torch\n\n1.  typenameis_tensoris_storage\n2. torch\n3. _C._init_nametorch/csrc/Module.cpp torchDoubleStorage torch.DoubleStorageFloatStorageHalfStorage\n4. _C._initExtensiontorch/csrc/Module.cpp \n    - layouttorchstridedsparse_coo_mkldnn\n    - torchany_formatpreserve_formatcontiguous_formatchannels_last\n    - torchuint8int8float64float32int32int64int16float16complex32complex64complex128boolqint8quint8qint32torch\n    - python1PyTensorType PyTensorTypeBackendScalarType2torch.tensortypetorch.FloatTensorTensormetaclass3pythonTensortorch.FloatTensor4Tensor torch 5FloatTensorTensor\n    - \n    -  THPxxxStorage_postInit(module)xxxTHPxxxStorage_Init moduletorchtorch._CPython storageFloattorch.FloatStorage\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n         TH_CONCAT_2(at::k, Real)at::kRealReal=Floatat::ScalarType::Float\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        THPStorageClassback+at::kReal\n\nimport torch \n\n# \nPyTorch","slug":"PyTorch-2","published":1,"updated":"2019-08-26T01:17:15.372Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379f40021dgvc23p093iy","content":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1><p>PyTorchpythonPyTorch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n\n<p> <code>torch/__init__.py</code>torch._CRTLD_GLOBAL|RTLD_LAZYRTLD_GLOBAL|RTLD_LAZYtorch._C</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags=sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ += [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] != &apos;_&apos; and</span><br><span class=\"line\">            not name.endswith(&apos;Base&apos;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n\n<p><strong>torch._C_Base</strong></p>\n<p><code>__init__.py</code>import torch._Cimportmodulepackagetorch._C torch._Ctorch/csrc/stub.cppshmtorch_pythonstub.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   // pythonimport _C </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>python3<code>import torch._C</code> PyInit__CPyInit_&lt;package&gt;initModuleinitModuleexterninitModuleshmtorch_python</p>\n<p>shmDomain Sockettorch/CMakeLists.txtshm</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>shmtorch/lib/libshmtorch._Ctorch_pythoninitModuletorch/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>TORCH_PYTHON_SRCStorch_pythontorch_pythontorch/CMakeLists.txt</p>\n<p>initModuletorch/csrc/Module.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       // cuda</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 // </span><br><span class=\"line\">  // methods</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // </span><br><span class=\"line\">  static struct PyModuleDef torchmodule = &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          // </span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       // </span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module = PyModule_Create(&amp;torchmodule)); // </span><br><span class=\"line\">  // </span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       // cuda</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // setter</span><br><span class=\"line\">  // namevincref</span><br><span class=\"line\">  // 10</span><br><span class=\"line\">  auto set_module_attr = [&amp;](const char* name, PyObject* v, bool incref = true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) == 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // </span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  // </span><br><span class=\"line\">  auto py_module = py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    // </span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  //  _THNN </span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>torch._Cattr</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p> THPUtils_addPyMethodDefs torch._C </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch/csrc/autograd/init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  </span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch/csrc/cuda/Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch/csrc/distributed/Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>torch._C </p>\n<ul>\n<li><p>torch._C_PtrWrapperGeneratorFatalErrorSizedtypeiinfolayoutmemory_formatdevice_LegacyVariableBase_TensorBase_VariableFunctions_FunctionBase_EngineBaseJITExceptionIODescriptor_THNN_THCUNN</p>\n<p>  torch._C._TensorBase</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n\n<p>  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n\n<p>  torch._C._FunctionBase </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n\n<p>   torch._C._VariableFunctions </p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arange</span><br><span class=\"line\">as_tensor</span><br><span class=\"line\">...</span><br><span class=\"line\">empty       <span class=\"comment\">#  torch.empty</span></span><br><span class=\"line\">empty_like</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>  _TensorBaseTensorTensor_FunctionBase</p>\n</li>\n<li><p>torch._C _wrap_tensor_impl_tensor_impl_raw_handle_demangle_log_api_usage_once_jit</p>\n</li>\n<li><p>torch._C _nncpp_onnx</p>\n</li>\n<li><p>torch._C has_cudnnhas_openmphas_mklhas_lapackhas_cudahas_mkldnn_GLIBCXX_USE_CXX11_APIdefault_generator</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._CTensor THPxxxStorage_init  THCPxxxStorage_init </p>\n<p>Module.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/TH.h&gt;               // TH=TorcH</span><br><span class=\"line\">#include &lt;c10/util/Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen/ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;      // THP=TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>ATenc10torchATen [A Tensor Library] Tensorc10 [caffe2ATen] Tensor</p>\n<p> TH/TH.h #include &lt;TH/THGeneral.h&gt;aten/src/THCMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>THGeneral.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/THP.h #include &lt;torch/src/Storage.h&gt;Storage.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>4include/include torch/csrc/generic/Storage.htorch/csrc/generic/Storage.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.h&quot;         // (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      // (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>TH/THGenerateAllType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>4includeinclude#undef TH_GENERIC_FILEincludeinclude torch/csrc/generic/Storage.h (0) (1)TH/THGenerateFloatTypes.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  TH_GENERIC_FILE</span><br><span class=\"line\">#include &lt;TH/THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     // TH_GENERIC_FILE </span><br></pre></td></tr></table></figure>\n\n<p>TH/THGenerateFloatType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         // (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n\n<p> (2) include torch/csrc/generic/Storate.hTH_GENERIC_FILE  (1) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n\n<p>#include &lt;TH/THGenerateDoubleType.h&gt;THPDoubleStorage_init</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n\n<p>4include</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/Storage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;                   // include THPxxxStorage_init </span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>4include torch/csrc/generic/Storage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.cpp&quot;              // (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   // (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods = methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members = THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset = THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>moduleTHPStorageBaseStr torch/csrc/Storage.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n\n<p>TH/THGeneral.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n\n<p>StorageBaseReal Int, Float, Double, Short, CharTHPxxxStorage_initReal=FloatTHPStorageBaseStrFloatStorageBasetorch._C FloatStorageBase python class torch._C.FloatStorageBase</p>\n<p>4includeinclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (11)include TH/THGenerateAllTypes.hTH/THGenerateFloatType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>include torch/csrc/generic/Storage.cppTH_GENERIC_FILE (12) THPFloatStorage_inittorch._C  FloatStorageBase</p>\n<p>IntCharByteDoubleHalfQUInt8</p>\n<p>torch/csrc/Module.cppinitModule THCPxxxStorage_init  THPxxxStorage_init  torch/csrc/cuda/Storage.h  torch/csrc/cuda/Storage.cpp </p>\n<p>torch._CFloatStorageBasetorch/csrc/generic/Storage.cpp THPStorageType</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType = &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               /* tp_name */</span><br><span class=\"line\">  sizeof(THPStorage),                          /* tp_basicsize */</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          /* tp_new */</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>pythonFloatStorageBaseC++THPStorage torch/csrc/StorageDef.hTHPStorage</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/generic/Storage.cpp  THPStoragetorch/csrc/Storage.cppinclude torch/csrc/THP.htorch/csrc/generic/Storage.cpp torch/csrc/THP.h include torch/csrc/Storage.htorch/csrc/Storage.hincludetorch/csrc/generic/Storage.hgeneric/Storage.hinclude torch/csrc/StorageDef.h</p>\n<p> THPStorage_(pynew) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // </span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); // self</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator = nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs != nullptr) &#123;                               // named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, &quot;allocator&quot;); // allocator</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      //  c10::Allocator </span><br><span class=\"line\">      allocator = static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs = PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args == 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs==1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   // cdata</span><br><span class=\"line\">        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata = ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       // THPStorage</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs == 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args == 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            // cdataTHWStorage</span><br><span class=\"line\">      self-&gt;cdata = THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     //  self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>FloatStorageBase THPStorage.cdataTHWStoragetorch/csrc/THP.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n\n<p> THStorage torch/csrc/Storage.cppinclude</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH/TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH/THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH/generic/THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10/core/StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p> TH/generic/THStorage.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n\n<p> c10/core/StorageImpl.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  // </span><br><span class=\"line\">  DataPtr data_ptr_;             // </span><br><span class=\"line\">  int64_t numel_;                // </span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         // </span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>THWStorage at::StorageImpl THPStorage_(pynew)  cdata THWStorage THWStorage_(NAME)NAME</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                // THStorage sizesize=0Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        // THStorage sizeAllocator</span><br><span class=\"line\">newWithAllocator   // THStorage size  Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n\n<p>THStorage_(NAME)  TH/generic/THStorage.hTH/generic/THStorageCopy.h cpp</p>\n<p>cuda#define THWStorage_(NAME) THCStorage_(NAME)THC/generic/THCStorage.hTHC/generic/THCStorageCopy.h</p>\n<p> THStorage_(newWithSize) TH/generic/THStorage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage = c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        // scalar_t </span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>StorageImplintrusive_ptrStorageImplintrusive_ptr  THStorage  at::StorageImplStorageImplc10::make_instrusiveStorageImpl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>StorageImpl</p>\n<p>FloatStorageBaseTH/THGenerateFloatType.h  4include</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    //  THQUANTIZED </span><br></pre></td></tr></table></figure>\n\n<p>caffe2::TypeMeta::Make caffe2::TypeMeta detail::TypeMetaData* data_new TypeMetaData</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n\n<p> _typeMetaDataInstance</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) = _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">// </span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n\n<p> float</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// </span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>c10/util/typeid.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    = _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>detail::_makeTypeMetaDataInstanceTypeMetaDataTypeMetaData floatid</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">// </span><br><span class=\"line\">using New = void*();                            // new</span><br><span class=\"line\">using PlacementNew = void(void*, size_t);       // new</span><br><span class=\"line\">using Copy = void(const void*, void*, size_t);  // </span><br><span class=\"line\">using PlacementDelete = void(void*, size_t);</span><br><span class=\"line\">using Delete = void(void*);</span><br><span class=\"line\">... //</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  // </span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   //  new</span><br><span class=\"line\">Copy* copy_;        // </span><br><span class=\"line\">Delete* delete_;    // </span><br><span class=\"line\">TypeIdentifier id_; // id</span><br><span class=\"line\">const char* name_;  // </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>floatdetail::_makeTypeMetaDataInstance </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 // T</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             //  new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  // id</span><br><span class=\"line\">          typeName&#125;;                 // float&quot;float&quot;</span><br></pre></td></tr></table></figure>\n\n<p>structstruct{}id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n\n<p>CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE TypeIdentifer(PreallocatedId)floatPreallocatedId6</p>\n<p> intdoubleint64_t</p>\n<p>PyTorchidid_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCETypeIdentifier::createTypeId()PyTorchid32_CaffeHighestPreallocatedTypeIdid1</p>\n<p>TypeMetaDataTypeMetaTypeMetaDataStorageImplTHStorage_(newWithSize)(ptrdiff_t size)StorageImpl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             // StorageImplTypeMetafloat</span><br><span class=\"line\">getTHDefaultAllocator(),  // posix_memalign</span><br><span class=\"line\">true                      // StorageImplresize</span><br></pre></td></tr></table></figure>\n\n<p>StorageImplTHPStorageStorageImplTHPStorage torch._C FloatStorageBase</p>\n<p>floatTHPStorageIntStorageBase</p>\n<p>FloatStorageBasemethods, members, properties generic/Storage.cppTHPStorage_(int)(PyObject* module)</p>\n<p> _THNN  _THCUNN  torch._C</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/nnTHNN.cppTHCUNN.cpp torch_python TARGET tools/setup_helpers/generate_code.py torch/CMakeLists.txt</p>\n<p><code>torch._C</code> <code>torch/__init__.py</code> import torch</p>\n<ol>\n<li><p> typenameis_tensoris_storage</p>\n</li>\n<li><p>torch</p>\n</li>\n<li><p>_C._init_nametorch/csrc/Module.cpp torchDoubleStorage torch.DoubleStorageFloatStorageHalfStorage</p>\n</li>\n<li><p>_C._initExtensiontorch/csrc/Module.cpp </p>\n<ul>\n<li><p>layouttorchstridedsparse_coo_mkldnn</p>\n</li>\n<li><p>torchany_formatpreserve_formatcontiguous_formatchannels_last</p>\n</li>\n<li><p>torchuint8int8float64float32int32int64int16float16complex32complex64complex128boolqint8quint8qint32torch</p>\n</li>\n<li><p>python1PyTensorType PyTensorTypeBackendScalarType2torch.tensortypetorch.FloatTensorTensormetaclass3pythonTensortorch.FloatTensor4Tensor torch 5FloatTensorTensor</p>\n</li>\n<li><p></p>\n</li>\n<li><p> THPxxxStorage_postInit(module)xxxTHPxxxStorage_Init moduletorchtorch._CPython storageFloattorch.FloatStorage</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n\n<p>   TH_CONCAT_2(at::k, Real)at::kRealReal=Floatat::ScalarType::Float</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`</span><br></pre></td></tr></table></figure>\n\n<p>  THPStorageClassback+at::kReal</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>import torch </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>PyTorch</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1><p>PyTorchpythonPyTorch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n\n<p> <code>torch/__init__.py</code>torch._CRTLD_GLOBAL|RTLD_LAZYRTLD_GLOBAL|RTLD_LAZYtorch._C</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags=sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ += [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] != &apos;_&apos; and</span><br><span class=\"line\">            not name.endswith(&apos;Base&apos;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n\n<p><strong>torch._C_Base</strong></p>\n<p><code>__init__.py</code>import torch._Cimportmodulepackagetorch._C torch._Ctorch/csrc/stub.cppshmtorch_pythonstub.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   // pythonimport _C </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>python3<code>import torch._C</code> PyInit__CPyInit_&lt;package&gt;initModuleinitModuleexterninitModuleshmtorch_python</p>\n<p>shmDomain Sockettorch/CMakeLists.txtshm</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>shmtorch/lib/libshmtorch._Ctorch_pythoninitModuletorch/CMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>TORCH_PYTHON_SRCStorch_pythontorch_pythontorch/CMakeLists.txt</p>\n<p>initModuletorch/csrc/Module.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       // cuda</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 // </span><br><span class=\"line\">  // methods</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // </span><br><span class=\"line\">  static struct PyModuleDef torchmodule = &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          // </span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       // </span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module = PyModule_Create(&amp;torchmodule)); // </span><br><span class=\"line\">  // </span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       // cuda</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // setter</span><br><span class=\"line\">  // namevincref</span><br><span class=\"line\">  // 10</span><br><span class=\"line\">  auto set_module_attr = [&amp;](const char* name, PyObject* v, bool incref = true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) == 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // </span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  // </span><br><span class=\"line\">  auto py_module = py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    // </span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  //  _THNN </span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>torch._Cattr</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p> THPUtils_addPyMethodDefs torch._C </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch/csrc/autograd/init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  </span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch/csrc/cuda/Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch/csrc/distributed/Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>torch._C </p>\n<ul>\n<li><p>torch._C_PtrWrapperGeneratorFatalErrorSizedtypeiinfolayoutmemory_formatdevice_LegacyVariableBase_TensorBase_VariableFunctions_FunctionBase_EngineBaseJITExceptionIODescriptor_THNN_THCUNN</p>\n<p>  torch._C._TensorBase</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n\n<p>  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n\n<p>  torch._C._FunctionBase </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n\n<p>   torch._C._VariableFunctions </p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arange</span><br><span class=\"line\">as_tensor</span><br><span class=\"line\">...</span><br><span class=\"line\">empty       <span class=\"comment\">#  torch.empty</span></span><br><span class=\"line\">empty_like</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>  _TensorBaseTensorTensor_FunctionBase</p>\n</li>\n<li><p>torch._C _wrap_tensor_impl_tensor_impl_raw_handle_demangle_log_api_usage_once_jit</p>\n</li>\n<li><p>torch._C _nncpp_onnx</p>\n</li>\n<li><p>torch._C has_cudnnhas_openmphas_mklhas_lapackhas_cudahas_mkldnn_GLIBCXX_USE_CXX11_APIdefault_generator</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._CTensor THPxxxStorage_init  THCPxxxStorage_init </p>\n<p>Module.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/TH.h&gt;               // TH=TorcH</span><br><span class=\"line\">#include &lt;c10/util/Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen/ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;      // THP=TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>ATenc10torchATen [A Tensor Library] Tensorc10 [caffe2ATen] Tensor</p>\n<p> TH/TH.h #include &lt;TH/THGeneral.h&gt;aten/src/THCMakeLists.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>THGeneral.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND(x,y,z,w)</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/THP.h #include &lt;torch/src/Storage.h&gt;Storage.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>4include/include torch/csrc/generic/Storage.htorch/csrc/generic/Storage.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.h&quot;         // (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      // (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>TH/THGenerateAllType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>4includeinclude#undef TH_GENERIC_FILEincludeinclude torch/csrc/generic/Storage.h (0) (1)TH/THGenerateFloatTypes.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  TH_GENERIC_FILE</span><br><span class=\"line\">#include &lt;TH/THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     // TH_GENERIC_FILE </span><br></pre></td></tr></table></figure>\n\n<p>TH/THGenerateFloatType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         // (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n\n<p> (2) include torch/csrc/generic/Storate.hTH_GENERIC_FILE  (1) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n\n<p>#include &lt;TH/THGenerateDoubleType.h&gt;THPDoubleStorage_init</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n\n<p>4include</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/Storage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;                   // include THPxxxStorage_init </span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>4include torch/csrc/generic/Storage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.cpp&quot;              // (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   // (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods = methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members = THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset = THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>moduleTHPStorageBaseStr torch/csrc/Storage.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n\n<p>TH/THGeneral.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n\n<p>StorageBaseReal Int, Float, Double, Short, CharTHPxxxStorage_initReal=FloatTHPStorageBaseStrFloatStorageBasetorch._C FloatStorageBase python class torch._C.FloatStorageBase</p>\n<p>4includeinclude torch/csrc/generic/Storage.cppTH_GENERIC_FILE (11)include TH/THGenerateAllTypes.hTH/THGenerateFloatType.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>include torch/csrc/generic/Storage.cppTH_GENERIC_FILE (12) THPFloatStorage_inittorch._C  FloatStorageBase</p>\n<p>IntCharByteDoubleHalfQUInt8</p>\n<p>torch/csrc/Module.cppinitModule THCPxxxStorage_init  THPxxxStorage_init  torch/csrc/cuda/Storage.h  torch/csrc/cuda/Storage.cpp </p>\n<p>torch._CFloatStorageBasetorch/csrc/generic/Storage.cpp THPStorageType</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType = &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               /* tp_name */</span><br><span class=\"line\">  sizeof(THPStorage),                          /* tp_basicsize */</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          /* tp_new */</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>pythonFloatStorageBaseC++THPStorage torch/csrc/StorageDef.hTHPStorage</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/generic/Storage.cpp  THPStoragetorch/csrc/Storage.cppinclude torch/csrc/THP.htorch/csrc/generic/Storage.cpp torch/csrc/THP.h include torch/csrc/Storage.htorch/csrc/Storage.hincludetorch/csrc/generic/Storage.hgeneric/Storage.hinclude torch/csrc/StorageDef.h</p>\n<p> THPStorage_(pynew) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // </span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); // self</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator = nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs != nullptr) &#123;                               // named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, &quot;allocator&quot;); // allocator</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      //  c10::Allocator </span><br><span class=\"line\">      allocator = static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs = PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args == 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs==1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   // cdata</span><br><span class=\"line\">        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata = ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       // THPStorage</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs == 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args == 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            // cdataTHWStorage</span><br><span class=\"line\">      self-&gt;cdata = THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     //  self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>FloatStorageBase THPStorage.cdataTHWStoragetorch/csrc/THP.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n\n<p> THStorage torch/csrc/Storage.cppinclude</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH/TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH/THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH/generic/THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10/core/StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p> TH/generic/THStorage.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n\n<p> c10/core/StorageImpl.h </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  // </span><br><span class=\"line\">  DataPtr data_ptr_;             // </span><br><span class=\"line\">  int64_t numel_;                // </span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         // </span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>THWStorage at::StorageImpl THPStorage_(pynew)  cdata THWStorage THWStorage_(NAME)NAME</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                // THStorage sizesize=0Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        // THStorage sizeAllocator</span><br><span class=\"line\">newWithAllocator   // THStorage size  Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n\n<p>THStorage_(NAME)  TH/generic/THStorage.hTH/generic/THStorageCopy.h cpp</p>\n<p>cuda#define THWStorage_(NAME) THCStorage_(NAME)THC/generic/THCStorage.hTHC/generic/THCStorageCopy.h</p>\n<p> THStorage_(newWithSize) TH/generic/THStorage.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage = c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        // scalar_t </span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>StorageImplintrusive_ptrStorageImplintrusive_ptr  THStorage  at::StorageImplStorageImplc10::make_instrusiveStorageImpl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>StorageImpl</p>\n<p>FloatStorageBaseTH/THGenerateFloatType.h  4include</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    //  THQUANTIZED </span><br></pre></td></tr></table></figure>\n\n<p>caffe2::TypeMeta::Make caffe2::TypeMeta detail::TypeMetaData* data_new TypeMetaData</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n\n<p> _typeMetaDataInstance</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) = _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">// </span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n\n<p> float</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// </span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>c10/util/typeid.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    = _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>detail::_makeTypeMetaDataInstanceTypeMetaDataTypeMetaData floatid</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">// </span><br><span class=\"line\">using New = void*();                            // new</span><br><span class=\"line\">using PlacementNew = void(void*, size_t);       // new</span><br><span class=\"line\">using Copy = void(const void*, void*, size_t);  // </span><br><span class=\"line\">using PlacementDelete = void(void*, size_t);</span><br><span class=\"line\">using Delete = void(void*);</span><br><span class=\"line\">... //</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  // </span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   //  new</span><br><span class=\"line\">Copy* copy_;        // </span><br><span class=\"line\">Delete* delete_;    // </span><br><span class=\"line\">TypeIdentifier id_; // id</span><br><span class=\"line\">const char* name_;  // </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>floatdetail::_makeTypeMetaDataInstance </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 // T</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             //  new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  // id</span><br><span class=\"line\">          typeName&#125;;                 // float&quot;float&quot;</span><br></pre></td></tr></table></figure>\n\n<p>structstruct{}id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n\n<p>CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE TypeIdentifer(PreallocatedId)floatPreallocatedId6</p>\n<p> intdoubleint64_t</p>\n<p>PyTorchidid_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCETypeIdentifier::createTypeId()PyTorchid32_CaffeHighestPreallocatedTypeIdid1</p>\n<p>TypeMetaDataTypeMetaTypeMetaDataStorageImplTHStorage_(newWithSize)(ptrdiff_t size)StorageImpl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             // StorageImplTypeMetafloat</span><br><span class=\"line\">getTHDefaultAllocator(),  // posix_memalign</span><br><span class=\"line\">true                      // StorageImplresize</span><br></pre></td></tr></table></figure>\n\n<p>StorageImplTHPStorageStorageImplTHPStorage torch._C FloatStorageBase</p>\n<p>floatTHPStorageIntStorageBase</p>\n<p>FloatStorageBasemethods, members, properties generic/Storage.cppTHPStorage_(int)(PyObject* module)</p>\n<p> _THNN  _THCUNN  torch._C</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>torch/csrc/nnTHNN.cppTHCUNN.cpp torch_python TARGET tools/setup_helpers/generate_code.py torch/CMakeLists.txt</p>\n<p><code>torch._C</code> <code>torch/__init__.py</code> import torch</p>\n<ol>\n<li><p> typenameis_tensoris_storage</p>\n</li>\n<li><p>torch</p>\n</li>\n<li><p>_C._init_nametorch/csrc/Module.cpp torchDoubleStorage torch.DoubleStorageFloatStorageHalfStorage</p>\n</li>\n<li><p>_C._initExtensiontorch/csrc/Module.cpp </p>\n<ul>\n<li><p>layouttorchstridedsparse_coo_mkldnn</p>\n</li>\n<li><p>torchany_formatpreserve_formatcontiguous_formatchannels_last</p>\n</li>\n<li><p>torchuint8int8float64float32int32int64int16float16complex32complex64complex128boolqint8quint8qint32torch</p>\n</li>\n<li><p>python1PyTensorType PyTensorTypeBackendScalarType2torch.tensortypetorch.FloatTensorTensormetaclass3pythonTensortorch.FloatTensor4Tensor torch 5FloatTensorTensor</p>\n</li>\n<li><p></p>\n</li>\n<li><p> THPxxxStorage_postInit(module)xxxTHPxxxStorage_Init moduletorchtorch._CPython storageFloattorch.FloatStorage</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n\n<p>   TH_CONCAT_2(at::k, Real)at::kRealReal=Floatat::ScalarType::Float</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`</span><br></pre></td></tr></table></figure>\n\n<p>  THPStorageClassback+at::kReal</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>import torch </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>PyTorch</p>\n"},{"title":"PyTorch-3","date":"2019-06-18T08:44:44.000Z","_content":" [PyTorch-2](PyTorch-2)  torch  package  PyTorch [](https://pytorch.org/docs/stable/torch.html)\n# torch \n `torch/__init__.py`  torch \n1. / typename, is_tensor, is_storage, _storage_classes \n2.  torch /\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3.  torch._C //\n4.  torch._C._VariableFunctions /\n   \nPyTorch  torch \n## torch.empty\n torch._C._VariableFunctions  torch/csrc/Module.cpp  THPVariable_initModule torch/csrc/autograd/python_variable.cpp  torch::autograd::initTorchFunctions torch/csrc/autograd/generated/python_torch_functions.cpp PyTorch \n1. caffe2/CMakeLists.txt \n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2.  tools/setup_helpers/generate_code.py generate_code \n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n torch/csrc/autograd/generated/python_torch_functions.cpp  tools/autograd/templates/python_torch_functions.cpp  ${py_methods}  ${py_method_defs}  torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml\n1.  caffe2/CMakeLists.txt \n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2.  cmake/Codegen.cmake  `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n    aten/src/ATen/native/native_functions.yaml  `empty` \n3. aten/src/ATen/gen.py  generate_outputs  Declarations.yaml \n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4.  2 install_dir  build/aten/src/ATen Declarations.yaml  build/aten/src/ATen\n   - CMakeLists.txt  add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt  add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt  add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt \n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n    Declarations.yaml aten/src/ATen/CMakeLists.txt  build/aten/src/ATen/Functions.h aten/src/ATen/CMakeLists.txt  INSTALL \n   \n tools/autograd/gen_python_functions.py  create_python_bindings  ${py_methods}  ${py_method_defs} \n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... //  env  key-value pair or  env  key  value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n PY_VARIABLE_METHOD_VARARGS Declarations.yaml, deprecated.yaml, derivatives.yaml env  PY_VARIABLE_METHOD_VARARGS  env  key \n\n## empty \n empty  torch/csrc/autograd/generated/python_torch_function.cpp\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // vector\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n empty  Tensor Tensor Tensor\n1. `out` None dtype, device, layout  requires_grad  Tensor\n2. `out` None,  `out`  Tensor  dtype, layout, device  `out`  requires_grad  requires_grad\n\n dispatch_empty torch/csrc/autograd/generated/python_torch_functions_dispatch.h python_torch_function.cpp  tools/autograd/templates/python_torch_functions_dispatch.h  tools/autograd/gen_python_functions.py  gen_py_torch_functionsdispatch_empty \n```\n// empty  Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty  Tensor 'out' options \ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n###  Tensor\n Tensor AutoNoGIL\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n GIL at::empty_out  GIL at::empty_out  GIL\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n at::empty_out  torch/lib/include/Aten/Functions.h\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n at::empty_out  Functions.h  aten/src/ATen/gen.py  generate_outputs  Declarations.yaml \n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n at::empty_out  detail::infer_type(result)  Tensor  result  TypeExtendedInference  empty_out TypeExtendedInferfaceTypeDefault torch/lib/include/ATen/TypeExtendedInferface.h torch/lib/include/ATen/TypeDefault.hTypeDefault build/aten/src/ATen/TypeDefault.cpp empty_out \n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n Declarations.yaml  at::native::empty_out  torch/lib/include/ATen/NativeFunctions.h Declarations.yaml  aten/src/ATen/native/TensorFactories.cpp\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n Tensor \n1.  Tensor \n   \n    Tensor  sparse_resize_and_clear_ torch/lib/include/ATen/core/Tensor.h Declarations.yaml  aten/src/ATen/gen.py aten/src/ATen/core/Tensor.h TensorMethods.h  Type.h sparse_resize_and_clear_  torch/lib/include/ATen/core/TensorMethods.h\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n    Tensor  Type Type  sparse_resize_and_clear_ Type  Type  .cpp Type  int,float,double  BackendCPU,CUDA,SparseCPU, SparseCUDA  SparseCPUByteType.h  SparseCPUByteType.cpp\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n    at::native::sparse_resize_and_clear_  torch/lib/include/ATen/NativeFunctions.h aten/src/ATen/native/sparse/SparseTensor.cpp\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n    Tensor  SparseTensorImpl  SparseTensorImpl  resize_and_clear_\n2.  Tensor \n   \n   Tensor  resize_  TensorMethods.h\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n    Tensor  resize_ CPUByteType.cpp \n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n    Tensor  size  resize  aten/src/ATen/native/Resize.cpp  resize_cpu_ \n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         //  Tensor \n       //  size  Tensor  resize size  Tensor size \n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   resize_impl_cpu_  cpu  resize  aten/src/ATen/native/Resize.h \n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           //  size  size \n           // size size \n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // stride=1\n           self->set_sizes_contiguous(size);    //  size  size\n           storage_size = self->numel();        //  size  size  (n1,n2,n3) n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize \n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset()  0\n           // \n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n    aten/src/TH/THStorageFunctions.cpp  THStorage_resize \n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           //  Tensor \n           //  Tensor \n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   //  size\n           storage->set_numel(size);                // \n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // \n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n    resize  N1resize  N2\n   1. N1 >= N2 size N1  N2 \n   2. N1 < N2 N1  N1  Tensor  allocator \n\n torch.empty \n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize  size\nprint(x)\ntorch.empty(1,2,out=x)  # resize  size\nprint(x)\ntorch.empty(4,4,out=x)  #  resize  size\n```\n\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n###  Tensor\n torch/csrc/autograd/generated/python_torch_functions_dispatch.h  tensor  dispatch_empty  torch::empty torch/csrc/autograd/generated/variable_factories.h jit  Tensor \n```c++\ninline at::Tensor empty(at::IntList size, const at::TensorOptions & options={}) {\n    ...     // jit tracing\n    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(false));\n    auto result = autograd::make_variable(tensor, options.requires_grad()); //  Tensor  Variable\n    ...     // jit tracing\n    return result\n}\n```\n at::empty  Functions.h\n```c++\nstatic inline Tensor empty(IntList size, const TensorOptions & options) {\n    return at::getType(options).empty(size, options);\n}\n```\n Tensor  empty  at::getType(options)  options  TypeExtendedInterface  instance options.backend(), options.dtype()  options.is_variable()  CPU  backend  aten/src/ATen/Context.cpp  Context  register_cpu_types(this)  register_cpu_type(Context* context)  build/aten/src/ATen/RegisterCPU.cpp  aten/src/ATen/gen.py  generate_outputs  gen.py  register_cpu_types \n```\nCPUByteType\nCPUCharType\nCPUDoubleType\nCPUFloatType\nCPUIntType\nCPULongType\nCPUShortType\n...\n```\n CPUByteType empty \n```\nTensor CPUByteType::empty(IntList size, const TensorOptions & options) const {\n    const DeviceGuard device_guard(options.device());   //  device  Tensor\n    return at::native::empty_cpu(size, options);\n}\n```\n at::native::empty_cpu  aten/src/ATen/native/TensorFactories.cpp \n```c++\nauto* allocator = at::getCPUAllocator();\nint64_t nelements = prod_intlist(size); // \nauto dtype = options.dtype();\nauto storage_impl = c10::make_intrusive<StorageImpl>(\n    dtype,\n    nelements,\n    allocator->allocate(nelements*dtype.itemsize()),\n    allocator,\n    /*resizeable=*/true\n);\nauto tensor = detail::make_tensor<TensorImpl>(storage_impl, at::CPUTensorId(), false);\n```\n c10::make_intrusive<StorageImpl>  new StorageImpl(...) wrap  intrusive_ptr [PyTorch-2](2019/06/13/PyTorch-2)  Tensor  StorageImpl StorageImpl  detail::make_tensor  Tensor at::getCPUAllocator  THDefaultAllocator  allocate  THAlloc THAlloc  THAllocInternal  malloc posix_memalign  \n\n\n```python\nimport torch\ntorch.empty(2,3)\n```\n\n```\ntensor([[1.6504e-12,3.0637e-41,1.6588e-12],\n        [3.0637e-41,4.4842e-44,0.0000e+00]])\n```\n\n### Tensor \n torch.empty  torch.Tensor  `torch/__init__.py`  `import autograd` `torch/autograd/__init__.py`\n```python\nif not torch._C._autograd_init():\n```\n_autograd_init  python  torch/csrc/Module.cpp  THPAutograd_initExtension  c++  torch/csrc/autograd/autograd.h  torch/csrc/autograd/init.cpp \n```c++\n//  torch/tensor.py \nauto tensor_module = THPObjectPtr(PyImport_ImportModule(\"torch.tensor\"));\n//  torch/tensor.py  Tensor \nTHPVariableClass = PyObject_GetAttrString(tensor_module, \"Tensor\");\n```\n `THPVariableClass`  torch/csrc/autograd/python_variable.h \n```c++\nTHP_API PyObject *THPVariableClass;\n```\n extern  torch/csrc/autograd/python_variable.cpp  torch.empty  c++  THPVariable_empty  dispatch_empty  Variable  wrap  PyObject wrap  torch/csrc/autograd/utils/wrap_outputs.h  THPVariable_Wrap torch/csrc/autograd/python_variable.cpp THPVariableClass  THPVariableClass  torch/tensor.py  Tensor  THPVariable_Wrap  THPVariable_NewWithVar  Variable  THPVariableClass  Tensor THPVariable_NewWithVar \n```c++\nstatic PyObject* THPVariable_NewWithVar(PyTypeObject* type, Variable var) {\nPyObject *obj=type->tp_alloc(type, 0);      //  torch.Tensor \nif(obj) {\n    auto v = (THPVariable*)obj; // cast  THPVariable  torch.Tensor  torch._C._TensorBase \n    new(&v->cdata) Variable(std::move(var));    //  VariableC++  Tensor\n    v->cdata.set_pyobj(obj);\n    ...\n}\nreturn obj;\n}\n```\n\n\n```python\n>>> type(torch.empty(2,3))\n<class 'torch.Tensor'>\n```\n\n# PS\n","source":"_posts/PyTorch-3.md","raw":"---\ntitle: PyTorch-3\ndate: 2019-06-18 16:44:44\ntags: PyTorch\ncategories: DL Framework\n---\n [PyTorch-2](PyTorch-2)  torch  package  PyTorch [](https://pytorch.org/docs/stable/torch.html)\n# torch \n `torch/__init__.py`  torch \n1. / typename, is_tensor, is_storage, _storage_classes \n2.  torch /\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3.  torch._C //\n4.  torch._C._VariableFunctions /\n   \nPyTorch  torch \n## torch.empty\n torch._C._VariableFunctions  torch/csrc/Module.cpp  THPVariable_initModule torch/csrc/autograd/python_variable.cpp  torch::autograd::initTorchFunctions torch/csrc/autograd/generated/python_torch_functions.cpp PyTorch \n1. caffe2/CMakeLists.txt \n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2.  tools/setup_helpers/generate_code.py generate_code \n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n torch/csrc/autograd/generated/python_torch_functions.cpp  tools/autograd/templates/python_torch_functions.cpp  ${py_methods}  ${py_method_defs}  torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml\n1.  caffe2/CMakeLists.txt \n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2.  cmake/Codegen.cmake  `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n    aten/src/ATen/native/native_functions.yaml  `empty` \n3. aten/src/ATen/gen.py  generate_outputs  Declarations.yaml \n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4.  2 install_dir  build/aten/src/ATen Declarations.yaml  build/aten/src/ATen\n   - CMakeLists.txt  add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt  add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt  add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt \n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n    Declarations.yaml aten/src/ATen/CMakeLists.txt  build/aten/src/ATen/Functions.h aten/src/ATen/CMakeLists.txt  INSTALL \n   \n tools/autograd/gen_python_functions.py  create_python_bindings  ${py_methods}  ${py_method_defs} \n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... //  env  key-value pair or  env  key  value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n PY_VARIABLE_METHOD_VARARGS Declarations.yaml, deprecated.yaml, derivatives.yaml env  PY_VARIABLE_METHOD_VARARGS  env  key \n\n## empty \n empty  torch/csrc/autograd/generated/python_torch_function.cpp\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // vector\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n empty  Tensor Tensor Tensor\n1. `out` None dtype, device, layout  requires_grad  Tensor\n2. `out` None,  `out`  Tensor  dtype, layout, device  `out`  requires_grad  requires_grad\n\n dispatch_empty torch/csrc/autograd/generated/python_torch_functions_dispatch.h python_torch_function.cpp  tools/autograd/templates/python_torch_functions_dispatch.h  tools/autograd/gen_python_functions.py  gen_py_torch_functionsdispatch_empty \n```\n// empty  Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty  Tensor 'out' options \ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n###  Tensor\n Tensor AutoNoGIL\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n GIL at::empty_out  GIL at::empty_out  GIL\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n at::empty_out  torch/lib/include/Aten/Functions.h\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n at::empty_out  Functions.h  aten/src/ATen/gen.py  generate_outputs  Declarations.yaml \n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n at::empty_out  detail::infer_type(result)  Tensor  result  TypeExtendedInference  empty_out TypeExtendedInferfaceTypeDefault torch/lib/include/ATen/TypeExtendedInferface.h torch/lib/include/ATen/TypeDefault.hTypeDefault build/aten/src/ATen/TypeDefault.cpp empty_out \n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n Declarations.yaml  at::native::empty_out  torch/lib/include/ATen/NativeFunctions.h Declarations.yaml  aten/src/ATen/native/TensorFactories.cpp\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n Tensor \n1.  Tensor \n   \n    Tensor  sparse_resize_and_clear_ torch/lib/include/ATen/core/Tensor.h Declarations.yaml  aten/src/ATen/gen.py aten/src/ATen/core/Tensor.h TensorMethods.h  Type.h sparse_resize_and_clear_  torch/lib/include/ATen/core/TensorMethods.h\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n    Tensor  Type Type  sparse_resize_and_clear_ Type  Type  .cpp Type  int,float,double  BackendCPU,CUDA,SparseCPU, SparseCUDA  SparseCPUByteType.h  SparseCPUByteType.cpp\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n    at::native::sparse_resize_and_clear_  torch/lib/include/ATen/NativeFunctions.h aten/src/ATen/native/sparse/SparseTensor.cpp\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n    Tensor  SparseTensorImpl  SparseTensorImpl  resize_and_clear_\n2.  Tensor \n   \n   Tensor  resize_  TensorMethods.h\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n    Tensor  resize_ CPUByteType.cpp \n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n    Tensor  size  resize  aten/src/ATen/native/Resize.cpp  resize_cpu_ \n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         //  Tensor \n       //  size  Tensor  resize size  Tensor size \n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   resize_impl_cpu_  cpu  resize  aten/src/ATen/native/Resize.h \n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           //  size  size \n           // size size \n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // stride=1\n           self->set_sizes_contiguous(size);    //  size  size\n           storage_size = self->numel();        //  size  size  (n1,n2,n3) n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize \n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset()  0\n           // \n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n    aten/src/TH/THStorageFunctions.cpp  THStorage_resize \n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           //  Tensor \n           //  Tensor \n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   //  size\n           storage->set_numel(size);                // \n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // \n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n    resize  N1resize  N2\n   1. N1 >= N2 size N1  N2 \n   2. N1 < N2 N1  N1  Tensor  allocator \n\n torch.empty \n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize  size\nprint(x)\ntorch.empty(1,2,out=x)  # resize  size\nprint(x)\ntorch.empty(4,4,out=x)  #  resize  size\n```\n\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n###  Tensor\n torch/csrc/autograd/generated/python_torch_functions_dispatch.h  tensor  dispatch_empty  torch::empty torch/csrc/autograd/generated/variable_factories.h jit  Tensor \n```c++\ninline at::Tensor empty(at::IntList size, const at::TensorOptions & options={}) {\n    ...     // jit tracing\n    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(false));\n    auto result = autograd::make_variable(tensor, options.requires_grad()); //  Tensor  Variable\n    ...     // jit tracing\n    return result\n}\n```\n at::empty  Functions.h\n```c++\nstatic inline Tensor empty(IntList size, const TensorOptions & options) {\n    return at::getType(options).empty(size, options);\n}\n```\n Tensor  empty  at::getType(options)  options  TypeExtendedInterface  instance options.backend(), options.dtype()  options.is_variable()  CPU  backend  aten/src/ATen/Context.cpp  Context  register_cpu_types(this)  register_cpu_type(Context* context)  build/aten/src/ATen/RegisterCPU.cpp  aten/src/ATen/gen.py  generate_outputs  gen.py  register_cpu_types \n```\nCPUByteType\nCPUCharType\nCPUDoubleType\nCPUFloatType\nCPUIntType\nCPULongType\nCPUShortType\n...\n```\n CPUByteType empty \n```\nTensor CPUByteType::empty(IntList size, const TensorOptions & options) const {\n    const DeviceGuard device_guard(options.device());   //  device  Tensor\n    return at::native::empty_cpu(size, options);\n}\n```\n at::native::empty_cpu  aten/src/ATen/native/TensorFactories.cpp \n```c++\nauto* allocator = at::getCPUAllocator();\nint64_t nelements = prod_intlist(size); // \nauto dtype = options.dtype();\nauto storage_impl = c10::make_intrusive<StorageImpl>(\n    dtype,\n    nelements,\n    allocator->allocate(nelements*dtype.itemsize()),\n    allocator,\n    /*resizeable=*/true\n);\nauto tensor = detail::make_tensor<TensorImpl>(storage_impl, at::CPUTensorId(), false);\n```\n c10::make_intrusive<StorageImpl>  new StorageImpl(...) wrap  intrusive_ptr [PyTorch-2](2019/06/13/PyTorch-2)  Tensor  StorageImpl StorageImpl  detail::make_tensor  Tensor at::getCPUAllocator  THDefaultAllocator  allocate  THAlloc THAlloc  THAllocInternal  malloc posix_memalign  \n\n\n```python\nimport torch\ntorch.empty(2,3)\n```\n\n```\ntensor([[1.6504e-12,3.0637e-41,1.6588e-12],\n        [3.0637e-41,4.4842e-44,0.0000e+00]])\n```\n\n### Tensor \n torch.empty  torch.Tensor  `torch/__init__.py`  `import autograd` `torch/autograd/__init__.py`\n```python\nif not torch._C._autograd_init():\n```\n_autograd_init  python  torch/csrc/Module.cpp  THPAutograd_initExtension  c++  torch/csrc/autograd/autograd.h  torch/csrc/autograd/init.cpp \n```c++\n//  torch/tensor.py \nauto tensor_module = THPObjectPtr(PyImport_ImportModule(\"torch.tensor\"));\n//  torch/tensor.py  Tensor \nTHPVariableClass = PyObject_GetAttrString(tensor_module, \"Tensor\");\n```\n `THPVariableClass`  torch/csrc/autograd/python_variable.h \n```c++\nTHP_API PyObject *THPVariableClass;\n```\n extern  torch/csrc/autograd/python_variable.cpp  torch.empty  c++  THPVariable_empty  dispatch_empty  Variable  wrap  PyObject wrap  torch/csrc/autograd/utils/wrap_outputs.h  THPVariable_Wrap torch/csrc/autograd/python_variable.cpp THPVariableClass  THPVariableClass  torch/tensor.py  Tensor  THPVariable_Wrap  THPVariable_NewWithVar  Variable  THPVariableClass  Tensor THPVariable_NewWithVar \n```c++\nstatic PyObject* THPVariable_NewWithVar(PyTypeObject* type, Variable var) {\nPyObject *obj=type->tp_alloc(type, 0);      //  torch.Tensor \nif(obj) {\n    auto v = (THPVariable*)obj; // cast  THPVariable  torch.Tensor  torch._C._TensorBase \n    new(&v->cdata) Variable(std::move(var));    //  VariableC++  Tensor\n    v->cdata.set_pyobj(obj);\n    ...\n}\nreturn obj;\n}\n```\n\n\n```python\n>>> type(torch.empty(2,3))\n<class 'torch.Tensor'>\n```\n\n# PS\n","slug":"PyTorch-3","published":1,"updated":"2019-08-26T07:43:50.509Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379f60023dgvce6txh0mv","content":"<p> <a href=\"PyTorch-2\">PyTorch-2</a>  torch  package  PyTorch <a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"torch-\"><a href=\"#torch-\" class=\"headerlink\" title=\"torch \"></a>torch </h1><p> <code>torch/__init__.py</code>  torch </p>\n<ol>\n<li><p>/ typename, is_tensor, is_storage, _storage_classes </p>\n</li>\n<li><p> torch /</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> torch._C //</p>\n</li>\n<li><p> torch._C._VariableFunctions /</p>\n</li>\n</ol>\n<p>PyTorch  torch </p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p> torch._C._VariableFunctions  torch/csrc/Module.cpp  THPVariable_initModule torch/csrc/autograd/python_variable.cpp  torch::autograd::initTorchFunctions torch/csrc/autograd/generated/python_torch_functions.cpp PyTorch </p>\n<ol>\n<li><p>caffe2/CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;/csrc/autograd/generated/python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools/setup_helpers/generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> tools/setup_helpers/generate_code.py generate_code </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p> torch/csrc/autograd/generated/python_torch_functions.cpp  tools/autograd/templates/python_torch_functions.cpp  ${py_methods}  ${py_method_defs}  torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml</p>\n<ol>\n<li><p> caffe2/CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(../cmake/Codegen.cmake)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> cmake/Codegen.cmake  <code>gen.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen/gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n\n<p> aten/src/ATen/native/native_functions.yaml  <code>empty</code> </p>\n</li>\n<li><p>aten/src/ATen/gen.py  generate_outputs  Declarations.yaml </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> 2 install_dir  build/aten/src/ATen Declarations.yaml  build/aten/src/ATen</p>\n<ul>\n<li>CMakeLists.txt  add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt  add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt  add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen/Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;/ATen)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p> Declarations.yaml aten/src/ATen/CMakeLists.txt  build/aten/src/ATen/Functions.h aten/src/ATen/CMakeLists.txt  INSTALL </p>\n</li>\n</ol>\n<p> tools/autograd/gen_python_functions.py  create_python_bindings  ${py_methods}  ${py_method_defs} </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS = CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, /*traceable=*/$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module=False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env = &#123;</span><br><span class=\"line\">            &apos;name&apos;: name,</span><br><span class=\"line\">            &apos;dispatch_name&apos;: &apos;dispatch_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;pycname&apos;: &apos;THPVariable_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;signature&apos;: [],</span><br><span class=\"line\">            &apos;max_args&apos;: max(len(o[&apos;arguments&apos;])+len(o[&apos;python_binding_arguments&apos;]) for o in declarations),</span><br><span class=\"line\">            &apos;unpack_self&apos;: [],</span><br><span class=\"line\">            &apos;dispatch&apos;: [],</span><br><span class=\"line\">            &apos;declare_namedtuple_return_types&apos;: &apos;&apos;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... //  env  key-value pair or  env  key  value</span><br><span class=\"line\">        if len(declarations) == 1 and len(declarations[0][&apos;args&apos;]) == 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl = PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&apos;flags&apos;] = &apos;METH_VARARGS | METH_KEYWORDS&apos;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&apos;flags&apos;] += &apos; | METH_STATIC&apos;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n\n<p> PY_VARIABLE_METHOD_VARARGS Declarations.yaml, deprecated.yaml, derivatives.yaml env  PY_VARIABLE_METHOD_VARARGS  env  key </p>\n<h2 id=\"empty-\"><a href=\"#empty-\" class=\"headerlink\" title=\"empty \"></a>empty </h2><p> empty  torch/csrc/autograd/generated/python_torch_function.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)&quot;,</span><br><span class=\"line\">    &#125;, /*tracebalbe*/true); // vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx == 0) &#123;       // vector</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  // parameter &apos;out&apos; is None</span><br><span class=\"line\">            auto size = r.intlist(0);</span><br><span class=\"line\">            auto dtype = r.scalartype(2);</span><br><span class=\"line\">            auto device = r.device(4);</span><br><span class=\"line\">            const auto options = TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> empty  Tensor Tensor Tensor</p>\n<ol>\n<li><code>out</code> None dtype, device, layout  requires_grad  Tensor</li>\n<li><code>out</code> None,  <code>out</code>  Tensor  dtype, layout, device  <code>out</code>  requires_grad  requires_grad</li>\n</ol>\n<p> dispatch_empty torch/csrc/autograd/generated/python_torch_functions_dispatch.h python_torch_function.cpp  tools/autograd/templates/python_torch_functions_dispatch.h  tools/autograd/gen_python_functions.py  gen_py_torch_functionsdispatch_empty </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// empty  Tensor &apos;out&apos;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">// empty  Tensor &apos;out&apos; options </span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"-Tensor\"><a href=\"#-Tensor\" class=\"headerlink\" title=\" Tensor\"></a> Tensor</h3><p> Tensor AutoNoGIL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n\n<p> GIL at::empty_out  GIL at::empty_out  GIL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  torch/lib/include/Aten/Functions.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  Functions.h  aten/src/ATen/gen.py  generate_outputs  Declarations.yaml </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&apos;Functions.h&apos;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  detail::infer_type(result)  Tensor  result  TypeExtendedInference  empty_out TypeExtendedInferfaceTypeDefault torch/lib/include/ATen/TypeExtendedInferface.h torch/lib/include/ATen/TypeDefault.hTypeDefault build/aten/src/ATen/TypeDefault.cpp empty_out </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(/* native_actuals */ result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Declarations.yaml  at::native::empty_out  torch/lib/include/ATen/NativeFunctions.h Declarations.yaml  aten/src/ATen/native/TensorFactories.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor </p>\n<ol>\n<li><p> Tensor </p>\n<p> Tensor  sparse_resize_and_clear_ torch/lib/include/ATen/core/Tensor.h Declarations.yaml  aten/src/ATen/gen.py aten/src/ATen/core/Tensor.h TensorMethods.h  Type.h sparse_resize_and_clear_  torch/lib/include/ATen/core/TensorMethods.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  Type Type  sparse_resize_and_clear_ Type  Type  .cpp Type  int,float,double  BackendCPU,CUDA,SparseCPU, SparseCUDA  SparseCPUByteType.h  SparseCPUByteType.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::native::sparse_resize_and_clear_  torch/lib/include/ATen/NativeFunctions.h aten/src/ATen/native/sparse/SparseTensor.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  SparseTensorImpl  SparseTensorImpl  resize_and_clear_</p>\n</li>\n<li><p> Tensor </p>\n<p>Tensor  resize_  TensorMethods.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  resize_ CPUByteType.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; CPUByteType::resize_(Tensor &amp; self, IntList size) <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  size  resize  aten/src/ATen/native/Resize.cpp  resize_cpu_ </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">    <span class=\"comment\">//  size  Tensor  resize size  Tensor size </span></span><br><span class=\"line\">    resize_impl_cpu_(self_, size, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(size.size()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>resize_impl_cpu_  cpu  resize  aten/src/ATen/native/Resize.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == size &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//  size  size </span></span><br><span class=\"line\">        <span class=\"comment\">// size size </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(size);    <span class=\"comment\">//  size  size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">//  size  size  (n1,n2,n3) n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize </span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset()  0</span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> aten/src/TH/THStorageFunctions.cpp  THStorage_resize </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*size);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">        <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::move(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">//  size</span></span><br><span class=\"line\">        storage-&gt;set_numel(size);                <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// </span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.get(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> resize  N1resize  N2</p>\n<ol>\n<li>N1 &gt;= N2 size N1  N2 </li>\n<li>N1 &lt; N2 N1  N1  Tensor  allocator </li>\n</ol>\n</li>\n</ol>\n<p> torch.empty </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize  size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize  size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\">#  resize  size</span></span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"-Tensor\"><a href=\"#-Tensor\" class=\"headerlink\" title=\" Tensor\"></a> Tensor</h3><p> torch/csrc/autograd/generated/python_torch_functions_dispatch.h  tensor  dispatch_empty  torch::empty torch/csrc/autograd/generated/variable_factories.h jit  Tensor </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">inline</span> at::<span class=\"function\">Tensor <span class=\"title\">empty</span><span class=\"params\">(at::IntList size, <span class=\"keyword\">const</span> at::TensorOptions &amp; options=&#123;&#125;)</span> </span>&#123;</span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad()); <span class=\"comment\">//  Tensor  Variable</span></span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty  Functions.h</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> Tensor <span class=\"title\">empty</span><span class=\"params\">(IntList size, <span class=\"keyword\">const</span> TensorOptions &amp; options)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::getType(options).empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  empty  at::getType(options)  options  TypeExtendedInterface  instance options.backend(), options.dtype()  options.is_variable()  CPU  backend  aten/src/ATen/Context.cpp  Context  register_cpu_types(this)  register_cpu_type(Context* context)  build/aten/src/ATen/RegisterCPU.cpp  aten/src/ATen/gen.py  generate_outputs  gen.py  register_cpu_types </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPUByteType</span><br><span class=\"line\">CPUCharType</span><br><span class=\"line\">CPUDoubleType</span><br><span class=\"line\">CPUFloatType</span><br><span class=\"line\">CPUIntType</span><br><span class=\"line\">CPULongType</span><br><span class=\"line\">CPUShortType</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p> CPUByteType empty </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor CPUByteType::empty(IntList size, const TensorOptions &amp; options) const &#123;</span><br><span class=\"line\">    const DeviceGuard device_guard(options.device());   //  device  Tensor</span><br><span class=\"line\">    return at::native::empty_cpu(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::native::empty_cpu  aten/src/ATen/native/TensorFactories.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span>* allocator = at::getCPUAllocator();</span><br><span class=\"line\"><span class=\"keyword\">int64_t</span> nelements = prod_intlist(size); <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = options.dtype();</span><br><span class=\"line\"><span class=\"keyword\">auto</span> storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class=\"line\">    dtype,</span><br><span class=\"line\">    nelements,</span><br><span class=\"line\">    allocator-&gt;allocate(nelements*dtype.itemsize()),</span><br><span class=\"line\">    allocator,</span><br><span class=\"line\">    <span class=\"comment\">/*resizeable=*/</span><span class=\"literal\">true</span></span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor = detail::make_tensor&lt;TensorImpl&gt;(storage_impl, at::CPUTensorId(), <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n\n<p> c10::make_intrusive<storageimpl>  new StorageImpl() wrap  intrusive_ptr <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a>  Tensor  StorageImpl StorageImpl  detail::make_tensor  Tensor at::getCPUAllocator  THDefaultAllocator  allocate  THAlloc THAlloc  THAllocInternal  malloc posix_memalign  </storageimpl></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1.6504e-12,3.0637e-41,1.6588e-12],</span><br><span class=\"line\">        [3.0637e-41,4.4842e-44,0.0000e+00]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tensor-\"><a href=\"#Tensor-\" class=\"headerlink\" title=\"Tensor \"></a>Tensor </h3><p> torch.empty  torch.Tensor  <code>torch/__init__.py</code>  <code>import autograd</code> <code>torch/autograd/__init__.py</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch._C._autograd_init():</span><br></pre></td></tr></table></figure>\n\n<p>_autograd_init  python  torch/csrc/Module.cpp  THPAutograd_initExtension  c++  torch/csrc/autograd/autograd.h  torch/csrc/autograd/init.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  torch/tensor.py </span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor_module = THPObjectPtr(PyImport_ImportModule(<span class=\"string\">\"torch.tensor\"</span>));</span><br><span class=\"line\"><span class=\"comment\">//  torch/tensor.py  Tensor </span></span><br><span class=\"line\">THPVariableClass = PyObject_GetAttrString(tensor_module, <span class=\"string\">\"Tensor\"</span>);</span><br></pre></td></tr></table></figure>\n\n<p> <code>THPVariableClass</code>  torch/csrc/autograd/python_variable.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THP_API PyObject *THPVariableClass;</span><br></pre></td></tr></table></figure>\n\n<p> extern  torch/csrc/autograd/python_variable.cpp  torch.empty  c++  THPVariable_empty  dispatch_empty  Variable  wrap  PyObject wrap  torch/csrc/autograd/utils/wrap_outputs.h  THPVariable_Wrap torch/csrc/autograd/python_variable.cpp THPVariableClass  THPVariableClass  torch/tensor.py  Tensor  THPVariable_Wrap  THPVariable_NewWithVar  Variable  THPVariableClass  Tensor THPVariable_NewWithVar </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject* <span class=\"title\">THPVariable_NewWithVar</span><span class=\"params\">(PyTypeObject* type, Variable var)</span> </span>&#123;</span><br><span class=\"line\">PyObject *obj=type-&gt;tp_alloc(type, <span class=\"number\">0</span>);      <span class=\"comment\">//  torch.Tensor </span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(obj) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> v = (THPVariable*)obj; <span class=\"comment\">// cast  THPVariable  torch.Tensor  torch._C._TensorBase </span></span><br><span class=\"line\">    <span class=\"keyword\">new</span>(&amp;v-&gt;cdata) Variable(<span class=\"built_in\">std</span>::move(var));    <span class=\"comment\">//  VariableC++  Tensor</span></span><br><span class=\"line\">    v-&gt;cdata.set_pyobj(obj);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">return</span> obj;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">&lt;<span class=\"class\"><span class=\"keyword\">class</span> '<span class=\"title\">torch</span>.<span class=\"title\">Tensor</span>'&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> <a href=\"PyTorch-2\">PyTorch-2</a>  torch  package  PyTorch <a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"torch-\"><a href=\"#torch-\" class=\"headerlink\" title=\"torch \"></a>torch </h1><p> <code>torch/__init__.py</code>  torch </p>\n<ol>\n<li><p>/ typename, is_tensor, is_storage, _storage_classes </p>\n</li>\n<li><p> torch /</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> torch._C //</p>\n</li>\n<li><p> torch._C._VariableFunctions /</p>\n</li>\n</ol>\n<p>PyTorch  torch </p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p> torch._C._VariableFunctions  torch/csrc/Module.cpp  THPVariable_initModule torch/csrc/autograd/python_variable.cpp  torch::autograd::initTorchFunctions torch/csrc/autograd/generated/python_torch_functions.cpp PyTorch </p>\n<ol>\n<li><p>caffe2/CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;/csrc/autograd/generated/python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools/setup_helpers/generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> tools/setup_helpers/generate_code.py generate_code </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p> torch/csrc/autograd/generated/python_torch_functions.cpp  tools/autograd/templates/python_torch_functions.cpp  ${py_methods}  ${py_method_defs}  torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml</p>\n<ol>\n<li><p> caffe2/CMakeLists.txt </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(../cmake/Codegen.cmake)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> cmake/Codegen.cmake  <code>gen.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen/gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n\n<p> aten/src/ATen/native/native_functions.yaml  <code>empty</code> </p>\n</li>\n<li><p>aten/src/ATen/gen.py  generate_outputs  Declarations.yaml </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p> 2 install_dir  build/aten/src/ATen Declarations.yaml  build/aten/src/ATen</p>\n<ul>\n<li>CMakeLists.txt  add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt  add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt  add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen/Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;/ATen)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p> Declarations.yaml aten/src/ATen/CMakeLists.txt  build/aten/src/ATen/Functions.h aten/src/ATen/CMakeLists.txt  INSTALL </p>\n</li>\n</ol>\n<p> tools/autograd/gen_python_functions.py  create_python_bindings  ${py_methods}  ${py_method_defs} </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS = CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, /*traceable=*/$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module=False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env = &#123;</span><br><span class=\"line\">            &apos;name&apos;: name,</span><br><span class=\"line\">            &apos;dispatch_name&apos;: &apos;dispatch_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;pycname&apos;: &apos;THPVariable_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;signature&apos;: [],</span><br><span class=\"line\">            &apos;max_args&apos;: max(len(o[&apos;arguments&apos;])+len(o[&apos;python_binding_arguments&apos;]) for o in declarations),</span><br><span class=\"line\">            &apos;unpack_self&apos;: [],</span><br><span class=\"line\">            &apos;dispatch&apos;: [],</span><br><span class=\"line\">            &apos;declare_namedtuple_return_types&apos;: &apos;&apos;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... //  env  key-value pair or  env  key  value</span><br><span class=\"line\">        if len(declarations) == 1 and len(declarations[0][&apos;args&apos;]) == 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl = PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&apos;flags&apos;] = &apos;METH_VARARGS | METH_KEYWORDS&apos;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&apos;flags&apos;] += &apos; | METH_STATIC&apos;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n\n<p> PY_VARIABLE_METHOD_VARARGS Declarations.yaml, deprecated.yaml, derivatives.yaml env  PY_VARIABLE_METHOD_VARARGS  env  key </p>\n<h2 id=\"empty-\"><a href=\"#empty-\" class=\"headerlink\" title=\"empty \"></a>empty </h2><p> empty  torch/csrc/autograd/generated/python_torch_function.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)&quot;,</span><br><span class=\"line\">    &#125;, /*tracebalbe*/true); // vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx == 0) &#123;       // vector</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  // parameter &apos;out&apos; is None</span><br><span class=\"line\">            auto size = r.intlist(0);</span><br><span class=\"line\">            auto dtype = r.scalartype(2);</span><br><span class=\"line\">            auto device = r.device(4);</span><br><span class=\"line\">            const auto options = TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> empty  Tensor Tensor Tensor</p>\n<ol>\n<li><code>out</code> None dtype, device, layout  requires_grad  Tensor</li>\n<li><code>out</code> None,  <code>out</code>  Tensor  dtype, layout, device  <code>out</code>  requires_grad  requires_grad</li>\n</ol>\n<p> dispatch_empty torch/csrc/autograd/generated/python_torch_functions_dispatch.h python_torch_function.cpp  tools/autograd/templates/python_torch_functions_dispatch.h  tools/autograd/gen_python_functions.py  gen_py_torch_functionsdispatch_empty </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// empty  Tensor &apos;out&apos;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">// empty  Tensor &apos;out&apos; options </span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"-Tensor\"><a href=\"#-Tensor\" class=\"headerlink\" title=\" Tensor\"></a> Tensor</h3><p> Tensor AutoNoGIL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n\n<p> GIL at::empty_out  GIL at::empty_out  GIL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  torch/lib/include/Aten/Functions.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  Functions.h  aten/src/ATen/gen.py  generate_outputs  Declarations.yaml </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&apos;Functions.h&apos;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n\n<p> at::empty_out  detail::infer_type(result)  Tensor  result  TypeExtendedInference  empty_out TypeExtendedInferfaceTypeDefault torch/lib/include/ATen/TypeExtendedInferface.h torch/lib/include/ATen/TypeDefault.hTypeDefault build/aten/src/ATen/TypeDefault.cpp empty_out </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(/* native_actuals */ result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Declarations.yaml  at::native::empty_out  torch/lib/include/ATen/NativeFunctions.h Declarations.yaml  aten/src/ATen/native/TensorFactories.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor </p>\n<ol>\n<li><p> Tensor </p>\n<p> Tensor  sparse_resize_and_clear_ torch/lib/include/ATen/core/Tensor.h Declarations.yaml  aten/src/ATen/gen.py aten/src/ATen/core/Tensor.h TensorMethods.h  Type.h sparse_resize_and_clear_  torch/lib/include/ATen/core/TensorMethods.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  Type Type  sparse_resize_and_clear_ Type  Type  .cpp Type  int,float,double  BackendCPU,CUDA,SparseCPU, SparseCUDA  SparseCPUByteType.h  SparseCPUByteType.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::native::sparse_resize_and_clear_  torch/lib/include/ATen/NativeFunctions.h aten/src/ATen/native/sparse/SparseTensor.cpp</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  SparseTensorImpl  SparseTensorImpl  resize_and_clear_</p>\n</li>\n<li><p> Tensor </p>\n<p>Tensor  resize_  TensorMethods.h</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  resize_ CPUByteType.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; CPUByteType::resize_(Tensor &amp; self, IntList size) <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  size  resize  aten/src/ATen/native/Resize.cpp  resize_cpu_ </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">    <span class=\"comment\">//  size  Tensor  resize size  Tensor size </span></span><br><span class=\"line\">    resize_impl_cpu_(self_, size, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(size.size()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>resize_impl_cpu_  cpu  resize  aten/src/ATen/native/Resize.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == size &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//  size  size </span></span><br><span class=\"line\">        <span class=\"comment\">// size size </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(size);    <span class=\"comment\">//  size  size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">//  size  size  (n1,n2,n3) n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize </span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset()  0</span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> aten/src/TH/THStorageFunctions.cpp  THStorage_resize </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*size);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">        <span class=\"comment\">//  Tensor </span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::move(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">//  size</span></span><br><span class=\"line\">        storage-&gt;set_numel(size);                <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// </span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.get(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> resize  N1resize  N2</p>\n<ol>\n<li>N1 &gt;= N2 size N1  N2 </li>\n<li>N1 &lt; N2 N1  N1  Tensor  allocator </li>\n</ol>\n</li>\n</ol>\n<p> torch.empty </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize  size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize  size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\">#  resize  size</span></span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"-Tensor\"><a href=\"#-Tensor\" class=\"headerlink\" title=\" Tensor\"></a> Tensor</h3><p> torch/csrc/autograd/generated/python_torch_functions_dispatch.h  tensor  dispatch_empty  torch::empty torch/csrc/autograd/generated/variable_factories.h jit  Tensor </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">inline</span> at::<span class=\"function\">Tensor <span class=\"title\">empty</span><span class=\"params\">(at::IntList size, <span class=\"keyword\">const</span> at::TensorOptions &amp; options=&#123;&#125;)</span> </span>&#123;</span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    at::Tensor tensor = at::empty(size, at::TensorOptions(options).is_variable(<span class=\"literal\">false</span>));</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> result = autograd::make_variable(tensor, options.requires_grad()); <span class=\"comment\">//  Tensor  Variable</span></span><br><span class=\"line\">    ...     <span class=\"comment\">// jit tracing</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::empty  Functions.h</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> Tensor <span class=\"title\">empty</span><span class=\"params\">(IntList size, <span class=\"keyword\">const</span> TensorOptions &amp; options)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::getType(options).empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> Tensor  empty  at::getType(options)  options  TypeExtendedInterface  instance options.backend(), options.dtype()  options.is_variable()  CPU  backend  aten/src/ATen/Context.cpp  Context  register_cpu_types(this)  register_cpu_type(Context* context)  build/aten/src/ATen/RegisterCPU.cpp  aten/src/ATen/gen.py  generate_outputs  gen.py  register_cpu_types </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CPUByteType</span><br><span class=\"line\">CPUCharType</span><br><span class=\"line\">CPUDoubleType</span><br><span class=\"line\">CPUFloatType</span><br><span class=\"line\">CPUIntType</span><br><span class=\"line\">CPULongType</span><br><span class=\"line\">CPUShortType</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p> CPUByteType empty </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor CPUByteType::empty(IntList size, const TensorOptions &amp; options) const &#123;</span><br><span class=\"line\">    const DeviceGuard device_guard(options.device());   //  device  Tensor</span><br><span class=\"line\">    return at::native::empty_cpu(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> at::native::empty_cpu  aten/src/ATen/native/TensorFactories.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span>* allocator = at::getCPUAllocator();</span><br><span class=\"line\"><span class=\"keyword\">int64_t</span> nelements = prod_intlist(size); <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> dtype = options.dtype();</span><br><span class=\"line\"><span class=\"keyword\">auto</span> storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class=\"line\">    dtype,</span><br><span class=\"line\">    nelements,</span><br><span class=\"line\">    allocator-&gt;allocate(nelements*dtype.itemsize()),</span><br><span class=\"line\">    allocator,</span><br><span class=\"line\">    <span class=\"comment\">/*resizeable=*/</span><span class=\"literal\">true</span></span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor = detail::make_tensor&lt;TensorImpl&gt;(storage_impl, at::CPUTensorId(), <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n\n<p> c10::make_intrusive<storageimpl>  new StorageImpl() wrap  intrusive_ptr <a href=\"2019/06/13/PyTorch-2\">PyTorch-2</a>  Tensor  StorageImpl StorageImpl  detail::make_tensor  Tensor at::getCPUAllocator  THDefaultAllocator  allocate  THAlloc THAlloc  THAllocInternal  malloc posix_memalign  </storageimpl></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1.6504e-12,3.0637e-41,1.6588e-12],</span><br><span class=\"line\">        [3.0637e-41,4.4842e-44,0.0000e+00]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tensor-\"><a href=\"#Tensor-\" class=\"headerlink\" title=\"Tensor \"></a>Tensor </h3><p> torch.empty  torch.Tensor  <code>torch/__init__.py</code>  <code>import autograd</code> <code>torch/autograd/__init__.py</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch._C._autograd_init():</span><br></pre></td></tr></table></figure>\n\n<p>_autograd_init  python  torch/csrc/Module.cpp  THPAutograd_initExtension  c++  torch/csrc/autograd/autograd.h  torch/csrc/autograd/init.cpp </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  torch/tensor.py </span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> tensor_module = THPObjectPtr(PyImport_ImportModule(<span class=\"string\">\"torch.tensor\"</span>));</span><br><span class=\"line\"><span class=\"comment\">//  torch/tensor.py  Tensor </span></span><br><span class=\"line\">THPVariableClass = PyObject_GetAttrString(tensor_module, <span class=\"string\">\"Tensor\"</span>);</span><br></pre></td></tr></table></figure>\n\n<p> <code>THPVariableClass</code>  torch/csrc/autograd/python_variable.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THP_API PyObject *THPVariableClass;</span><br></pre></td></tr></table></figure>\n\n<p> extern  torch/csrc/autograd/python_variable.cpp  torch.empty  c++  THPVariable_empty  dispatch_empty  Variable  wrap  PyObject wrap  torch/csrc/autograd/utils/wrap_outputs.h  THPVariable_Wrap torch/csrc/autograd/python_variable.cpp THPVariableClass  THPVariableClass  torch/tensor.py  Tensor  THPVariable_Wrap  THPVariable_NewWithVar  Variable  THPVariableClass  Tensor THPVariable_NewWithVar </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> PyObject* <span class=\"title\">THPVariable_NewWithVar</span><span class=\"params\">(PyTypeObject* type, Variable var)</span> </span>&#123;</span><br><span class=\"line\">PyObject *obj=type-&gt;tp_alloc(type, <span class=\"number\">0</span>);      <span class=\"comment\">//  torch.Tensor </span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(obj) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span> v = (THPVariable*)obj; <span class=\"comment\">// cast  THPVariable  torch.Tensor  torch._C._TensorBase </span></span><br><span class=\"line\">    <span class=\"keyword\">new</span>(&amp;v-&gt;cdata) Variable(<span class=\"built_in\">std</span>::move(var));    <span class=\"comment\">//  VariableC++  Tensor</span></span><br><span class=\"line\">    v-&gt;cdata.set_pyobj(obj);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">return</span> obj;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(torch.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">&lt;<span class=\"class\"><span class=\"keyword\">class</span> '<span class=\"title\">torch</span>.<span class=\"title\">Tensor</span>'&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p></p>\n"},{"title":"gcc-src","date":"2019-07-02T05:49:38.000Z","_content":" c++  c++ \n\n ubuntu gcc  7.3.0 /usr/include/c++/7/ \n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone  github \n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html)  gcc \n\nC++ \n\n\n\n# Allocator\n __allocator_traits_base  libstdc++-v3/include/bits/alloc_traits.h \n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    //  _Tp \nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n __rebind __rebind\n1.  void __rebind  \n2.  void\n    -  _Tp::template rebind<_Up>::other  __rebind \n    -  __rebind \n\n\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n _Alloc::template rebind<_Up>::other  __rebind  _Alloc __alloc_rebind  _Alloc<_Up, ...>::type\n\n allocator_traits \n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n pointer  _Alloc::pointer  value_type*_Alloc::pointer  value_type* __pointer   \n std/type_traits  __detected_or_t  __rebind  allocator_traits \n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n _Ptr type  _Func<_Alloc> _Func  type  `pointer_traits<pointer>::template rebind<_Tp>` pointer  value_type*  bits/ptr_traits.h  pointer_traits \n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n _Ptr::type  _Tp* allocator_traits  _Diff type _Size  type  _Ptr::type, _Diff::type  _Size::type \n```c++\n//  _Alloc::const_pointer  _Alloc::const_pointer const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n//  _Alloc::void_pointer  void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n//  _Alloc::difference_type  pointer_traits<pointer>::difference_type\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n//  _Alloc::size_type  difference_type \nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n\n\n __alloc_traits ext/alloc_traits.h \n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     //  __cplusplus >= 201103L\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits \n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // \n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n std::allocator_traits  allocate \n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD \nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n_Alloc  allocate  __a  __n  allocate  __hint  __hint  allocate  allocate  allocatedeallocate, construct, destroy, max_size  _Tp  _Tp \n\n std::allocator_traits::construct \n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n _Tp* _Tp  __gnu_cxx::__alloc_traits  construct \n```c++\n// \ntemplate<typename _Ptr>\nusing __is_custom_pointer\n//  _Ptr  pointer  _Ptr  => __is_custom_pointer \n//  _Ptr  pointer  __is_custom_pointer \n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n//  \ntemplate<typename _Ptr, typename... _Args>\n//  __is_custom_pointer<_Ptr> enable_if<xx>::type \nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\nconst / rebind _Alloc  _Alloc  value_type  allocator_traits  value_type  rebind  allocator_traits::value_type  Allocator\n\n/\n\nstd::allocator  bits/allocator.h  __allocator_base __gnu_cxx::new_allocator  new_allocator  allocate, deallocate, max_size, construct, destroy \n\n# Iterator\n bit/stl_iterator_base_types.h \n```c++\n//  Iterator \n// \ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n iterator_tag // std::iterator  iterator_traits \n\n bits/stl_iterator.h \n\n1. input  input  input   \n   \n2. output  input   \n   \n3. forward  input  output  input/output forward  multipass \n4. bidirectional  forword \n5. random-access  bidirectional \n\n\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // \n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // \npublic:\n    // \n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// \n                        // \n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // \n        return *this;\n    }\n}\n```\n r i r  i  i \n1. : *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\nr  i \n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // \n{\nprotected:\n    _Container* container;  // \npublic:\n    back_insert_iterator&   // \n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       //  output \n    back_insert_iterator&\n    operator++() {return *this; }       // \n    back_insert_iterator&\n}\n```\n front_insert_iterator, insert_iterator \n\n## __normal_iterator\n _Iterator, _Container_Container  __normal_iterator \n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator  _M_current \n    ...\npublic:\n    // \n    ...\n    //  _M_current \n    //  __normal_iterator  _M_current \n    //  normal \n}\n```\n\n## move_iterator\n move move_iterator  move move  copy\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator \n    typedef typename __traits_type::reference   __base_ref;     //  _Iterator \npublic:\n    ...\n    // __base_ref __base_ref \n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // \n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // \n}\n```\n\n\n# Container\n## Vector\n vector  bits/stl_vector.h  _Vector_base\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n _Alloc  value_type  _Tp rebind  value_type  _Tp  alloctor alloctor<_Tp> _Tp_alloc_type pointer _Tp_alloc_type::pointer _Tp* std::allocator  _Tp_alloc_type::pointer  _Tp* _Vector_base::pointer  _Tp*\n\n _Vector_base \n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   //  vector \n    pointer _M_finish;  //  vector  past-the-last-element \n    pointer _M_end_of_storage;  // vector  past-the-max-element \n\n    // \n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // \n    // vector  overflow  _GLIBCXX_SANITIZE_VECTOR AddressSanitizer\n}\n```\n _Vector_base _Vector_base \n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // \n    ...\n    // / \n\n    pointer _M_allocator(size_t __n) {      //  n  pointer \n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        //  __n=0 nullptr _M_impl \n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // \n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n _Vector_base  vector  vector \n```c++\n// vector  vector  _Tp vector  _Alloc\n//   _Alloc  std::allocator<_Tp> _Tp \n//   _Alloc  _Tp  _Alloc::rebind  alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n vector  iterator pointer  __gnu::cxx::__normal_iterator  _Iterator  \n vector  vector  _M_impl._M_finish  past-the-last-element  vector \n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n\n\n vector resize  vector  new_size > old_size resize  new_size<=old_size _M_impl._M_finish [_M_start, _M_finish) _M_finish \n\n vector  push_back \n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        //  __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  //  _M_finish  __x\n                                        //   _M_finish \n}\n```\n bits/vector.tcc  _M_realloc_insert \n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    //  2  _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; //  past-the-last \n    const size_type __elems_before = __position - begin();// \n    pointer __new_start(this->_M_allocate(__len));   //  __len \n    pointer __new_finish(__new_start);  //  past-the-last \n    __try\n    {\n        // \n        _Alloc_traits::construct(this->_M_impl,                     // \n                                 __new_start + __elems_before,      // \n                                 std::forward<_Args>(__args)...);   // \n        //  vector  __new_finish  nullptr\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // \n            // \n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            //  __new_finish  1\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   //  __new_finish  past-of-last \n        }\n        ...\n        // \n        // \n        // \n    }\n}\n```\nvector \n\n","source":"_posts/gcc-src.md","raw":"---\ntitle: gcc-src\ndate: 2019-07-02 13:49:38\ntags: c++\n---\n c++  c++ \n\n ubuntu gcc  7.3.0 /usr/include/c++/7/ \n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone  github \n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html)  gcc \n\nC++ \n\n\n\n# Allocator\n __allocator_traits_base  libstdc++-v3/include/bits/alloc_traits.h \n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    //  _Tp \nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n __rebind __rebind\n1.  void __rebind  \n2.  void\n    -  _Tp::template rebind<_Up>::other  __rebind \n    -  __rebind \n\n\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n _Alloc::template rebind<_Up>::other  __rebind  _Alloc __alloc_rebind  _Alloc<_Up, ...>::type\n\n allocator_traits \n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n pointer  _Alloc::pointer  value_type*_Alloc::pointer  value_type* __pointer   \n std/type_traits  __detected_or_t  __rebind  allocator_traits \n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n _Ptr type  _Func<_Alloc> _Func  type  `pointer_traits<pointer>::template rebind<_Tp>` pointer  value_type*  bits/ptr_traits.h  pointer_traits \n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n _Ptr::type  _Tp* allocator_traits  _Diff type _Size  type  _Ptr::type, _Diff::type  _Size::type \n```c++\n//  _Alloc::const_pointer  _Alloc::const_pointer const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n//  _Alloc::void_pointer  void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n//  _Alloc::difference_type  pointer_traits<pointer>::difference_type\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n//  _Alloc::size_type  difference_type \nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n\n\n __alloc_traits ext/alloc_traits.h \n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     //  __cplusplus >= 201103L\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits \n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // \n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n std::allocator_traits  allocate \n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD \nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n_Alloc  allocate  __a  __n  allocate  __hint  __hint  allocate  allocate  allocatedeallocate, construct, destroy, max_size  _Tp  _Tp \n\n std::allocator_traits::construct \n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n _Tp* _Tp  __gnu_cxx::__alloc_traits  construct \n```c++\n// \ntemplate<typename _Ptr>\nusing __is_custom_pointer\n//  _Ptr  pointer  _Ptr  => __is_custom_pointer \n//  _Ptr  pointer  __is_custom_pointer \n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n//  \ntemplate<typename _Ptr, typename... _Args>\n//  __is_custom_pointer<_Ptr> enable_if<xx>::type \nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\nconst / rebind _Alloc  _Alloc  value_type  allocator_traits  value_type  rebind  allocator_traits::value_type  Allocator\n\n/\n\nstd::allocator  bits/allocator.h  __allocator_base __gnu_cxx::new_allocator  new_allocator  allocate, deallocate, max_size, construct, destroy \n\n# Iterator\n bit/stl_iterator_base_types.h \n```c++\n//  Iterator \n// \ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n iterator_tag // std::iterator  iterator_traits \n\n bits/stl_iterator.h \n\n1. input  input  input   \n   \n2. output  input   \n   \n3. forward  input  output  input/output forward  multipass \n4. bidirectional  forword \n5. random-access  bidirectional \n\n\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // \n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // \npublic:\n    // \n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// \n                        // \n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // \n        return *this;\n    }\n}\n```\n r i r  i  i \n1. : *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\nr  i \n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // \n{\nprotected:\n    _Container* container;  // \npublic:\n    back_insert_iterator&   // \n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       //  output \n    back_insert_iterator&\n    operator++() {return *this; }       // \n    back_insert_iterator&\n}\n```\n front_insert_iterator, insert_iterator \n\n## __normal_iterator\n _Iterator, _Container_Container  __normal_iterator \n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator  _M_current \n    ...\npublic:\n    // \n    ...\n    //  _M_current \n    //  __normal_iterator  _M_current \n    //  normal \n}\n```\n\n## move_iterator\n move move_iterator  move move  copy\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator \n    typedef typename __traits_type::reference   __base_ref;     //  _Iterator \npublic:\n    ...\n    // __base_ref __base_ref \n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // \n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // \n}\n```\n\n\n# Container\n## Vector\n vector  bits/stl_vector.h  _Vector_base\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n _Alloc  value_type  _Tp rebind  value_type  _Tp  alloctor alloctor<_Tp> _Tp_alloc_type pointer _Tp_alloc_type::pointer _Tp* std::allocator  _Tp_alloc_type::pointer  _Tp* _Vector_base::pointer  _Tp*\n\n _Vector_base \n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   //  vector \n    pointer _M_finish;  //  vector  past-the-last-element \n    pointer _M_end_of_storage;  // vector  past-the-max-element \n\n    // \n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // \n    // vector  overflow  _GLIBCXX_SANITIZE_VECTOR AddressSanitizer\n}\n```\n _Vector_base _Vector_base \n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // \n    ...\n    // / \n\n    pointer _M_allocator(size_t __n) {      //  n  pointer \n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        //  __n=0 nullptr _M_impl \n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // \n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n _Vector_base  vector  vector \n```c++\n// vector  vector  _Tp vector  _Alloc\n//   _Alloc  std::allocator<_Tp> _Tp \n//   _Alloc  _Tp  _Alloc::rebind  alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n vector  iterator pointer  __gnu::cxx::__normal_iterator  _Iterator  \n vector  vector  _M_impl._M_finish  past-the-last-element  vector \n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n\n\n vector resize  vector  new_size > old_size resize  new_size<=old_size _M_impl._M_finish [_M_start, _M_finish) _M_finish \n\n vector  push_back \n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        //  __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  //  _M_finish  __x\n                                        //   _M_finish \n}\n```\n bits/vector.tcc  _M_realloc_insert \n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    //  2  _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; //  past-the-last \n    const size_type __elems_before = __position - begin();// \n    pointer __new_start(this->_M_allocate(__len));   //  __len \n    pointer __new_finish(__new_start);  //  past-the-last \n    __try\n    {\n        // \n        _Alloc_traits::construct(this->_M_impl,                     // \n                                 __new_start + __elems_before,      // \n                                 std::forward<_Args>(__args)...);   // \n        //  vector  __new_finish  nullptr\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // \n            // \n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            //  __new_finish  1\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   //  __new_finish  past-of-last \n        }\n        ...\n        // \n        // \n        // \n    }\n}\n```\nvector \n\n","slug":"gcc-src","published":1,"updated":"2019-07-13T10:32:47.398Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzy379f70025dgvcm3gn228f","content":"<p> c++  c++ </p>\n<p> ubuntu gcc  7.3.0 /usr/include/c++/7/ </p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone  github </li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a>  gcc </li>\n</ol>\n<p>C++ </p>\n<p></p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p> __allocator_traits_base  libstdc++-v3/include/bits/alloc_traits.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//  _Tp </span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> __rebind __rebind</p>\n<ol>\n<li> void __rebind  </li>\n<li> void<ul>\n<li> _Tp::template rebind&lt;_Up&gt;::other  __rebind </li>\n<li> __rebind </li>\n</ul>\n</li>\n</ol>\n<p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p> _Alloc::template rebind&lt;_Up&gt;::other  <strong>rebind  _Alloc </strong>alloc_rebind  _Alloc&lt;_Up, &gt;::type</p>\n<p> allocator_traits </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> pointer  _Alloc::pointer  value_type<em>_Alloc::pointer  value_type</em> __pointer <br> std/type_traits  __detected_or_t  __rebind  allocator_traits </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> _Ptr type  _Func&lt;_Alloc&gt; _Func  type  <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code> pointer  value_type*  bits/ptr_traits.h  pointer_traits </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind = _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> _Ptr::type  _Tp* allocator_traits  _Diff type _Size  type  _Ptr::type, _Diff::type  _Size::type </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  _Alloc::const_pointer  _Alloc::const_pointer const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::void_pointer  void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::difference_type  pointer_traits&lt;pointer&gt;::difference_type</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::size_type  difference_type </span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p> __alloc_traits ext/alloc_traits.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">//  __cplusplus &gt;= 201103L</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> std::allocator_traits  allocate </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD </span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>_Alloc  allocate  __a  __n  allocate  __hint  __hint  allocate  allocate  allocatedeallocate, construct, destroy, max_size  _Tp  _Tp </p>\n<p> std::allocator_traits::construct </p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p> _Tp* _Tp  <strong>gnu_cxx::</strong>alloc_traits  construct </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">//  _Ptr  pointer  _Ptr  =&gt; __is_custom_pointer </span></span><br><span class=\"line\"><span class=\"comment\">//  _Ptr  pointer  __is_custom_pointer </span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//  </span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">//  __is_custom_pointer&lt;_Ptr&gt; enable_if&lt;xx&gt;::type </span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>const / rebind _Alloc  _Alloc  value_type  allocator_traits  value_type  rebind  allocator_traits::value_type  Allocator</p>\n<p>/</p>\n<p>std::allocator  bits/allocator.h  __allocator_base __gnu_cxx::new_allocator  new_allocator  allocate, deallocate, max_size, construct, destroy </p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p> bit/stl_iterator_base_types.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  Iterator </span></span><br><span class=\"line\"><span class=\"comment\">// </span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n\n<p> iterator_tag // std::iterator  iterator_traits </p>\n<p> bits/stl_iterator.h </p>\n<ol>\n<li>input  input  input <br></li>\n<li>output  input <br></li>\n<li>forward  input  output  input/output forward  multipass </li>\n<li>bidirectional  forword </li>\n<li>random-access  bidirectional </li>\n</ol>\n<p></p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // </span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// </span></span><br><span class=\"line\">                        <span class=\"comment\">// </span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> r i r  i  i </p>\n<ol>\n<li>: *r = *(i-1)</li>\n<li>++r = i, r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>r  i </p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// </span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">//  output </span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// </span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> front_insert_iterator, insert_iterator </p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p> _Iterator, _Container_Container  __normal_iterator </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator  _M_current </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//  _M_current </span></span><br><span class=\"line\">    <span class=\"comment\">//  __normal_iterator  _M_current </span></span><br><span class=\"line\">    <span class=\"comment\">//  normal </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p> move move_iterator  move move  copy</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">//  _Iterator </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// __base_ref __base_ref </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// </span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::move(_M_current[__n]); &#125;  <span class=\"comment\">// </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p> vector  bits/stl_vector.h  _Vector_base</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Alloc  value_type  _Tp rebind  value_type  _Tp  alloctor alloctor&lt;_Tp&gt; _Tp_alloc_type pointer _Tp_alloc_type::pointer _Tp<em> std::allocator  _Tp_alloc_type::pointer  _Tp</em> _Vector_base::pointer  _Tp*</p>\n<p> _Vector_base </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">//  vector </span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">//  vector  past-the-last-element </span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector  past-the-max-element </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"comment\">// vector  overflow  _GLIBCXX_SANITIZE_VECTOR AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Vector_base _Vector_base </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// / </span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">//  n  pointer </span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">//  __n=0 nullptr _M_impl </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Vector_base  vector  vector </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector  vector  _Tp vector  _Alloc</span></span><br><span class=\"line\"><span class=\"comment\">//   _Alloc  std::allocator&lt;_Tp&gt; _Tp </span></span><br><span class=\"line\"><span class=\"comment\">//   _Alloc  _Tp  _Alloc::rebind  alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> vector  iterator pointer  <strong>gnu::cxx::</strong>normal_iterator  _Iterator<br> vector  vector  <em>M</em>impl.<em>M</em>finish  past-the-last-element  vector </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\">begin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\">end() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(end()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(begin()); &#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p> vector resize  vector  new_size &gt; old_size resize  new_size&lt;=old_size <em>M</em>impl.<em>M</em>finish [<em>M</em>start, <em>M</em>finish) <em>M</em>finish </p>\n<p> vector  push_back </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//  __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(end(), __x);  <span class=\"comment\">//  _M_finish  __x</span></span><br><span class=\"line\">                                        <span class=\"comment\">//   _M_finish </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> bits/vector.tcc  <em>M</em>realloc_insert </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//  2  _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">//  past-the-last </span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - begin();<span class=\"comment\">// </span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">//  __len </span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">//  past-the-last </span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// </span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// </span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">//  vector  __new_finish  nullptr</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// </span></span><br><span class=\"line\">            <span class=\"comment\">// </span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">//  __new_finish  1</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">//  __new_finish  past-of-last </span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>vector </p>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> c++  c++ </p>\n<p> ubuntu gcc  7.3.0 /usr/include/c++/7/ </p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone  github </li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a>  gcc </li>\n</ol>\n<p>C++ </p>\n<p></p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p> __allocator_traits_base  libstdc++-v3/include/bits/alloc_traits.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//  _Tp </span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> __rebind __rebind</p>\n<ol>\n<li> void __rebind  </li>\n<li> void<ul>\n<li> _Tp::template rebind&lt;_Up&gt;::other  __rebind </li>\n<li> __rebind </li>\n</ul>\n</li>\n</ol>\n<p></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p> _Alloc::template rebind&lt;_Up&gt;::other  <strong>rebind  _Alloc </strong>alloc_rebind  _Alloc&lt;_Up, &gt;::type</p>\n<p> allocator_traits </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> pointer  _Alloc::pointer  value_type<em>_Alloc::pointer  value_type</em> __pointer <br> std/type_traits  __detected_or_t  __rebind  allocator_traits </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> _Ptr type  _Func&lt;_Alloc&gt; _Func  type  <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code> pointer  value_type*  bits/ptr_traits.h  pointer_traits </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind = _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p> _Ptr::type  _Tp* allocator_traits  _Diff type _Size  type  _Ptr::type, _Diff::type  _Size::type </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  _Alloc::const_pointer  _Alloc::const_pointer const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::void_pointer  void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::difference_type  pointer_traits&lt;pointer&gt;::difference_type</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">//  _Alloc::size_type  difference_type </span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p> __alloc_traits ext/alloc_traits.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">//  __cplusplus &gt;= 201103L</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> std::allocator_traits  allocate </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD </span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>_Alloc  allocate  __a  __n  allocate  __hint  __hint  allocate  allocate  allocatedeallocate, construct, destroy, max_size  _Tp  _Tp </p>\n<p> std::allocator_traits::construct </p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p> _Tp* _Tp  <strong>gnu_cxx::</strong>alloc_traits  construct </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">//  _Ptr  pointer  _Ptr  =&gt; __is_custom_pointer </span></span><br><span class=\"line\"><span class=\"comment\">//  _Ptr  pointer  __is_custom_pointer </span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//  </span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">//  __is_custom_pointer&lt;_Ptr&gt; enable_if&lt;xx&gt;::type </span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>const / rebind _Alloc  _Alloc  value_type  allocator_traits  value_type  rebind  allocator_traits::value_type  Allocator</p>\n<p>/</p>\n<p>std::allocator  bits/allocator.h  __allocator_base __gnu_cxx::new_allocator  new_allocator  allocate, deallocate, max_size, construct, destroy </p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p> bit/stl_iterator_base_types.h </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  Iterator </span></span><br><span class=\"line\"><span class=\"comment\">// </span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n\n<p> iterator_tag // std::iterator  iterator_traits </p>\n<p> bits/stl_iterator.h </p>\n<ol>\n<li>input  input  input <br></li>\n<li>output  input <br></li>\n<li>forward  input  output  input/output forward  multipass </li>\n<li>bidirectional  forword </li>\n<li>random-access  bidirectional </li>\n</ol>\n<p></p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // </span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// </span></span><br><span class=\"line\">                        <span class=\"comment\">// </span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> r i r  i  i </p>\n<ol>\n<li>: *r = *(i-1)</li>\n<li>++r = i, r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>r  i </p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// </span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">//  output </span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// </span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> front_insert_iterator, insert_iterator </p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p> _Iterator, _Container_Container  __normal_iterator </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator  _M_current </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//  _M_current </span></span><br><span class=\"line\">    <span class=\"comment\">//  __normal_iterator  _M_current </span></span><br><span class=\"line\">    <span class=\"comment\">//  normal </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p> move move_iterator  move move  copy</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">//  _Iterator </span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// __base_ref __base_ref </span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// </span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::move(_M_current[__n]); &#125;  <span class=\"comment\">// </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p> vector  bits/stl_vector.h  _Vector_base</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Alloc  value_type  _Tp rebind  value_type  _Tp  alloctor alloctor&lt;_Tp&gt; _Tp_alloc_type pointer _Tp_alloc_type::pointer _Tp<em> std::allocator  _Tp_alloc_type::pointer  _Tp</em> _Vector_base::pointer  _Tp*</p>\n<p> _Vector_base </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">//  vector </span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">//  vector  past-the-last-element </span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector  past-the-max-element </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// </span></span><br><span class=\"line\">    <span class=\"comment\">// vector  overflow  _GLIBCXX_SANITIZE_VECTOR AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Vector_base _Vector_base </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// </span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// / </span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">//  n  pointer </span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">//  __n=0 nullptr _M_impl </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> _Vector_base  vector  vector </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector  vector  _Tp vector  _Alloc</span></span><br><span class=\"line\"><span class=\"comment\">//   _Alloc  std::allocator&lt;_Tp&gt; _Tp </span></span><br><span class=\"line\"><span class=\"comment\">//   _Alloc  _Tp  _Alloc::rebind  alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> vector  iterator pointer  <strong>gnu::cxx::</strong>normal_iterator  _Iterator<br> vector  vector  <em>M</em>impl.<em>M</em>finish  past-the-last-element  vector </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\">begin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\">end() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(end()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(begin()); &#125;</span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p> vector resize  vector  new_size &gt; old_size resize  new_size&lt;=old_size <em>M</em>impl.<em>M</em>finish [<em>M</em>start, <em>M</em>finish) <em>M</em>finish </p>\n<p> vector  push_back </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//  __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(end(), __x);  <span class=\"comment\">//  _M_finish  __x</span></span><br><span class=\"line\">                                        <span class=\"comment\">//   _M_finish </span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> bits/vector.tcc  <em>M</em>realloc_insert </p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//  2  _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">//  past-the-last </span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - begin();<span class=\"comment\">// </span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">//  __len </span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">//  past-the-last </span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// </span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// </span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">//  vector  __new_finish  nullptr</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// </span></span><br><span class=\"line\">            <span class=\"comment\">// </span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">//  __new_finish  1</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">//  __new_finish  past-of-last </span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">        <span class=\"comment\">// </span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>vector </p>\n<p></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjzy379620008dgvcss88f34e","category_id":"cjzy37966000cdgvc242rnacc","_id":"cjzy3796c000mdgvc8cz47xc6"},{"post_id":"cjzy379c2001edgvc1wahyx27","category_id":"cjzy37966000cdgvc242rnacc","_id":"cjzy379cd001ldgvcshvqn83j"},{"post_id":"cjzy379e6001tdgvcvw7kygjd","category_id":"cjzy37966000cdgvc242rnacc","_id":"cjzy379e8001wdgvcghta3054"},{"post_id":"cjzy379f40021dgvc23p093iy","category_id":"cjzy37966000cdgvc242rnacc","_id":"cjzy379fe0027dgvcge2hwcim"},{"post_id":"cjzy379f60023dgvce6txh0mv","category_id":"cjzy37966000cdgvc242rnacc","_id":"cjzy379ff0029dgvcc1j5vj62"},{"post_id":"cjzy379f2001xdgvckxdxe07t","category_id":"cjzy379f4001zdgvchfwipm58","_id":"cjzy379ff002bdgvcedgzl72z"}],"PostTag":[{"post_id":"cjzy3795l0000dgvcowedm8pb","tag_id":"cjzy3795v0004dgvc9ytv8832","_id":"cjzy37964000adgvcugl5er9u"},{"post_id":"cjzy37964000bdgvcjmis7zg5","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy37968000fdgvcrghqbav9"},{"post_id":"cjzy3795s0002dgvchi1va9xb","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy37969000hdgvc1plyei9d"},{"post_id":"cjzy37968000gdgvc27iowwy7","tag_id":"cjzy3795v0004dgvc9ytv8832","_id":"cjzy3796b000kdgvcy2mpx1l4"},{"post_id":"cjzy3795x0005dgvcr20nvfez","tag_id":"cjzy37967000edgvc5shzcl04","_id":"cjzy3796d000odgvczftxkjb9"},{"post_id":"cjzy3795x0005dgvcr20nvfez","tag_id":"cjzy3796a000jdgvcxpm0z3mv","_id":"cjzy3796d000pdgvc8mmkzsf5"},{"post_id":"cjzy3795z0007dgvc28xd127q","tag_id":"cjzy3796c000ndgvci6o13z0v","_id":"cjzy3796e000rdgvcx7k2xcot"},{"post_id":"cjzy379620008dgvcss88f34e","tag_id":"cjzy3796d000qdgvch0pkzwi0","_id":"cjzy3796f000tdgvcgkcq7mt3"},{"post_id":"cjzy37966000ddgvcu4700weq","tag_id":"cjzy3796e000sdgvcg6105c95","_id":"cjzy3796f000vdgvcbmyna550"},{"post_id":"cjzy37969000idgvchkwkaz1h","tag_id":"cjzy3796f000udgvcftfd0r2l","_id":"cjzy3796h000xdgvcmnksk0bm"},{"post_id":"cjzy3796b000ldgvcs131qnkc","tag_id":"cjzy3796f000udgvcftfd0r2l","_id":"cjzy3796i000ydgvcal6cgvq5"},{"post_id":"cjzy379br000zdgvcajs3c9oo","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379bu0011dgvck290lagh"},{"post_id":"cjzy379bs0010dgvcd8yiapcv","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379bv0013dgvc00bk8972"},{"post_id":"cjzy379bu0012dgvcjjxidx7g","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379bw0015dgvcrjso33vl"},{"post_id":"cjzy379bv0014dgvcdyyigmr5","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379by0017dgvcylbciwmo"},{"post_id":"cjzy379bx0016dgvc9wfkx7iv","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379bz0019dgvcam0mmwrk"},{"post_id":"cjzy379by0018dgvc8zweroo8","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379c0001bdgvcm2z1071d"},{"post_id":"cjzy379bz001adgvct874b4zz","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379c2001ddgvcggfq3lf5"},{"post_id":"cjzy379c1001cdgvcieym9jpw","tag_id":"cjzy3795v0004dgvc9ytv8832","_id":"cjzy379c3001fdgvc796d52ev"},{"post_id":"cjzy379c2001edgvc1wahyx27","tag_id":"cjzy3796d000qdgvch0pkzwi0","_id":"cjzy379ca001hdgvcgfidx55m"},{"post_id":"cjzy379c7001gdgvcixoiqipn","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379cc001jdgvc6zccifp8"},{"post_id":"cjzy379cb001idgvc6e83ruww","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379ce001mdgvc9fhdcxae"},{"post_id":"cjzy379cc001kdgvclw2pjtqb","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379cf001odgvc82a69eae"},{"post_id":"cjzy379ce001ndgvcm8w0miq4","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379cg001pdgvct2klkgpc"},{"post_id":"cjzy379e3001qdgvczx4srz2a","tag_id":"cjzy3795v0004dgvc9ytv8832","_id":"cjzy379e5001sdgvcbngbjnbg"},{"post_id":"cjzy379e4001rdgvccnao3c28","tag_id":"cjzy379630009dgvccsyzx2pq","_id":"cjzy379e7001udgvc80o0j3bp"},{"post_id":"cjzy379e6001tdgvcvw7kygjd","tag_id":"cjzy3796d000qdgvch0pkzwi0","_id":"cjzy379e7001vdgvcf04gmtt7"},{"post_id":"cjzy379f2001xdgvckxdxe07t","tag_id":"cjzy37967000edgvc5shzcl04","_id":"cjzy379f40020dgvcxpfchiey"},{"post_id":"cjzy379f2001xdgvckxdxe07t","tag_id":"cjzy3796a000jdgvcxpm0z3mv","_id":"cjzy379f50022dgvcowll4olj"},{"post_id":"cjzy379f3001ydgvcgv0y6avu","tag_id":"cjzy37967000edgvc5shzcl04","_id":"cjzy379f70024dgvct633hoi2"},{"post_id":"cjzy379f3001ydgvcgv0y6avu","tag_id":"cjzy3796a000jdgvcxpm0z3mv","_id":"cjzy379f80026dgvck2flqkn9"},{"post_id":"cjzy379f40021dgvc23p093iy","tag_id":"cjzy3796d000qdgvch0pkzwi0","_id":"cjzy379fe0028dgvcifsix0qh"},{"post_id":"cjzy379f60023dgvce6txh0mv","tag_id":"cjzy3796d000qdgvch0pkzwi0","_id":"cjzy379ff002adgvc38p0uh21"},{"post_id":"cjzy379f70025dgvcm3gn228f","tag_id":"cjzy3796e000sdgvcg6105c95","_id":"cjzy379ff002cdgvcq330z7ej"}],"Tag":[{"name":"GAN","_id":"cjzy3795v0004dgvc9ytv8832"},{"name":"object detection","_id":"cjzy379630009dgvccsyzx2pq"},{"name":"math","_id":"cjzy37967000edgvc5shzcl04"},{"name":"DP","_id":"cjzy3796a000jdgvcxpm0z3mv"},{"name":"tool","_id":"cjzy3796c000ndgvci6o13z0v"},{"name":"PyTorch","_id":"cjzy3796d000qdgvch0pkzwi0"},{"name":"c++","_id":"cjzy3796e000sdgvcg6105c95"},{"name":"CV","_id":"cjzy3796f000udgvcftfd0r2l"}]}}