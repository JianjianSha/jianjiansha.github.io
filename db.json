{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/BBox-reg_fig3.png","path":"images/BBox-reg_fig3.png","modified":1,"renderable":0},{"_id":"source/images/BBox-reg_fig4.png","path":"images/BBox-reg_fig4.png","modified":1,"renderable":0},{"_id":"source/images/DetNet_fig1.png","path":"images/DetNet_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DetNet_fig2.png","path":"images/DetNet_fig2.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig2.png","path":"images/FSAF_fig2.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig3.png","path":"images/FSAF_fig3.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig4.png","path":"images/FSAF_fig4.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig5.png","path":"images/FSAF_fig5.png","modified":1,"renderable":0},{"_id":"source/images/FSAF_fig6.png","path":"images/FSAF_fig6.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig3.png","path":"images/GA-RPN_fig3.png","modified":1,"renderable":0},{"_id":"source/images/GIoU_fig2.png","path":"images/GIoU_fig2.png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig3.png","path":"images/M2Det_fig3.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(b).png","path":"images/TridentNet_fig1(b).png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(c).png","path":"images/TridentNet_fig1(c).png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig3.png","path":"images/libra-rcnn_fig3.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_figa.png","path":"images/libra-rcnn_figa.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig1.png","path":"images/mAP_fig1.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig2.png","path":"images/mAP_fig2.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig3.png","path":"images/mAP_fig3.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig4.png","path":"images/mAP_fig4.png","modified":1,"renderable":0},{"_id":"source/images/mAP_fig5.png","path":"images/mAP_fig5.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig3.png","path":"images/mask-rcnn_fig3.png","modified":1,"renderable":0},{"_id":"source/images/BBox-reg_fig2.png","path":"images/BBox-reg_fig2.png","modified":1,"renderable":0},{"_id":"source/images/DeRPN_fig1.png","path":"images/DeRPN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig1.png","path":"images/GA-RPN_fig1.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig2.png","path":"images/GA-RPN_fig2.png","modified":1,"renderable":0},{"_id":"source/images/GA-RPN_fig4.png","path":"images/GA-RPN_fig4.png","modified":1,"renderable":0},{"_id":"source/images/GIoU_fig1.png","path":"images/GIoU_fig1.png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig1.png","path":"images/M2Det_fig1.png","modified":1,"renderable":0},{"_id":"source/images/M2Det_fig4.png","path":"images/M2Det_fig4.png","modified":1,"renderable":0},{"_id":"source/images/RepPoints_fig2.png","path":"images/RepPoints_fig2.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig1(a).png","path":"images/TridentNet_fig1(a).png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig2.png","path":"images/TridentNet_fig2.png","modified":1,"renderable":0},{"_id":"source/images/TridentNet_fig3.png","path":"images/TridentNet_fig3.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig1.png","path":"images/libra-rcnn_fig1.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig2.png","path":"images/libra-rcnn_fig2.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig4.png","path":"images/libra-rcnn_fig4.png","modified":1,"renderable":0},{"_id":"source/images/libra-rcnn_fig5.png","path":"images/libra-rcnn_fig5.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig1.png","path":"images/mask-rcnn_fig1.png","modified":1,"renderable":0},{"_id":"source/images/mask-rcnn_fig4.png","path":"images/mask-rcnn_fig4.png","modified":1,"renderable":0},{"_id":"source/images/RepPoints_fig1.png","path":"images/RepPoints_fig1.png","modified":1,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/affix.js","path":"js/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/exturl.js","path":"js/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/js.cookie.js","path":"js/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/post-details.js","path":"js/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scroll-cookie.js","path":"js/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scrollspy.js","path":"js/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"source/images/BBox-reg_fig1.png","path":"images/BBox-reg_fig1.png","modified":1,"renderable":0},{"_id":"source/images/DeRPN_fig2.png","path":"images/DeRPN_fig2.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"source/images/M2Det_fig2.png","path":"images/M2Det_fig2.png","modified":1,"renderable":0},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"source/images/DSOD_fig1.png","path":"images/DSOD_fig1.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1560516826130},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1560516826130},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1560516826130},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1560516826129},{"_id":"themes/next/.all-contributorsrc","hash":"43eb0149c78e464c695f0dd758bb8c59353182b3","modified":1560516826129},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1560516826140},{"_id":"themes/next/.gitignore","hash":"8bf1bfc917aa8dd2d886fd36f764447a2b561e1e","modified":1560516826139},{"_id":"themes/next/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1560516826140},{"_id":"themes/next/_config.yml","hash":"a87cac157119b7bc9c16d55a965cfd96a9248463","modified":1560652971556},{"_id":"themes/next/README.md","hash":"3f72e5a5051ca2bdaccdda684c46dc4fdb4413a6","modified":1560516826141},{"_id":"themes/next/bower.json","hash":"8076a6e58a99d1188d335a6456a0de0fda163338","modified":1560516826142},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1560516826139},{"_id":"themes/next/package.json","hash":"037ed50fbce1520918bf8c3a1c14b6e07676783e","modified":1560516826195},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1560516826142},{"_id":"themes/next/gulpfile.coffee","hash":"6407d9063bd88ede299ff7c2a59cf2c82e079476","modified":1560516826152},{"_id":"source/_posts/BBox-Reg-Uncertainty.md","hash":"af31eca9b7a7e09588671fba50245e1c326943dd","modified":1561713660064},{"_id":"source/_posts/DSOD.md","hash":"1074946adbcf09724075cb4a9a805195b44d2c3f","modified":1563343920637},{"_id":"source/_posts/DetNet.md","hash":"ce3d911a8602593276ff1f81a5b1328a477ef1ac","modified":1563348045979},{"_id":"source/_posts/DeRPN.md","hash":"a0a74041a81f2111d53ef0f6f95f698f7300470e","modified":1563268517107},{"_id":"source/_posts/FSAF.md","hash":"b008fe64013cfaa36df10791f3f42dda0eab7aee","modified":1561637721594},{"_id":"source/_posts/GA-RPN.md","hash":"6e16e51f06c48693e144172fd32859013b1d2a9b","modified":1561637765328},{"_id":"source/_posts/GIoU.md","hash":"6aff371ba813f07062ded15e606dcdc14204ce02","modified":1561453809665},{"_id":"source/_posts/Hexo-Sync.md","hash":"b8cc3f0b1e846edd52d10c5d41c4e7afeb20f555","modified":1560518432167},{"_id":"source/_posts/M2Det.md","hash":"6eb8aec3c5176af1faf540594c09d1d5c4b70455","modified":1561802397459},{"_id":"source/_posts/PyTorch-1.md","hash":"02541d37b0459041c1ac8e7a0e7980c5d904d6c7","modified":1561025269801},{"_id":"source/_posts/PyTorch-2.md","hash":"ac8f1a76ee498bfe89e4622468ef93e5c088cc7a","modified":1561026317453},{"_id":"source/_posts/PyTorch-3.md","hash":"f2b1007ca20da82779bf17723401c13f6d7f9c97","modified":1562745801154},{"_id":"source/_posts/RepPoints.md","hash":"3e3d4aa9322e89944463456db26918cf1349949d","modified":1563448962079},{"_id":"source/_posts/TridentNet.md","hash":"de2029038a1b6d9388722a83d5dbd6afa44c654f","modified":1561637849335},{"_id":"source/_posts/cpp-aux-tools.md","hash":"a297a5eedda27f8313b9b582ce19bf0757d4ce98","modified":1562844177310},{"_id":"source/_posts/cv-mtds.md","hash":"b46e3d21b161a5d3eb590811337ae0bf6a427f48","modified":1563269478679},{"_id":"source/_posts/gcc-src.md","hash":"5a6ea3c2c2896aa71ca5f9e6b50fac1070100b6d","modified":1563013967398},{"_id":"source/_posts/libra-rcnn.md","hash":"d20c22a87cf5ddb28c447391ff6f4d40da666a28","modified":1562330122631},{"_id":"source/_posts/loss.md","hash":"e2f2848272240eb043f2a492356e6a946143804c","modified":1563328501435},{"_id":"source/_posts/mAP.md","hash":"07f0e284d51af27a2c28b57c5bae20aa57da9fcf","modified":1561100541616},{"_id":"source/_posts/mask-rcnn.md","hash":"5c9efbe2d02dc03eb6d39fe4c6d582d5cc6f4c91","modified":1562669847936},{"_id":"source/categories/index.md","hash":"df4e0622c971f2ed6ca0fede98457aef01367ad0","modified":1560518664346},{"_id":"source/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1561700897878},{"_id":"source/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1561700932509},{"_id":"source/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1563348004387},{"_id":"source/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1563348028431},{"_id":"source/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1561601705733},{"_id":"source/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1561601750043},{"_id":"source/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1561606224212},{"_id":"source/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1561615069653},{"_id":"source/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1561620213819},{"_id":"source/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1561532150029},{"_id":"source/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1560737748441},{"_id":"source/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1561782174383},{"_id":"source/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1561107751780},{"_id":"source/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1561107868725},{"_id":"source/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1562323541605},{"_id":"source/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1562205999211},{"_id":"source/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1560737862918},{"_id":"source/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1560739283012},{"_id":"source/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1560741089754},{"_id":"source/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1560761461928},{"_id":"source/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1560776786462},{"_id":"source/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1562670075135},{"_id":"source/tags/index.md","hash":"9b0cad73fc3ec06304fd78b076232fd6c4147bf8","modified":1560518657742},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"f7ddb7faed8031a9f40eae4ee7bb48c1bc50fd14","modified":1560516826131},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"046262c4b2f54b5ed8ac19b0c99aad04968e01e5","modified":1560516826132},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"1e212fe229bd659726b4a3bcf4b5b14e0310ba3a","modified":1560516826132},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"f1631b9bef922e7bc2db1e33badfad70fd88d459","modified":1560516826134},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1560516826137},{"_id":"themes/next/.github/auto_assign.yml","hash":"9fe0dbe3f6edc59bf10ea25b14eba0e92e2c8f42","modified":1560516826135},{"_id":"themes/next/.github/lock.yml","hash":"4f1070097b614b24050f238694127e3573ce8472","modified":1560516826136},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"e06053d417579ed967a94166deb6bda5ce41d805","modified":1560516826136},{"_id":"themes/next/.github/stale.yml","hash":"85975c43d606c39b91c0ad32197154be9d482a09","modified":1560516826138},{"_id":"themes/next/.github/config.yml","hash":"cbd06d0c40afa9fdf056765120e9085826b00d20","modified":1560516826136},{"_id":"themes/next/.github/release-drafter.yml","hash":"d01b1e8f462af114e3934fef2ee654634d86b406","modified":1560516826137},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1560516826138},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1e49c08b446678336b2eacc8bf581faca969f34b","modified":1560516826143},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1560516826143},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1560516826144},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1560516826144},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"721a1aa9feed1b580ab99af8e69ed22699121e88","modified":1560516826145},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1560516826146},{"_id":"themes/next/docs/MATH.md","hash":"7d0330c250082a86897d1c96fbb4ef5df59538af","modified":1560516826146},{"_id":"themes/next/languages/de.yml","hash":"79b3221344da335743b5ef5a82efa9338d64feb0","modified":1560516826153},{"_id":"themes/next/.github/weekly-digest.yml","hash":"6db3bcad65c3156de298f6a3ffd3ba887af4aa4f","modified":1560516826139},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1560516826145},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1560516826154},{"_id":"themes/next/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1560516826154},{"_id":"themes/next/languages/es.yml","hash":"db1a9f2af477212544c830c2dd986400e26ddd6a","modified":1560516826154},{"_id":"themes/next/languages/fa.yml","hash":"3227072c7e1bfb16ec0517394b60632f4be921dd","modified":1560516826155},{"_id":"themes/next/.github/topissuebot.yml","hash":"5091c3bc6f3df303d16d853ce65a302601c1e875","modified":1560516826138},{"_id":"themes/next/languages/fr.yml","hash":"2429c90dad5bb865e3a969be2b373f19b3a77b3b","modified":1560516826155},{"_id":"themes/next/languages/id.yml","hash":"f3302a4dfdc9be38a52d6e081411574b1ea01671","modified":1560516826156},{"_id":"themes/next/languages/ja.yml","hash":"3f25eca504ee5a519987b4402731f1bb7f5191c9","modified":1560516826156},{"_id":"themes/next/languages/ko.yml","hash":"75f2fe142f76bf623e34ed3570598226f55f2b8b","modified":1560516826157},{"_id":"themes/next/languages/pt-BR.yml","hash":"c7de8b77f44e75be4f04423088a1c891537aa601","modified":1560516826157},{"_id":"themes/next/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1560516826158},{"_id":"themes/next/languages/it.yml","hash":"31eb878b53d60ff47e3e534cdd7a839c8801ac6e","modified":1560516826156},{"_id":"themes/next/languages/nl.yml","hash":"08f16ce395dacc88847fc30dc6b985ce22fb8948","modified":1560516826157},{"_id":"themes/next/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1560516826158},{"_id":"themes/next/languages/pt.yml","hash":"ca5072c967e5eb1178ffed91827459eda6e4e6e2","modified":1560516826158},{"_id":"themes/next/languages/uk.yml","hash":"6320439c6e9ff81e5b8f8129ca16e9a744b37032","modified":1560516826159},{"_id":"themes/next/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1560516826159},{"_id":"themes/next/languages/vi.yml","hash":"e2f0dd7f020a36aa6b73ed4d00dcc4259a7e5e9d","modified":1560516826159},{"_id":"themes/next/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1560516826160},{"_id":"themes/next/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1560516826160},{"_id":"themes/next/layout/category.swig","hash":"ad0ac6a1ff341f8eab9570e7fb443962948c5f9d","modified":1560516826193},{"_id":"themes/next/layout/archive.swig","hash":"61bc56e77e653684fc834f63dcbdadf18687c748","modified":1560516826193},{"_id":"themes/next/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1560516826194},{"_id":"themes/next/layout/post.swig","hash":"af74e97d57cf00cde6f8dbd4364f27910915454e","modified":1560516826194},{"_id":"themes/next/layout/_layout.swig","hash":"ba786b1baba49021928e2e508da53f2fd1369b3f","modified":1560516826161},{"_id":"themes/next/layout/page.swig","hash":"5d06ee8f477ffc39932d0251aa792ffcaf8faf14","modified":1560516826194},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1560516826198},{"_id":"themes/next/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1560516826197},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1560516826266},{"_id":"themes/next/layout/schedule.swig","hash":"e79f43df0e9a6cf48bbf00882de48c5a58080247","modified":1560516826195},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1560516826267},{"_id":"source/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1561700861047},{"_id":"source/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1563268584724},{"_id":"source/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1561511191938},{"_id":"source/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1561527456160},{"_id":"source/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1561515013760},{"_id":"source/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1560737735216},{"_id":"source/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1561776113410},{"_id":"source/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1561782128055},{"_id":"source/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1563448839370},{"_id":"source/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1561107716675},{"_id":"source/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1561338611567},{"_id":"source/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1561363621295},{"_id":"source/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1562205321629},{"_id":"source/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1562207542722},{"_id":"source/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1562311073193},{"_id":"source/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1562323605049},{"_id":"source/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1562670019815},{"_id":"source/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1562670127461},{"_id":"themes/next/layout/tag.swig","hash":"283519d4d5b67814412863a3e0212bac18bcc5a0","modified":1560516826195},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1560516826143},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1560516826267},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826242},{"_id":"source/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1563448770086},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"795b8ddb251da8e2327299d5f7dbf446fb9867c6","modified":1560516826133},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"245917ffaa296bc2d9a85444acf639077ca25944","modified":1560516826133},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"59b2b45e151972bbe08582cde22f398e58832765","modified":1560516826134},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"ae22e700b7c63c60746321719a20d34022ad78d9","modified":1560516826134},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1560516826147},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1560516826147},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1560516826148},{"_id":"themes/next/docs/ru/README.md","hash":"aeb95129ab1da9ec41786bfa86dc32c739ee6358","modified":1560516826147},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"aaf25d304793344e2d026062768c93005723f5c6","modified":1560516826148},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"018a259694f4a8c7c384e1f323531442cba5fbf3","modified":1560516826149},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1560516826150},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"16d98708de86efe40ebcb02c02a01af0f160b80a","modified":1560516826149},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1560516826150},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"b17fc344ff61603f83387c0f9b2b2189aae81d50","modified":1560516826151},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"db2797f161e1e7a4987cbfa3d1be682266dfbba6","modified":1560516826151},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1560516826152},{"_id":"themes/next/docs/zh-CN/README.md","hash":"4016948fdb971e4f905efb7a5bb3add3dd58e7a8","modified":1560516826152},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1560516826161},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1560516826161},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1560516826160},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"91017f58f83d9505ce99109fffdc51c032bf017e","modified":1560516826164},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"891ab67815969dd8736cb22fbbb3f791b8fff4e4","modified":1560516826163},{"_id":"themes/next/layout/_partials/comments.swig","hash":"d0b9e841d55c974d02f43823a06a2627f8e46431","modified":1560516826164},{"_id":"themes/next/layout/_partials/footer.swig","hash":"9a79dde1412b1b1473380e8b6cacfe1930ed321b","modified":1560516826164},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"1ad13269b43b900356f3bdab7947d6a86f035a2c","modified":1560516826165},{"_id":"themes/next/layout/_macro/post.swig","hash":"c77a7928d65bfe0fb712a2931b4cd7045666508c","modified":1560516826163},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1560516826170},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1560516826170},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"cccd93d30787675010b1a74ef02eb5b813ec1d96","modified":1560516826174},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"c2e8f4b3a2bf991320ecc827dcdc227399ad5b51","modified":1560516826175},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"50c3ae6b50f173ae70f8c3312f7c6da1097eb9b6","modified":1560516826175},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"efb3404a3303622f3be60944d9d1926972c5c248","modified":1560516826175},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"8a992b7fe42b9c1a5eb9d937b0827aed91586d94","modified":1560516826178},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1560516826183},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"87bcb495f7ddd81cc3fe2c2a886e51c08053019b","modified":1560516826183},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"87182367d7954457cb2498bbfa9445c03c2d619e","modified":1560516826184},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"b9b57e1735035319e200c22cf46a38b52b4e0d9c","modified":1560516826178},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"80dfc0879866e6512cb67590a3b2d8741a66f980","modified":1560516826189},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1560516826190},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"12bf51c55449d0e838f93a4aae9f6d25c0a27ba2","modified":1560516826187},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"4ae61c7efb16e962385bfe522a38c4d29cdcccbe","modified":1560516826190},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"7db4ad4a8dd5420dad2f6890f5299945df0af970","modified":1560516826189},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"8b1322a091355853db62a5aafb8886fdbd8ab56a","modified":1560516826190},{"_id":"themes/next/scripts/filters/exturl.js","hash":"b19c7c1021e57367b3b3bbf5678381017ed5667d","modified":1560516826196},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cdb6152582313268d970ffeef99b4a8a7850f034","modified":1560516826196},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"a40ce6bc852bb4bff8b9f984fa064741dd151e96","modified":1560516826196},{"_id":"themes/next/scripts/tags/button.js","hash":"95a520f6529424a03c7ead6dbfd5e626d672febb","modified":1560516826198},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1560516826198},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a6b2264215c555c553b2c5db85fa90678798d0d5","modified":1560516826199},{"_id":"themes/next/scripts/tags/exturl.js","hash":"f9f25905adecfb8be49def4ff3b0b8bbc6955d84","modified":1560516826199},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"23d839333422375e85d44e476f554faf49973a3c","modified":1560516826199},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1560516826200},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1560516826199},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1560516826200},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1560516826201},{"_id":"themes/next/scripts/tags/pdf.js","hash":"ab995f0fc60d60f637220e2651111b775b8a06de","modified":1560516826201},{"_id":"themes/next/scripts/tags/tabs.js","hash":"72a5adbd8f300bee1d0c289367598ca06b2bed17","modified":1560516826201},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1560516826202},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"c476dc3693a9dd0be2d136a45b0d7fdef55d4d92","modified":1560516826191},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1560516826243},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1560516826243},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1560516826244},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1560516826244},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1560516826244},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1560516826245},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1560516826246},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1560516826247},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1560516826247},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1560516826247},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560516826247},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1560516826248},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560516826248},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1560516826248},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1560516826249},{"_id":"themes/next/source/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1560516826249},{"_id":"themes/next/source/js/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1560516826249},{"_id":"themes/next/source/js/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1560516826250},{"_id":"themes/next/source/js/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1560516826250},{"_id":"themes/next/source/js/js.cookie.js","hash":"f11e84def0352b7dd6393f1b83e55a40ab468686","modified":1560516826250},{"_id":"themes/next/source/js/next-boot.js","hash":"696a0c2cf158001576d56b48195ec8e39e835b47","modified":1560516826251},{"_id":"themes/next/source/js/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1560516826252},{"_id":"themes/next/source/js/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1560516826253},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"b44010cd577e4d063c3406772938c4b117ec7b7b","modified":1560516826192},{"_id":"themes/next/source/js/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1560516826253},{"_id":"themes/next/source/js/utils.js","hash":"fed16cd4fa5fac8cb4a63633d1840792a056f2be","modified":1560516826254},{"_id":"source/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1561700827659},{"_id":"source/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1563268633859},{"_id":"themes/next/source/js/motion.js","hash":"d0a6d9dbcc57159e54bbb1f683b86632ae0b78f0","modified":1560516826251},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"882cd0b68c493af1b6d945660f9c21085e006ffc","modified":1560516826191},{"_id":"themes/next/source/css/main.styl","hash":"5e7d28bc539e84f8b03e68df82292f7fc0f2d023","modified":1560516826242},{"_id":"source/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1561776076542},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826230},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826231},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826231},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826241},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1560516826242},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1560516826246},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1560516826246},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"b57bf9c865bed0f22157176a8085de168a1aef77","modified":1560516826165},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1560516826162},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"fd079a414ca0f42f4cddd00247a9d5a5f58c4d8e","modified":1560516826166},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1560516826162},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"03f669356bbaa70144b743f3312178e1981ac3a8","modified":1560516826167},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"f46699a9daa5fef599733cbab35cb75cf7a05444","modified":1560516826169},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"2b905ddd5dea8558c3fd95aacad241da6b6800f4","modified":1560516826168},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"39c4ad0e36b7c1260da98ba345f7bd72a2ac0f2e","modified":1560516826168},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"e015c7d9b84062b60b15b36be3ef11929dd10943","modified":1560516826168},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"39fa6780b9515bc343898ff615c858206728cc3c","modified":1560516826166},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"be6683db6a269d83bb0441d7cf74db63a240fa8a","modified":1560516826171},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1560516826167},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f331ad02beea8990066d32ad6ec9f859672c3615","modified":1560516826171},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1560516826172},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"f62b801c7999da67b4bdca9c5e373b9b5ed039dc","modified":1560516826171},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"fb7727e8ec63a58238a7206bf70eb273c8879993","modified":1560516826171},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1560516826173},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c609097b95eb6127c2784f47f2230e6e6efc0be2","modified":1560516826173},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f14e9e8c27af82f1bfe794e252dec0d7e521f503","modified":1560516826172},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"647e8677d1ccfb3f7918dd3ea2ff7078504a845d","modified":1560516826174},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"31245e09ce0465b994cebd94223a531585c4eab4","modified":1560516826173},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cf87ab778618a32119ec1c4ac2079a51385b1913","modified":1560516826176},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a62c93f19429f159bcf0c2e533ffc619aa399755","modified":1560516826176},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"3c548934b97cc426544947f7a2ae35c270b5e33f","modified":1560516826176},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"828eb9c47f34090c841a2e9a0b3f31b0e4ccf40a","modified":1560516826177},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7ef07edd2a97a3774229990d2f0a6eefa31bd015","modified":1560516826177},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"66d562b3778dbc839f7c00103bd0099c5d61602a","modified":1560516826178},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"54b43d406cf37932e7b60f46814e864d31b1842c","modified":1560516826174},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"2e1de38f44af00209129d4051b7ae307cb11ad68","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"335005a9f8b36349f0ad0a7beeba6969c55fc7f7","modified":1560516826180},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"83dd7df11b100bae38c9faab9a478f92149a0315","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"f648e5bf8c5dfc74143233976ed4ff5978deda43","modified":1560516826180},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"73576c9683d9ad9b124916dc6c660607fe7cc1fa","modified":1560516826179},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"d68da660cd1cc8fb3ff0a81178decadb620afc11","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"39928f358dd13d9fc1a4641800e57be157ecd815","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"623e73bedef067ac24a398ef27c8197295da872d","modified":1560516826181},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a22d1ea29a5ffe46199ab7d108a291a05af8d5b6","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"d18c87d7839e7407e39acd2998bcc9e0b34611b0","modified":1560516826182},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"4cff8bf5c42c62f7f0ac1f0d70f839dae39ba77a","modified":1560516826183},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"71fb01bcad43bc9410ab19190373b9f7e59215b5","modified":1560516826182},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"4e86e1ace90a70bb8862f5e6de9dbe7bfc046bee","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"280ff4282396beb53fb3913f58c6b5890bd1c9ef","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"b3818fd0b3028dadf341b6d0b180e1243683de6a","modified":1560516826186},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"3da014b25f9ac804eda1614591706e3733c0d2c5","modified":1560516826184},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"9a4923d2aa5182531ea7a7fb9abe824450026208","modified":1560516826185},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"bc3fc9d053b3d1fc0cd3918bf9a629a6f38f6414","modified":1560516826184},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1a5d94f5779a2ce13abc886dd78e0617f89c34b9","modified":1560516826187},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"1b72c755101c9dfb85da13df9a0abccf37cd1dd2","modified":1560516826187},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"43a20fa0e9ae2f4254f04813f9c619dd36b49ae5","modified":1560516826188},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"ea1c136f960667a0a13b334db497b9b19c41f629","modified":1560516826188},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"143ef265c96a8ea2fb93c36c5ffb9c5e940f7693","modified":1560516826191},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"078bd2d5815eb23e8c5f74467dc0042babea00ae","modified":1560516826192},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1560516826230},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"1aabac9e37a8f4451c86d09037b3a1f8b30eaf5e","modified":1560516826231},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"767ba29f258db5d2e5baf875a6f36ac1d44df6a3","modified":1560516826189},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1560516826230},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e9b0752f08398709e787546a246baca12b4c557f","modified":1560516826241},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1560516826241},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"21a14a19149f1cb8e011c477f29dd1352675605b","modified":1560516826231},{"_id":"themes/next/source/js/schemes/muse.js","hash":"ccc0c5cd4ec6f8159c98990ad83f11a5c0b0234c","modified":1560516826252},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"3eea56cc9ce47bb4760930c4c69cebf847a7fbb2","modified":1560516826253},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1560516826254},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1560516826255},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ebc95eeb8966d17cdc7dd0de009deaef1fe65064","modified":1560516826242},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1560516826255},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1560516826255},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1560516826265},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1560516826266},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"da7049f3d9a157abe0ecc62611edcf43605ba84d","modified":1560516826241},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1560516826255},{"_id":"source/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1562554796933},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"d45ca53af17d1d83fd27f8ed0917a72f0060e1a9","modified":1560516826192},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1560516826266},{"_id":"themes/next/source/lib/jquery/index.js","hash":"b15f7cfa79519756dff1ad22553fd0ed09024343","modified":1560516826262},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"9fd526db0527c71243f05e18086f937dc67b1c3e","modified":1560516826203},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"5c0ca7f801859cff254d2f5b7d1a70d66ff61a8d","modified":1560516826202},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"463817cbbd804ce134cb3e7e721431cb0e1616f2","modified":1560516826203},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"1a4ac0d119f2126ef8951897338706edce112235","modified":1560516826227},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"997058180065d986e05df72992cc2cbfd7febd7e","modified":1560516826204},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"cfa64bd8ee2ff9f943673e339d69341e76fbf031","modified":1560516826216},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"a4c6ee546a94fd69e5b7a1e4c054ab8cacb73d2a","modified":1560516826211},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"40144394fdfe05d400f39f6763f66f75479a2e34","modified":1560516826228},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1560516826203},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9c6194177533619a6f6685dc7e890dcbec456241","modified":1560516826229},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"afdd21533db18d846e1a2663b1199761b1bd2c1e","modified":1560516826216},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"35c6fd7eab3779bd9e38b7ba8825ab0c67a1be7a","modified":1560516826228},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"fec36a14080104b5862e9f021eab117d87c5f7c5","modified":1560516826229},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"107f42aa590ec4ba0765a0bc5d735f0f09edc0ff","modified":1560516826230},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1560516826229},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1560516826233},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"a609ff811f2b2764f5470236fe2fb1f3aa6ccba5","modified":1560516826232},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1560516826233},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"8da8416213127595dfc4d2b358639194647e7bd3","modified":1560516826233},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"40f266e24af4dedc9497056ab18ebcfda38dd47d","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"06d9d00257abd28414ec0b746f866bf9911cf5ec","modified":1560516826234},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"25f05ed8da68d034dce7f06e0f20f6cd55841070","modified":1560516826235},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"157e6915dcf5990566e463acffa71043b2651c07","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"234b44cfd03f9c9e3e179ff5fd698ac876341913","modified":1560516826237},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"04706657af638f2746ae59520e6fc78577c7682c","modified":1560516826236},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"b9619c9827f969ca2e2f5878552362a7b858918f","modified":1560516826239},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"e73d6da74c5755442e831d8fd7d922c5b32bd892","modified":1560516826239},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1560516826238},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0d6f0df798449b710e1e5dbd43d470089b2a3c95","modified":1560516826240},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"5b5e0a02a7bf63de9efcd33a4e482939cce5822d","modified":1560516826240},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0b3001909f3446843b226030524ea8498d4d8997","modified":1560516826239},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1560516826257},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1560516826256},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"adb7379f3b9001840eb38b260434e89365771a81","modified":1560516826240},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1560516826257},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"2df409df04fdb52d7234876a9f6e502edd4e3929","modified":1560516826205},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"024e8ff40ca881c6fbf45712897e22f58a3811ab","modified":1560516826207},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"9c1a082e6c1f96187a099c3f4cb5424c0c9fd06e","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"a8f4d4b86acaa34c99111b2dde5d0779cc7e0de6","modified":1560516826205},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"33200f60bd6a8bbfc66dd49a239bcc75c2f564c1","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"3a0efe849587b34f20d4e260028dc799215b0bb3","modified":1560516826207},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"e5a5f8747fdf2ca960e4e73c081b8952afd62224","modified":1560516826208},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"b8647d6140141b0a160607f6353e4d4594cca92e","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"c9cfb4b99e1ec8ec9cf075cb761b8f7fa5fe63fd","modified":1560516826206},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"9a8fb61bd2d184de9d206e62ba8961d1845c5669","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"c27527cdeb9e3a9f447f7238f442a5dc33fde4e6","modified":1560516826210},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fa1cea6fcc3f552d57cc7d28380a304859139bf6","modified":1560516826209},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"c97c819a65f6967485184399397601e5133deda6","modified":1560516826210},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1560516826211},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"fc94dd09b4245143b452d6cf2fc4c12134d99d6d","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"f3b0d259e991ac86454ae5eac6bc94dc8691d8c9","modified":1560516826211},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a73346f999b31355075cd58637946a8950cf6f7e","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"b6a241626783d2ac115d683fd59ec283af68e5bb","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"f14cefc99309934d4103a3aa785e1258d858813f","modified":1560516826212},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"4aad8e36178faaa71a767af0084d578df4c09f73","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d77f85d3af2d7090d84b28ab01c6a49f92eec647","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"13d365ee626c01f17ec664b3f54f51d8b9ee7cf4","modified":1560516826213},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ccd0b1309acff0c676fdcc848a8ae2d05f0369ab","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"c8009fd9598a661b7d23158b5121b6ac266939e9","modified":1560516826215},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1560516826214},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"981795aad232c8bd3f52a0ed8720db696d18a234","modified":1560516826215},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"539fc0880b2e035e8316d5d4b423703195c1b7ba","modified":1560516826215},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"a5484d8436b2b7862faf6e7309a9e7b88cdd0027","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"ab1776c5dc537beabb0ab81a0f04e08bebad070b","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"615fca7dff197a2ca3df674cf963ce70b8525985","modified":1560516826216},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"e58bb8b7127aa21e8260493a425ec00fcb25d338","modified":1560516826218},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"b36eea093bd4b32056b5de6f370ff57e50b25a49","modified":1560516826218},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"0eadef0381f696de7f88d7dc5f0ddc3cd5d309b3","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"9204c79c05d620ecd5d411cdf11e27441b6281dc","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"03a4e75e963e3e7cc393d588b1495a88d52e0e40","modified":1560516826217},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"7e2ba73073daaea0a18c3d67ff137dd683af7011","modified":1560516826219},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"ed3a2960ebce7396d1893bb8e08c99c7d9259140","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"3cb387fa70017f3c24a1a1884461d29deda54585","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"fde59300ec38868676ff5ed495b9dc9b02d07ffc","modified":1560516826220},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"3baeeb51cfe123e99235ee1816d0e1f6a97c7852","modified":1560516826222},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"b43421291bf85b589e8d0ec853e238d36ab80631","modified":1560516826221},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"020fac447d7a17c03e2802f0f724ae0738088354","modified":1560516826222},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"da7a21f5a2f7dcf4c5a4788d7670159ca4132b65","modified":1560516826223},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"1df9d36e2b0e9c94e0a959acc136026405ae0d73","modified":1560516826223},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"4305813408a1cd6aba764a7769b94b081d383d4f","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"d9c244b1c3a09a7fccd3c3f732e6fb112a8cd565","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"2a1008f1044b450b806adc166754ba9513e68375","modified":1560516826225},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"2fbe52f955da41c7a14eb09918bf86a252e4504f","modified":1560516826224},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"ed8a12982c0497eeb9d7642781abeb801428f83d","modified":1560516826225},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"6880467b4f6d7b057fb8291aa10966429a0a3bff","modified":1560516826226},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"9801977a23268e36c5deefd270423f6f1a0c3bb2","modified":1560516826226},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"35dc9f3990fadff3ea038d4e8ac75923219886ed","modified":1560516826226},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"7cf42f96ba6b249c75e00dad251ebacf7de61e6c","modified":1560516826227},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1560516826264},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1560516826236},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1560516826238},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1560516826235},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1560516826261},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1560516826261},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1560516826260},{"_id":"public/categories/index.html","hash":"f41bf0055d098f80209e3b9ff1b090aa65853170","modified":1563448988295},{"_id":"public/tags/index.html","hash":"d29482622cfa9fbbcb8b951d9deff7bfc82eff9d","modified":1563448988304},{"_id":"public/2019/07/11/cpp-aux-tools/index.html","hash":"41634b4465978111f03470d183c078dfbcc4ce98","modified":1563448988305},{"_id":"public/archives/2019/07/index.html","hash":"5ff75f489c1fbab13d4ffb56bb4e7557e1fcc095","modified":1563448988306},{"_id":"public/categories/DL-Framework/index.html","hash":"0d3deaf64b04d4a4d69ffdde04ee76d0018db9ac","modified":1563448988306},{"_id":"public/tags/object-detection/page/2/index.html","hash":"97941af810a283762d5fcf8b7004b2818b053c35","modified":1563448988306},{"_id":"public/tags/tool/index.html","hash":"ef5cc1915861f63b83cbe56783604f866869f439","modified":1563448988306},{"_id":"public/tags/c/index.html","hash":"29ecc885fc9ffe0e9ce5b5b4a48d77b8840cecb7","modified":1563448988306},{"_id":"public/tags/CV/index.html","hash":"0f4f4f8c7e90c943ee8b0df27075ce78cdae5e91","modified":1563448988306},{"_id":"public/2019/07/15/DeRPN/index.html","hash":"1b3fb840408709509d2f3e7a18799e2bf64abf0a","modified":1563448988306},{"_id":"public/2019/07/08/mask-rcnn/index.html","hash":"5ec88287d5e67de9a20aa69b6c5491c8c37d6c79","modified":1563448988306},{"_id":"public/2019/07/08/DSOD/index.html","hash":"48b9d5d698ab16b397e27c5327ffd91c260d0f6b","modified":1563448988307},{"_id":"public/2019/07/03/libra-rcnn/index.html","hash":"a57fff8e64092a2efe564db3e57e26540f25c3c2","modified":1563448988307},{"_id":"public/2019/07/02/gcc-src/index.html","hash":"7fae71eb516d68dbfa1995b3fd7ca0ffcd1bc809","modified":1563448988307},{"_id":"public/2019/06/28/M2Det/index.html","hash":"1f658df575d969a193351a6e4d4e91270db4d47e","modified":1563448988307},{"_id":"public/2019/06/28/BBox-Reg-Uncertainty/index.html","hash":"27d2e1bb2c7af4121e79f823edf5b0093bfcc348","modified":1563448988307},{"_id":"public/2019/06/27/FSAF/index.html","hash":"81290ad72a11630d06a86e3cb28b32c08c6aacc3","modified":1563448988307},{"_id":"public/2019/06/25/GA-RPN/index.html","hash":"a7da2e9ab0142ba3e87f69f96ad2f31ff76cea65","modified":1563448988307},{"_id":"public/2019/06/24/cv-mtds/index.html","hash":"f72bb8926b56ccef78bf1007408391e75b9e016e","modified":1563448988307},{"_id":"public/2019/06/21/TridentNet/index.html","hash":"b6ce484b7abc986f45c5ccb446bb00d73b92fe5c","modified":1563448988307},{"_id":"public/2019/06/18/PyTorch-3/index.html","hash":"01cd62c7abfb1cd4560db54697dd8d6540168ec6","modified":1563448988307},{"_id":"public/2019/06/16/mAP/index.html","hash":"04f41482fc69c4babf1d92ca012be036cbb4f34c","modified":1563448988307},{"_id":"public/2019/06/13/GIoU/index.html","hash":"906d701931353f6b1e378bb6caadc4ddb27e8cd7","modified":1563448988307},{"_id":"public/2019/06/13/PyTorch-2/index.html","hash":"6aeaf4e0bedc9bcaa4ebe84597d537be50d594ee","modified":1563448988307},{"_id":"public/2019/06/13/Hexo-Sync/index.html","hash":"29246230827f62f71f60e579620005e0c50b0ef3","modified":1563448988307},{"_id":"public/2019/06/12/PyTorch-1/index.html","hash":"89747635130f8085cbede415c5732ea018f6f0a8","modified":1563448988307},{"_id":"public/archives/index.html","hash":"7ea61ad483c752e42872e3ba4df8ca3fbc5326ed","modified":1563448988307},{"_id":"public/archives/2019/index.html","hash":"35f015521fa2fc11ac7cfe82dc8281a659166f8d","modified":1563448988307},{"_id":"public/archives/2019/06/index.html","hash":"28a27c1b55ceadc2e03a7eb26f81752e03dc1949","modified":1563448988308},{"_id":"public/index.html","hash":"7b7bb692b41d14054fa9bdec7d23b7d2e9bd2ef8","modified":1563448988308},{"_id":"public/page/2/index.html","hash":"683bc7efc522f88eae0af604799dbc5c66bbfba8","modified":1563448988308},{"_id":"public/tags/object-detection/index.html","hash":"b68b6b7e1f36a05031461ed15b930c91f0af531c","modified":1563448988308},{"_id":"public/archives/page/2/index.html","hash":"e479c63dbd1999344448c6ff56e58bde74528792","modified":1563448988318},{"_id":"public/archives/2019/page/2/index.html","hash":"99c81b269492f6959f8f7f3ca2016cbaf65015c3","modified":1563448988318},{"_id":"public/page/3/index.html","hash":"be36fa5eeb800469d5eb0b369c43fa1819cfc000","modified":1563448988318},{"_id":"public/tags/PyTorch/index.html","hash":"cd4f519678429db4be2e0b61c072d80f1f4a3e66","modified":1563448988318},{"_id":"public/2019/07/17/RepPoints/index.html","hash":"6791e95578f2e4e7fa8a2fd0a560d57746c6d110","modified":1563448988318},{"_id":"public/2019/07/17/DetNet/index.html","hash":"41d4aa8fd6fe6b8b5e4b9bf15216576922ecdd69","modified":1563448988318},{"_id":"public/2019/07/16/loss/index.html","hash":"2fc1aad1d94d664a8838b265e654d1013745b2f3","modified":1563448988318},{"_id":"public/images/FSAF_fig3.png","hash":"c792cc01936994074b8e6fce3b9884801d7e9f4b","modified":1563448988325},{"_id":"public/images/FSAF_fig4.png","hash":"a7fd53318b0879dba659aa13e5bb8e87ddaa57f3","modified":1563448988325},{"_id":"public/images/GA-RPN_fig3.png","hash":"12f2d60e449b7213de492d18a2b0ada527788d94","modified":1563448988325},{"_id":"public/images/M2Det_fig3.png","hash":"1a1f004e3ae0786c11e9ae915a229b41634e7e9f","modified":1563448988325},{"_id":"public/images/mAP_fig5.png","hash":"8727119e5d2a352699c59a7ef5429a2d30b3cef9","modified":1563448988325},{"_id":"public/images/mask-rcnn_fig3.png","hash":"593aa6f15b90d819e5690a2cd8befa3ba24ebccf","modified":1563448988325},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1563448988325},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1563448988325},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1563448988325},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1563448988325},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1563448988325},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1563448988325},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1563448988325},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1563448988325},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1563448988325},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1563448988325},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1563448988325},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1563448988326},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1563448988326},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1563448988326},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1563448988326},{"_id":"public/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1563448988326},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1563448988326},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1563448988326},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1563448988326},{"_id":"public/images/DetNet_fig1.png","hash":"0be4e55fcb24f78c0ae7e0abd848c863858c907e","modified":1563448988326},{"_id":"public/images/DetNet_fig2.png","hash":"dffd982d9558203676d5df62ded8eb2f4fa314d1","modified":1563448988326},{"_id":"public/images/BBox-reg_fig3.png","hash":"941dbd9edad7d9e4961866872d93c87f35ae1c40","modified":1563448988799},{"_id":"public/images/FSAF_fig2.png","hash":"5980bbb5993d141c28aecc700f01bdbaf793d018","modified":1563448988799},{"_id":"public/images/FSAF_fig5.png","hash":"afd140d4277cdf34e02d9607a950e805c33398e8","modified":1563448988800},{"_id":"public/images/FSAF_fig6.png","hash":"6d4ddfb5ca31af495ad5ba5f80d66f8cb0c5e79b","modified":1563448988800},{"_id":"public/images/TridentNet_fig1(c).png","hash":"3c3ec6c57f7ebb7180af2c2dc0649857ac37ad9b","modified":1563448988800},{"_id":"public/images/libra-rcnn_fig3.png","hash":"29973e85ddcc86bea3fb84d725471e061a479e75","modified":1563448988800},{"_id":"public/images/mAP_fig1.png","hash":"e1e227bb2bf05159c46b812d4acb9683631e8aba","modified":1563448988800},{"_id":"public/images/libra-rcnn_figa.png","hash":"761b2d9d6c57c8279ee717b4b011a19c987e5259","modified":1563448988800},{"_id":"public/images/mAP_fig2.png","hash":"6fe61598c855d8086c10c04187c9956d7b14b5be","modified":1563448988800},{"_id":"public/images/mAP_fig3.png","hash":"5289afe8e4ec978a94571d7d001563713404155f","modified":1563448988800},{"_id":"public/images/mAP_fig4.png","hash":"a13992fc8671dd3c1f5a44a9cd714176f580681d","modified":1563448988800},{"_id":"public/images/GA-RPN_fig1.png","hash":"3879e968e038df6b23ad8793a1dc5933b14454ac","modified":1563448988800},{"_id":"public/images/GA-RPN_fig2.png","hash":"fb3585918ffff3813d462bd78799b847989598c7","modified":1563448988800},{"_id":"public/images/GIoU_fig1.png","hash":"8b5d938397e94b905245363c4dee5f4266e734b9","modified":1563448988800},{"_id":"public/images/M2Det_fig1.png","hash":"002f75d4ab7fa714afbf0f8d64fadb7ec1799775","modified":1563448988801},{"_id":"public/images/TridentNet_fig2.png","hash":"e186f0cd983d28f81df120a01c8611e8f9accdaa","modified":1563448988801},{"_id":"public/images/TridentNet_fig3.png","hash":"52502eade50078f68ca751915681e536a96094b4","modified":1563448988801},{"_id":"public/images/libra-rcnn_fig1.png","hash":"4150832cd9a4c2f9532ad0309e299154fd581d87","modified":1563448988801},{"_id":"public/images/libra-rcnn_fig2.png","hash":"be628467551b790e0cd0e9e3cdd1f041548f60db","modified":1563448988801},{"_id":"public/images/libra-rcnn_fig4.png","hash":"cc0dafa1503dad25981ed08a5f33431a5ce40e93","modified":1563448988801},{"_id":"public/images/libra-rcnn_fig5.png","hash":"33795e57abe6b93ce0e70a46dbb37c6348308c2e","modified":1563448988801},{"_id":"public/images/mask-rcnn_fig1.png","hash":"6ee2989bcd121be915722f0d7ae9ba97039681b6","modified":1563448988801},{"_id":"public/images/mask-rcnn_fig4.png","hash":"a6000ce7a01688cefdd8af1d3c1bab492954320c","modified":1563448988801},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1563448988801},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1563448988801},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1563448988801},{"_id":"public/images/RepPoints_fig2.png","hash":"ef1068b6bf393ea829f199e37e5f14edc70641c3","modified":1563448988802},{"_id":"public/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1563448988812},{"_id":"public/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1563448988812},{"_id":"public/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1563448988812},{"_id":"public/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1563448988812},{"_id":"public/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1563448988813},{"_id":"public/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1563448988813},{"_id":"public/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1563448988813},{"_id":"public/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1563448988813},{"_id":"public/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1563448988813},{"_id":"public/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1563448988813},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1563448988813},{"_id":"public/css/main.css","hash":"b772bac01b39b92a98fcfa7371a69181d94b2a9a","modified":1563448988813},{"_id":"public/images/BBox-reg_fig4.png","hash":"651ed85e9511c2ea9ff0c9c76248e71a950c41d4","modified":1563448988813},{"_id":"public/images/TridentNet_fig1(b).png","hash":"03489e4b700364b62b4693418137113f38f03c82","modified":1563448988813},{"_id":"public/images/BBox-reg_fig2.png","hash":"645d710112f0a5b25f1d34020c1f7e56cfc2ad62","modified":1563448988813},{"_id":"public/images/DeRPN_fig1.png","hash":"09d707a0fd6534c106e4f503c104c36aed31293e","modified":1563448988813},{"_id":"public/images/M2Det_fig4.png","hash":"27227839728d56d5c61c57629546b8b7e4d0fb00","modified":1563448988813},{"_id":"public/images/TridentNet_fig1(a).png","hash":"89a0e081b4dba10cbeca8e1af76d23cfa203eb72","modified":1563448988813},{"_id":"public/images/DeRPN_fig2.png","hash":"30ef45adc8c9335bc896b5e2ec09b4096d17ec67","modified":1563448988813},{"_id":"public/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1563448988820},{"_id":"public/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1563448988820},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1563448988821},{"_id":"public/images/GIoU_fig2.png","hash":"874dbb5acd431fd27c0be189513190d30449b57c","modified":1563448988821},{"_id":"public/images/GA-RPN_fig4.png","hash":"7d7c11a72c263bdc87e8898b4018d3a96e6f7f1b","modified":1563448988821},{"_id":"public/images/M2Det_fig2.png","hash":"335a17497636d7b03535a133a982f2e89266cea7","modified":1563448988821},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1563448988821},{"_id":"public/images/BBox-reg_fig1.png","hash":"1458cac1708ced6ad430da020ae1c4c84a0ddc6e","modified":1563448988823},{"_id":"public/images/RepPoints_fig1.png","hash":"132a00a2205e81d684269a7a89ff3f61ac520217","modified":1563448988823},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1563448988825},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1563448988859},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1563448988859},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1563448988860},{"_id":"public/images/DSOD_fig1.png","hash":"19c19bc5ee6dadb3498de545b5eae190a17309ba","modified":1563448988860},{"_id":"public/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1563448988866},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1563448988868}],"Category":[{"name":"DL Framework","_id":"cjy8l5pz50018xgvcu51xvdx6"}],"Data":[],"Page":[{"title":"分类","date":"2019-06-14T13:23:43.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2019-06-14 21:23:43\ntype: categories\n---\n","updated":"2019-06-14T13:24:24.346Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjy8l5ps30001xgvcm6he5sw0","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","date":"2019-06-14T13:23:22.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2019-06-14 21:23:22\ntype: tags\n---\n","updated":"2019-06-14T13:24:17.742Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjy8l5pti000jxgvc6va31vnr","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"DetNet","date":"2019-07-17T02:05:50.000Z","mathjax":true,"_content":"论文 [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n\n本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。\n\nDetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。\n\n# DetNet\n如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：\n![](/images/DetNet_fig1.png)<center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center>\n\n1. 网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。\n2. 大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。\n3. 小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。\n\nDetNet 经过如下设计可解决以上问题：\n1. 直接为目标检测量身定制 stage 的数量\n2. 即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。\n\n## DetNet 设计\n使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1, 64 \\\\\\\\ 3 \\times 3, 64 \\\\\\\\ 1 \\times 1, 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1, 128 \\\\\\\\ 3 \\times 3, 128 \\\\\\\\ 1 \\times 1, 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1, 256 \\\\\\\\ 3 \\times 3, 256 \\\\\\\\ 1 \\times 1, 1024\\end{bmatrix} \\times 6$|\n\n从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：\n![](/images/DetNet_fig2.png)<center>fig 2. DetNet 的结构细节</center>\n\n1. 从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。\n2. 从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。\n3. bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64->128->256->512）。\n\nDetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。\n\n# 实验\n实验和结果分析，略\n","source":"_posts/DetNet.md","raw":"---\ntitle: DetNet\ndate: 2019-07-17 10:05:50\ntags: object detection\nmathjax: true\n---\n论文 [DetNet: A Backbone network for Object Detection](https://arxiv.org/abs/1804.06215)\n\n本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。\n\nDetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。\n\n# DetNet\n如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：\n![](/images/DetNet_fig1.png)<center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center>\n\n1. 网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。\n2. 大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。\n3. 小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。\n\nDetNet 经过如下设计可解决以上问题：\n1. 直接为目标检测量身定制 stage 的数量\n2. 即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。\n\n## DetNet 设计\n使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，\n|   ResNet        | output size | 50-layer             |\n|:--------:       | :------:    |   :-------:          |\n| conv1           | 112x112     | 7x7,64, stride 2     |\n|   maxpool       | 56x56       | 3x3, stride 2        |\n| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1, 64 \\\\\\\\ 3 \\times 3, 64 \\\\\\\\ 1 \\times 1, 256\\end{bmatrix} \\times 3$|\n|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1, 128 \\\\\\\\ 3 \\times 3, 128 \\\\\\\\ 1 \\times 1, 512\\end{bmatrix} \\times 4$|\n|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1, 256 \\\\\\\\ 3 \\times 3, 256 \\\\\\\\ 1 \\times 1, 1024\\end{bmatrix} \\times 6$|\n\n从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：\n![](/images/DetNet_fig2.png)<center>fig 2. DetNet 的结构细节</center>\n\n1. 从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。\n2. 从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。\n3. bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64->128->256->512）。\n\nDetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。\n\n# 实验\n实验和结果分析，略\n","slug":"DetNet","published":1,"updated":"2019-07-17T07:20:45.979Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5prw0000xgvcwe1v6lrj","content":"<p>论文 <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>\n<p>本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。</p>\n<p>DetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。</p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p>如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：<br><img src=\"/images/DetNet_fig1.png\" alt><center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center></p>\n<ol>\n<li>网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。</li>\n<li>大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。</li>\n<li>小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。</li>\n</ol>\n<p>DetNet 经过如下设计可解决以上问题：</p>\n<ol>\n<li>直接为目标检测量身定制 stage 的数量</li>\n<li>即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。</li>\n</ol>\n<h2 id=\"DetNet-设计\"><a href=\"#DetNet-设计\" class=\"headerlink\" title=\"DetNet 设计\"></a>DetNet 设计</h2><p>使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，<br>|   ResNet        | output size | 50-layer             |<br>|:——–:       | :——:    |   :——-:          |<br>| conv1           | 112x112     | 7x7,64, stride 2     |<br>|   maxpool       | 56x56       | 3x3, stride 2        |<br>| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1, 64 \\\\ 3 \\times 3, 64 \\\\ 1 \\times 1, 256\\end{bmatrix} \\times 3$|<br>|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1, 128 \\\\ 3 \\times 3, 128 \\\\ 1 \\times 1, 512\\end{bmatrix} \\times 4$|<br>|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1, 256 \\\\ 3 \\times 3, 256 \\\\ 1 \\times 1, 1024\\end{bmatrix} \\times 6$|</p>\n<p>从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：<br><img src=\"/images/DetNet_fig2.png\" alt><center>fig 2. DetNet 的结构细节</center></p>\n<ol>\n<li>从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。</li>\n<li>从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。</li>\n<li>bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64-&gt;128-&gt;256-&gt;512）。</li>\n</ol>\n<p>DetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验和结果分析，略</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1804.06215\" target=\"_blank\" rel=\"noopener\">DetNet: A Backbone network for Object Detection</a></p>\n<p>本文创作动机是当前大多数的目标检测器都是在 ImageNet 上预训练后 finetune 到目标检测集，目标检测器的 backbone 原本是为了图像分类任务而设计的，这样的 backbone 显然不是最佳的，较大的下采样率带来较大的感受野 RF，这对图像分类是有益的，对目标检测尤其是小目标而言则是不利的，所以像 FPN 和 RetinaNet 就使用了额外的网络结构（extra stage）来处理目标的多尺度问题，但是这总归不是一个优雅的解决办法，所以本文提出了 DetNet，这是一个专为目标检测而设计的新型 backbone。</p>\n<p>DetNet 保持了 FPN 中的额外网络结构（extra stage），毕竟是目标的多尺度问题的一个较为不错的解决方案。与 FPN 等基于 ImageNet 预训练的目标检测器不同的是，DetNet 的深层依然有较高的空间分辨率，不过考虑到高分辨率与计算资源的矛盾，我们采用了一种低复杂度的 dilated bottleneck 结构。</p>\n<h1 id=\"DetNet\"><a href=\"#DetNet\" class=\"headerlink\" title=\"DetNet\"></a>DetNet</h1><p>如图 1(A) 是 FPN 的部分网络结构，图像分类任务和目标分类任务本身就存在很大的不同，并且基于此结构的模型训练还存在以下问题：<br><img src=\"/images/DetNet_fig1.png\" alt><center>A. 具有传统的 backbone 的 FPN 结构；B. 图像分类中传统的 backbone；C. DetNet 的 backbone，比 FPN 的分辨率高</center></p>\n<ol>\n<li>网络 stage 的数量不同。图像分类的网络包含 5 个 stages，每个 stage 下采样率为 2，故输出分辨率为 32 倍的下采样，而 FPN 拥有更多的 stages，比如增加 P6 以处理更大的目标，在 RetinaNet 中也同样增加了 P6 和 P7。</li>\n<li>大目标的可视性较差。具有 32 的步幅的 feature map 包含较强的语义信息，然而这对目标定位是不利的，FPN 中大目标是由较深 layer 进行预测，难以回归到准确的目标边界。</li>\n<li>小目标的不可见性。大的步幅显然会导致小目标的丢失，所以 FPN 在较浅 layer 上预测小目标，然而浅 layer 只有很弱的语义信息，可能不足以预测目标分类，故为了加强浅 layer 的目标分类能力，将深 layer 的特征上采样后合并进浅层特征，如图 1 A 所示，只不过，如果小目标在较深 layer 中已经丢失，那么深层特征上就没有小目标的 context 信息，这样的深层特征合并进浅层特征并不会增强对小目标的分类能力。</li>\n</ol>\n<p>DetNet 经过如下设计可解决以上问题：</p>\n<ol>\n<li>直接为目标检测量身定制 stage 的数量</li>\n<li>即使 stage 的数量很多，如 6~7 个 stage，对于 deep layer，在保持较大感受野（有利于分类）的同时有较大的分辨率（有利于目标定位）。</li>\n</ol>\n<h2 id=\"DetNet-设计\"><a href=\"#DetNet-设计\" class=\"headerlink\" title=\"DetNet 设计\"></a>DetNet 设计</h2><p>使用 ResNet-50 作为 baseline。在 ResNet-50 的基础之上构建 DetNet-59（类似地也可以在 ResNet-101 基础上构建 DetNet，在本文中这不是重点）。DetNet 的 stage 1,2,3,4 与 ResNet-50 的 stage 1,2,3,4 完全相同。这里给出 ResNet-50 前四个 stage 的结构描述，<br>|   ResNet        | output size | 50-layer             |<br>|:——–:       | :——:    |   :——-:          |<br>| conv1           | 112x112     | 7x7,64, stride 2     |<br>|   maxpool       | 56x56       | 3x3, stride 2        |<br>| conv2_x         | 56x56       | $\\begin{bmatrix} 1 \\times 1, 64 \\\\ 3 \\times 3, 64 \\\\ 1 \\times 1, 256\\end{bmatrix} \\times 3$|<br>|conv3_x          | 28x28       | $\\begin{bmatrix} 1 \\times 1, 128 \\\\ 3 \\times 3, 128 \\\\ 1 \\times 1, 512\\end{bmatrix} \\times 4$|<br>|conv4_x          | 14x14       | $\\begin{bmatrix} 1 \\times 1, 256 \\\\ 3 \\times 3, 256 \\\\ 1 \\times 1, 1024\\end{bmatrix} \\times 6$|</p>\n<p>从第五个 stage 开始介绍 DetNet，如图 2 D 所示，DetNet-59 的设计细节如下：<br><img src=\"/images/DetNet_fig2.png\" alt><center>fig 2. DetNet 的结构细节</center></p>\n<ol>\n<li>从上图中可见，我们在 backbone 中引入了 extra stage，即 P6，与 FPN 中一样，也是用于目标检测，只不过，从 stage 4 开始，我们就固定了步幅 16，即每个 stage 的输出空间大小。</li>\n<li>从 stage 4 开始的空间大小就固定不变，本文引入一种 dilated bottleneck 和 1x1 卷积并列的结构，用于之后每个 stage 的最开始，如图 2 B。</li>\n<li>bottleneck 中的 dilated conv 可以增大感受野。由于 dilated conv 较为耗时，所以 stage 5 和 6 的 channel 与 stage 4 保持相同（维持在256），这一点与传统 backbone 设计不一样，传统 backbone 的后一个 stage 的 channel 是前一个 stage 的两倍（如 ResNet-50 中的 64-&gt;128-&gt;256-&gt;512）。</li>\n</ol>\n<p>DetNet 作为 backbone 可以很方便地移植到（具有/不具有 feature pyramid 的）目标检测器中。不失代表性地，我们采用 FPN 作为主检测器，除了 backbone 不同，其他结构与原先 FPN 中保持相同。由于 stage 4 之后的 stage 输出大小不变，所以将 stage 4,5,6 的输出相加，如图 2 E。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验和结果分析，略</p>\n"},{"title":"Hexo Sync","date":"2019-06-13T01:57:11.000Z","_content":"\n场景：\n```\n在A, B两台电脑上同步Hexo博客\n```\n假设在 computer B 上已经初次建立hexo博客 https://shajian.github.io， computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\n在github仓库 shajian.github.io 上新建branch，比如\"hexo\"，这样，\"mater\"主分支用于维护hexo生成的静态博客文件/目录，\"hexo\"分支用于维护hexo部署环境下的所有文件/目录。\n\n在 computer A 上 clone 这个仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n然后可在使用\n```\nhexo new \"<title>\"\n```\n写新文章或直接去source/_posts下修改已有文章，\n部署\n```\nhexo g -d\n```\n然后提交到仓库的hexo分支，进行备份\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n然后就可以去 https://shajian.github.io 浏览本地新增/修改文章内容了。\n\n在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行\n```\n$ npm install\n```\n此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。\n\ncomputer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新\n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n切换到 hexo 分支后，可以进行修改和新增文章了。\n\n由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行\n```\nhexo g -d\n```\n由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。","source":"_posts/Hexo-Sync.md","raw":"---\ntitle: Hexo Sync\ndate: 2019-06-13 9:57:11\ntags: tool\n---\n\n场景：\n```\n在A, B两台电脑上同步Hexo博客\n```\n假设在 computer B 上已经初次建立hexo博客 https://shajian.github.io， computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：\n```\n_config.yml\ndb.json\nnode_modules\npackage.json\npackage-lock.json\npublic\nscaffolds\nsource\nthemes\n```\n在github仓库 shajian.github.io 上新建branch，比如\"hexo\"，这样，\"mater\"主分支用于维护hexo生成的静态博客文件/目录，\"hexo\"分支用于维护hexo部署环境下的所有文件/目录。\n\n在 computer A 上 clone 这个仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n$ git branch\n* hexo\n  master\n```\n将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，\n```\n$ rm -rf .\n# do not use \"cp -R path/to/myblog/* ./\" which ignores hidden files/directories\n$ cp -R path/to/myblog/. ./\n```\n然后可在使用\n```\nhexo new \"<title>\"\n```\n写新文章或直接去source/_posts下修改已有文章，\n部署\n```\nhexo g -d\n```\n然后提交到仓库的hexo分支，进行备份\n```\n$ git add .\n$ git commit -m \"new post 'title'\"\n$ git push origin hexo\n```\n\n然后就可以去 https://shajian.github.io 浏览本地新增/修改文章内容了。\n\n在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，\n```\n$ git clone https://github/shajian/shajian.github.io.git\n$ cd shajian.github.io\n$ git checkout hexo\n```\n如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行\n```\n$ npm install\n```\n此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。\n\ncomputer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新\n```\n$ git checkout master\n$ git pull origin master\n$ git checkout hexo\n$ git pull origin hexo\n```\n切换到 hexo 分支后，可以进行修改和新增文章了。\n\n由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行\n```\nhexo g -d\n```\n由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。","slug":"Hexo-Sync","published":1,"updated":"2019-06-14T13:20:32.167Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ps40002xgvc4fodk46g","content":"<p>场景：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在A, B两台电脑上同步Hexo博客</span><br></pre></td></tr></table></figure>\n\n<p>假设在 computer B 上已经初次建立hexo博客 <a href=\"https://shajian.github.io，\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io，</a> computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n\n<p>在github仓库 shajian.github.io 上新建branch，比如”hexo”，这样，”mater”主分支用于维护hexo生成的静态博客文件/目录，”hexo”分支用于维护hexo部署环境下的所有文件/目录。</p>\n<p>在 computer A 上 clone 这个仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n\n<p>将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path/to/myblog/* ./&quot; which ignores hidden files/directories</span><br><span class=\"line\">$ cp -R path/to/myblog/. ./</span><br></pre></td></tr></table></figure>\n\n<p>然后可在使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>写新文章或直接去source/_posts下修改已有文章，<br>部署</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>然后提交到仓库的hexo分支，进行备份</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &apos;title&apos;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>然后就可以去 <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> 浏览本地新增/修改文章内容了。</p>\n<p>在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n\n<p>如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n\n<p>此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。</p>\n<p>computer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>切换到 hexo 分支后，可以进行修改和新增文章了。</p>\n<p>由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>场景：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在A, B两台电脑上同步Hexo博客</span><br></pre></td></tr></table></figure>\n\n<p>假设在 computer B 上已经初次建立hexo博客 <a href=\"https://shajian.github.io，\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io，</a> computer B 本地的文件夹（hexo部署环境目录）为 path/to/myblog，其内部文件/目录如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_config.yml</span><br><span class=\"line\">db.json</span><br><span class=\"line\">node_modules</span><br><span class=\"line\">package.json</span><br><span class=\"line\">package-lock.json</span><br><span class=\"line\">public</span><br><span class=\"line\">scaffolds</span><br><span class=\"line\">source</span><br><span class=\"line\">themes</span><br></pre></td></tr></table></figure>\n\n<p>在github仓库 shajian.github.io 上新建branch，比如”hexo”，这样，”mater”主分支用于维护hexo生成的静态博客文件/目录，”hexo”分支用于维护hexo部署环境下的所有文件/目录。</p>\n<p>在 computer A 上 clone 这个仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git branch</span><br><span class=\"line\">* hexo</span><br><span class=\"line\">  master</span><br></pre></td></tr></table></figure>\n\n<p>将目录 shajian.github.io 内的所有文件/目录全部删除，然后将 path/to/myblog内的全部内容复制过来，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf .</span><br><span class=\"line\"># do not use &quot;cp -R path/to/myblog/* ./&quot; which ignores hidden files/directories</span><br><span class=\"line\">$ cp -R path/to/myblog/. ./</span><br></pre></td></tr></table></figure>\n\n<p>然后可在使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &quot;&lt;title&gt;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>写新文章或直接去source/_posts下修改已有文章，<br>部署</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>然后提交到仓库的hexo分支，进行备份</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m &quot;new post &apos;title&apos;&quot;</span><br><span class=\"line\">$ git push origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>然后就可以去 <a href=\"https://shajian.github.io\" target=\"_blank\" rel=\"noopener\">https://shajian.github.io</a> 浏览本地新增/修改文章内容了。</p>\n<p>在 computer B 上删除 path/to/myblog 目录，然后重新 clone 仓库，并切换到 hexo 分支，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github/shajian/shajian.github.io.git</span><br><span class=\"line\">$ cd shajian.github.io</span><br><span class=\"line\">$ git checkout hexo</span><br></pre></td></tr></table></figure>\n\n<p>如果仓库有 .gitignore 文件且包含 node_modules 目录，则执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n\n<p>此时，要修改还是新增文章，步骤均与上面 computer A上的操作一致。</p>\n<p>computer A 和 B 本地均有仓库后，以后每次修改还是新增文章，首先需要将仓库更新到最新</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git pull origin master</span><br><span class=\"line\">$ git checkout hexo</span><br><span class=\"line\">$ git pull origin hexo</span><br></pre></td></tr></table></figure>\n\n<p>切换到 hexo 分支后，可以进行修改和新增文章了。</p>\n<p>由于 .depoly_git 下其实就是对应 master 主分支的内容，这也是一个git 仓库目录，内含 .git 文件夹，所以提交的时候无法提交 .deploy_git 内部的文件/目录，不过这个没关系，例如前面，在 computer B 上 clone 仓库后，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n\n<p>由 hexo 向 .deploy_git 填充生成的文件/目录，而不需要在 hexo 分支上备份这些内容。</p>\n"},{"title":"RepPoints","date":"2019-07-17T08:05:41.000Z","mathjax":true,"_content":"论文 [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n\n大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。\n\n如图 1，\n![](/images/RepPoints_fig1.png)\n\nRepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。~~RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。~~\n\n# RepPoints\n## BBox 表示\n目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。\n\n坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n回归损失使用 smooth L1 损失。\n\n## RepPoints\n前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，\n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n其中 n 为采样点数量，本文设置为 9。\n\nRepPoints 的坐标调整可表示为，\n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n其中 $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。\n\n__RepPoints 变换到 gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，\n1. Min-max function  \n   $\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。\n\n__RepPoints 的学习__ 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。\n\n# RPDet\n类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，\n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector) 结构如图 2，\n![](/images/RepPoints_fig2.png)\n\n其中 N 表示 RepPoints 的数量。\n\n采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。\n\n得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。\n\n## 与可变形 RoI pooling 的关系\n可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。\n\n# 实验\n略\n\n# 结论\n本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。","source":"_posts/RepPoints.md","raw":"---\ntitle: RepPoints\ndate: 2019-07-17 16:05:41\ntags: object detection\nmathjax: true\n---\n论文 [RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)\n\n大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。\n\n如图 1，\n![](/images/RepPoints_fig1.png)\n\nRepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。~~RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。~~\n\n# RepPoints\n## BBox 表示\n目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，\n\nbbox anchors -(bbox reg)->  bbox proposals (S1)\n             -(bbox reg)->  bbox proposals (S2)\n             ...\n             -(bbox reg)->  bbox object targets\n\n开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。\n\n坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，\n$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$\n\n记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，\n$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$\n\n回归损失使用 smooth L1 损失。\n\n## RepPoints\n前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，\n$$\\mathcal R = \\{(x_k,y_k)\\}_{k=1}^n$$\n其中 n 为采样点数量，本文设置为 9。\n\nRepPoints 的坐标调整可表示为，\n$$\\mathcal R_r = \\{(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)\\}_{k=1}^n \\qquad (5)$$\n其中 $\\{(\\Delta x_k,\\ \\Delta y_k)\\}_{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。\n\n__RepPoints 变换到 gt box:__ $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，\n1. Min-max function  \n   $\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n2. Partial min-max function  \n   $\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$\n\n3. Moment-based function  \n   $\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。\n\n__RepPoints 的学习__ 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。\n\n# RPDet\n类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，\n\nobject centers -(RP refine)-> RepPoints proposals(S1) -(RP refine)-> RepPoints proposals(S2) ... -(RP refine)-> RepPoints object targets\n\nRPDet (RepPoints Detector) 结构如图 2，\n![](/images/RepPoints_fig2.png)\n\n其中 N 表示 RepPoints 的数量。\n\n采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。\n\n得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。\n\n## 与可变形 RoI pooling 的关系\n可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。\n\n# 实验\n略\n\n# 结论\n本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。","slug":"RepPoints","published":1,"updated":"2019-07-18T11:22:42.079Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ps70004xgvc0xvgiymg","content":"<p>论文 <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>\n<p>大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。</p>\n<p>如图 1，<br><img src=\"/images/RepPoints_fig1.png\" alt></p>\n<p>RepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。<del>RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-表示\"><a href=\"#BBox-表示\" class=\"headerlink\" title=\"BBox 表示\"></a>BBox 表示</h2><p>目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             …<br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p>开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。</p>\n<p>坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p>记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p>回归损失使用 smooth L1 损失。</p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，<br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br>其中 n 为采样点数量，本文设置为 9。</p>\n<p>RepPoints 的坐标调整可表示为，<br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br>其中 ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。</p>\n<p><strong>RepPoints 变换到 gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。</p>\n</li>\n</ol>\n<p><strong>RepPoints 的学习</strong> 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。</p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，</p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2) … -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector) 结构如图 2，<br><img src=\"/images/RepPoints_fig2.png\" alt></p>\n<p>其中 N 表示 RepPoints 的数量。</p>\n<p>采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。</p>\n<p>得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。</p>\n<h2 id=\"与可变形-RoI-pooling-的关系\"><a href=\"#与可变形-RoI-pooling-的关系\" class=\"headerlink\" title=\"与可变形 RoI pooling 的关系\"></a>与可变形 RoI pooling 的关系</h2><p>可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>略</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1904.11490\" target=\"_blank\" rel=\"noopener\">RepPoints: Point Set Representation for Object Detection</a></p>\n<p>大多数目标检测器使用 bbox 表示目标的位置，但是本文认为 bbox 对目标的位置描述较为粗糙，所以提出了一种新型的更加精确的目标表示方法 RepPoints（representative points），RepPoints 使用一组采样点进行目标定位和识别。通过训练给定目标的 gt 位置和分类，RepPoints 可以学习自动排布这些点使得这些点能勾画出目标边缘并在语义上确定目标所在区域。这是一种 anchor-free 的目标检测器，避开了 anchor 导致的搜索空间大和 anchor 难于设计等缺点。</p>\n<p>如图 1，<br><img src=\"/images/RepPoints_fig1.png\" alt></p>\n<p>RepPoints 使用一个点集，这个点集中各点位于目标边缘，并且语义上表明了目标所在的局部区域。<del>RepPoints 与其他非矩阵表示的目标检测器区别在于其他目标检测器采用 bottom-up 的方式确定一些独立的点（比如角点或极点），然后依靠手工设计的聚合方法聚合这些点以获得预测 box，而 RepPoints 则是 top-down 的方式从输入 image/目标特征中学习并且能够端到端的训练。</del></p>\n<h1 id=\"RepPoints\"><a href=\"#RepPoints\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h1><h2 id=\"BBox-表示\"><a href=\"#BBox-表示\" class=\"headerlink\" title=\"BBox 表示\"></a>BBox 表示</h2><p>目标 bbox 可表示为 $\\mathcal B=(x,y,w,h)$。我们回归一下 multi-stage 目标检测器，每经过一个 stage，目标定位都会得到调整，过程如下，</p>\n<p>bbox anchors -(bbox reg)-&gt;  bbox proposals (S1)<br>             -(bbox reg)-&gt;  bbox proposals (S2)<br>             …<br>             -(bbox reg)-&gt;  bbox object targets</p>\n<p>开始时使用多个具有不同 scale 和 aspect ratio 的 anchors。anchor 中心处的特征（向量）用于预测目标分类的得分（二值分类，前景/背景），以及预测坐标偏差。调整后的 bbox 称为 proposal (S1)。在第二 stage，从 S1 中继续抽取特征，通常是 RoIpooling/RoIAlign，对于 two-stage 目标检测器，S1 中抽取的特征将用于最终的 box 的分类预测和坐标偏差预测。对于 multi-stage 目标检测器，S1 中抽取的特征用于预测生成 S2，逐次进行此过程，直到最后一个 stage 用于预测最终的 bbox target。</p>\n<p>坐标回归预测为一个 4-d 向量 $(\\Delta x_p, \\Delta y_p, \\Delta w_p, \\Delta h_p)$，再结合 bbox proposal 的坐标 $\\mathcal B_p=(x_p,y_p,w_p,h_p)$ 可解码出调整后的预测 bbox，<br>$$\\mathcal B_r=(x_p+w_p \\Delta x_p, \\ y_p+h_p\\Delta y_p, \\ w_p e^{\\Delta w_p}, \\ h_p e^{\\Delta h_p})$$</p>\n<p>记目标的 gt 位置为 $\\mathcal B_t=(x_t,y_t,w_t,h_t)$，gt 坐标偏差（gt target）为，<br>$$\\hat {\\mathcal F}(\\mathcal B_p, \\mathcal B_t)=(\\frac {x_t-x_p} {w_p},\\ \\frac {y_t-y_p} {h_p},\\ \\log \\frac {w_t} {w_p}, \\ \\log \\frac {h_t} {h_p})$$</p>\n<p>回归损失使用 smooth L1 损失。</p>\n<h2 id=\"RepPoints-1\"><a href=\"#RepPoints-1\" class=\"headerlink\" title=\"RepPoints\"></a>RepPoints</h2><p>前文提到，4-d bbox 是一种比较粗糙的目标位置表示，不能反映目标的性质、姿势以及语义上的区域。RepPoints 能够解决这些问题，使用一组自适应采样点，<br>$$\\mathcal R = {(x_k,y_k)}_{k=1}^n$$<br>其中 n 为采样点数量，本文设置为 9。</p>\n<p>RepPoints 的坐标调整可表示为，<br>$$\\mathcal R_r = {(x_k+\\Delta x_k,\\ y_k+\\Delta y_k)}<em>{k=1}^n \\qquad (5)$$<br>其中 ${(\\Delta x_k,\\ \\Delta y_k)}</em>{k=1}^n$ 表示新采样点与旧采样点之间的偏差，refine 旧采样点之后得到新采样点。</p>\n<p><strong>RepPoints 变换到 gt box:</strong> $\\mathcal {T: R}_P \\rightarrow \\mathcal B_P$，其中 $\\mathcal R_P$ 表示目标 P 的 RepPoints，$\\mathcal T(\\mathcal R_p)$ 表示伪 box，变换函数考虑以下三种，</p>\n<ol>\n<li><p>Min-max function<br>$\\mathcal {T=T_1}$：根据所有 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Partial min-max function<br>$\\mathcal {T=T_2}$：根据部分 RepPoints 确定横轴和纵轴上的最小最大值，以得到 $\\mathcal B_p$</p>\n</li>\n<li><p>Moment-based function<br>$\\mathcal {T=T_3}$：使用 RepPoints 的期望值和二阶矩（方差）来计算 $\\mathcal B_p$ 的中心和 scale，其中 scale 需要乘上全局共享的可学习系数 $\\lambda_x, \\ \\lambda_y$。使用坐标的均值作为 box 的中心，这一点不难理解，RepPoints 为目标边缘的点，故目标越大，RepPoints 坐标的方差越大，两者应该成正比，所以将方差乘以系数可得到目标 size。考虑到任意目标的 RepPoint 都不是固定的，所以系数可以全局共享，并且可通过 point loss 学习得到。</p>\n</li>\n</ol>\n<p><strong>RepPoints 的学习</strong> 学习过程由目标定位损失和目标分类损失驱动。对于定位损失，首先使用上述某个转换函数将 RepPoints 转换为 pseudo box，然后计算与 gt box 之间的距离，这里使用左上角和右下角的 smooth L1 距离作为定位损失。</p>\n<h1 id=\"RPDet\"><a href=\"#RPDet\" class=\"headerlink\" title=\"RPDet\"></a>RPDet</h1><p>类似地，multi-stage 中使用 RepPoints 的目标表示演进过程为，</p>\n<p>object centers -(RP refine)-&gt; RepPoints proposals(S1) -(RP refine)-&gt; RepPoints proposals(S2) … -(RP refine)-&gt; RepPoints object targets</p>\n<p>RPDet (RepPoints Detector) 结构如图 2，<br><img src=\"/images/RepPoints_fig2.png\" alt></p>\n<p>其中 N 表示 RepPoints 的数量。</p>\n<p>采用 FPN backbone，得到的 feature maps，一支经过 3x3 卷积得到 offset field，这是一个与输入 feature maps 相同 spatial size 的 2N-channel 的特征，每个空间位置的 feature vector 是 2N-d 的，表示这个位置处的 offsets，offsets 是用于 deformable conv，同时也是 RepPoints，相当于给 deformable conv 的 offsets 赋予了物理含义，变换这个 offsets 得到 pseudo box，与 gt box 之间的左上和右下两点的 point loss（smooth L1 损失）用于优化这一分支。</p>\n<p>得到的 offsets 用于 deformable conv，然后再经过卷积得到第二组的 offsets，根据 (5) 式得到 refinement 之后的 RepPoints（也就是说第二组的 offsets 其实是 $\\Delta x, \\Delta y$？），表示最终的目标定位，另一分支经过卷积得到目标分类 score maps。</p>\n<h2 id=\"与可变形-RoI-pooling-的关系\"><a href=\"#与可变形-RoI-pooling-的关系\" class=\"headerlink\" title=\"与可变形 RoI pooling 的关系\"></a>与可变形 RoI pooling 的关系</h2><p>可变形卷积和可变形 RoI pooling 用于改善特征抽取，而 RepPoints 是一种灵活的目标几何表示方式，可以抽取语义特征，从而准确地定位目标。作者认为，可变形 RoI pooling 无法学习到表示目标精确位置的采样点，原因为：假设可以学习到目标位置的几何表示，那么对于同一目标的两个靠的很近的 proposals，可变形 RoI pooling 将生成相同的特征，这表示目标检测器会失败；然而，可变形 RoI pooling 已经通过实验证明可以区分两个靠的很近的 proposals，这说明，可变形 RoI pooling 无法学习到目标的准确位置表示。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>略</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>本文比较晦涩难懂，需要多读几遍，并且期待作者放出源码。</p>\n"},{"title":"cpp-aux-tools","date":"2019-07-11T11:16:23.000Z","_content":"来看一个 c++ 程序片段\n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n编译\n```\ngcc test.cpp -o test.o\n```\n查看 f 的 low-level assembler 名称（name mangling），\n```\nnm test.o | grep f\n// 输出\n// 000000000000008b T _Z4fi\n```\n逆过程为\n```\nc++filt -n _Z4fi\n// 输出\n// f(int)\n```","source":"_posts/cpp-aux-tools.md","raw":"---\ntitle: cpp-aux-tools\ndate: 2019-07-11 19:16:23\ntags: c++\n---\n来看一个 c++ 程序片段\n```c++\n// test.cpp\nint f(int i) { return 0; }\n```\n编译\n```\ngcc test.cpp -o test.o\n```\n查看 f 的 low-level assembler 名称（name mangling），\n```\nnm test.o | grep f\n// 输出\n// 000000000000008b T _Z4fi\n```\n逆过程为\n```\nc++filt -n _Z4fi\n// 输出\n// f(int)\n```","slug":"cpp-aux-tools","published":1,"updated":"2019-07-11T11:22:57.310Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ps90005xgvcnt75sjvd","content":"<p>来看一个 c++ 程序片段</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n\n<p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n\n<p>查看 f 的 low-level assembler 名称（name mangling），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">// 输出</span><br><span class=\"line\">// 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n\n<p>逆过程为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">// 输出</span><br><span class=\"line\">// f(int)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>来看一个 c++ 程序片段</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// test.cpp</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"number\">0</span>; &#125;</span><br></pre></td></tr></table></figure>\n\n<p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc test.cpp -o test.o</span><br></pre></td></tr></table></figure>\n\n<p>查看 f 的 low-level assembler 名称（name mangling），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nm test.o | grep f</span><br><span class=\"line\">// 输出</span><br><span class=\"line\">// 000000000000008b T _Z4fi</span><br></pre></td></tr></table></figure>\n\n<p>逆过程为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c++filt -n _Z4fi</span><br><span class=\"line\">// 输出</span><br><span class=\"line\">// f(int)</span><br></pre></td></tr></table></figure>"},{"title":"CV 中的常用方法总结","date":"2019-06-24T09:33:22.000Z","mathjax":true,"_content":"总结 CV 中的一些概念和操作（并不局限于 CV）。\n# RF\n第 k layer 上的感受野大小为\n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。\n# NMS\n以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：\n1. proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内\n2. 过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）\n3. 按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）\n4. 非极大抑制 NMS\n5. 按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）\n\nNMS 过程如下：\n1. 对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中\n2. 找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，`K.append(I[0])`。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）\n3. 重复过程 2，直到当前 I 为空 \n4. K 中保存了 NMS 之后的 proposals 的列表下标值\n\n# Soft-NMS\nNMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：\n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   * 分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，\n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n   可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，\n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n   在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。\n\n# Deconvolution\n在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。\n(to be continued...)","source":"_posts/cv-mtds.md","raw":"---\ntitle: CV 中的常用方法总结\ndate: 2019-06-24 17:33:22\ntags: CV\nmathjax: true\n---\n总结 CV 中的一些概念和操作（并不局限于 CV）。\n# RF\n第 k layer 上的感受野大小为\n$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$\n其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。\n# NMS\n以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：\n1. proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内\n2. 过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）\n3. 按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）\n4. 非极大抑制 NMS\n5. 按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）\n\nNMS 过程如下：\n1. 对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中\n2. 找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，`K.append(I[0])`。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）\n3. 重复过程 2，直到当前 I 为空 \n4. K 中保存了 NMS 之后的 proposals 的列表下标值\n\n# Soft-NMS\nNMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 [Improving Object Detection With One Line of Code](https://arxiv.org/pdf/1704.04503.pdf)\n\nSoft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：\n\n\n__Input__\n   * $\\mathcal B=\\{b_1,...,b_N\\}, \\mathcal S = \\{s_1,...,S_N\\}, N_t, m$\n   * 分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$\n\n$\\mathcal D \\leftarrow \\{\\}$\n\n__while__ $\\mathcal B \\ne \\varnothing$ __do__\n\n&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$\n\n&emsp; &emsp; $\\mathcal M \\leftarrow b_m$\n\n&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$\n\n&emsp; &emsp; __for__ $b_i \\in \\mathcal B$ __do__\n\n&emsp; &emsp; &emsp; &emsp; __if__ $iou(\\mathcal M, b_i) > N_t$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __if__ $m=1$ __then__\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __else if__ $m=2$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$\n\n&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; &emsp; &emsp; __end__\n\n&emsp; &emsp; __end__\n\n__end__\n\n__return__ $\\mathcal {D,S}$\n\n现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，\n1. NMS\n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 0 & iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$\n\n2. Soft-NMS\n   \n   $$f(i) = \\begin{cases} 1 & iou(\\mathcal M, b_i) < N_t \\\\ 1-iou(\\mathcal M, b_i) & iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$\n   可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，\n   $$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$\n   在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。\n\n# Deconvolution\n在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。\n(to be continued...)","slug":"cv-mtds","published":1,"updated":"2019-07-16T09:31:18.679Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5psa0006xgvcs9cm2yqo","content":"<p>总结 CV 中的一些概念和操作（并不局限于 CV）。</p>\n<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p>第 k layer 上的感受野大小为<br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p>以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：</p>\n<ol>\n<li>proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内</li>\n<li>过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）</li>\n<li>按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）</li>\n<li>非极大抑制 NMS</li>\n<li>按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）</li>\n</ol>\n<p>NMS 过程如下：</p>\n<ol>\n<li>对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中</li>\n<li>找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，<code>K.append(I[0])</code>。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）</li>\n<li>重复过程 2，直到当前 I 为空 </li>\n<li>K 中保存了 NMS 之后的 proposals 的列表下标值</li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：</p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,…,b_N}, \\mathcal S = {s_1,…,S_N}, N_t, m$</li>\n<li>分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p>现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，</p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br>可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，<br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br>在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。</p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p>在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。<br>(to be continued…)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>总结 CV 中的一些概念和操作（并不局限于 CV）。</p>\n<h1 id=\"RF\"><a href=\"#RF\" class=\"headerlink\" title=\"RF\"></a>RF</h1><p>第 k layer 上的感受野大小为<br>$$l_k=l_{k-1}+[(f_k-1)\\prod_{i=0}^{k-1}s_i]$$<br>其中，$s_i$ 为第 i layer 上卷积的步幅，$f_k$ 为第 k layer 上卷积核大小，$l_0=1, \\ s_0=1$。</p>\n<h1 id=\"NMS\"><a href=\"#NMS\" class=\"headerlink\" title=\"NMS\"></a>NMS</h1><p>以 Faster R-CNN 为例，Test 阶段时，ProposalLayer 生成密集均匀分布的 anchors，RPN 得到所有 anchors 的得分（置信度）以及偏差回归值，根据偏差值对 anchors 进行坐标转换得到 proposals，proposals 的得分就是对应 anchors 的得分，然后经过如下处理：</p>\n<ol>\n<li>proposal 的坐标不能超过输入 image 的范围 [0,w-1], [0,h-1]，故需要对超过范围的 proposal 进行 clip 以使得 proposal 坐标位于范围内</li>\n<li>过滤极小尺度的 proposal，proposal 对应在原始 image 上的 box 尺度必须大于 16（配置值）</li>\n<li>按 proposals 得分倒排，保留 top N1 的 proposals（N1 为配置值）</li>\n<li>非极大抑制 NMS</li>\n<li>按 proposals 得分倒排，保留 top N2 的 proposals（N2 为配置值）</li>\n</ol>\n<p>NMS 过程如下：</p>\n<ol>\n<li>对于所有的 proposals 列表 P，计算其面积列表 A，根据 proposals 的得分倒排得到其列表下标 I。最终要保留的 proposals 的列表下标将被保存到 K 中</li>\n<li>找到当前得分最高的 proposal，其列表下标为 I[0]，将其添加到最终需要保留的 K 中，<code>K.append(I[0])</code>。计算当前 I 中与此最高得分的 proposal 的 IOUs，从 I 列表中移除 IOU 大于阈值（配置值）的那些 proposals 的下标值（注意，包括 I[0] 处的 proposal 也被移除，因为 I[0] 已经添加到 K 中）</li>\n<li>重复过程 2，直到当前 I 为空 </li>\n<li>K 中保存了 NMS 之后的 proposals 的列表下标值</li>\n</ol>\n<h1 id=\"Soft-NMS\"><a href=\"#Soft-NMS\" class=\"headerlink\" title=\"Soft-NMS\"></a>Soft-NMS</h1><p>NMS会过滤到两个靠的很近的 boxes 中得分较低的那个 box，但是有时候确实是存在两个靠的很近的 gt boxes，强行过滤到得分较低的 box 会导致 recall 较低，所以此时可改用 soft-NMS，源于论文 <a href=\"https://arxiv.org/pdf/1704.04503.pdf\" target=\"_blank\" rel=\"noopener\">Improving Object Detection With One Line of Code</a></p>\n<p>Soft-NMS 与 NMS 的最主要区别是 NMS 将近邻低得分 box 重置其得分为 0，而 Soft-NMS 则是根据一个函数降低其得分，使得近邻 box 的置信度更低，但仍然在检测 rank list 中。算法如下：</p>\n<p><strong>Input</strong></p>\n<ul>\n<li>$\\mathcal B={b_1,…,b_N}, \\mathcal S = {s_1,…,S_N}, N_t, m$</li>\n<li>分别表示初始检测 boxes，相应的 scores，NMS 阈值（0.7）， $m=1 \\rightarrow \\text{NMS}; \\ m=2\\rightarrow \\text{Soft-NMS}$</li>\n</ul>\n<p>$\\mathcal D \\leftarrow {}$</p>\n<p><strong>while</strong> $\\mathcal B \\ne \\varnothing$ <strong>do</strong></p>\n<p>&emsp; &emsp; $m \\leftarrow \\arg \\max \\mathcal S$</p>\n<p>&emsp; &emsp; $\\mathcal M \\leftarrow b_m$</p>\n<p>&emsp; &emsp; $\\mathcal D \\leftarrow \\mathcal {D \\cup M}; \\mathcal B \\leftarrow \\mathcal{B-M}$</p>\n<p>&emsp; &emsp; <strong>for</strong> $b_i \\in \\mathcal B$ <strong>do</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>if</strong> $iou(\\mathcal M, b_i) &gt; N_t$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>if</strong> $m=1$ <strong>then</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $\\mathcal {B \\leftarrow B} - b_i; \\mathcal {S \\leftarrow S} - s_i$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>else if</strong> $m=2$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; $s_i \\leftarrow s_i f[iou(\\mathcal M, b_i)]$</p>\n<p>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; &emsp; &emsp; <strong>end</strong></p>\n<p>&emsp; &emsp; <strong>end</strong></p>\n<p><strong>end</strong></p>\n<p><strong>return</strong> $\\mathcal {D,S}$</p>\n<p>现在来看 Soft-NMS 中的得分衰减因子 f 函数，首先无论 NMS 还是 Soft-NMS，我们都可以统一可将 box 的得分修改为 $s_i=s_i f(i)$，其中 f 函数为，</p>\n<ol>\n<li><p>NMS<br>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 0 &amp; iou(\\mathcal M, b_i) \\ge N_t\\end{cases}$$</p>\n</li>\n<li><p>Soft-NMS</p>\n<p>$$f(i) = \\begin{cases} 1 &amp; iou(\\mathcal M, b_i) &lt; N_t \\ 1-iou(\\mathcal M, b_i) &amp; iou(\\mathcal M, b_i) \\ge N_t \\end{cases}$$<br>可见，靠的越近 iou 越大，得分衰减的越厉害。但是这个式子中函数值在 iou=N<sub>t</sub> 处附近不连续，可使用高斯惩罚函数，<br>$$f(i)=e^{-\\frac {iou(\\mathcal M, b_i)^2} \\sigma}, \\ \\forall b_i \\notin \\mathcal D$$<br>在 iou=0 时取得最大值，参数 $\\sigma$ 控制衰减速度，$\\sigma$ 越小表示得分随 iou 增大而衰减越快。</p>\n</li>\n</ol>\n<h1 id=\"Deconvolution\"><a href=\"#Deconvolution\" class=\"headerlink\" title=\"Deconvolution\"></a>Deconvolution</h1><p>在很多 CV 任务例如 semantic segmentation 中，需要上采样，简单的上采样可以使用 bilinear interpolation，但有时为了得到更好的上采样结果会使用反卷积。<br>(to be continued…)</p>\n"},{"title":"loss","date":"2019-07-16T09:32:26.000Z","mathjax":true,"_content":"总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）\n\n# Cross-Entropy Loss\n交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,...,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,...,t_C)$，其中\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n于是交叉熵损失为\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in \\{0,1\\}$，\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n为方便起见，记\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n于是，\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\n如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n其中 $(1-p_t)^{\\gamma}$ 称为调节因子。\n\nFocal loss 的性质：\n1. $p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大\n2. $p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小\n\n# MSE\n均方误差为\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。\n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n缺点：当 $|Y_i-\\hat Y_i|>1$ 时，误差会被放大很多，导致模型训练不稳定。\n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n缺点：当 $|Y_i-\\hat Y_i|<1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。\n## Smooth L1 Loss\n结合以上两点，得到 Smooth L1 损失，\n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。\n\n","source":"_posts/loss.md","raw":"---\ntitle: loss\ndate: 2019-07-16 17:32:26\ntags: CV\nmathjax: true\n---\n总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）\n\n# Cross-Entropy Loss\n交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,...,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,...,t_C)$，其中\n$$t_i=\\begin{cases} 1 & i=c \\\\\\\\ 0 & i\\ne c \\end{cases}$$\n于是交叉熵损失为\n$$CE=-\\sum_{i=1}^C t_i \\log p_i$$\n\n## Binary Cross-Entropy Loss\n特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in \\{0,1\\}$，\n$$CE=-t \\log p - (1-t) \\log (1-p)$$\n为方便起见，记\n$$p_t=\\begin{cases} p & t=1 \\\\\\\\ 1-p & t=0 \\end{cases}$$\n于是，\n$$ CE=-\\log p_t $$\n\n## Balanced Cross-Entropy Loss\n如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，\n$$CE=-\\alpha_t \\log p_t$$\n\n## Focal Loss\n虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，\n$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$\n其中 $(1-p_t)^{\\gamma}$ 称为调节因子。\n\nFocal loss 的性质：\n1. $p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大\n2. $p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小\n\n# MSE\n均方误差为\n$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$\n表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。\n\n## L2 Loss\n$$L_2=(Y_i-\\hat Y_i)^2$$\n缺点：当 $|Y_i-\\hat Y_i|>1$ 时，误差会被放大很多，导致模型训练不稳定。\n## L1 Loss\n$$L_1=|Y_i-\\hat Y_i|$$\n缺点：当 $|Y_i-\\hat Y_i|<1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。\n## Smooth L1 Loss\n结合以上两点，得到 Smooth L1 损失，\n$$L=smooth_{L_1}(Y_i-\\hat Y_i)\n\\\\\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n\n## Regularized Loss\n机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。\n\n","slug":"loss","published":1,"updated":"2019-07-17T01:55:01.435Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5psc0009xgvc7egaresy","content":"<p>总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）</p>\n<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,…,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,…,t_C)$，其中<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br>于是交叉熵损失为<br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p>特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in {0,1}$，<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br>为方便起见，记<br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br>于是，<br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br>其中 $(1-p_t)^{\\gamma}$ 称为调节因子。</p>\n<p>Focal loss 的性质：</p>\n<ol>\n<li>$p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大</li>\n<li>$p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小</li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p>均方误差为<br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br>表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。</p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br>缺点：当 $|Y_i-\\hat Y_i|&gt;1$ 时，误差会被放大很多，导致模型训练不稳定。</p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br>缺点：当 $|Y_i-\\hat Y_i|&lt;1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。</p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p>结合以上两点，得到 Smooth L1 损失，<br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p>机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>总结一些常见的损失（虽然我把本文归类到 CV，但实际上这些损失函数并不仅仅用于 CV 中，只是目前我只关注 CV 而已）</p>\n<h1 id=\"Cross-Entropy-Loss\"><a href=\"#Cross-Entropy-Loss\" class=\"headerlink\" title=\"Cross-Entropy Loss\"></a>Cross-Entropy Loss</h1><p>交叉熵损失常用于分类任务中，比如共有 C 中可能的分类，（softmax 之后的）预测向量为 $P=(p_1,…,p_C)$，其中 $p_i$ 表示分类为 i 的概率，且有 $\\sum_i^C p_i=1$，目标真实分类为 c，那么 gt target 为 $T=(t_1,…,t_C)$，其中<br>$$t_i=\\begin{cases} 1 &amp; i=c \\\\ 0 &amp; i\\ne c \\end{cases}$$<br>于是交叉熵损失为<br>$$CE=-\\sum_{i=1}^C t_i \\log p_i$$</p>\n<h2 id=\"Binary-Cross-Entropy-Loss\"><a href=\"#Binary-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Binary Cross-Entropy Loss\"></a>Binary Cross-Entropy Loss</h2><p>特别地，当分类数量 C=2 时，目标为正的预测概率为 p，真实分类为 t，$t \\in {0,1}$，<br>$$CE=-t \\log p - (1-t) \\log (1-p)$$<br>为方便起见，记<br>$$p_t=\\begin{cases} p &amp; t=1 \\\\ 1-p &amp; t=0 \\end{cases}$$<br>于是，<br>$$ CE=-\\log p_t $$</p>\n<h2 id=\"Balanced-Cross-Entropy-Loss\"><a href=\"#Balanced-Cross-Entropy-Loss\" class=\"headerlink\" title=\"Balanced Cross-Entropy Loss\"></a>Balanced Cross-Entropy Loss</h2><p>如果样本分类分布不均（long-tail distribution），即少数分类的占据了绝大多数样本，而其他分类的样本数量则非常少，比如二分类中，分类为 1 的样本很少而分类为 0 的样本很多，那么从分类为 1 的样本中学习到的信息就有限，或者说分类为 1 的样本对损失贡献较小从而对优化过程作用较弱，故引入权重因子，t=1 具有权重 $\\alpha$，t=0 具有权重 $1-\\alpha$，$\\alpha \\in [0,1]$。实际操作中，设置 $\\alpha$ 反比例于分类样本频次，或将 $\\alpha$ 作为超参数通过交叉验证设置其值（RetinaNet 中设置为 0.25）。于是平衡交叉熵损失为，<br>$$CE=-\\alpha_t \\log p_t$$</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>虽然 balanced cross-entropy loss 中 $\\alpha$ 平衡了正负样本，但是并没有区分简单样本和困难样本，我们知道 $p_t \\gg 0.5$ 属于简单样本，当简单样本数量很多时，其贡献的总损失不容忽视，显然，我们更应该重视困难样本，因为从困难样本中更能学习到有用（对模型至关重要的）信息，所以，降低简单样本的损失权重，比如这里的 Focal loss，<br>$$FL=-(1-p_t)^{\\gamma} \\log p_t \\ , \\ \\gamma \\ge 0$$<br>其中 $(1-p_t)^{\\gamma}$ 称为调节因子。</p>\n<p>Focal loss 的性质：</p>\n<ol>\n<li>$p_t$ 较小，表示误分类，困难样本，此时 $(1-p_t)^{\\gamma}$ 相对较大</li>\n<li>$p_t$ 较大，表示分类正确，简单样本，此时 $(1-p_t)^{\\gamma}$ 相对较小</li>\n</ol>\n<h1 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h1><p>均方误差为<br>$$MSE = \\frac 1 n \\sum_{i=1}^n (Y_i-\\hat Y_i)^2$$<br>表示 n 个样本的 L2 范数误差的平均，其中 $Y_i, \\hat Y_i$ 分别表示第 i 个样本的真实值和预测值。</p>\n<h2 id=\"L2-Loss\"><a href=\"#L2-Loss\" class=\"headerlink\" title=\"L2 Loss\"></a>L2 Loss</h2><p>$$L_2=(Y_i-\\hat Y_i)^2$$<br>缺点：当 $|Y_i-\\hat Y_i|&gt;1$ 时，误差会被放大很多，导致模型训练不稳定。</p>\n<h2 id=\"L1-Loss\"><a href=\"#L1-Loss\" class=\"headerlink\" title=\"L1 Loss\"></a>L1 Loss</h2><p>$$L_1=|Y_i-\\hat Y_i|$$<br>缺点：当 $|Y_i-\\hat Y_i|&lt;1$ 时，梯度（的绝对值）不变，导致优化过程出现震荡。</p>\n<h2 id=\"Smooth-L1-Loss\"><a href=\"#Smooth-L1-Loss\" class=\"headerlink\" title=\"Smooth L1 Loss\"></a>Smooth L1 Loss</h2><p>结合以上两点，得到 Smooth L1 损失，<br>$$L=smooth_{L_1}(Y_i-\\hat Y_i)<br>\\\\ smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$</p>\n<h2 id=\"Regularized-Loss\"><a href=\"#Regularized-Loss\" class=\"headerlink\" title=\"Regularized Loss\"></a>Regularized Loss</h2><p>机器学习中，为防止过拟合加入正则项损失，通常是参数的 L1 范数或 L2 范数，略。</p>\n"},{"title":"BBox-Reg-Uncertainty","date":"2019-06-28T01:23:16.000Z","mathjax":true,"_content":"论文：[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n# 简介\n大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center>\n\n当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 __回归 loss 较大__。\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center>\n\n为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 __回归 loss 较小__。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为\n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n这是离散分布的情况，对于连续分布则为，\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n注意，此时 p(x) 和 q(x) 表示概率密度而非概率。\n显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。\n\n使用 KL loss 学习 bbox 回归有以下三个优点：\n1. 可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小\n2. 学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题\n3. 学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用\n\n\n\n我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。\n\n# 方法\n## BBox 参数化\n基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，\n![](/images/BBox-reg_fig3.png)\n\n令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ 为：\n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为$\\{x_1,y_1,x_2,y_2\\}$。\n\n我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，\n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。\n\n~~（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）~~\n\ngt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，\n$$P_D(x)=\\delta(x-x_g)$$\n其中 $x_g$ 是 gt box 位置 x 坐标。\n\n## 使用 KL Loss 的 BBox 回归\n最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，\n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。\n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。\n\n如图 4，\n![](/images/BBox-reg_fig4.png)\n\n当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，\n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。\n\n当 $|x_g - x_e| > 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，\n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。\n\n## Variance Voting\n得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU>0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，\n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes\n\n$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量\n\n$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵\n\n$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整\n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 [CV 中的常用方法总结](/2019/06/24/cv-mtds)。\n\n算法 1 中，对于当前得分最高的检测 box，记为 b， $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU>0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：\n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。\n\n# 实验\n实验介绍及结果分析略，请阅读原文以获得更详细的信息。\n\n# 结论\n大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。\n\n从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。","source":"_posts/BBox-Reg-Uncertainty.md","raw":"---\ntitle: BBox-Reg-Uncertainty\ndate: 2019-06-28 09:23:16\ntags: object detection\nmathjax: true\n---\n论文：[Bounding Box Regression with Uncertainty for Accurate Object Detection](https://arxiv.org/abs/1809.08545)\n# 简介\n大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，\n![](/images/BBox-reg_fig1.png) <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center>\n\n当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 __回归 loss 较大__。\n![](/images/BBox-reg_fig2.png) <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center>\n\n为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 __回归 loss 较小__。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为\n$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$\n这是离散分布的情况，对于连续分布则为，\n$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$\n注意，此时 p(x) 和 q(x) 表示概率密度而非概率。\n显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。\n\n使用 KL loss 学习 bbox 回归有以下三个优点：\n1. 可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小\n2. 学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题\n3. 学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用\n\n\n\n我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。\n\n# 方法\n## BBox 参数化\n基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，\n![](/images/BBox-reg_fig3.png)\n\n令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 $\\{t_i| i=x_1,y_1,x_2,y_2\\}$ 为：\n\n$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}\n\\\\\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}\n\\\\\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$\n\n其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为$\\{x_1,y_1,x_2,y_2\\}$。\n\n我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，\n$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$\n其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。\n\n~~（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）~~\n\ngt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，\n$$P_D(x)=\\delta(x-x_g)$$\n其中 $x_g$ 是 gt box 位置 x 坐标。\n\n## 使用 KL Loss 的 BBox 回归\n最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，\n$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$\n其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。\n$$\\begin{aligned} L_{reg} &=D_{KL}(P_D(x)||P_{\\Theta}(x)) \n\\\\\\\\ &=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx\n\\\\\\\\ &=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx\n\\\\\\\\ &=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx\n\\\\\\\\ &=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))\n\\end{aligned}$$\n\n其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。\n\n如图 4，\n![](/images/BBox-reg_fig4.png)\n\n当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$\n当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，\n$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$\n损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，\n$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}\n\\\\\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$\n\n由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时\n$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$\n反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。\n\n当 $|x_g - x_e| > 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，\n$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 & |x_g - x_e| \\le 1\n\\\\\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 & |x_g - x_e| > 1 \\end{cases}$$\n\n根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。\n\n## Variance Voting\n得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU>0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，\n\n__Algorithm 1__ var voting\n*****\n$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes\n\n$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量\n\n$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵\n\n$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整\n\n$\\mathcal B=\\{b_1,...,b_N\\}, \\ \\mathcal S=\\{s_1,...,s_N\\}, \\ \\mathcal C=\\{\\sigma_1^2,...,\\sigma_N^2\\}$\n\n$\\mathcal D \\leftarrow \\{\\}, \\ \\mathcal T \\leftarrow \\mathcal B$\n\n__while__ $\\mathcal T \\ne \\varnothing$ __do__\n\n- $m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）\n- $\\mathcal T \\leftarrow \\mathcal T - b_m$\n- <font color='cyan'>$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font>\n- <font color='gree'>$idx \\leftarrow IoU(b_m, B) > 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font>\n- <font color='gree'> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font>\n- <font color='gree'> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font>\n- $\\mathcal D \\leftarrow \\mathcal D \\cup b_m$\n \n__end while__\n\n__return__ $\\mathcal {D, S}$\n***\n\n我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 [CV 中的常用方法总结](/2019/06/24/cv-mtds)。\n\n算法 1 中，对于当前得分最高的检测 box，记为 b， $\\{x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}\\}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU>0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：\n$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}\n\\\\\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}\n\\\\\\\\ \\text{s.t.  IoU}(b_i, b) >0$$\n上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。\n\n# 实验\n实验介绍及结果分析略，请阅读原文以获得更详细的信息。\n\n# 结论\n大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。\n\n从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。","slug":"BBox-Reg-Uncertainty","published":1,"updated":"2019-06-28T09:21:00.064Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptj000kxgvcmf2hc3ek","content":"<p>论文：<a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，<br><img src=\"/images/BBox-reg_fig1.png\" alt> <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center></p>\n<p>当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 <strong>回归 loss 较大</strong>。\n<img src=\"/images/BBox-reg_fig2.png\" alt> <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center></p>\n<p>为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 <strong>回归 loss 较小</strong>。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为<br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br>这是离散分布的情况，对于连续分布则为，<br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br>注意，此时 p(x) 和 q(x) 表示概率密度而非概率。<br>显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。</p>\n<p>使用 KL loss 学习 bbox 回归有以下三个优点：</p>\n<ol>\n<li>可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小</li>\n<li>学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题</li>\n<li>学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用</li>\n</ol>\n<p>我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"BBox-参数化\"><a href=\"#BBox-参数化\" class=\"headerlink\" title=\"BBox 参数化\"></a>BBox 参数化</h2><p>基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，<br><img src=\"/images/BBox-reg_fig3.png\" alt></p>\n<p>令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 ${t_i| i=x_1,y_1,x_2,y_2}$ 为：</p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p>其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为${x_1,y_1,x_2,y_2}$。</p>\n<p>我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，<br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br>其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。</p>\n<p><del>（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）</del></p>\n<p>gt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，<br>$$P_D(x)=\\delta(x-x_g)$$<br>其中 $x_g$ 是 gt box 位置 x 坐标。</p>\n<h2 id=\"使用-KL-Loss-的-BBox-回归\"><a href=\"#使用-KL-Loss-的-BBox-回归\" class=\"headerlink\" title=\"使用 KL Loss 的 BBox 回归\"></a>使用 KL Loss 的 BBox 回归</h2><p>最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，<br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br>其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。<br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。</p>\n<p>如图 4，<br><img src=\"/images/BBox-reg_fig4.png\" alt></p>\n<p>当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br>当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br>损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，<br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p>由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br>反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。</p>\n<p>当 $|x_g - x_e| &gt; 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，<br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p>根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p>得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU&gt;0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，</p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes</p>\n<p>$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量</p>\n<p>$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵</p>\n<p>$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整</p>\n<p>$\\mathcal B={b_1,…,b_N}, \\ \\mathcal S={s_1,…,s_N}, \\ \\mathcal C={\\sigma_1^2,…,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color=\"cyan\">$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color=\"gree\">$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color=\"gree\"> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color=\"gree\"> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p>我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 <a href=\"/2019/06/24/cv-mtds\">CV 中的常用方法总结</a>。</p>\n<p>算法 1 中，对于当前得分最高的检测 box，记为 b， ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU&gt;0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：<br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br>上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍及结果分析略，请阅读原文以获得更详细的信息。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。</p>\n<p>从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://arxiv.org/abs/1809.08545\" target=\"_blank\" rel=\"noopener\">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>大型目标检测集如 ImageNet，MS-COCO 和 CrowdHuman 等都致力于定义足够明确的 ground truth bounding box。但是有时候 gt bbox 的边界是不明确的，使得难以去打标签，也难以学习 bbox 回归函数（的参数），如图 1，<br><img src=\"/images/BBox-reg_fig1.png\" alt> <center>Fig 1 MS-COCO 数据集中 gt bbox 不明确的情况。(a)(c) 标签不准确导致歧义；(b) 遮挡导致歧义；(d) 目标边界本身就不明确</center></p>\n<p>当前 SOTA 目标检测器如 Faster R-CNN，Cascade R-CNN 和 Mask R-CNN 等均依赖于 bbox 回归来定位目标。传统的 bbox 回归损失如 smooth-L1 没有考虑到 gt box 的不明确性，所以损失较大，并且认为分类得分越高时 bbox 回归越准确（应该说的是 Inference 阶段），但事实不总是如此，如图 2，分类得分高的 bbox 但是回归不够准确，回归不准确还是说明 <strong>回归 loss 较大</strong>。\n<img src=\"/images/BBox-reg_fig2.png\" alt> <center>Fig 2 MS-COCO 上使用 VGG-16 Faster R-CNN 的失败案例。(a) 两个预测框均不准确；(b) 高分类得分 bbox 的左边界不准确</center></p>\n<p>为了解决以上问题，我们介绍一种新型 bbox 回归损失 KL loss，同时学习 bbox 回归和定位不确定性，从而使得 <strong>回归 loss 较小</strong>。学习 gt box 的不确定性肯定是针对整个数据集的，首先将预测 box 和 gt box 分别建模为 Gaussian 分布和 Dirac delta 函数。KL loss 定义为预测分布和 gt 分布之间的 KL 散度，我们知道 KL 散度用于衡量两个分布之间的距离（其实不满足距离的对称性，即不满足交换律）或者说差异，差异越大，KL 散度越大。假设目标分布为 P(x)，使用 Q(x) 去匹配目标分布，那么 KL 散度为<br>$$D_{KL}(P||Q)=\\sum_{i=1}^N P(x_i) \\log \\frac {P(x_i)} {Q(x_i)}$$<br>这是离散分布的情况，对于连续分布则为，<br>$$D_{KL}(P||Q)=E_P \\left[\\log \\frac {p(x)} {q(x)} \\right]=\\int p(x) \\log \\frac {p(x)} {q(x)} dx$$<br>注意，此时 p(x) 和 q(x) 表示概率密度而非概率。<br>显然如果 P,Q 完全匹配，那么 KL 散度达到最小值 0。</p>\n<p>使用 KL loss 学习 bbox 回归有以下三个优点：</p>\n<ol>\n<li>可以成功捕获数据集中的不明确性，对于有歧义的 bbox，回归损失更小</li>\n<li>学习到的方差在后续处理中非常有用。我们提出 var voting (variance voting)，通过使用附近 box 的位置和位置方差来票选（加权平均）出当前候选 box 的位置。这么做是为了解决 Fig 2 中的问题</li>\n<li>学习到的概率分布是可解释的。由于分布反应的是预测 box 的不确定性，故在汽车自动驾驶或机器人等下游应用中非常有用</li>\n</ol>\n<p>我们提出了 KL loss 和 var voting，为了验证这两者的通用性，我们使用了 PASCAL VOC 2007 和 MS-COCO 两个 benchmark，多个目标检测器包括 VGG-CNN-M-1024, VGG-16, ResNet-5-FPN 以及 Mask R-CNN（前两者属于 Faster R-CNN），实验证明使用我们提出的方法均提升了目标定位的准确率。</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"BBox-参数化\"><a href=\"#BBox-参数化\" class=\"headerlink\" title=\"BBox 参数化\"></a>BBox 参数化</h2><p>基于 Faster R-CNN 或 Mask R-CNN 如图 3，我们分别回归 bbx 的四条边坐标，即 Box 分支输出 shape 为 (N, 84)，其中 N 表示使用 proposals 的 batch size，84 是 21 个分类下 4 个坐标预测（这里以 PASCAL VOC 为例，共 21 个分类），Box std 分支输出 shape 也是 (N, 84)，表示 21 分类下 4 条边坐标分布的标准差 $\\sigma$，坐标是分类相关的（not class-agnostic），前面简介部分所讲的将 box 建模为高斯分布，就是指四条边的坐标均为高斯分布，具体请往下看，<br><img src=\"/images/BBox-reg_fig3.png\" alt></p>\n<p>令 $(x_1,y_1,x_2,y_2) \\in \\mathcal R^4$ 表示预测 bbox，那么偏差 ${t_i| i=x_1,y_1,x_2,y_2}$ 为：</p>\n<p>$$t_{x_1}=\\frac {x_1-x_{1a}} {w_a}, \\quad t_{x_2}=\\frac {x_2-x_{2a}} {w_a}<br>\\\\ t_{y_1}=\\frac {y_1-y_{1a}} {h_a}, \\quad t_{y_2}=\\frac {y_2-y_{2a}} {h_a}<br>\\\\ t_{x_1}^{\\ast}=\\frac {x_1^{\\ast}-x_{1a}} {w_a}, \\quad t_{x_2}^{\\ast}=\\frac {x_2^{\\ast}-x_{2a}} {w_a}<br>\\\\ t_{y_1}^{\\ast}=\\frac {y_1^{\\ast}-y_{1a}} {h_a}, \\quad t_{y_2}^{\\ast}=\\frac {y_2^{\\ast}-y_{2a}} {h_a}$$</p>\n<p>其中带 * 的为 gt offset，不带 * 的为预测 offset，$(x_{1a},y_{1a},x_{2a},y_{2a})$ 为 anchor box。后面的讨论中，由于各坐标独立进行优化，故我们统一使用 x 表示这四个坐标，x 取值为${x_1,y_1,x_2,y_2}$。</p>\n<p>我们的网络不仅仅预测 bbox 的定位，还预测其概率分布。这种分布可以是复杂的如多变量高斯分布或混合高斯分布，但是本文为了简单起见，我们假定各坐标互相独立，故使用单变量高斯分布，<br>$$P_{\\Theta}(x)=\\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}}$$<br>其中 $\\Theta$ 是可学习的参数，$x_e$ 是 bbox 位置估计，标准差 $\\sigma$ 衡量位置估计的不确定性，越大越不确定。当 $\\sigma \\rightarrow 0$，表示网络对 bbox 位置估计非常十分自信。</p>\n<p><del>（以 Faster R-CNN 为例说明，bbox 回归分支其实是两组输出 blob，分别使用两个全连接层得到，分别表示 4 个 坐标估计以及 4 个坐标分布的标准差，所以可以说，$\\Theta$ 就是这两个全连接层的权重参数。这段话不一定准确，需要看源码待定）</del></p>\n<p>gt box 也可以使用高斯分布，只是其中标准差无限趋于 0： $\\sigma \\rightarrow 0$，此时退化为 Dirac delta 函数，<br>$$P_D(x)=\\delta(x-x_g)$$<br>其中 $x_g$ 是 gt box 位置 x 坐标。</p>\n<h2 id=\"使用-KL-Loss-的-BBox-回归\"><a href=\"#使用-KL-Loss-的-BBox-回归\" class=\"headerlink\" title=\"使用 KL Loss 的 BBox 回归\"></a>使用 KL Loss 的 BBox 回归</h2><p>最小化 $P_{\\Theta}(x)$ 和 $P_D(x)$ 之间的 KL 散度来估计参数 $\\hat \\Theta$，即，使用 KL 损失优化网络参数，<br>$$\\hat \\Theta = \\arg \\min_{\\Theta} \\frac 1 N \\sum D_{KL}(P_D(x)||P_{\\Theta}(x))$$<br>其中 N 表示样本数量，x 表示 4 个坐标中的一个。KL 散度作为回归损失，而分类损失维持原来不变。<br>$$\\begin{aligned} L_{reg} &amp;=D_{KL}(P_D(x)||P_{\\Theta}(x))<br>\\\\ &amp;=\\int P_D(x) \\log P_D(x) dx - \\int P_D(x) \\log P_{\\Theta}(x) dx<br>\\\\ &amp;=-H(P_D(x))-\\int P_D(x) \\log \\frac 1 {\\sqrt {2 \\pi \\sigma^2}}e^{- \\frac {(x-x_e)^2} {2 \\sigma^2}} dx<br>\\\\ &amp;=-H(P_D(x))+ \\log \\sqrt{2\\pi \\sigma^2}\\int P_D(x) dx+\\int P_D(x) \\frac {(x-x_e)^2} {2 \\sigma^2} dx<br>\\\\ &amp;=\\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2 + \\frac {\\log 2\\pi} 2 - H(P_D(x))<br>\\end{aligned}$$</p>\n<p>其中，$H(P_D(x))$ 是 Dirac delta 分布的信息熵。</p>\n<p>如图 4，<br><img src=\"/images/BBox-reg_fig4.png\" alt></p>\n<p>当 box 位置 $x_e$ 估计不正确时，我们希望方差 $\\sigma^2$ 更大，从而降低回归损失 $L_{reg}$。由于 $H(P_D(x)), \\log (2\\pi)/2$ 均与估计参数 $\\Theta$ 无关，故有，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2}{2\\sigma^2}+\\frac {\\log \\sigma^2} 2$$<br>当 $\\sigma=1$，KL Loss 退化为标准的欧氏距离，<br>$$L_{reg} \\propto \\frac {(x_g-x_e)^2} 2$$<br>损失关于估计位置 $x_e$ 和定位标准差 $\\sigma$ 可导，<br>$$\\frac d {dx_e}L_{reg}=\\frac {x_e-x_g} {\\sigma^2}<br>\\\\ \\frac d {dx_e}L_{reg}=-\\frac {(x_e-x_g)^2} {\\sigma^3} + \\frac 1 \\sigma$$</p>\n<p>由于 $\\sigma$ 位于分母上，所以训练初期可能会出现梯度爆炸，为了避免这种现象，在训练阶段，使用 $\\alpha=\\log \\sigma^2$ 代替 $\\sigma$，即，图 3 中 Box std 输出为 $\\alpha$，此时<br>$$L_{reg} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2$$<br>反向传播时使用 $L_{reg}$ 关于 $\\alpha$ 的梯度。测试阶段，则将 $\\alpha$ 转变为 $\\sigma$，即测试阶段中，需要将 Box std 的输出经过 $\\sigma=\\sqrt{e^{\\alpha}}$ 转换才能得到标准差。</p>\n<p>当 $|x_g - x_e| &gt; 1$ 时，我们参考 smooth-L1 改写回归损失，这是为了避免 $x_g,x_e$ 相差太多时，损失过大造成训练不稳定，于是最终有，<br>$$L_{reg} \\begin {cases} \\propto \\frac {e^{-\\alpha}} 2 (x_g-x_e)^2+\\frac \\alpha 2 &amp; |x_g - x_e| \\le 1<br>\\\\ = e^{-\\alpha} (|x_g-x_e|-\\frac 1 2 )+\\frac \\alpha 2 &amp; |x_g - x_e| &gt; 1 \\end{cases}$$</p>\n<p>根据以上分析可见，网络 bbox 回归分支输出两组数据，分别是预测位置 offset 以及位置分布标准差 $\\sigma$。训练阶段，将预测 $\\sigma$ 改为预测 $\\alpha$，$\\alpha$ 预测的那个全连接层参数使用随机 Gaussian 初始化，这个 Gaussian 使用标准差 0.0001，期望 0。</p>\n<h2 id=\"Variance-Voting\"><a href=\"#Variance-Voting\" class=\"headerlink\" title=\"Variance Voting\"></a>Variance Voting</h2><p>得到预测位置坐标的方差 $\\sigma^2$ 后，根据附近 bbox 的位置方差票选出当前候选框的位置，这里附近是指与当前 box 有重叠（IoU&gt;0）的 box。使用 Variance Voting 是为了解决 Fig 2 中的问题。算法如下，</p>\n<p><strong>Algorithm 1</strong> var voting</p>\n<hr>\n<p>$\\mathcal B$ 是 Nx4 的矩阵，表示初始检测 boxes</p>\n<p>$\\mathcal S$ 为相应的检测得分，是长度为 N 的一维向量</p>\n<p>$\\mathcal C$ 是相应的方差，也是一个 Nx4 的矩阵</p>\n<p>$\\mathcal D$ 为最终的检测结果集，$\\sigma_t$ 是 var voting 的一个参数，其值可调整</p>\n<p>$\\mathcal B={b_1,…,b_N}, \\ \\mathcal S={s_1,…,s_N}, \\ \\mathcal C={\\sigma_1^2,…,\\sigma_N^2}$</p>\n<p>$\\mathcal D \\leftarrow {}, \\ \\mathcal T \\leftarrow \\mathcal B$</p>\n<p><strong>while</strong> $\\mathcal T \\ne \\varnothing$ <strong>do</strong></p>\n<ul>\n<li>$m \\leftarrow \\arg\\max \\mathcal T$ （论文中为 $\\arg \\max \\mathcal S$，但是我觉得不对）</li>\n<li>$\\mathcal T \\leftarrow \\mathcal T - b_m$</li>\n<li><font color=\"cyan\">$\\mathcal S \\leftarrow \\mathcal S f(IoU(b_m, \\mathcal T)) \\qquad \\qquad \\qquad \\qquad \\ \\ \\triangleright$ soft-NMS </font></li>\n<li><font color=\"gree\">$idx \\leftarrow IoU(b_m, B) &gt; 0 \\qquad \\qquad \\qquad \\qquad \\triangleright$    var voting </font></li>\n<li><font color=\"gree\"> $p \\leftarrow exp(-(1-IoU(b_m, \\mathcal B[idx]))^2/\\sigma_t)$ </font></li>\n<li><font color=\"gree\"> $b_m \\leftarrow p(\\mathcal B[idx]/\\mathcal C[idx])/p(1 / \\mathcal C[idx])$</font></li>\n<li>$\\mathcal D \\leftarrow \\mathcal D \\cup b_m$</li>\n</ul>\n<p><strong>end while</strong></p>\n<p><strong>return</strong> $\\mathcal {D, S}$</p>\n<hr>\n<p>我们已经知道，当前检测 box 的近邻 box 指与当前 box 的 IoU 超过一定阈值的 box。NMS 是移除得分较低的近邻预测 box ，soft-NMS 是 NMS 的修改版，将得分较低的近邻预测 box 重新修改为一个更低的得分，简单来讲就是得分低，则进一步抑制其得分，衰减因子为函数 $f(IoU(b_m,b_i))$ 的值，关于这两者的具体解释可参考 <a href=\"/2019/06/24/cv-mtds\">CV 中的常用方法总结</a>。</p>\n<p>算法 1 中，对于当前得分最高的检测 box，记为 b， ${x_1,y_1,x_2,y_2,s,\\sigma_{x1},\\sigma_{y1},\\sigma_{x2},\\sigma_{y2}}$，先使用 soft-NMS 衰减其近邻 boxes 的得分，然后获取其附近（IoU&gt;0） boxes，根据附近 boxes $\\sigma$ 的加权来计算当前 box 的新位置，这里加权是基于这样一个认识：某个附近 box 如果越靠近当前 box，那么用它的值来计算当前 box 就越有把握，不确定性越低。用 x 表示坐标（例如 x<sub>1</sub> 坐标），x<sub>i</sub> 表示第 i 个 box 的坐标，坐标新值按如下计算：<br>$$p_i = e^{-(1-IoU(b_i,b))^2/\\sigma_t}<br>\\\\ x=\\frac {\\sum_i p_i x_i/\\sigma_{x,i}^2} {\\sum_i p_i / \\sigma_{x,i}^2}<br>\\\\ \\text{s.t.  IoU}(b_i, b) &gt;0$$<br>上面两式非常明显了，我们不直接使用检测 box 的初始预测位置值，而是通过附近 boxes 的位置和位置方差加权平均值作为当前 box 的位置坐标值。当附近 box 与当前 box 靠的越近，IoU 越大，然后 p<sub>i</sub> 越大，然后 voting 当前 box 的坐标时，权值越大，即贡献越大。另外，上两式也表明附近 box 的方差也影响权值， 当 $\\sigma^2$ 越小，权值越大，贡献也越大。以上 voting 过程没有考虑分类得分值，因为低得分的 box 其定位置信度可能还更高，所以让分类得分影响权值，也许会降低准确性。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍及结果分析略，请阅读原文以获得更详细的信息。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>大型数据集中 gt box 的不确定性会阻碍 SOTA 检测器性能的提升。分类置信度与定位置信度不是强相关的。本文提出新型 bbox 回归损失用于学习目标的准确定位。使用 KL Loss 训练网络学习预测每个坐标的分布方差。预测的方差用在 var voting 中，从而改良 box 的坐标。</p>\n<p>从网络结构上来看，在 Faster R-CNN/Mask R-CNN 基础上修改回归预测分支，使用 KL Loss 替换 smooth L1 Loss，并使用 var voting 得到坐标新值，其中坐标初始预测值（也就是算法 1 中的输入 $\\mathcal B$）与 Faster R-CNN 中相同。</p>\n"},{"title":"DSOD","date":"2019-07-08T01:14:40.000Z","mathjax":true,"_content":"论文 [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n# Introduction\n近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：\n1. 有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。\n2. 学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。\n3. 领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。\n\n于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。\n\n# DSOD\n## 结构\nDSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。\n![](/images/DSOD_fig1.png)<center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center>\n\n整个 DSOD 网络结构如表 1 所示。\n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD 结构 </center>\n\nDSOD 设计原则如下：\n### 无 Proposal\n我们调查了如下三类 SOTA 的目标检测器：\n1. R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。\n2. Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals\n3. YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。\n\n发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。\n\n于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。\n\n### 深度监督\n中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。\n\n### Stem Block\nStem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。\n\n### 密集预测结构\n图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。\n\n在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。\n\n# Experiments\n实验部分略，可阅读原文以获取详细信息。\n\n# Conclusion\n提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。","source":"_posts/DSOD.md","raw":"---\ntitle: DSOD\ndate: 2019-07-08 09:14:40\ntags: object detection\nmathjax: true\n---\n论文 [DSOD: Learning Deeply Supervised Object Detectors from Scratch](https://arxiv.org/abs/1708.01241)\n# Introduction\n近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：\n1. 有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。\n2. 学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。\n3. 领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。\n\n于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。\n\n# DSOD\n## 结构\nDSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。\n![](/images/DSOD_fig1.png)<center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center>\n\n整个 DSOD 网络结构如表 1 所示。\n\n|      Layers      | Output Size (Input 3x100x100) |       DSOD        |\n|      :----:      |  :--------:                   |     :-----:       |\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 2|\n| Stem Convolution | 64x150x150                    | 3x3 conv, stride 1|\n| Stem Convolution | 128x150x150                   | 3x3 conv, stride 1|\n| Stem Convolution | 128x75x75                     | 2x2 max pool, stride 2|\n| Dense Block (1)  | 416x75x75                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 6$|\n| Transition Layer (1)| 416x75x75 <br> 416x38x38   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (2)  | 800x38x38                     | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition Layer (2)| 800x38x38 <br> 800x19x19   | 1x1 conv <br> 2x2 max pool, stride 2|\n| Dense Block (3)  | 1184x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (1)| 1184x19x19     | 1x1 conv          |\n| Dense Block (4)  | 1568x19x19                    | $\\begin{bmatrix} 1 \\times 1 & conv \\\\\\\\ 3 \\times 3 & conv\\end{bmatrix} \\times 8$|\n| Transition w/o Pooling Layer (2)| 1568x19x19     | 1x1 conv          |\n| DSOD Prediction Layers | -                       | Plain/Dense       |\n\n<center>Table 1: DSOD 结构 </center>\n\nDSOD 设计原则如下：\n### 无 Proposal\n我们调查了如下三类 SOTA 的目标检测器：\n1. R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。\n2. Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals\n3. YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。\n\n发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。\n\n于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。\n\n### 深度监督\n中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。\n\n### Stem Block\nStem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。\n\n### 密集预测结构\n图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。\n\n在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。\n\n# Experiments\n实验部分略，可阅读原文以获取详细信息。\n\n# Conclusion\n提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。","slug":"DSOD","published":1,"updated":"2019-07-17T06:12:00.637Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptl000lxgvcz7c717a6","content":"<p>论文 <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：</p>\n<ol>\n<li>有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。</li>\n<li>学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。</li>\n<li>领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。</li>\n</ol>\n<p>于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。</p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>DSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。<br><img src=\"/images/DSOD_fig1.png\" alt><center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center></p>\n<p>整个 DSOD 网络结构如表 1 所示。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD 结构 </center>\n\n<p>DSOD 设计原则如下：</p>\n<h3 id=\"无-Proposal\"><a href=\"#无-Proposal\" class=\"headerlink\" title=\"无 Proposal\"></a>无 Proposal</h3><p>我们调查了如下三类 SOTA 的目标检测器：</p>\n<ol>\n<li>R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。</li>\n<li>Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals</li>\n<li>YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。</li>\n</ol>\n<p>发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。</p>\n<p>于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。</p>\n<h3 id=\"深度监督\"><a href=\"#深度监督\" class=\"headerlink\" title=\"深度监督\"></a>深度监督</h3><p>中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。</p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。</p>\n<h3 id=\"密集预测结构\"><a href=\"#密集预测结构\" class=\"headerlink\" title=\"密集预测结构\"></a>密集预测结构</h3><p>图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。</p>\n<p>在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，可阅读原文以获取详细信息。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1708.01241\" target=\"_blank\" rel=\"noopener\">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>近几年来提出了很多新型 CNN 网络结构，如 Inception、ResNet 以及 DenseNet 等，带动了包括目标检测在内的诸多 CV 任务的发展。通常来讲，目标检测都是在 backbone 后增加检测子网络，backbone 在分类 benchmark 如 ImageNet 进行预训练，然后使用目标检测数据集对整个网络进行 fine-tune，也就是所谓的迁移学习。但是这种设计范式具有三个不足之处：</p>\n<ol>\n<li>有限的结构设计空间。基于 ImageNet 预训练的 backbone 通常是较为庞大的网络，参数量巨大，所以用在目标检测时，不容易调整网络结构。</li>\n<li>学习偏向性。由于分类和目标检测任务两者的损失函数以及分类分布情况均不相同，导致不同的搜索/优化空间，对目标检测任务而言，模型学习可能偏向一个局部最优解。</li>\n<li>领域不匹配。fine-tuning 虽然可以缓和不同数据集的不同分类分布，但是当源域（ImageNet）与目标域（深度图像，医学图像等）有着严重不匹配时，这依然是个问题。</li>\n</ol>\n<p>于是我们考虑两个问题：目标检测网络是否可以 train from scratch？如果可以，是否存在一些网络结构设计原则使得保持高检测准确率的同时让网络轻量？我们提出深度监督目标检测器 DSOD 以满足以上两个问题。</p>\n<h1 id=\"DSOD\"><a href=\"#DSOD\" class=\"headerlink\" title=\"DSOD\"></a>DSOD</h1><h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>DSOD 与 SSD 类似，是一个多尺度的无 proposal（one-stage）的目标检测网络。DSOD 结构分为两部分：用于抽取特征的 backbone 子网络，以及在多尺度响应图（response maps）上预测子网络（这里也称前端子网络）。backbone 是深度监督的 DenseNet 的变体（深度监督指的是对网络隐藏层和输出层直接使用目标检测数据集监督训练，而不是先使用 ImageNet 预训练，再使用目标检测数据集 fine-tune），这个 DenseNet 组成包括一个 stem block，四个 dense block，两个 transition layer 以及两个不带池化层的 transition layer。前端子网络使用一个dense结构融合了多尺度预测响应，如图 1 展示了 DSOD 前端子网络，以及 SSD 中使用的朴素多尺度预测 maps 结构。<br><img src=\"/images/DSOD_fig1.png\" alt><center>Fig 1: 预测子网络。左边是 SSD 中所用的朴素结构；右边是 dense 结构</center></p>\n<p>整个 DSOD 网络结构如表 1 所示。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Layers</th>\n<th align=\"center\">Output Size (Input 3x100x100)</th>\n<th align=\"center\">DSOD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">64x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x150x150</td>\n<td align=\"center\">3x3 conv, stride 1</td>\n</tr>\n<tr>\n<td align=\"center\">Stem Convolution</td>\n<td align=\"center\">128x75x75</td>\n<td align=\"center\">2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (1)</td>\n<td align=\"center\">416x75x75</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 6$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (1)</td>\n<td align=\"center\">416x75x75 <br> 416x38x38</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (2)</td>\n<td align=\"center\">800x38x38</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition Layer (2)</td>\n<td align=\"center\">800x38x38 <br> 800x19x19</td>\n<td align=\"center\">1x1 conv <br> 2x2 max pool, stride 2</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (3)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (1)</td>\n<td align=\"center\">1184x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">Dense Block (4)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">$\\begin{bmatrix} 1 \\times 1 &amp; conv \\\\ 3 \\times 3 &amp; conv\\end{bmatrix} \\times 8$</td>\n</tr>\n<tr>\n<td align=\"center\">Transition w/o Pooling Layer (2)</td>\n<td align=\"center\">1568x19x19</td>\n<td align=\"center\">1x1 conv</td>\n</tr>\n<tr>\n<td align=\"center\">DSOD Prediction Layers</td>\n<td align=\"center\">-</td>\n<td align=\"center\">Plain/Dense</td>\n</tr>\n</tbody></table>\n<center>Table 1: DSOD 结构 </center>\n\n<p>DSOD 设计原则如下：</p>\n<h3 id=\"无-Proposal\"><a href=\"#无-Proposal\" class=\"headerlink\" title=\"无 Proposal\"></a>无 Proposal</h3><p>我们调查了如下三类 SOTA 的目标检测器：</p>\n<ol>\n<li>R-CNN 和 Fast R-CNN，使用外部目标 proposal 生成器如 selective search。</li>\n<li>Faster R-CNN 和 R-FCN 使用 RPN 生成 region proposals</li>\n<li>YOLO 和 SSD，属于 single-shot 不生成 proposals（proposal-free），直接回归得到目标位置。</li>\n</ol>\n<p>发现仅第三类（proposal-free）方法可以在没有预训练模型的情况下收敛成功。我们猜测这是由于前两类方法中的 RoI pooling 从每个 region proposal 中生成特征，这个 pooling 阻碍了梯度从 region 到 conv feature 的平滑反向传播。基于 proposal 的方法在有预训练的情况下工作良好是因为 RoI pooling 之前的 layers 的参数初始化足够好，而在 train from scratch 时由于没有预训练，所以那些 layers 参数初始化不够好，并在训练过程中梯度无法平法的反向传播过去，导致无法很好的更新这部分 layers 的参数。</p>\n<p>于是，第一个设计原则为：training from scratch 需要 proposal-free 网络。</p>\n<h3 id=\"深度监督\"><a href=\"#深度监督\" class=\"headerlink\" title=\"深度监督\"></a>深度监督</h3><p>中心思想是使用统一的目标函数对网络最初的隐藏层进行直接监督。这里我们使用密集层间连接如同 DenseNets 中那样来增强深度监督，即在一个 block 中当前 layer 与前面所有 layers 均有直接连接（也称 dense block），DenseNet 中初始的 layers 可通过 skip connections 得到来自目标函数的额外监督，所以只需要一个位于网络顶层的目标函数即可实现深度监督，并且能缓和梯度消失的问题。在 DenseNet 中，每个 transition layer 均包含池化层，所以要维持相同尺度的输出并增加网络深度，那么只能在 dense block 内部增加 layers，而我们所用的 Transition w/o pooling layer 由于不带有池化层，故消除了这种限制。</p>\n<h3 id=\"Stem-Block\"><a href=\"#Stem-Block\" class=\"headerlink\" title=\"Stem Block\"></a>Stem Block</h3><p>Stem block 包含三个 3x3 卷积以及一个 2x2 最大值池化，其中第一个卷积步幅为 2。这个 stem block 明显提高了我们实验性能，相比较于 DenseNet 中的原始设计（7x7 卷积步幅为 2，后跟一个步幅为 2 的 3x3 最大值池化），stem block 可以降低输入 image 中的信息损失。</p>\n<h3 id=\"密集预测结构\"><a href=\"#密集预测结构\" class=\"headerlink\" title=\"密集预测结构\"></a>密集预测结构</h3><p>图 1 展示了两种预测子结构：1. 朴素结构（源于 SSD）以及 2. 我们提出的密集结构。输入 image 大小为 300x300，6 种不同尺度的 feature maps 用于预测目标，其中 Scale-1 feature maps 来自 backbone 中间层，此 feature maps 尺度最大，为 38x38，用于小目标预测，其余五个尺度的 feature maps 来自于 backbone 之后的子结构。这个子结构构造方法为：如图 1 右边仅靠中心竖线的虚线框，相邻两个尺度 feature maps 之间使用 transition layer 连接起来，这个 transition layer 具有 bottleneck 结构：一个 1x1 卷积用于降低 previous scale 的 feature maps 的通道数，以及一个 3x3 卷积下采样得到 next scale 的 feature maps。</p>\n<p>在图 1 中所示的 SSD 原始预测子结构中，每个尺度的特征均由上一个尺度的特征直接转变而来。我们提出的预测子结构是一个密集结构，融合了多尺度特征。为简单起见，限制每个尺度输出相等通道的 feature maps 用于预测。在 DSOD 中，除 scale-1 之外的每个尺度中，feature maps 有一半是通过一系列的 conv 从上一尺度中学习而来，这一系列的 conv 即图 1 右边仅靠中心竖线的虚线框所标注，剩余的一半 feature maps 则直接从相邻的高分辨率的 feature maps 中降采样得到，图 1 中最右边的虚线框标注，这个降采样包含 2x2 步幅为 2 的 max pooling，以及一个 1x1 步幅为 1 的 conv，其中 max pooling 是为了两边的 feature maps 的分辨率匹配从而能够 concatenate 起来，而 1x1 conv 则是为了将 feature maps 的通道数降为一半。max pooling 层位于 1x1 conv 之前可以降低计算损害。对每个 scale 而言，仅学习一半的新 feature maps，并重新利用一半的 previous feature maps。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，可阅读原文以获取详细信息。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出 DSOD 用于 training from scratch，而这总训练方式适合 single-shot 的目标检测器，在 SSD 基础上，使用 DenseNet 作为 backbone，同时预测子网络也采用类似 DenseNet 的密集连接网络，实现了深度监督。</p>\n"},{"title":"DeRPN","date":"2019-07-15T07:04:18.000Z","_content":"论文 [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n\ntwo-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，\n![](/images/DeRPN_fig1.png)\n\nDeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。\n\n# 方法论\n## 建模\n我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：\n$$\\mathbf {t = W_t x+b_r}\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma(\\mathbf {W_c x + b_c})$$\n其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。\n\n显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，\n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\n其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。\n### 匹配复杂度\n假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。\n\n## 维度分解\n### Anchor strings\nRPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 $\\{a_n\\}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。\n\n图 2 为 DeRPN 网络，\n![](/images/DeRPN_fig2.png) <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center>\n\n如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，\n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 $\\{a_n\\}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。\n\n上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。\n\n上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，\n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n剩下的就不多说了。\n\n忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛\n\n### Label assignment\n对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。\n\n### Consistent network\nDeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 $\\{a_n\\}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。\n\n### Scale-sensitive loss function\n目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\n这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^*$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^*$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，\n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，\n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# 维度合并\nDeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。\n\n__像素级别的合并算法__ 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$，每个组合后的 bbox 的概率使用调和平均计算得到，\n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。\n\n类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。\n\n# 实验\n请阅读原文，略。\n\n# 结论\n1. 介绍了 DeRPN，将目标的宽和高两个维度进行分解\n2. 使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没","source":"_posts/DeRPN.md","raw":"---\ntitle: DeRPN\ndate: 2019-07-15 15:04:18\ntags: object detection\n---\n论文 [DeRPN: Taking a further step toward more general object detection](https://arxiv.org/abs/1811.06700)\n\ntwo-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，\n![](/images/DeRPN_fig1.png)\n\nDeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。\n\n# 方法论\n## 建模\n我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：\n$$\\mathbf {t = W_t x+b_r}\n\\\\\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))\n\\\\\\\\ P_B=\\sigma(\\mathbf {W_c x + b_c})$$\n其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。\n\n显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，\n$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))\n\\\\\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))\n\\\\\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$\n相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，\n$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))\n\\\\\\\\ P_B=g(P_s^w, P_s^h)$$\n其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。\n### 匹配复杂度\n假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。\n\n## 维度分解\n### Anchor strings\nRPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 $\\{a_n\\}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。\n\n图 2 为 DeRPN 网络，\n![](/images/DeRPN_fig2.png) <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center>\n\n如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，\n$$M_j=\\{i|\\arg \\min_i |\\log e_j - \\log a_i|\\} \\cup \\{i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta\\}, \\ (i=1,...,N) \\quad(9)$$\n$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 $\\{a_n\\}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。\n\n上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。\n\n上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，\n$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$\n剩下的就不多说了。\n\n忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛\n\n### Label assignment\n对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。\n\n### Consistent network\nDeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 $\\{a_n\\}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。\n\n### Scale-sensitive loss function\n目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，\n$$L(\\{p_i\\},\\{t_i\\})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^*) \\cdot \\Bbb I\\{i \\in R_j\\} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^*)\\cdot \\Bbb I\\{i \\in G_j\\} \\quad (10)\n\\\\\\\\ R_j=\\{k|s_k=a_j, k=1,...,M\\} \\quad (11)\n\\\\\\\\ G_j=\\{k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,...,M\\} \\quad (12)$$\n\n这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^*$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^*$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，\n$$L_{cls}(p_i,p_i^*)=- p_i^*\\log p_i-(1-p_i^*)\\log (1-p_i)\n\\\\\\\\ L_{reg}(t_i,t_i^*)=\\sum_{j \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^j,t_i^{j*})$$\n\n预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，\n$$x=x_a+w_a \\times t_x \\quad (13)\n\\\\\\\\ y=y_a+h_a \\times t_y \\quad (14)\n\\\\\\\\ w=w_a \\times e^{t_w} \\qquad (15)\n\\\\\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$\n\n# 维度合并\nDeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。\n\n__像素级别的合并算法__ 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w=\\{(x,y^{(k)},w,h^{(k)}\\}$，每个组合后的 bbox 的概率使用调和平均计算得到，\n$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$\n其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。\n\n类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h=\\{(x^{(k)},y,w^{(k)},h\\}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。\n\n# 实验\n请阅读原文，略。\n\n# 结论\n1. 介绍了 DeRPN，将目标的宽和高两个维度进行分解\n2. 使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没","slug":"DeRPN","published":1,"updated":"2019-07-16T09:15:17.107Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptm000nxgvcca3398j3","content":"<p>论文 <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>\n<p>two-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，<br><img src=\"/images/DeRPN_fig1.png\" alt></p>\n<p>DeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。</p>\n<h1 id=\"方法论\"><a href=\"#方法论\" class=\"headerlink\" title=\"方法论\"></a>方法论</h1><h2 id=\"建模\"><a href=\"#建模\" class=\"headerlink\" title=\"建模\"></a>建模</h2><p>我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：<br>$$\\mathbf {t = W_t x+b_r}<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma(\\mathbf {W_c x + b_c})$$<br>其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。</p>\n<p>显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br>相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，<br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。</p>\n<h3 id=\"匹配复杂度\"><a href=\"#匹配复杂度\" class=\"headerlink\" title=\"匹配复杂度\"></a>匹配复杂度</h3><p>假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。</p>\n<h2 id=\"维度分解\"><a href=\"#维度分解\" class=\"headerlink\" title=\"维度分解\"></a>维度分解</h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 ${a_n}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。</p>\n<p>图 2 为 DeRPN 网络，<br><img src=\"/images/DeRPN_fig2.png\" alt> <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center></p>\n<p>如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，<br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,…,N) \\quad(9)$$<br>$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 ${a_n}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。</p>\n<p>上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。</p>\n<p>上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，<br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br>剩下的就不多说了。</p>\n<p>忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛</p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p>对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。</p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 ${a_n}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。</p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p>目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，<br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,…,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,…,M} \\quad (12)$$</p>\n<p>这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^<em>$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^</em>$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，<br>$$L_{cls}(p_i,p_i^<em>)=- p_i^</em>\\log p_i-(1-p_i^<em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^</em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j*})$$</p>\n<p>预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，<br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"维度合并\"><a href=\"#维度合并\" class=\"headerlink\" title=\"维度合并\"></a>维度合并</h1><p>DeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。</p>\n<p><strong>像素级别的合并算法</strong> 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w={(x,y^{(k)},w,h^{(k)}}$，每个组合后的 bbox 的概率使用调和平均计算得到，<br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br>其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。</p>\n<p>类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h={(x^{(k)},y,w^{(k)},h}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>请阅读原文，略。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><ol>\n<li>介绍了 DeRPN，将目标的宽和高两个维度进行分解</li>\n<li>使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1811.06700\" target=\"_blank\" rel=\"noopener\">DeRPN: Taking a further step toward more general object detection</a></p>\n<p>two-stage SOTA 目标检测器通常会使用 anchor，比如 Faster R-CNN 中的 RPN，但是对于不同的数据集，则需要重新设计超参数，如 anchor 的 scale 和 aspect ratio，并且一旦选定就固定了，这在被检测目标尺度变化较大时，检测性能往往不理想，当然，也有人尝试使用 K-means 聚类计算得到 anchor，但是对最终的检测性能的提升非常有限。本文提出 DeRPN 用于解决 RPN 的这一不足之处，如图 1(b)，<br><img src=\"/images/DeRPN_fig1.png\" alt></p>\n<p>DeRPN 通过分离宽度和高度来分解检测维度（维度分解）。利用灵活的 anchor strings（不理解这个概念没关系，阅读完下一节就理解了），使得可以选择最佳 anchor 来匹配目标。</p>\n<h1 id=\"方法论\"><a href=\"#方法论\" class=\"headerlink\" title=\"方法论\"></a>方法论</h1><h2 id=\"建模\"><a href=\"#建模\" class=\"headerlink\" title=\"建模\"></a>建模</h2><p>我们知道目标检测网络通常都是一个 CNN 网络用于抽取特征，记抽取到的特征为 $\\mathbf x$，然后经过两个并行的检测分支：回归和分类，其中回归是在 anchor box （$B_a$）基础上进行回归得到目标位置，而分类分支则在最后的预测值上应用 sigmoid（二分类）或 softmax（多分类），记此函数为 $\\sigma$，从而得到 bbox 的分类置信度（概率），用数学语言描述则为：<br>$$\\mathbf {t = W_t x+b_r}<br>\\\\ B(x,y,w,h)=\\psi(\\mathbf t, B_a(x_a,y_a,w_a,h_a))<br>\\\\ P_B=\\sigma(\\mathbf {W_c x + b_c})$$<br>其中 $\\mathbf {W_r, b_r}$ 表示回归分支的权重和偏置，$\\mathbf {W_c, b_c}$ 表示分类分支的权重和偏置，$\\psi$ 表示预测 box 的位置解码，例如 Faster R-CNN 中根据位置偏差 $\\mathbf t$ 和 region proposals 的坐标计算出预测 box 的坐标。</p>\n<p>显然由于目标形状的多样性，anchor 的数量会非常大，这不利于训练，而且我们也很难设计出合适的 anchor 形状，所以当 anchor 严重偏离 gt box 时，检测性能下降! 目标检测的维度分解具体是指分离宽度和高度，以减轻目标不同尺度带来的影响。我们引入 anchor string，$(S_a^w(x_a,w_a), S_a^h(y_a,h_a))$，各自分别作为目标宽度和高度的回归参照，anchor string 分别独立预测 $(S_w(x,w), S_h(y,h))$ 以及对应的分类概率 $(P_s^w, P_s^h)$，此过程的数学语言描述为，<br>$$\\mathbf t^w=\\mathbf {W_r}^w \\mathbf {x+ b_r}^w \\qquad S_w(x,w)=\\psi(\\mathbf t^w, S_a^w(x_a,w_a))<br>\\\\ \\mathbf t^h=\\mathbf {W_r}^h \\mathbf {x+ b_r}^h \\qquad S_h(x,w)=\\psi(\\mathbf t^h, S_a^h(y_a,h_a))<br>\\\\ P_s^w=\\sigma (\\mathbf {W_c}^w \\mathbf {x+b_c}^w) \\qquad P_s^h=\\sigma (\\mathbf {W_c}^h \\mathbf {x+b_c}^h)$$<br>相比上一组计算式，容易看出确实是将宽度和高度分类开来（包括分类概率也分解为两个维度上各自独立的分类概率）。现在我们从分解开来的两个维度预测恢复出 bbox 的位置以及分类置信度，<br>$$B(x,y,w,h)=f(S_w(x,w),S_h(y,h))<br>\\\\ P_B=g(P_s^w, P_s^h)$$<br>其中，f 表示合并两个维度的一种策略函数，g 计算合并后 bbox 的分类置信度（可以是算术平均，或调和平均）。</p>\n<h3 id=\"匹配复杂度\"><a href=\"#匹配复杂度\" class=\"headerlink\" title=\"匹配复杂度\"></a>匹配复杂度</h3><p>假设数据集中目标的宽度或高度共有 n 种情况，那么一共有 $n^2$ 种情况需要 anchor box 去匹配，即，匹配复杂度为 $O(n^2)$，而在维度分解下，n 种宽度和高度分别独立地由 anchor string 去匹配，匹配复杂度降为 $O(n)$。</p>\n<h2 id=\"维度分解\"><a href=\"#维度分解\" class=\"headerlink\" title=\"维度分解\"></a>维度分解</h2><h3 id=\"Anchor-strings\"><a href=\"#Anchor-strings\" class=\"headerlink\" title=\"Anchor strings\"></a>Anchor strings</h3><p>RPN 以 anchor string 作为回归参照，DeRPN 则将二维 box 拆分为两个独立的一维部分作为回归参照，称为 anchor string。虽说 anchor string 可以匹配任意object 的宽度或高度，设置 anchor string 为一个等比数列 ${a_n}$，例如 (16,32,64,128,256,512,1024)，此时可用于匹配目标宽度或高度的范围为 $[8\\sqrt 2,1024 \\sqrt 2]$，通常这已经足够覆盖很多场景下的目标尺寸了。解释一下这个的 $\\sqrt 2$，记一个 anchor string 长度值（等比数列中的一项）为 $a_i$，这个 anchor string 可匹配的目标边长范围为$[a_i/\\sqrt 2, a_i\\sqrt 2]$，由于等比数列中公比为2，此时这个等比数列中各项所匹配的目标边长范围无缝连接，形成一个大的范围 $[8\\sqrt 2,1024 \\sqrt 2]$。</p>\n<p>图 2 为 DeRPN 网络，<br><img src=\"/images/DeRPN_fig2.png\" alt> <center>(a) 目标宽度和高度分别独立使用 anchor string 匹配，粗线表示匹配较好的 anchor string；(b) 在 anchor string 上应用分类和回归，虚线表示置信度低的 anchor string；(c) 合并预测的宽度和高度生成 bbox；(d) 使用置信度阈值和 NMS 过滤得到 region proposals。</center></p>\n<p>如何为目标选择最佳匹配的 anchor string？在 RPN 中，通过 anchor box 与 gt box 的 IoU 决定是否选择 anchor 参与训练。比如， anchor 的最大 IoU 超过 0.7，或者 gt 的最大 IoU 对应的 anchor 均可作为正例。在 DeRPN 中则基于长度将 anchor string 与目标进行匹配，评估最佳匹配 anchor string 的方法为，<br>$$M_j={i|\\arg \\min_i |\\log e_j - \\log a_i|} \\cup {i,i+1| \\begin{vmatrix}\\frac {e_j} {a_i} - \\sqrt q \\end{vmatrix} \\le \\beta}, \\ (i=1,…,N) \\quad(9)$$<br>$M_j$ 表示与第 j 个目标匹配的 anchor string 的索引，$e_j$ 是目标边长（宽或高），N 是等比数列 ${a_n}$ 中的项数，q 是等比数列的公比（本文中设置为 2）。</p>\n<p>上式中，第一项表示选择与目标边长最接近的 anchor string，这是一种很直观的选择策略，然而还有第二种选择策略，见上式第二项，我们将条件约束稍作变形得 $(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$，范围 $[(\\sqrt q-\\beta)\\times a_i, (\\sqrt q+\\beta)\\times a_i]$ 称为 i 关联的转移区间，$\\beta$ 控制区间长度，如果目标边长 $e_j$ 位于此范围内，那么选择 i 和 i+1 作为匹配的 anchor string 的索引。</p>\n<p>上文我们说到 $a_i$ 可匹配的目标边长范围为 $[a_i/ \\sqrt q,a_i\\sqrt q]$，按道理说，如果 $e_j$ 落于这个区间，就选择 i 作为匹配的索引就好了鸭（不考虑边长等于区间端点值的情况，事实上这种情况的可能性为0），但是考虑到图像噪声和 gt 标记偏离正确位置等因素，按照这个选择策略选择的 i 不一定准确，而图像噪声和 gt 标记偏离正确位置等因素所带来的影响相对较小，所以我们选择连续的两个 anchor string 索引即可保证目标能落入这两个连续 anchor string 的可匹配范围，$a_i, a_{i+1}$ 的可匹配范围为 $[a_i/\\sqrt q, a_i \\sqrt q] \\cup [a_i \\sqrt q,qa_i\\sqrt q]$，其（非几何）“中心”为 $a_i \\sqrt q$，所以很自然地，如果目标边长 $e_j$ 在这个“中心”附近，就选择 i 和 i+1 作为匹配索引，判断是否在附近的条件不难理解，<br>$$(\\sqrt q-\\beta)\\times a_i \\le e_j \\le (\\sqrt q+\\beta)\\times a_i$$<br>剩下的就不多说了。</p>\n<p>忽略转移区间，可以知道 anchor string 与目标边长之间的最大偏移比例为 $\\sqrt q$（如果考虑转移区间，最大偏移比例则为 $\\max(\\sqrt q + \\beta, q/(\\sqrt q-\\beta))$，也就比 $\\sqrt q$ 大一点点），这表示 DeRPN 中回归损失是有界的，而 RPN 中较小的 IoU 则会导致较大的回归损失，经验表明，如果 anchor box 严重偏离 gt，RPN 甚至无法收敛</p>\n<h3 id=\"Label-assignment\"><a href=\"#Label-assignment\" class=\"headerlink\" title=\"Label assignment\"></a>Label assignment</h3><p>对齐的 anchor string 位于 feature map 上目标中心处，其中与目标匹配较好的（根据式 (9)）则标记为正。除了对齐的 anchor string，还使用了 observe-to-distribute 策略来选择其他 anchor string：1. 观察每个 anchor string 的回归结果，回归之后，结合宽度/高度的预测得到 region proposal，如果这个 region proposal 与某个 gt 的 IoU 大于一定阈值（0.6），那么就将正标签分发到对应的 anchor string 上。不满足以上任何条件的 anchor string 则标记为负。</p>\n<h3 id=\"Consistent-network\"><a href=\"#Consistent-network\" class=\"headerlink\" title=\"Consistent network\"></a>Consistent network</h3><p>DeRPN 与 RPN 的网络结构是一致的，故可方便地移植到当前 two-stage 目标检测器中。如图 2 所示，由一个 3x3 的卷积层，后跟两个并列的 1x1 卷积层，分别用于分类和回归，组成了 DeRPN 网络。记 anchor string 长度的等比数列为 ${a_n}$，数量为 N，宽度和高度独立使用 anchor string，分类预测 $2\\times 2N$ 个得分来估计 anchor string 是否匹配目标边长（二值分类置信度），anchor string 预测目标的宽需要两个值 $(x,w)$，同理对于目标的高也需要两个值 $(y,h)$，故回归一共预测 $2 \\times 2N$ 个值。</p>\n<h3 id=\"Scale-sensitive-loss-function\"><a href=\"#Scale-sensitive-loss-function\" class=\"headerlink\" title=\"Scale-sensitive loss function\"></a>Scale-sensitive loss function</h3><p>目标的尺度分布不是均匀的，大目标比小目标更多。如果简单地将目标混合起来计算损失，那么小目标对损失的影响将会被大目标带来的影响所淹没，本文提出一种新型的尺度敏感的损失函数，公平地对待不同尺度的目标，<br>$$L({p_i},{t_i})=\\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|R_j|} L_{cls}(p_i,p_i^<em>) \\cdot \\Bbb I{i \\in R_j} + \\lambda \\sum_{j=1}^N \\sum_{i=1}^M \\frac 1 {|G_j|} L_{reg} (t_i,t_i^</em>)\\cdot \\Bbb I{i \\in G_j} \\quad (10)<br>\\\\ R_j={k|s_k=a_j, k=1,…,M} \\quad (11)<br>\\\\ G_j={k|s_k \\in A, s_k=a_j, p_i^*=1, k=1,…,M} \\quad (12)$$</p>\n<p>这里，N 是等比数列的项数，M 是 batch size，s 表示 anchor string，$p_i$ 表示一个批次中第 i 个 anchor string 的预测概率，$p_i^<em>$ 表示 gt label，当 anchor string 为正时等于 1， 否则等于 0。$t_i$ 表示参数化坐标的预测向量，$t_i^</em>$ 为相应的 gt 向量。A 表示对齐的 anchor string 集合。$R_j$ 这个索引集包含了具有相同尺度的 anchor string，其中 j 用于指示尺度 $a_j$。$G_j$ 这个索引集包含了具有相同尺度的对齐的正 anchor string，同样 j 用于指示尺度 $a_j$。上式表明每个尺度下的目标损失均根据这个尺度下的 anchor string 数量进行归一化，这可以有效地避免小目标优化作用被大目标淹没。分类损失使用交叉熵，回归损失使用 smooth L1 损失，<br>$$L_{cls}(p_i,p_i^<em>)=- p_i^</em>\\log p_i-(1-p_i^<em>)\\log (1-p_i)<br>\\\\ L_{reg}(t_i,t_i^</em>)=\\sum_{j \\in {x,y,w,h}} smooth_{L_1}(t_i^j,t_i^{j*})$$</p>\n<p>预测值 t 表示坐标偏差，这一点与 Fast/Faster R-CNN 中完全一样，故可根据下式解码出预测 box 坐标，<br>$$x=x_a+w_a \\times t_x \\quad (13)<br>\\\\ y=y_a+h_a \\times t_y \\quad (14)<br>\\\\ w=w_a \\times e^{t_w} \\qquad (15)<br>\\\\ h=h_a \\times e^{t_h} \\qquad (16)$$</p>\n<h1 id=\"维度合并\"><a href=\"#维度合并\" class=\"headerlink\" title=\"维度合并\"></a>维度合并</h1><p>DeRPN 使用维度分解来预测，然而最终的 region proposal 是二维的 bbox，故需要合并宽和高以恢复出 region proposal。</p>\n<p><strong>像素级别的合并算法</strong> 根据预测坐标偏差 t 和 anchor string 可以解码出宽和高，记所有预测宽的集合为 W，根据预测宽的概率选择 top-N，记 $W_N$，对于这 top-N 中任意一个宽的预测 (x,w)（对应的概率为 $p^W$），我们在 (x,w) 所在的像素位置处选择 top-k 的目标高的预测 $(y^{(k)},h^{(k)})$，于是得到一系列的 bbox $B_w={(x,y^{(k)},w,h^{(k)}}$，每个组合后的 bbox 的概率使用调和平均计算得到，<br>$$p^B=2/ \\left(\\frac 1 {p^W}+\\frac 1 {p^H}\\right)$$<br>其中 $p^W$ 为 (x,w) 对应的预测概率，$p^H$ 为 $(y^{(k)},h^{(k)})$ 对应的预测概率。</p>\n<p>类似地，对于 top-N 预测概率的目标高 $H_N$，按上面的策略选择得到 $B_h={(x^{(k)},y,w^{(k)},h}$，对这两个集合的并 $B=B_w \\cup B_h$ 使用 NMS，然后再选择 top-M 作为 region proposals。尽管这个合并过程引入了一些背景 bbox，但是第二 stage 的目标检测器可以通过分类分支抑制它们。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>请阅读原文，略。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><ol>\n<li>介绍了 DeRPN，将目标的宽和高两个维度进行分解</li>\n<li>使用了新型损失函数，避免了小目标（少数）的优化作用被大目标（多数）淹没</li>\n</ol>\n"},{"title":"FSAF","date":"2019-06-27T01:14:42.000Z","mathjax":true,"_content":"论文：[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n\n目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：\n1. 启发式导向的特征选择\n2. 基于 overlap 选取 anchor \n\n第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。\n\n![](/images/FSAF_fig2.png)\n\n本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center>\n\n对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。\n\n# FSAF 模块\n我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：\n1. 如何创建 anchor-free 分支\n2. 如何生成 anchor-free 分支的监督信号（GT target）\n3. 如何为每个实例动态选择 feature level\n4. 如何联合训练/测试 anchor-free 分支和 anchor-based 分支\n\n## 网络框架\n图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 $\\{P_l|l\\in [3,7]\\}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。\n\n基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，\n\n![](/images/FSAF_fig4.png)<center>Fig 4 具有 FSAF 的 RetinaNet 框架</center>\n\n这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。\n\n## Ground-truth and Loss\n给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n（到这里就发现与 [GA-RPN](/2019/06/25/GA-RPN) 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）\n\n图 5 是一个 car 实例的 GT 生成（GT target）的例子\n\n![](/images/FSAF_fig5.png)\n\n__分类输出：__ 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：\n- 位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域\n- 位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域\n- 如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略\n\n这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。\n\n如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。\n\n__Box 回归输出：__ 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 [FCOS](FCOS) 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为\n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n具体可参考 UnitBox。\n\nInference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。\n\n## 在线特征选择\nFSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。\n\n给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。\n\n图 6 显示了在线特征选择的过程。\n\n![](/images/FSAF_fig6.png) <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center>\n\n首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。\n\n我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l'}$，其中\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。\n\n## Joint Inference and Training\n当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。\n\n__Inference:__ FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。\n\n__初始化：__ backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。\n\n__优化：__ 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。\n\n# 实验\n实验介绍以及结果分析略，请阅读原文以获取详细信息。\n\n#总结\n本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。","source":"_posts/FSAF.md","raw":"---\ntitle: FSAF\ndate: 2019-06-27 09:14:42\ntags: object detection\nmathjax: true\n---\n论文：[Feature Selective Anchor-Free Module for Single-Shot Object Detection](https://arxiv.org/pdf/1903.00621)\n\n目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：\n1. 启发式导向的特征选择\n2. 基于 overlap 选取 anchor \n\n第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。\n\n![](/images/FSAF_fig2.png)\n\n本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，\n\n![](/images/FSAF_fig3.png) <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center>\n\n对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。\n\n# FSAF 模块\n我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：\n1. 如何创建 anchor-free 分支\n2. 如何生成 anchor-free 分支的监督信号（GT target）\n3. 如何为每个实例动态选择 feature level\n4. 如何联合训练/测试 anchor-free 分支和 anchor-based 分支\n\n## 网络框架\n图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 $\\{P_l|l\\in [3,7]\\}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。\n\n基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，\n\n![](/images/FSAF_fig4.png)<center>Fig 4 具有 FSAF 的 RetinaNet 框架</center>\n\n这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。\n\n## Ground-truth and Loss\n给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有\n$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l\n\\\\\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$\n（到这里就发现与 [GA-RPN](/2019/06/25/GA-RPN) 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）\n\n图 5 是一个 car 实例的 GT 生成（GT target）的例子\n\n![](/images/FSAF_fig5.png)\n\n__分类输出：__ 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：\n- 位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域\n- 位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域\n- 如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略\n\n这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。\n\n如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，\n$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t\n\\\\\\\\p_t=\\begin{cases} p & y=1 \\\\\\\\1-p & y=0 \\end{cases}\n\\\\\\\\\\alpha_t=\\begin{cases} \\alpha & y=1 \\\\\\\\1-\\alpha & y=0 \\end{cases}$$\nanchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。\n\n__Box 回归输出：__ 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：\n$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$\n其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 [FCOS](FCOS) 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为\n$$L_{IoU}=-\\log IoU\n\\\\\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$\n具体可参考 UnitBox。\n\nInference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。\n\n## 在线特征选择\nFSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。\n\n给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下\n$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)\n\\\\\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$\n其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。\n\n图 6 显示了在线特征选择的过程。\n\n![](/images/FSAF_fig6.png) <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center>\n\n首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，\n$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$\n对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。\n\n我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l'}$，其中\n$$l' = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$\n其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，\n$$ \\sqrt{wh}/2^{l'} \\approx 224/2^{l_0}$$\n可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。\n\n## Joint Inference and Training\n当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。\n\n__Inference:__ FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。\n\n__初始化：__ backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。\n\n__优化：__ 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。\n\n# 实验\n实验介绍以及结果分析略，请阅读原文以获取详细信息。\n\n#总结\n本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。","slug":"FSAF","published":1,"updated":"2019-06-27T12:15:21.594Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptn000pxgvcxllat9gl","content":"<p>论文：<a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>\n<p>目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：</p>\n<ol>\n<li>启发式导向的特征选择</li>\n<li>基于 overlap 选取 anchor </li>\n</ol>\n<p>第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。</p>\n<p><img src=\"/images/FSAF_fig2.png\" alt></p>\n<p>本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt> <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center></p>\n<p>对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。</p>\n<h1 id=\"FSAF-模块\"><a href=\"#FSAF-模块\" class=\"headerlink\" title=\"FSAF 模块\"></a>FSAF 模块</h1><p>我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：</p>\n<ol>\n<li>如何创建 anchor-free 分支</li>\n<li>如何生成 anchor-free 分支的监督信号（GT target）</li>\n<li>如何为每个实例动态选择 feature level</li>\n<li>如何联合训练/测试 anchor-free 分支和 anchor-based 分支</li>\n</ol>\n<h2 id=\"网络框架\"><a href=\"#网络框架\" class=\"headerlink\" title=\"网络框架\"></a>网络框架</h2><p>图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 ${P_l|l\\in [3,7]}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。</p>\n<p>基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt><center>Fig 4 具有 FSAF 的 RetinaNet 框架</center></p>\n<p>这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p>给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br>（到这里就发现与 <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a> 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）</p>\n<p>图 5 是一个 car 实例的 GT 生成（GT target）的例子</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt></p>\n<p><strong>分类输出：</strong> 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：</p>\n<ul>\n<li>位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域</li>\n<li>位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域</li>\n<li>如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略</li>\n</ul>\n<p>这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。</p>\n<p>如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。</p>\n<p><strong>Box 回归输出：</strong> 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br>其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 <a href=\"FCOS\">FCOS</a> 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为<br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br>具体可参考 UnitBox。</p>\n<p>Inference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。</p>\n<h2 id=\"在线特征选择\"><a href=\"#在线特征选择\" class=\"headerlink\" title=\"在线特征选择\"></a>在线特征选择</h2><p>FSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。</p>\n<p>给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。</p>\n<p>图 6 显示了在线特征选择的过程。</p>\n<p><img src=\"/images/FSAF_fig6.png\" alt> <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center></p>\n<p>首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br>对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。</p>\n<p>我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l’}$，其中<br>$$l’ = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，<br>$$ \\sqrt{wh}/2^{l’} \\approx 224/2^{l_0}$$<br>可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。</p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p>当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。</p>\n<p><strong>Inference:</strong> FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。</p>\n<p><strong>初始化：</strong> backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。</p>\n<p><strong>优化：</strong> 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍以及结果分析略，请阅读原文以获取详细信息。</p>\n<p>#总结<br>本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://arxiv.org/pdf/1903.00621\" target=\"_blank\" rel=\"noopener\">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></p>\n<p>目标检测中一个具有挑战性的问题是目标尺度的变化，即，在检测极小目标或极大目标时，往往检测性能不够好。为了达到尺度不变性，SOTA 检测器使用 feature pyramid 或 image pyramid。比如使用 feature pyramid 时，高 level 的 feature 对应大 anchor，低 level 的 feature 对应小 anchor，如图 2，高 level 的 feature 拥有更多的语义信息，适合检测大目标，而低 level 的 feature 由于保持了细粒度的信息，所以适合检测小目标。但是这种网络设计有两个局限：</p>\n<ol>\n<li>启发式导向的特征选择</li>\n<li>基于 overlap 选取 anchor </li>\n</ol>\n<p>第 1 点是指对某个目标的检测选择哪个 level 的 feature 是启发式的，或者说是通过不断的实验、试错从而找到这个问题的解，这就导致为某个目标所选的 feature level 可能不是最优的。第 2 点则指出每个目标总是需要根据 IoU 去匹配到最近的 anchor 上去。</p>\n<p><img src=\"/images/FSAF_fig2.png\" alt></p>\n<p>本文则提出一个简单而有效的方法同时解决以上两个局限，此方法名为 feature selective anchor-free (FSAF)，目的是为了让每个目标实例选择最佳 feature level。如图 3，</p>\n<p><img src=\"/images/FSAF_fig3.png\" alt> <center>Fig 3 FSAF 插入到传统基于 anchor 的检测模块中。训练阶段，根据特征选择将每个目标实例分配到一个pyramid level上</center></p>\n<p>对 feature pyramid 的每个 level 均使用一个 anchor-free 分支，此分支与 anchor-based 分支类似，包含一个分类子网络和回归子网络（图 3 中没有展示出来）。目标实例可被分配到任意 level 的 anchor-free 分支。训练阶段，基于实例内容而不仅仅是实例 box 为每个目标实例动态选择最佳 feature level。这个 feature level 负责学习并检测这个被分配过来的目标实例。Inference 阶段，可以单独使用 FSAF 模块或者将其与 anchor-based 分支结合使用。此外，anchor-free 分支和在线特征选择可以使用复杂的结构，但是在我们的实验中，我们选择简单的 FSAF 模块结构，所以 FSAF 模块的计算量与整个网络相比是很小的。</p>\n<h1 id=\"FSAF-模块\"><a href=\"#FSAF-模块\" class=\"headerlink\" title=\"FSAF 模块\"></a>FSAF 模块</h1><p>我们来看下如何实现 FSAF 模块以及如何将其整合到具有 feature pyramid 的 single-shot 检测器（如 SSD, DSSD, RetinaNet）中。不失一般性，我们将 FSAF 应用到 SOTA 的 RetinaNet。从以下几个方面来说明我们的设计：</p>\n<ol>\n<li>如何创建 anchor-free 分支</li>\n<li>如何生成 anchor-free 分支的监督信号（GT target）</li>\n<li>如何为每个实例动态选择 feature level</li>\n<li>如何联合训练/测试 anchor-free 分支和 anchor-based 分支</li>\n</ol>\n<h2 id=\"网络框架\"><a href=\"#网络框架\" class=\"headerlink\" title=\"网络框架\"></a>网络框架</h2><p>图 4 是将 FSAF 应用到 RetinaNet 的网络结构。简单而言，Retina 由一个 backbone 网络以及两个特定任务的子网络组成。从 backbone 网络中构建 feature pyramid，其 level 为 ${P_l|l\\in [3,7]}$，$P_l$ 分辨率为输入 image 的 $1/2^l$ 倍。图 4 中为了简单起见仅显示了三个 level 的 feature pyramid，每个 level 负责检测一定 scale 范围的目标，每个 feature level 后接分类子网络和回归子网络，这俩子网络均为小型全卷积网络。</p>\n<p>基于 RetinaNet，FSAF 仅在每个 feature level 增加两个卷积层，如图 4，</p>\n<p><img src=\"/images/FSAF_fig4.png\" alt><center>Fig 4 具有 FSAF 的 RetinaNet 框架</center></p>\n<p>这两个卷积层分别负责 anchor-free 分支的分类预测和回归预测。具体地，具有 3x3 大小的 K 个卷积核的卷积层附在分类子网络上，这个卷积层后跟一个 sigmoid 函数用于将分类得分归一化，与 anchor-based 的分类卷积层并列，用于预测空间每个位置点的 K 个分类的得分（置信度）。回归子网络则类似的使用 3x3 大小的 4 个卷积核的卷积层后跟一个 ReLu 函数，用于预测 anchor-free 方式的 box 偏差。anchor-free 分支和 anchor-based 分支以多任务方式联合运作并共享所属 level 的 feature。</p>\n<h2 id=\"Ground-truth-and-Loss\"><a href=\"#Ground-truth-and-Loss\" class=\"headerlink\" title=\"Ground-truth and Loss\"></a>Ground-truth and Loss</h2><p>给定一个目标，我们知道其分类 k 和 bbox 坐标 b=[x,y,w,h]。此目标可被分配到任意 feature level，定义此目标映射到 $P_l$ 的 box 为 $b_p^l=[x_p^l,y_p^l,w_p^l,h_p^l]$，由于 $P_l$ 分辨率是输入 image 的 $1/2^l$，故 $b_p^l=b/2^l$。定义一个有效 box $b_e^l=[x_e^l,y_e^l,w_e^l,h_e^l]$ 和一个 ignore box $b_i^l=[x_i^l,y_i^l,w_i^l,h_i^l]$，均为 $b_p^l$ 的线性缩放，比例分别为 $\\epsilon_e, \\ \\epsilon_i$，于是有<br>$$x_e^l=x_p^l, \\ y_e^l=y_p^l, \\ w_e^l=\\epsilon_e w_p^l, \\ h_e^l=\\epsilon_e h_p^l<br>\\\\x_i^l=x_p^l, \\ y_i^l=y_p^l, \\ w_i^l=\\epsilon_i w_p^l, \\ h_i^l=\\epsilon_i h_p^l$$<br>（到这里就发现与 <a href=\"/2019/06/25/GA-RPN\">GA-RPN</a> 中完全一样有木有，所以 anchor-free 到底是什么，是不是也突然明白了什么，如果与 GA-RPN 中一样的话，那么 anchor-free 就是指没有预设 scale 和 aspect ratio 生成的均匀密集分布的 anchor，也就是说 anchor-free 还是有 anchor 的，只不过其 shape 是任意的、动态生成的，而不是 anchor-based 那样固定的 scale 和 aspect ratio。好的，先不管是不是这样，我们继续往下讨论。）</p>\n<p>图 5 是一个 car 实例的 GT 生成（GT target）的例子</p>\n<p><img src=\"/images/FSAF_fig5.png\" alt></p>\n<p><strong>分类输出：</strong> 分类的 GT output 是 K-channel maps，每个 map 对应一个分类。假设目标分类为 k，那么对第 k 个 GT map 有：</p>\n<ul>\n<li>位于 $b_e^l$ 内的为正例，值为 1，如图 5 中白色区域</li>\n<li>位于 $b_i^l - b_e^l$ 内的点忽略，此区域的梯度不进行反向传播，如图 5 中灰色区域</li>\n<li>如果存在邻近 feature level，那么其上的 $b_i^{l-1}, b_i^{l+1}$ 区域也被忽略</li>\n</ul>\n<p>这里需要注意的是，由于在线特征选择模块，单个实例最终只用在最佳的某个 feature level 上。</p>\n<p>如果两个实例的有效 box 重叠了，较小尺度的实例优先权更高。gt map 的剩余区域则是负例，值为 0，如图 5 中黑色区域。分类损失使用 Focal loss，<br>$$FL(p_t)=-\\alpha_t (1-p_t)^{\\gamma} \\log p_t<br>\\\\p_t=\\begin{cases} p &amp; y=1 \\\\1-p &amp; y=0 \\end{cases}<br>\\\\\\alpha_t=\\begin{cases} \\alpha &amp; y=1 \\\\1-\\alpha &amp; y=0 \\end{cases}$$<br>anchor-free 的总分类损失为除 ignore box 之外的区域内所有点的 focal loss 之和，并除以有效 box 内点的数量进行归一化。</p>\n<p><strong>Box 回归输出：</strong> 回归输出的 gt 为 4-channal maps，表示 4 个偏差值（与分类无关，否则就是 4K-channel 了）。实例仅影响 gt maps 上 $b_e^l$ 区域的值，对 $b_e^l$ 内某一像素点位置 (i,j)，我们使用一个 4-d 向量来表示 $b_p^l$：<br>$$\\mathbf d_{i,j}^l=[d_{t_{i,j}}^l,d_{l_{i,j}}^l,d_{b_{i,j}}^l,d_{r_{i,j}}^l]$$<br>其中 $d_t^l,d_l^l,d_b^l,d_r^l$ 分别为当前位置点 (i,j) 到 $b_p^l$ 的 top,left,bottom,right 四条边的距离。这个与 <a href=\"FCOS\">FCOS</a> 是差不多的，毕竟都是 anchor-free 的。然后点 (i,j) 处的 4-d 向量归一化为 $\\mathbf d_{i,j}^l/S$，根据经验 S=4（可能是训练过程中发现这样归一化后不容易出现梯度饱和的现象，或者是训练更加稳定）。有效 box 之外的区域的梯度全部忽略。采用 IoU 损失来优化此分支参数。anchor-free 的总回归损失为所有有效 box 区域的 IoU 损失的平均（损失之和对有效 box 内点的数量取平均），其中单点 IoU 损失为<br>$$L_{IoU}=-\\log IoU<br>\\\\ IoU = \\frac {I(b_p,b_{gt})} {U(b_p,b_{gt})}$$<br>具体可参考 UnitBox。</p>\n<p>Inference 阶段，从分类输出和回归输出中解码出预测 box。在位置 (i,j)，假设预测偏差输出为 $[\\hat o_{t_{i,j}},\\hat o_{l_{i,j}},\\hat o_{b_{i,j}},\\hat o_{r_{i,j}}]$，那么预测距离为 $[S\\hat o_{t_{i,j}},S\\hat o_{l_{i,j}},S\\hat o_{b_{i,j}},S\\hat o_{r_{i,j}}]$，于是左上角和右下角坐标分别为 $(i-S\\hat o_{t_{i,j}},j-S\\hat o_{l_{i,j}}), \\ (i+\\hat o_{b_{i,j}},j+\\hat o_{r_{i,j}})$，最后再乘以 $2^l$ 就恢复到输入 image 上的预测框，其置信度得分和分类则可以根据分类输出 maps 上 (i,j) 处的 K-d 向量决定。</p>\n<h2 id=\"在线特征选择\"><a href=\"#在线特征选择\" class=\"headerlink\" title=\"在线特征选择\"></a>在线特征选择</h2><p>FSAF 模块为每个实例选择最佳 level 的 feature $P_l$，这种选择是基于实例的内容，而 anchor-based 中则是基于实例 box 大小，显然基于实例内容更加合理。</p>\n<p>给力实例 $I$，定义其在 $P_l$ 上的分类损失和回归损失分别为 $L_{FL}^I(l), \\ L_{IoU}^I(l)$，计算式如下<br>$$L_{FL}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} FL(l,i,j)<br>\\\\L_{IoU}^I(l)=\\frac 1 {N(b_e^l)} \\sum_{i,j \\in b_e^l} IoU(l,i,j)$$<br>其中，$N(b_e^l)$ 为有效 box 内点的数量。注意因为只考虑实例 $I$ 的损失，故分类损失只考虑了正例损失的那部分。</p>\n<p>图 6 显示了在线特征选择的过程。</p>\n<p><img src=\"/images/FSAF_fig6.png\" alt> <center>Fig 6 在线特征选择机制。每个实例通过所有level的anchor-free分支以相应的计算平均分类损失和平均回归损失，然后具有最小两种损失之和的分支为最佳分支，在此分支上设立此实例的监督信号（gt target）</center></p>\n<p>首先实例 $I$ 前向传播到 feature pyramid，然后计算每个 anchor-free 分支的 $L_{FL}^I(l) + L_{IoU}^I(l)$ 的和，最后根据最小损失之和选择最佳 feature pyramid leve $P_l$，<br>$$l^*=\\arg \\min_l L_{FL}^I(l) + L_{IoU}^I(l)$$<br>对于一个训练批次，更新某 level 的特征仅使用分配到此 level 上的实例。直觉上，根据这种方法选择的特征最适合对实例进行建模，因为此时的损失在特征空间构成损失下限，而经过训练又进一步地拉低损失下限。Inference 阶段，我们不需要手动选择使用哪个特征，在线选择的最适合的特征将会输出高置信度得分。</p>\n<p>我们比较了启发式特征选择和在线特征选择。启发式特征选择仅依赖于 box size，例如在 FPN 检测器中，实例 $I$ 将被分配到 $P_{l’}$，其中<br>$$l’ = \\lfloor l_0+\\log_2(\\sqrt{wh}/224) \\rfloor$$<br>其中，(w,h) 是实例 size，224 是典型的 ImageNet 预训练尺寸，224x224 应该映射到 $l_0$ 这个 target level 上。如何理解上式？首先 $P_l$ 的分辨率是原始输入 image 的 $1/2^l$，然后将上式变形如下就能理解了，<br>$$ \\sqrt{wh}/2^{l’} \\approx 224/2^{l_0}$$<br>可见是将一个 scale 范围按 $1/2^l$ 的比例分配。我们这里选择 $l_0=5$，因为 ResNet 使用 conv5_x 卷积组的 feature map 进行分类预测。</p>\n<h2 id=\"Joint-Inference-and-Training\"><a href=\"#Joint-Inference-and-Training\" class=\"headerlink\" title=\"Joint Inference and Training\"></a>Joint Inference and Training</h2><p>当 FSAF 模块插入到 RetinaNet 中时，如图 4，我们保持原来的 anchor-based 分支不变，所有的超参也不变。</p>\n<p><strong>Inference:</strong> FSAF 仅增加少量的卷积层。对于 anchor-free 分支，我们对每个 pyramid level 的分类输出使用置信度阈值 0.05 进行过滤，然后分别选取 top 1k 得分的位置点，从这些位置解码出预测 box，所有 level 的预测 box 与 anchor-based 分支的预测 box 合并起来，并使用非极大抑制 NMS，NMS 阈值为 0.5，得到最后的检测结果。</p>\n<p><strong>初始化：</strong> backbone 网络使用 ImageNet1k 预训练。RetinaNet 中的 layers 与原始 RetinaNet 中 layers 的初始化相同。FSAF 中的 分类分支的 layers 初始化所用的高斯分布权值 $\\sigma=0.01$，偏置 bias 为 $-\\log((1-\\pi)/\\pi)$，其中 $\\pi$ 指明训练初始时各像素点输出是否存在目标的得分值在 $\\pi$ 上下。我们遵循原始 RetineNet 中的设置 $\\pi=0.01$。所有回归分支的 layers 初始化使用偏置 b=0.1，高斯权值 $\\sigma=0.01$。以上初始化过程由于避免生成较大的损失，从而有助于训练初期过程的稳定。</p>\n<p><strong>优化：</strong> 整个网络的损失来自于 anchor-free 分支和 anchor-based 分支。记原始 RetinaNet 的总损失为 $L^{ab}$，而 $L_{cls}^{af}, \\ L_{reg}^{af}$ 分布为 anchor-free 分支的分类损失和回归损失。那么，整个网络的总损失为 $L=L^{ab}+\\lambda (L_{cls}^{af} + L_{reg}^{af})$，其中 $\\lambda$ 用于平衡两者，实验中设置 $\\lambda=0.5$。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验介绍以及结果分析略，请阅读原文以获取详细信息。</p>\n<p>#总结<br>本文指出了具有 feature pyramid 的 anchor-based single-shot 目标检测器中启发式选择特征的不足之处，并提出 FSAF 模块以解决这个问题，FSAF 使用了 anchor-free 分支以及在线特征选择，显著提高了检测性能，inference 的耗费增加较少，但是性能超过最近的 SOTA single-shot 检测器。</p>\n"},{"title":"M2Det","date":"2019-06-28T09:59:08.000Z","mathjax":true,"_content":"论文：[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n\n# Introduction\n我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，\n![](/images/M2Det_fig1.png)\n\nSSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：\n1. pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。\n2. pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。\n\n一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。\n\n本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，\n![](/images/M2Det_fig2.png)\n\n首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。\n\n为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。\n\n# Method\nM2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。\n\n## MLFPN\n如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：\n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。\n\n### FFM\nFFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 __对称__ 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center>\n\n### TUM\nTUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。\n\n### SFAM\nSFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center>\n\n第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为 \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n其中 $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，\n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$。\n\n### 网络配置\n分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。\n\nMLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为\n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。\n\n在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 [soft-NMS](/2019/06/24/cv-mtds) 进一步处理检测结果。\n\n# 实验\n实验略，请阅读原文以获取详细信息\n\n# 结论\n提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。","source":"_posts/M2Det.md","raw":"---\ntitle: M2Det\ndate: 2019-06-28 17:59:08\ntags: object detection\nmathjax: true\n---\n论文：[M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network](https://arxiv.org/abs/1811.04533)\n\n# Introduction\n我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，\n![](/images/M2Det_fig1.png)\n\nSSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：\n1. pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。\n2. pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。\n\n一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。\n\n本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，\n![](/images/M2Det_fig2.png)\n\n首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。\n\n为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。\n\n# Method\nM2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。\n\n## MLFPN\n如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：\n$$[x_1^l,...x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) & l=1\n\\\\\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) & l=2,...L \\end{cases}$$\n其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。\n\n### FFM\nFFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 __对称__ 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。\n![](/images/M2Det_fig4.png) <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center>\n\n### TUM\nTUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。\n\n### SFAM\nSFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，\n![](/images/M2Det_fig3.png)<center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center>\n\n第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为 \n$$\\mathbf X=[\\mathbf X_1,...,\\mathbf X_i]$$\n其中 $\\mathbf X_i=Concat(x_i^1,...x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，\n$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$\n其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，\n\n$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$\n\n最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},...,\\tilde {\\mathbf X_i^C}]$。\n\n### 网络配置\n分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。\n\nMLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为\n$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$\n其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。\n\n在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 [soft-NMS](/2019/06/24/cv-mtds) 进一步处理检测结果。\n\n# 实验\n实验略，请阅读原文以获取详细信息\n\n# 结论\n提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。","slug":"M2Det","published":1,"updated":"2019-06-29T09:59:57.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptp000rxgvc349pv0gm","content":"<p>论文：<a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，<br><img src=\"/images/M2Det_fig1.png\" alt></p>\n<p>SSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：</p>\n<ol>\n<li>pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。</li>\n<li>pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。</li>\n</ol>\n<p>一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。</p>\n<p>本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，<br><img src=\"/images/M2Det_fig2.png\" alt></p>\n<p>首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。</p>\n<p>为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。</p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。</p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p>如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：<br>$$[x_1^l,…x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,…L \\end{cases}$$<br>其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。</p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 <strong>对称</strong> 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。<br><img src=\"/images/M2Det_fig4.png\" alt> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，<br><img src=\"/images/M2Det_fig3.png\" alt><center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center></p>\n<p>第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为<br>$$\\mathbf X=[\\mathbf X_1,…,\\mathbf X_i]$$<br>其中 $\\mathbf X_i=Concat(x_i^1,…x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，<br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p>最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},…,\\tilde {\\mathbf X_i^C}]$。</p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h3><p>分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。</p>\n<p>MLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为<br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。</p>\n<p>在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> 进一步处理检测结果。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验略，请阅读原文以获取详细信息</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://arxiv.org/abs/1811.04533\" target=\"_blank\" rel=\"noopener\">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>我们知道在目标检测任务中目标尺度的变化一直是一个具有挑战的问题，通常有两种解决思路：image pyramid 和 feature pyramid。前者在训练阶段其实可以看作是一种数据增强，优点是可以让网络学习到统一的特征表达能力，缺点是测试阶段计算量和内存占用均增大，因为不同 size 的 image 需要分别通过网络预测，然后再合并预测结果。后者是从输入 image 中抽取不同 level 的 feature （不同scale 的 feature maps）形成 feature pyramid，相比于前者，降低了计算量和内存占用，但是不足之处在于构造 feature pyramid 时使用 backbone 网络中固有的 multi-scale feature maps，虽然这些 feature maps 可以形成 feature pyramid，但是其本是为了分类任务而设计的。如图 1，<br><img src=\"/images/M2Det_fig1.png\" alt></p>\n<p>SSD 使用 backbone 的两个 layers，然后在此基础上再以步幅 2 连续构造 4 个卷积 layers，这 6 个 layers 的输出构成 feature pyramid；STDN 使用 DenseNet 的最后一个 block 并通过 pooling 和 scale-transfer 操作来构造出 feature pyramid；FPN 以 top-down 方式并增加一个横向连接，融合高层和底层特征，从而构造出 feature pyramid。通常来说，以上方法均存在以下两点不足：</p>\n<ol>\n<li>pyramid 中的 feature 用于目标检测的表征能力还不够，因为是从 backbone 中抽取出来的，而 backbone 是为分类任务设计的。</li>\n<li>pyramid 中每个 level 的 feature 用于检测相应某个 scale 范围内的目标，而 feature 主要（FPN 这类）或者仅仅（除 FPN 这类以外的）由 backbone 的单一 layer 生成，故 feature 主要或仅仅包含单一 level 的信息。</li>\n</ol>\n<p>一般而言，高层特征由于包含更多的语义信息对于分类任务更具有判别力，而低层特征保持了局部信息所以更适合目标定位任务。并且，低层特征适合描述具有简单外观的目标，而高层特征则适合描述具有复杂外观的目标。实际上，size 相差无几的目标其外观很可能差别非常大，例如交通信号灯和一个远处的人，两者 size 差不多，但是人的外观显然更加复杂，因此，用同一 level 的 feature maps 预测这两者，检测性能不是最优。</p>\n<p>本文构造出一个更加有效的 feature pyramid 用于检测不同 scale 的目标，并能解决上述问题。如图 2，<br><img src=\"/images/M2Det_fig2.png\" alt></p>\n<p>首先融合 backbone 的 multi-level features（来自于多 layers 输出）作为 base feature，将这个 base feature 喂给一个交替连接 Thinned U-shape Modules(TUM) 和 Feature Fusion Modules(FFM) 的 block（如图 2 中红色框），从而得到更具表征能力的 multi-level multi-scale features，multi-leve 是指 shallow, medium, deep 等 level，multi-scale 是指每个 level 均具有多尺度 features。值得注意的是，每个 U 型模块中的解码层深度相同，这是为了在下一步 Scale-wise Feature Aggregation Module（SFAM） 中，将每个 level 中 scale 相同的特征聚合起来构成最终的 feature pyramid，这个 SFAM 操作相当于将 multi-level multi-scale 变成 multi-scale multi-level，也就是说，用于检测每个 scale 范围目标的 feature 均包含浅层特征和深层特征。显然，用于生成最终 feature pyramid 的解码层特征比原先 backbone 中的 layers 更深，所以也就更具有表征能力。我们称此 feature pyramid 模块为 Multi-Level Feature Pyramid Network（MLFPN）。</p>\n<p>为了评估 MLFPN 的有效性，我们设计并训练了一个端到端的 one-stage 目标检测器称为 M2Det，这是将 MLFPN 合并入 SSD 得到的检测器。M2Det 获得了新 SOTA 结果，使用单尺度 inference 时，FPS=11.8，AP=41.0，而使用多尺度 inference 时，AP 高达 44.2，超过 MS-COCO 上其他 one-stage 检测结果。</p>\n<h1 id=\"Method\"><a href=\"#Method\" class=\"headerlink\" title=\"Method\"></a>Method</h1><p>M2Det 网络结构如图 2，使用 backbone 和 MLFPN 得到 feature pyramid，其他网络部分与 SSD 类似，生成密集预测 bbox 以及分类得分，然后使用 NMS 得到最后的检测结果。MLFPN 包含：FFM, TUM 以及 SFAM。FFMv1 丰富了 base feature 中的语义信息，因为融合了 backbone 多级 feature maps。每个 TUM 均生成一组多尺度特征，每一个尺度用于检测对应尺度范围的目标。交替连接 TUM 和 FFMv2 以抽取 multi-level multi scale features。此外，SFAM 按 scale 聚合多个 level 的 features（concatenate features）。关于这三个核心模块的细节以及 M2Det 的配置介绍如下。</p>\n<h2 id=\"MLFPN\"><a href=\"#MLFPN\" class=\"headerlink\" title=\"MLFPN\"></a>MLFPN</h2><p>如图 2，首先，FFMv1 融合了浅层和深层的特征得到 base feature，例如，融合 VGG 中 conv4_3 和 conv5_3 的特征。然后，交替连接 TUM 和 FFMv2，每个 TUM 生成不同 scale 的 feature maps，FFMv2 则融合 base feature 和上一 TUM 中最大 scale 的 feature，融合后的 feature maps 作为下一 TUM 的输入。注意第一个 TUM 仅从 base feature 中学习。输出的 multi-level multi-scale features 按如下方式计算：<br>$$[x_1^l,…x_i^l]=\\begin{cases} \\mathbf T_l(\\mathbf X_{base}) &amp; l=1<br>\\\\ \\mathbf T_l(\\mathbf F (\\mathbf X_{base}, \\mathbf x_i^{l-1})) &amp; l=2,…L \\end{cases}$$<br>其中，$\\mathbf X_{base}$ 表示 base feature，$x_i^l$ 表示第 $l$ 个 TUM 中第 $i$ 个 scale 的 feature，L 表示 TUM 数量，$\\mathbf T_l$ 表示 第 $l$ 个 TUM 处理，$\\mathbf F$ 表示 FFMv2 融合过程。</p>\n<h3 id=\"FFM\"><a href=\"#FFM\" class=\"headerlink\" title=\"FFM\"></a>FFM</h3><p>FFM 是如何融合多个 feature 的呢？使用 1x1 卷积压缩这些 features，然后使用 concatenation 操作聚合这些 features。由于 FFMv1 将 backbone 中不同 scale 的两个 features 作为输入，所以需要将其中深层特征 upsample 使得与浅层特征的 scale 相同，然后再执行 concatenation 操作。TUM 的网络结构是 <strong>对称</strong> 的，所以 FFMv2 的两个输入 base feature 与 上一 TUM 的最大的输出 feature 具有相同的 scale，故直接 concatenate 起来作为下一 TUM 的输入。FFMv1 和 FFMv2 的结构如图 4 (a)(b)。<br><img src=\"/images/M2Det_fig4.png\" alt> <center>Fig 4 (a) FFMv1. (b) FFMv2. (c) TUM。每个 block 中数字分别表示：输入通道，卷积核 size，步幅，输出通道</center></p>\n<h3 id=\"TUM\"><a href=\"#TUM\" class=\"headerlink\" title=\"TUM\"></a>TUM</h3><p>TUM 是一个 Thin U-shape 结构，如图 4(c)，encoder 是一系列的 stride=2 的 3x3 卷积，decoder 将这些卷积层的输出作为 feature maps 的参考集合，而 FPN 则使用 backbone 中的 layer 输出。此外，我们在 upsample 和 element-wise sum 操作之后增加了一个 1x1 卷积，以增强学习能力并保持特征的平滑。所有 TUM 的 decoder 输出形成 multi-level multi-scale features，其中，靠前的 TUM 生成浅层的 multi-scale features，中间的 TUM 生成中层的 multi-scale features，而靠后的 TUM 生成深层的 multi-scale features。</p>\n<h3 id=\"SFAM\"><a href=\"#SFAM\" class=\"headerlink\" title=\"SFAM\"></a>SFAM</h3><p>SFAM 用于聚合所有 TUM 输出的 multi-level multi-scale features，如图 3，<br><img src=\"/images/M2Det_fig3.png\" alt><center>Fig 3 SFAM 结构。第一阶段是按 scale 沿 channel 维度 concatenate 特征，第二阶段使用 SE attention 以适应的方式聚合特征</center></p>\n<p>第一阶段是将 scale 相等的 features 沿通道方向 concatenate，聚合后的 feature pyramid 可表示为<br>$$\\mathbf X=[\\mathbf X_1,…,\\mathbf X_i]$$<br>其中 $\\mathbf X_i=Concat(x_i^1,…x_i^L) \\in \\mathcal R^{W_i \\times H_i \\times C}$ 表示第 $i$ 个 scale 的（由浅层到深层）特征，$W_i \\times H_i$ 表示第 $i$ 个 scale 的 feature map 的 size，这里所有 scale 所有 level 的 feature maps 的通道 $C$ 均相等，如图 4 中 $C=128$。但是仅仅 concatenate 这些 features，其适应性还不足（有点生硬），所以第二阶段，我们采用了通道注意力模块使得 features 专注于那些能从中获得最大收益的通道。参考 SE block，在 squeeze 这一步，我们使用全局平均池化（global average pooling）按通道生成统计量 $\\mathbf z \\in \\mathcal R^C$，然后再 excitation 这一步，使用两个 fc 层学习注意力机制以获得通道依赖性，<br>$$\\mathbf s = \\mathbf F_{ex}(\\mathbf {z,W})=\\sigma (\\mathbf W_2 \\delta(\\mathbf W_1 \\mathbf z))$$<br>其中，$\\sigma$ 表示 ReLu，$\\delta$ 表示 sigmoid，$\\mathbf W_1 \\in \\mathcal R^{\\frac C r \\times C}, \\ \\mathbf W_2 \\in \\mathcal R^{C \\times \\frac C r}$， r 是缩小比例（实验中 r=16），然后重新对特征按通道加权得到最终的特征，</p>\n<p>$$\\tilde {\\mathbf X_i^c}=\\mathbf F_{scale}(\\mathbf X_i^c, s_c)=s_c \\cdot \\mathbf X_i^c$$</p>\n<p>最后的特征为 $\\tilde {\\mathbf X_i}=[\\tilde {\\mathbf X_i^1},…,\\tilde {\\mathbf X_i^C}]$。</p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h3><p>分别使用 VGG 和 ResNet 作为 M2Det 的 backbone，backbone 使用 ImageNet2012 进行预训练。MLFPN 包含 8 个 TUM，每个 TUM 包含 5 个 convs 和 5 个上采样操作，故共输出 6 个 scale 的 features。为了降低参数量，TUM 的每个 scale 的特征仅使用 256 个通道，参见图 4 (c) 中最上面一排。整个网络的输入大小遵循原始的 SSD, RefineDet 和 RetinaNet，分别为 320, 512 和 800。</p>\n<p>MLFPN 之后，得到 6 组 pyramid features，scale 分别为 1x1，3x3，5x5，10x10，20x20，40x40，我们为每个 scale 的 pyramid features 分别增加两个卷积层，用于定位回归和分类。6 组 pyramid features 上 anchor(prior) box 的默认 scale （不考虑 aspect ratio）与原始 SSD 中保持一致，稍微回顾一下这一点，假设共 m 组 features（这里 m = 6），第 k 组 features 上的 anchor box 的默认 scale 为<br>$$s_k=s_{min}+\\frac {s_{max}-s_{min}} {m-1} (k-1)$$<br>其中，$s_{min}=0.2, \\ s_{max}=0.9$（当然，还需要乘上每组 features 相对于输入 image 的步幅（下采样率）才是最终的 anchor 的默认 scale）。</p>\n<p>在 pyramidal features 上每个像素点位置，设置 6 个 anchors，包含 3 个 aspect ratios（参考 SSD）。使用阈值 0.05 过滤掉较低得分的检测，然后使用线性核函数的 <a href=\"/2019/06/24/cv-mtds\">soft-NMS</a> 进一步处理检测结果。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验略，请阅读原文以获取详细信息</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 MLFPN 以解决目标检测中 multi-scale 问题。构造 M2Det 目标检测器取得了 SOTA 的 one-stage 检测结果。</p>\n"},{"title":"GA-RPN","date":"2019-06-25T09:01:57.000Z","mathjax":true,"_content":"论文：[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n\n目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。\n\n合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：\n1. anchor 中心应与 feature map 的像素点对准\n2. 感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致\n\n滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：\n1. 对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。\n2. 为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量\n\n本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：\n1. 根据观察，image 中的目标位置不是均匀分布的\n2. 目标的 scale 与目标位置和 image 内容相关\n   \n于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：\n1. 确定可能包含目标的子区域\n2. 根据子区域位置确定其 shape\n\n可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。\n\n使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。\n\n# Guided Anchoring\n目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：\n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n这种因式分解基于如下两点：\n1. 给定 image，目标仅位于某些特定区域\n2. shape 即 scale 和 aspect ratio 与 anchor 的位置有关\n   \n根据以上公式，我们的 anchor 生成模块如图 1，\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center>\n\n给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。\n\n由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。\n\n## Anchor 位置预测\nanchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。\n\n使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。\n\n预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。\n\n## Anchor shape 预测\n确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。\n\n由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，\n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。\n\n以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。\n\n## Anchor-Guided 特征适配\n传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，\n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。\n\n## 训练\n### 联合目标函数\n本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：\n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g',y_g',w_g',h_g')$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：\n1. 中心区域 \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）\n\n2. 忽略区域\n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$，其中$\\sigma_2 > \\sigma_1$，IR 内的像素点被标记为 `ignore`，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 `[0.3,0.7]` 范围则标记为 -1，标记为 -1 的 anchor 不参与训练\n\n3. 外围区域\n   \n   $OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）\n\n由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。\n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n分两步来决定最佳 shape target：\n1. 将 anchor 与某个 gt box 匹配起来\n2. 预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box\n\nFaster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。\n\n但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：\n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。\n\n总结一下以上过程：\n1. 选择 9 组 (w,h)\n2. 给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)\n3. 最大 vIoU 的那个 gt box 与此位置 anchor 相匹配\n4. shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失\n\n那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？\n\n当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。\n\n## 使用高质量 proposals\n得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，\n![](/images/GA-RPN_fig3.png) <center>Fig 3 不同 IoU 下的 proposals 数量</center>\n\n比起 RPN，GA-RPN 有如下两个明显优点：\n1. 正例 proposals 数量更多\n2. 高 IoU 处两者的 proposals 数量比例更明显\n\n在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。\n\n除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。\n\n![](/images/GA-RPN_fig4.png)\n\n# 实验\n实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。\n\n# 结论\n提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。","source":"_posts/GA-RPN.md","raw":"---\ntitle: GA-RPN\ndate: 2019-06-25 17:01:57\ntags: object detection\nmathjax: true\n---\n论文：[Region Proposal by Guided Anchoring](https://arxiv.org/abs/1901.03278)\n\n目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。\n\n合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：\n1. anchor 中心应与 feature map 的像素点对准\n2. 感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致\n\n滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：\n1. 对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。\n2. 为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量\n\n本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：\n1. 根据观察，image 中的目标位置不是均匀分布的\n2. 目标的 scale 与目标位置和 image 内容相关\n   \n于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：\n1. 确定可能包含目标的子区域\n2. 根据子区域位置确定其 shape\n\n可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。\n\n使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。\n\n# Guided Anchoring\n目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：\n$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$\n这种因式分解基于如下两点：\n1. 给定 image，目标仅位于某些特定区域\n2. shape 即 scale 和 aspect ratio 与 anchor 的位置有关\n   \n根据以上公式，我们的 anchor 生成模块如图 1，\n\n![](/images/GA-RPN_fig1.png) <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center>\n\n给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。\n\n由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。\n\n## Anchor 位置预测\nanchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。\n\n使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。\n\n预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。\n\n## Anchor shape 预测\n确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。\n\n由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，\n$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$\n于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。\n\n以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。\n\n## Anchor-Guided 特征适配\n传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，\n$$\\mathbf f_i'= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$\n其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。\n\n## 训练\n### 联合目标函数\n本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：\n$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$\n\n### Anchor location targets\n为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g',y_g',w_g',h_g')$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：\n1. 中心区域 \n   \n   $CR=\\mathcal R(x_g',y_g',\\sigma_1 w_g', \\sigma_1 h_g')$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）\n\n2. 忽略区域\n   \n   $IR=\\mathcal R(x_g',y_g',\\sigma_2 w_g', \\sigma_2 h_g') \\setminus CR$，其中$\\sigma_2 > \\sigma_1$，IR 内的像素点被标记为 `ignore`，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 `[0.3,0.7]` 范围则标记为 -1，标记为 -1 的 anchor 不参与训练\n\n3. 外围区域\n   \n   $OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）\n\n由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。\n![](/images/GA-RPN_fig2.png)\n\n### Anchor shape targets\n\n分两步来决定最佳 shape target：\n1. 将 anchor 与某个 gt box 匹配起来\n2. 预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box\n\nFaster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。\n\n但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}=\\{(x_0,y_0,w,h)|w>0,h>0\\}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，\n$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w>0,h>0} IoU_{normal}(a_{wh},gt)$$\n对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：\n$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$\n其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。\n\n总结一下以上过程：\n1. 选择 9 组 (w,h)\n2. 给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)\n3. 最大 vIoU 的那个 gt box 与此位置 anchor 相匹配\n4. shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失\n\n那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？\n\n当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。\n\n## 使用高质量 proposals\n得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，\n![](/images/GA-RPN_fig3.png) <center>Fig 3 不同 IoU 下的 proposals 数量</center>\n\n比起 RPN，GA-RPN 有如下两个明显优点：\n1. 正例 proposals 数量更多\n2. 高 IoU 处两者的 proposals 数量比例更明显\n\n在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。\n\n除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。\n\n![](/images/GA-RPN_fig4.png)\n\n# 实验\n实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。\n\n# 结论\n提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。","slug":"GA-RPN","published":1,"updated":"2019-06-27T12:16:05.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptq000txgvcet4kqeiz","content":"<p>论文：<a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>\n<p>目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。</p>\n<p>合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：</p>\n<ol>\n<li>anchor 中心应与 feature map 的像素点对准</li>\n<li>感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致</li>\n</ol>\n<p>滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：</p>\n<ol>\n<li>对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。</li>\n<li>为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量</li>\n</ol>\n<p>本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：</p>\n<ol>\n<li>根据观察，image 中的目标位置不是均匀分布的</li>\n<li>目标的 scale 与目标位置和 image 内容相关</li>\n</ol>\n<p>于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：</p>\n<ol>\n<li>确定可能包含目标的子区域</li>\n<li>根据子区域位置确定其 shape</li>\n</ol>\n<p>可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。</p>\n<p>使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p>目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：<br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br>这种因式分解基于如下两点：</p>\n<ol>\n<li>给定 image，目标仅位于某些特定区域</li>\n<li>shape 即 scale 和 aspect ratio 与 anchor 的位置有关</li>\n</ol>\n<p>根据以上公式，我们的 anchor 生成模块如图 1，</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt> <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center></p>\n<p>给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。</p>\n<p>由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。</p>\n<h2 id=\"Anchor-位置预测\"><a href=\"#Anchor-位置预测\" class=\"headerlink\" title=\"Anchor 位置预测\"></a>Anchor 位置预测</h2><p>anchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。</p>\n<p>使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。</p>\n<p>预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。</p>\n<h2 id=\"Anchor-shape-预测\"><a href=\"#Anchor-shape-预测\" class=\"headerlink\" title=\"Anchor shape 预测\"></a>Anchor shape 预测</h2><p>确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。</p>\n<p>由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，<br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br>于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。</p>\n<p>以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。</p>\n<h2 id=\"Anchor-Guided-特征适配\"><a href=\"#Anchor-Guided-特征适配\" class=\"headerlink\" title=\"Anchor-Guided 特征适配\"></a>Anchor-Guided 特征适配</h2><p>传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，<br>$$\\mathbf f_i’= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><h3 id=\"联合目标函数\"><a href=\"#联合目标函数\" class=\"headerlink\" title=\"联合目标函数\"></a>联合目标函数</h3><p>本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：<br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p>为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g’,y_g’,w_g’,h_g’)$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：</p>\n<ol>\n<li><p>中心区域 </p>\n<p>$CR=\\mathcal R(x_g’,y_g’,\\sigma_1 w_g’, \\sigma_1 h_g’)$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）</p>\n</li>\n<li><p>忽略区域</p>\n<p>$IR=\\mathcal R(x_g’,y_g’,\\sigma_2 w_g’, \\sigma_2 h_g’) \\setminus CR$，其中$\\sigma_2 &gt; \\sigma_1$，IR 内的像素点被标记为 <code>ignore</code>，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 <code>[0.3,0.7]</code> 范围则标记为 -1，标记为 -1 的 anchor 不参与训练</p>\n</li>\n<li><p>外围区域</p>\n<p>$OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）</p>\n</li>\n</ol>\n<p>由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。<br><img src=\"/images/GA-RPN_fig2.png\" alt></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p>分两步来决定最佳 shape target：</p>\n<ol>\n<li>将 anchor 与某个 gt box 匹配起来</li>\n<li>预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box</li>\n</ol>\n<p>Faster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。</p>\n<p>但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br>对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：<br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br>其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。</p>\n<p>总结一下以上过程：</p>\n<ol>\n<li>选择 9 组 (w,h)</li>\n<li>给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)</li>\n<li>最大 vIoU 的那个 gt box 与此位置 anchor 相匹配</li>\n<li>shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失</li>\n</ol>\n<p>那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？</p>\n<p>当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。</p>\n<h2 id=\"使用高质量-proposals\"><a href=\"#使用高质量-proposals\" class=\"headerlink\" title=\"使用高质量 proposals\"></a>使用高质量 proposals</h2><p>得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，<br><img src=\"/images/GA-RPN_fig3.png\" alt> <center>Fig 3 不同 IoU 下的 proposals 数量</center></p>\n<p>比起 RPN，GA-RPN 有如下两个明显优点：</p>\n<ol>\n<li>正例 proposals 数量更多</li>\n<li>高 IoU 处两者的 proposals 数量比例更明显</li>\n</ol>\n<p>在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。</p>\n<p>除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt></p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://arxiv.org/abs/1901.03278\" target=\"_blank\" rel=\"noopener\">Region Proposal by Guided Anchoring</a></p>\n<p>目标检测中，通常使用 anchor 来生成 proposal（two-stage）或者直接对 anchor 进行分类和回归（one-stage）。以 two-stage 的 Faster R-CNN 为例，先在 feature map 上生成密集均匀分布的 anchors，然后对其进行二分类预测及坐标回归得到 proposals，最后再对 proposals 进行分类和坐标回归微调。</p>\n<p>合理的 anchor 设计遵循两个通用规则：alignment 和 consistency：</p>\n<ol>\n<li>anchor 中心应与 feature map 的像素点对准</li>\n<li>感受野 RF 和语义范围应与 anchor 的尺度和形状保持一致</li>\n</ol>\n<p>滑窗就是一种简单且被广泛采用的 anchor 生成机制，大多数目标检测方法均采用滑窗来生成均匀密集的 anchors，即，在 feature map 上每个像素点位置按预先设置的 scale 和 aspect ratio 生成 k 个 anchors。然而，这种 anchor 生成机制的困难在于：</p>\n<ol>\n<li>对不同的检测问题，需要预先精心设计适合的 scale 和 aspect ratio，否则会影响检测性能指标。</li>\n<li>为了得到高的 recall，需要生成大量的 anchors，这导致大部分 anchors 为负例（non-object），同时，大量的 anchors 增加计算量</li>\n</ol>\n<p>本文提出一个有效的 anchor 生成方法，此方法受以下观点启发：</p>\n<ol>\n<li>根据观察，image 中的目标位置不是均匀分布的</li>\n<li>目标的 scale 与目标位置和 image 内容相关</li>\n</ol>\n<p>于是我们的稀疏非均匀 anchor 生成方法步骤为（guided anchoring）：</p>\n<ol>\n<li>确定可能包含目标的子区域</li>\n<li>根据子区域位置确定其 shape</li>\n</ol>\n<p>可学习的 anchor shape 虽然合理，但是打破了前述的 consistency 规则，即，学习到（动态生成）的 anchor shape 可能与 RF 和 语义 scope 不一致。由于现在的 scale 和 aspect ratio 是可变的而非固定的，所以 feature map 上不同的像素点的 anchor shape 也不尽相同，需要学习适配 anchor shape 的表征以维持 consistency 原则。为了解决此问题，我们介绍了一个有效的模块：基于 anchor shape 来修改 features 使其适配，此即 feature adaptation 机制。</p>\n<p>使用前述的 guided anchoring 和 feature adaptation 机制，我们制定了 Guided Anchoring Region Proposal Network （GP-RPN）。由于动态预测 anchors，recall 值比常规 RPN（baseline，使用滑窗生成密集均匀分布的 anchor）高了 9.1%，而 anchors 数量下降了 90%。通过预测得到 scale 和 aspect ratio 而非固定的预设值，我们的检测方法能更加有效地处理 aspect ratio 小于 1/2 或 大于 2 的宽/高目标。除了用于生成 region proposals， guided anchoring 方法可集成到任何依赖 anchor 的检测器中（比如 SSD，直接拿 anchor 分类和回归得到最终的预测 box）。guided anchoring 机制能使得各种目标检测器获得一致的性能提升，比如在 COCO 数据集上， GA-Fast-RCNN，GA-Faster-RCNN 和 GA-RetinaNet 的 mAP 比相应的使用滑窗的 baseline 分别提升了 2.2%, 2.7% 和 1.2%。</p>\n<h1 id=\"Guided-Anchoring\"><a href=\"#Guided-Anchoring\" class=\"headerlink\" title=\"Guided Anchoring\"></a>Guided Anchoring</h1><p>目标的 location 和 shape 可以使用 (x,y,w,h) 来刻画，其中 (x,y) 是中心点的坐标，(w,h) 表示宽高。给定一个 image $I$，那么目标 location 和 shape 遵循如下分布：<br>$$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$$<br>这种因式分解基于如下两点：</p>\n<ol>\n<li>给定 image，目标仅位于某些特定区域</li>\n<li>shape 即 scale 和 aspect ratio 与 anchor 的位置有关</li>\n</ol>\n<p>根据以上公式，我们的 anchor 生成模块如图 1，</p>\n<p><img src=\"/images/GA-RPN_fig1.png\" alt> <center>Fig 1 框架结构。每个feature map 均使用 anchor 生成模块，模块中有两个分支，分别预测 anchor 位置和 shape。应用 feature 适配模块到 feature map 上得到新的 feature map，使其注意到 anchor</center></p>\n<p>给定 image $I$，首先得到 feature map $F_I$，在 $F_I$ 上 位置预测分支生成一个概率 map，表示每个位置处存在目标的概率，shape 预测分支生成位置相关的 shape，即预测每个位置的 w,h。使用一个概率阈值，选择大于阈值的位置，以及这些位置上最有可能的 shape，从而生成 anchor。考虑到 anchor 的 shape 可变，不同位置处的 feature 应该捕获不同范围内的视觉内容，所以我们进一步引入了特征适配模块，根据 anchor 的shape 使 feature 适配。</p>\n<p>由于最近的研究表面，使用不同 leve 的 feature maps 有助于目标检测，如 FPN 和 RetinaNet，所以如图 1，我们也使用了多 level 的 anchor 生成机制，需要注意的是，不同 level 的 anchor 生成分支所用的参数是相同的。</p>\n<h2 id=\"Anchor-位置预测\"><a href=\"#Anchor-位置预测\" class=\"headerlink\" title=\"Anchor 位置预测\"></a>Anchor 位置预测</h2><p>anchor 位置预测分支生成的概率 map $p(\\cdot|F_I)$ 与 feature map $F_I$ 大小相同，其上每点位置的概率值 $p(i,j|F_I)$ 对应原输入 image $I$ 上位置 $((i+\\frac 1 2)s,(j+\\frac 1 2)s)$，其中 s 是  $F_I$ 相对于 $I$ 的步幅，即两个相邻 anchor 中心点的距离，$p(i,j|F_I)$ 表示在 $F_I$ 位置 (i,j) 处是某个目标中心的概率。</p>\n<p>使用一个子网络 $\\mathcal N_L$ 来预测得到 $p(i,j|F_I)$，$\\mathcal N_L$ 组成包括一个 1x1 的卷积和一个 element-wise 的 sigmoid 函数。当然更复杂的 $\\mathcal N_L$ 可以使得预测更加准确，但是为了平衡计算效率和准确性，我们仍然采用当前 $\\mathcal N_L$ 的组成。</p>\n<p>预定义一个概率阈值 $\\epsilon_L$，概率 map 上小于阈值的位置均被过滤掉，也就是说，过滤掉以这些位置为中心的 region（由于当前只考虑了 anchor 中心，尚未考虑 shape，所以此时称 region），这可以过滤掉 90% 的 region 且能同时维持相同的 recall（与普通的 RPN 相比）。如图 4(b)，像天空和大海所在的 region 均被排除，而集中于人和冲浪板。由于不需要考虑那些排除掉的 region，我们将卷积替换为 masked convolution 使推断过程更加高效。</p>\n<h2 id=\"Anchor-shape-预测\"><a href=\"#Anchor-shape-预测\" class=\"headerlink\" title=\"Anchor shape 预测\"></a>Anchor shape 预测</h2><p>确定了 anchor 位置之后，下一步就是确定 anchor 的 shape。如图 1，此预测分支与传统的 bbox 回归预测不同，因为此分支不改变 anchor 的位置，所以不会打破前述 alignment 原则。给定 $F_I$，此分支预测每个位置上的最佳 shape (w,h)，这个最佳 shape 是指 anchor 与最近的 gt box 有最高覆盖度。</p>\n<p>由于 w,h 的范围较大，直接预测这两个值不稳定，故做如下转换将输出值域控制在 [-1,1] 这样一个较小的范围内，<br>$$w=\\sigma \\cdot s \\cdot e^{dw}, \\quad h = \\sigma \\cdot s \\cdot e^{dh}$$<br>于是 shape 分支预测输出为 (dw,dh)，其中 s 为 $F_I$ 相对于 $I$ 的步幅，$\\sigma$ 为经验尺度因子，我们实验中 $\\sigma=8$。使用子网络 $\\mathcal N_S$ 得到 shape 预测，$\\mathcal N_S$ 组成包含一个 1x1 卷积核输出通道为 2 的卷积层，以及一个 element-wise 转换层，前者生成的 2 通道分别对应 dw map 和 dh map，后者实现上式转换得到 w map 和 h map。</p>\n<p>以上 anchor 生成模块的设计与传统的 anchor 生成机制（滑窗）有本质不同，使用我们这里的 anchor 生成机制，每个位置仅一个 anchor，其 shape 动态预测得到，而传统的滑窗根据不同 scale 和 aspect ratio 每个位置生成 k 个 anchor。实验证明，由于我们的 anchor 生成机制中，shape 与 location 有密切关联，所以可以获取更高的 recall，且由于不是预设固定的 aspect ratio 而是动态预测 shape，我们的 anchor 机制可以捕获那些极高或者极宽的目标。</p>\n<h2 id=\"Anchor-Guided-特征适配\"><a href=\"#Anchor-Guided-特征适配\" class=\"headerlink\" title=\"Anchor-Guided 特征适配\"></a>Anchor-Guided 特征适配</h2><p>传统的 RPN 或者 one-stage 检测器采用滑窗机制生成 anchors 均匀分布在 feature map 上，每个位置处 anchor 的 shape/scale 均相同，所以 feature map 能学习到一致的表征。但是在我们的 anchor 机制中，由于 shape 任意可变的，所以不适合像传统方法那样在 feature map 上使用全卷积分类器对 anchor 进行分类（例如 RPN 的二分类或者 one-stage 的前景分类）。最好的做法是，大 anchor 的 feature 应该使用大 region 的内容，小 anchor 的 feature 则使用小范围内容。于是我们进一步提出 anchor-guided 特征适配模块，根据 anchor shape 对 feature 进行转换以使其适配，如下，<br>$$\\mathbf f_i’= \\mathcal N_T(\\mathbf f_i, w_i,h_i)$$<br>其中，$\\mathbf f_i$ 是 位置 i 处的 feature，$(w_i,h_i)$ 是此处的 anchor shape。由于此特征转换是位置相关的，所以我们采用 3x3 的可变形卷积来实现 $\\mathcal N_T$，如图 1，首先根据 anchor shape 分支的输出预测 offset，然后应用可变形卷积到原始 feature map 上，在转换后的 feature map 上，我们可以按传统方法进行分类和 bbox 回归。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><h3 id=\"联合目标函数\"><a href=\"#联合目标函数\" class=\"headerlink\" title=\"联合目标函数\"></a>联合目标函数</h3><p>本文提出的网络框架使用多任务损失进行端到端优化，除了传统的分类损失和回归损失，还包括 anchor 的位置损失和 shape 损失，目标函数的优化使用如下联合损失：<br>$$\\mathcal L=\\lambda_1 \\mathcal L_{loc}+ \\lambda_2 \\mathcal L_{shape} + \\mathcal L_{cls} + \\mathcal L_{reg}$$</p>\n<h3 id=\"Anchor-location-targets\"><a href=\"#Anchor-location-targets\" class=\"headerlink\" title=\"Anchor location targets\"></a>Anchor location targets</h3><p>为了训练 anchor 位置分支，对每个 image 我们需要一个 二值 label map，每个像素点值 1 表示有效位置，0 表示无效位置。这个二值 label map 根据 gt box 得到。我们希望在目标中心的周围附近放置较多的 anchor，而远离目标中心的位置则放置较少的 anchor。首先，将 gt box $(x_g,y_g,w_g,h_g)$ 映射到 feature map $F_I$上得到 $(x_g’,y_g’,w_g’,h_g’)$，然后使用 $\\mathcal R(x,y,w,h)$ 表示一个矩形区域。Anchors 将被放置到 gt box 中心的附近以得到较大的 IOU，对每个 gt box 定义如下三种类型的矩形区域：</p>\n<ol>\n<li><p>中心区域 </p>\n<p>$CR=\\mathcal R(x_g’,y_g’,\\sigma_1 w_g’, \\sigma_1 h_g’)$，此区域中心与 gt box 中心重合，宽高分别是 gt box 宽高的 $\\sigma_1$ 倍。CR 内像素点值为 1（positive）</p>\n</li>\n<li><p>忽略区域</p>\n<p>$IR=\\mathcal R(x_g’,y_g’,\\sigma_2 w_g’, \\sigma_2 h_g’) \\setminus CR$，其中$\\sigma_2 &gt; \\sigma_1$，IR 内的像素点被标记为 <code>ignore</code>，不参与训练，这一点类似于 Faster R-CNN 中训练 RPN，anchor 与 gt box 的 IOU 大于 0.7 时为标记为 1 positive，小于 0.3 时标记为 0 negative，而位于 <code>[0.3,0.7]</code> 范围则标记为 -1，标记为 -1 的 anchor 不参与训练</p>\n</li>\n<li><p>外围区域</p>\n<p>$OR=F_I \\setminus IR$，OR 内的像素点值为 0（negative）</p>\n</li>\n</ol>\n<p>由于我们使用多 level features，每个 level 的 feature map 应该仅瞄准特定 scale 范围的目标，故对某个 feature map 匹配的 scale 范围内的目标，我们设置相应（这些目标）的 CR，对 IR 也是同样处理，如图 2。如果多个目标重叠，那么 CR 抑制 IR， IR 抑制 OR，显然这是合理的，因为 CR, IR, OR 优先级应该逐步下降，才能保证 recall。由于 CR 只占 feature map 中的一小部分，所以我们采用 Focal Loss 平衡正负例来训练 anchor 位置分支。<br><img src=\"/images/GA-RPN_fig2.png\" alt></p>\n<h3 id=\"Anchor-shape-targets\"><a href=\"#Anchor-shape-targets\" class=\"headerlink\" title=\"Anchor shape targets\"></a>Anchor shape targets</h3><p>分两步来决定最佳 shape target：</p>\n<ol>\n<li>将 anchor 与某个 gt box 匹配起来</li>\n<li>预测 anchor 的宽高，使其最佳覆盖所匹配的 gt box</li>\n</ol>\n<p>Faster R-CNN 为 anchor 选定一个具有最大 IOU 的 gt box，然后根据 anchor 和 gt box 计算 $(t_x,t_y,t_w,t_h)$ 作为回归 target，这里的 anchor 其 $(x,y,w,h)$ 是已知的预设值。</p>\n<p>但是这种方法不适合我们的 anchor 生成机制，因为 anchor 的 w,h 不再是固定的预设值，而是变化的，也就是说，我们 shape 分支预测得到某位置的 $(w_p,h_p)$ 值，但是我们怎么确定该位置处的回归 target $(t_w,t_h)$ 呢？为了解决此问题，我们定义一个 anchor 变量 $a_{\\mathbf {wh}}={(x_0,y_0,w,h)|w&gt;0,h&gt;0}$ 与一个 gt box $gt=(x_g,y_g,w_g,h_g)$ 之间的 IoU （记作 vIoU）为，<br>$$\\text{vIoU}(a_{\\mathbf {wh}},gt)=\\max_{w&gt;0,h&gt;0} IoU_{normal}(a_{wh},gt)$$<br>对于任意给定的 anchor 位置 $(x_0,y_0)$ 和 gt box $gt$，上式的解析解是非常复杂的，在一个端到端的网络中很难去实现这个计算，因此使用一个替代方法得到近似解。给定位置 $(x_0,y_0)$，我们取一些 w 和 h 的常见值来模拟所有 w 和 h 的枚举，然后计算所取（这些常见 w 和 h）的 anchor 与某个 gt box 的 IoU，其中最大 IoU 作为 $\\text{vIoU}(a_{\\mathbf {wh}}, gt)$ 的近似。在我们的实验中，我们选取了 9 组 (w,h) 来估算 vIoU。这 9 组 (w,h) 使用 RetinaNet 中的 scales 和 aspect ratios 生成。理论而言，使用越多的 (w,h) 那么 vIoU 的近似越准确，当然计算量也跟着增加。我们采用 bounded iou loss 的变体来优化 shape 预测分支，这个损失如下：<br>$$\\mathcal L_{shape}=\\mathcal L_1(1-\\min(\\frac w {w_g}, \\frac {w_g} w)) + \\mathcal L_1 (1-\\min(\\frac h {h_g}, \\frac {h_g} h))$$<br>其中 (w,h) 是预测 anchor shape，(w<sub>g</sub>,h<sub>g</sub>) 是与 anchor 有着最大 vIoU 的那个 gt box 的 shape。从上式损失函数中可见，我们希望 $\\min(\\frac w {w_g}, \\frac {w_g} w)$ 和 $\\min(\\frac h {h_g}, \\frac {h_g} h)$ 越大越好，也就是说，w 越接近 w<sub>g</sub>，h 越接近 h<sub>g</sub>，就越好。</p>\n<p>总结一下以上过程：</p>\n<ol>\n<li>选择 9 组 (w,h)</li>\n<li>给定位置 (x<sub>0</sub>,y<sub>0</sub>)，计算 anchor 与所有 gt box 的 vIoU，每个 vIoU 的计算均使用 9 组 (w,h)</li>\n<li>最大 vIoU 的那个 gt box 与此位置 anchor 相匹配</li>\n<li>shape 预测分支在此位置预测的 (w,h) 与此位置 anchor 匹配的 gt box 的 (w<sub>g</sub>,h<sub>g</sub>) 一起计算得到此处的 shape 损失</li>\n</ol>\n<p>那么，为何不直接用 shape 分支预测的 (w,h) 与所有 gt box 计算 IoU，然后选择最大 IoU 的那个 gt box 作为该位置 anchor 所匹配的 gt box 呢？</p>\n<p>当然不行，由于 shape 分支预测的 (w,h) 在每次训练迭代过程中均会变化，如果使用上述方法求匹配的 gt box，那么该位置 anchor 所匹配的 gt box 在每次迭代时都有可能不一样，如果 anchor 训练回归的 target 都一直会变化，那就没法训练了。</p>\n<h2 id=\"使用高质量-proposals\"><a href=\"#使用高质量-proposals\" class=\"headerlink\" title=\"使用高质量 proposals\"></a>使用高质量 proposals</h2><p>得到 guided anchoring 加强的 RPN（GA-RPN）可以生成更高质量的 proposals。通过使用这些高质量的 proposals，我们探索了如何提高传统的 two-stage 目标检测器的性能。首先，研究了 RPN 和 GA-RPN 生成的 proposals 的 IoU 分布，如图 3，<br><img src=\"/images/GA-RPN_fig3.png\" alt> <center>Fig 3 不同 IoU 下的 proposals 数量</center></p>\n<p>比起 RPN，GA-RPN 有如下两个明显优点：</p>\n<ol>\n<li>正例 proposals 数量更多</li>\n<li>高 IoU 处两者的 proposals 数量比例更明显</li>\n</ol>\n<p>在现有模型下将 RPN 直接替换为 GA-RPN 然后端到端训练（从头开始训练），然而，如果采用相同的训练设置，性能指标提升会非常有限（不到 1 一个点）。通过我们的观察发现使用高质量 proposals 的前提条件是训练样本的分布需要与 proposal 分布一致。因此，设置一个更高的正负例阈值，从而使用更少的样本去训练。</p>\n<p>除了端到端训练，GA-RPN 还可以通过微调提升一个训练好的 two-stage 检测器的性能。具体而言，给定一个训练好的模型，我们舍弃其中的 proposal 生成模块，例如舍弃 RPN，然后使用预先计算好的 GA-RPN proposals 来微调这个模型，仅需要几个 epochs（默认是 3 个 epochs）即可。GA-RPN proposals 还可以用于 inference。这种简单的微调机制因为只需要少数 epochs，所以可以大大提高性能。</p>\n<p><img src=\"/images/GA-RPN_fig4.png\" alt></p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验参数、实现细节以及结果分析这里不展开讨论，直接阅读原文。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 Guided Anchoring 机制，利用语义特征生成位置非均匀且 shape 任意的 anchor。</p>\n"},{"title":"TridentNet","date":"2019-06-21T08:24:19.000Z","mathjax":true,"_content":"论文：[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n\n代码：[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# 简介\n目标检测通常分为：\n1. one stage，如 YOLO, SSD\n2. two stage，如 Faster R-CNN, R-FCN\n\n这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：\n1. 生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时\n2. 利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD\n3. 2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。\n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。\n\n# 感受野\nbackbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。\n\n假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。\n\n实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 _conv4_ stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font>\n\n从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：\n1. 网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的\n2. 尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小\n\n# Trident 网络\nTridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。\n## 网络结构\n如图 2，\n![](/images/TridentNet_fig2.png)\n\n网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。\n\n__多分支块__ 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。\n\n以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。\n\n__分支间共享权重__ 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。\n参数共享优点有三：\n1. 降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数\n2. 对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）\n3. 因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。\n\n## scale-aware 训练机制\n根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。\n\n每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 `(w,h)`，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。\n\nscale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。\n\n## Inference 和近似\nInference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。\n\n__快速推断近似__ 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。\n\n# 实验\n实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。\n\n## 实现细节\n使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。\n\n性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。\n\n## 消融学习\n\n__TridentNet 组件__ Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。\n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n   如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。\n2. __Scale-aware__\n   Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。\n3. __Weight-sharing__\n   Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。\n\n__分支数量__ Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ 目标检测结果。ResNet-50，使用不同分支数量</font>\n\n其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。\n\n# 结论\n提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。","source":"_posts/TridentNet.md","raw":"---\ntitle: TridentNet\ndate: 2019-06-21 16:24:19\ntags: object detection\nmathjax: true\n---\n论文：[Scale-Aware Trident Networks for Object Detection](https://arxiv.org/abs/1901.01892)\n\n代码：[TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n# 简介\n目标检测通常分为：\n1. one stage，如 YOLO, SSD\n2. two stage，如 Faster R-CNN, R-FCN\n\n这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：\n1. 生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时\n2. 利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD\n3. 2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。\n   \n![](/images/TridentNet_fig1(a).png) <center> fig1(a)</center>\n![](/images/TridentNet_fig1(b).png) <center> fig1(b)</center>\n![fig1(c)](/images/TridentNet_fig1(c).png) <center> fig1(c)</center>\n\n本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。\n\n# 感受野\nbackbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。\n\n假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。\n\n实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 _conv4_ stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，\n\n| Backbone  |   Dilation | AP    | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| ----------|----------- | :---: | :------------: | :------------: | :------------: |\n| ResNet-50 |  1         | 0.332 | __0.174__      | 0.384          | 0.464          |\n| ResNet-50 |  2         | 0.342 | 0.168          | __0.386__      | 0.486          |\n| ResNet-50 |  3         | 0.341 | 0.162          | 0.383          | __0.492__      |\n| ResNet-101|  1         | 0.372 | __0.200__      | __0.430__      | 0.528          |\n| ResNet-101|  2         | 0.380 | 0.191          | 0.427          | __0.538__      |\n| ResNet-101|  3         | 0.371 | 0.181          | 0.410          | __0.538__      |\n\n<font size=2> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font>\n\n从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：\n1. 网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的\n2. 尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小\n\n# Trident 网络\nTridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。\n## 网络结构\n如图 2，\n![](/images/TridentNet_fig2.png)\n\n网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。\n\n__多分支块__ 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。\n\n以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。\n\n__分支间共享权重__ 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。\n参数共享优点有三：\n1. 降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数\n2. 对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）\n3. 因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。\n\n## scale-aware 训练机制\n根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。\n\n每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 `(w,h)`，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。\n\nscale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。\n\n## Inference 和近似\nInference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。\n\n__快速推断近似__ 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。\n\n# 实验\n实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。\n\n## 实现细节\n使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。\n\n性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。\n\n## 消融学习\n\n__TridentNet 组件__ Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。\n![](/images/TridentNet_fig3.png)\n\n1. __Multi-branch__\n   如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。\n2. __Scale-aware__\n   Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。\n3. __Weight-sharing__\n   Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。\n\n__分支数量__ Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。\n\n| Branches | AP    | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> |\n| :------: |:-----:| :-------------: | :------------: | :------------: | :------------: |\n| 1        | 33.2  | 53.8            | 17.4           |  38.4          | 46.4           |\n| 2        | 35.9  | 56.7            | __19.0__       |  40.6          | 51.2           |\n| 3        | __36.6__  | __57.3__    | 18.3           |  __41.4__      | __52.3__       |\n| 4        | 36.5  | __57.3__        | 18.8           |  __41.4__      | 51.9           |\n\n<font size=2> Table 3 COCO _minival_ 目标检测结果。ResNet-50，使用不同分支数量</font>\n\n其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。\n\n# 结论\n提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。","slug":"TridentNet","published":1,"updated":"2019-06-27T12:17:29.335Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5pts000vxgvc346f74ae","content":"<p>论文：<a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>\n<p>代码：<a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目标检测通常分为：</p>\n<ol>\n<li>one stage，如 YOLO, SSD</li>\n<li>two stage，如 Faster R-CNN, R-FCN</li>\n</ol>\n<p>这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：</p>\n<ol>\n<li>生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时</li>\n<li>利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD</li>\n<li>2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。</li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p>本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。</p>\n<h1 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h1><p>backbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。</p>\n<p>假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。</p>\n<p>实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 <em>conv4</em> stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font></p>\n<p>从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：</p>\n<ol>\n<li>网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的</li>\n<li>尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小</li>\n</ol>\n<h1 id=\"Trident-网络\"><a href=\"#Trident-网络\" class=\"headerlink\" title=\"Trident 网络\"></a>Trident 网络</h1><p>TridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。</p>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>如图 2，<br><img src=\"/images/TridentNet_fig2.png\" alt></p>\n<p>网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。</p>\n<p><strong>多分支块</strong> 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。</p>\n<p>以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。</p>\n<p><strong>分支间共享权重</strong> 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。<br>参数共享优点有三：</p>\n<ol>\n<li>降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数</li>\n<li>对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）</li>\n<li>因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。</li>\n</ol>\n<h2 id=\"scale-aware-训练机制\"><a href=\"#scale-aware-训练机制\" class=\"headerlink\" title=\"scale-aware 训练机制\"></a>scale-aware 训练机制</h2><p>根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。</p>\n<p>每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 <code>(w,h)</code>，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。</p>\n<p>scale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。</p>\n<h2 id=\"Inference-和近似\"><a href=\"#Inference-和近似\" class=\"headerlink\" title=\"Inference 和近似\"></a>Inference 和近似</h2><p>Inference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。</p>\n<p><strong>快速推断近似</strong> 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。</p>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><p>使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。</p>\n<p>性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。</p>\n<h2 id=\"消融学习\"><a href=\"#消融学习\" class=\"headerlink\" title=\"消融学习\"></a>消融学习</h2><p><strong>TridentNet 组件</strong> Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。<br><img src=\"/images/TridentNet_fig3.png\" alt></p>\n<ol>\n<li><strong>Multi-branch</strong><br>如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。</li>\n<li><strong>Scale-aware</strong><br>Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。</li>\n<li><strong>Weight-sharing</strong><br>Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。</li>\n</ol>\n<p><strong>分支数量</strong> Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 3 COCO <em>minival</em> 目标检测结果。ResNet-50，使用不同分支数量</font></p>\n<p>其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://arxiv.org/abs/1901.01892\" target=\"_blank\" rel=\"noopener\">Scale-Aware Trident Networks for Object Detection</a></p>\n<p>代码：<a href=\"https://github.com/TuSimple/simpledet\" target=\"_blank\" rel=\"noopener\">TuSimple/simpledet</a></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目标检测通常分为：</p>\n<ol>\n<li>one stage，如 YOLO, SSD</li>\n<li>two stage，如 Faster R-CNN, R-FCN</li>\n</ol>\n<p>这些方法在目标尺度变化范围较大时，均存在问题，尤其在目标尺度很小或很大时，性能较差。为了解决目标尺寸多变性的问题，有如下方法：</p>\n<ol>\n<li>生成多尺度 image pyramids 作为网络输入，如图 1(a)，缺点是计算量大，耗时</li>\n<li>利用网络中的不同分辨率的 feature maps，不同分辨率的 feature maps 负责检测不同尺度的目标，如 SSD</li>\n<li>2 方法中 low level 的 feature 注重于局部细节，而 high level 的 feature 因为感受野 RF 更大，则注重于整体（语义）为了补偿 low level 的 feature 所缺失的语义，FPN 在原有 bottom-up 的基础上增加 top-down pathway 和 径向连接，如图 1(b)。但是由于不同分辨率 features 来自网络不同的 layers，所以对不同尺度的目标的表征能力差异较大，所以 feature pyramids 不能认为是 image pyramids 的替代。</li>\n</ol>\n<p><img src=\"/images/TridentNet_fig1(a).png\" alt> <center> fig1(a)</center><br><img src=\"/images/TridentNet_fig1(b).png\" alt> <center> fig1(b)</center><br><img src=\"/images/TridentNet_fig1(c).png\" alt=\"fig1(c)\"> <center> fig1(c)</center></p>\n<p>本文提出的新网络结构能适应不同的目标尺度，如图 1(c)，使用 trident 块生成多个尺度相关的 feature maps。trident 块的各个分支结构相同，且共享权重参数，但是由于使用了空洞卷积（膨胀系数不同），所以具有不同的 RF，每个分支负责处理一定尺度范围的目标。由于参数共享，所以 inference 阶段，可以使用一个主分支来近似 TridentNet 。</p>\n<h1 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h1><p>backbone 中的影响最终目标检测的几个设计因素为：下采样率、网络深度和感受野。更深的网络和更低的下采样率会增加网络的复杂度，但往往也有益于检测。为了研究 RF 在检测中的作用，可以将 backbone 的一些卷积层的卷积改为空洞卷积。</p>\n<p>假设膨胀率为 $d_s$，那么一个膨胀后的 3x3 卷积的 RF 与 kernel size 为 $3+2(d_s-1)$ 卷积核的 RF 相当。记当前 feature map 相对于输入 image 的下采样率为 s，那么此时膨胀率为 $d_s$ 的卷积相较于普通卷积，其 RF 将增加 $2(d_s-1)s$，因此，如果将 n 个卷积改为空洞卷积，那么 RF 将增加 $2(d_s-1)sn$，其中，这 n 个卷积所作用的 feature map 相对于输入 image 的下采样率均为 s。</p>\n<p>实验基于 COCO benchmark 使用 Faster R-CNN，backbone 分别使用 ResNet-50 和 ResNet-101，在 <em>conv4</em> stage 的 residual block 上 3x3 卷积层使用空洞卷积，膨胀率在 1-3 之间。测试结果指标 AP 分别基于： a. 所有目标；b. 小目标；c. 中等目标；d. 大目标，结果如表 1，</p>\n<table>\n<thead>\n<tr>\n<th>Backbone</th>\n<th>Dilation</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ResNet-50</td>\n<td>1</td>\n<td align=\"center\">0.332</td>\n<td align=\"center\"><strong>0.174</strong></td>\n<td align=\"center\">0.384</td>\n<td align=\"center\">0.464</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>2</td>\n<td align=\"center\">0.342</td>\n<td align=\"center\">0.168</td>\n<td align=\"center\"><strong>0.386</strong></td>\n<td align=\"center\">0.486</td>\n</tr>\n<tr>\n<td>ResNet-50</td>\n<td>3</td>\n<td align=\"center\">0.341</td>\n<td align=\"center\">0.162</td>\n<td align=\"center\">0.383</td>\n<td align=\"center\"><strong>0.492</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>1</td>\n<td align=\"center\">0.372</td>\n<td align=\"center\"><strong>0.200</strong></td>\n<td align=\"center\"><strong>0.430</strong></td>\n<td align=\"center\">0.528</td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>2</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.191</td>\n<td align=\"center\">0.427</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n<tr>\n<td>ResNet-101</td>\n<td>3</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.181</td>\n<td align=\"center\">0.410</td>\n<td align=\"center\"><strong>0.538</strong></td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 1 COCO 数据集上具有不同 RF 的 Faster R-CNN 的检测结果</font></p>\n<p>从表中可见，当 RF 增加时，ResNet-50 和 ResNet-101 上的小目标的检测性能持续下降，而大目标的检测性能则越来越好。不难发现：</p>\n<ol>\n<li>网络的 RF 能影响不同尺度的目标上的检测性能。一个合适的 RF 是与目标尺度强相关的</li>\n<li>尽管 ResNet-101 拥有足够大的理论 RF 以覆盖大尺度（大于 96x96）的目标，但是当增大膨胀率，仍能提高大目标上的性能。这说明实际上有效 RF 比理论 RF 要小</li>\n</ol>\n<h1 id=\"Trident-网络\"><a href=\"#Trident-网络\" class=\"headerlink\" title=\"Trident 网络\"></a>Trident 网络</h1><p>TridentNet 包括共享权重参数的 trident 块，以及一个精心设计的与 scale-aware 训练机制。</p>\n<h2 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h2><p>如图 2，<br><img src=\"/images/TridentNet_fig2.png\" alt></p>\n<p>网络输入为一个单尺度的 image，然后通过并行的分支生成不同尺度的 feature maps，这些并行分支共享权重参数，但是其卷积层的空洞卷积具有不同的膨胀率。</p>\n<p><strong>多分支块</strong> 在目标检测器的 backbone 中，使用 trident 块代替普通卷积。trident 块包含多个并行的分支，这些分支结构与原先普通卷积相同，只是膨胀率不同（普通卷积可以看作是膨胀率为 1 的空洞卷积）。</p>\n<p>以 ResNet 为例作为 backbone，bottleneck 风格（ResNet-50, ResNet=101 等）的 residual 块包含三个卷积：1x1，3x3，1x1。trident 块则基于 residual 块构建，即，将单个 residual 块改为并行的多个 residual 块，其中每个块中 3x3 的空洞卷积的膨胀率不同。通过堆叠多个 trident 块我们可以有效的调整不同分支上的感受野 RF。通常将 backbone 中最后一个 stage 中的 residual 块替换为 trident 块，这是因为靠后的 stage 其 stride 较大，所以并行分支中的 RF 差距较大。</p>\n<p><strong>分支间共享权重</strong> 多分支的一个显著问题是参数数量成倍增加，可能会导致过拟合，故分支间除了空洞卷积的膨胀不同，结构和参数均相同，包括每个分支的 RPN 和 Fast R-CNN head（分类预测和回归预测）。<br>参数共享优点有三：</p>\n<ol>\n<li>降低参数数量。相比于常规目标检测器，TridentNet 不需要额外的参数</li>\n<li>对不同尺度的目标，输入均通过统一的转换得到 feature maps，具有相同的表征能力。（这是与 feature pyramid 的区别）</li>\n<li>因为是多分支，相当于增加了训练参数的样本。换句话说，在不同的 RF 下，训练同样的参数以应对不同的尺度范围。</li>\n</ol>\n<h2 id=\"scale-aware-训练机制\"><a href=\"#scale-aware-训练机制\" class=\"headerlink\" title=\"scale-aware 训练机制\"></a>scale-aware 训练机制</h2><p>根据预先定义好的膨胀率，trident 框架将生成尺度相关的 feature maps。但是尺度不匹配可能会导致性能降级，例如表 1 中具有大膨胀率的分支检测小目标。因此，很自然地做法就是不同分支负责检测不同尺度的目标。我们提出了 scale-aware 训练机制，加强各分支对尺度认识，从而避免在不匹配的分支上训练具有极端尺度的目标（极大 or 极小）。</p>\n<p>每个分支定义一个有效范围 $[l_i,u_i]$。训练时，某个分支上训练所使用的 proposal 和 gt box 其尺度应该落入此分支的有效范围。具体而言，某个 ROI 大小为 <code>(w,h)</code>，如果 $l_i \\le \\sqrt{wh} \\le u_i$，那么这个 ROI 适合在分支 i 上训练。</p>\n<p>scale-aware 训练机制可以应用于 RPN 和 Fast R-CNN 上。原先 RPN 用于判断 anchors 目标/非目标 的二值分类，以及 box 回归。在 scale-aware 训练机制下，根据 gt box 尺度决定其用在哪个分支上，然后判断这个分支上的 anchor 是否是目标或非目标。训练 Fast R-CNN head 时，每个分支根据其有效范围筛选出有效的 proposal。</p>\n<h2 id=\"Inference-和近似\"><a href=\"#Inference-和近似\" class=\"headerlink\" title=\"Inference 和近似\"></a>Inference 和近似</h2><p>Inference 阶段，所有分支均生成检测结果，然后根据分支的有效范围筛选出有效的检测结果。然后使用 NMS 或 soft-NMS 合并多个分支的检测结果。</p>\n<p><strong>快速推断近似</strong> 为了进一步提高速度，在 inference 阶段我们可以仅使用一个主分支来近似 TridentNet。具体来说，设置主分支的有效范围为 [0,&infin;] 以预测所有尺度的目标。例如图 2 中的三分支网络，我们使用中间分支作为主分支，因为中间分支的有效范围覆盖了大目标和小目标。使用主分支近似 TridentNet 时，没有额外的计算和参数，故与原先的 Faster R-CNN 检测时间相当，与 TridentNet 相比，性能下降较小。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验采用 COCO 数据集，模型训练使用 80k 训练图片和 35k 的验证图片子集（_trainval35k_），模型评估使用 5k 验证图片子集（_minival_）。</p>\n<h2 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h2><p>使用 Faster R-CNN 的 MXNet 版本作为 baseline。网络 backbone 使用 ImageNet 进行预训练，然后迁移网络到检测数据集上微调。resize 输入 image，使得短边为 800 像素。Baseline 和 TridentNet 均进行 end-to-end 训练。我们在 8 块 GPU 上训练，batch size 为16。总共训练了 12 epochs，学习率初始值为 0.02，在第 8 个 和 第 10 个 epoch 之后分别下降 10%。使用 ResNet 的 conv4 stage 的输出作为 backbone 的 feature maps，而 conv5 stage 作为 baseline 和 TridentNet 的 rcnn head。对 TridentNet 的每个分支， 从每个 image 中采样 128 个 ROIs。若无特别说明，我们使用三分支结构作为默认 TridentNet 结构，膨胀率分别为 1，2，3.采用 scale-aware 训练机制时，设置三个分支的有效范围为 [0,90]，[30,160]，[90,&infin;]。</p>\n<p>性能评估时采用 COCO 标准评估指标 AP，和 $AP_{50}/AP_{75}$，以及 $AP_s, AP_m, AP_l$，目标尺度范围分别为 小于 32x32, 32x32 ~ 96x96, 大于 96x96。</p>\n<h2 id=\"消融学习\"><a href=\"#消融学习\" class=\"headerlink\" title=\"消融学习\"></a>消融学习</h2><p><strong>TridentNet 组件</strong> Baseline (Table 2(a)) 的评估结果分别使用 ResNet-101 和 ResNet-101-Deformable 作为 backbone。然后我们逐步在 Baseline 上应用 多分支、权重共享和 scale-aware 训练机制。<br><img src=\"/images/TridentNet_fig3.png\" alt></p>\n<ol>\n<li><strong>Multi-branch</strong><br>如 Table 2(b)，多分支版本比 baseline 的性能有所提升，尤其在大目标检测上，这种提升更加明显。这说明即使只应用最简单的多分支结构，也能受益于不同的 RF。</li>\n<li><strong>Scale-aware</strong><br>Table 2(d) 显示了在 Table 2(b) 多分支版本上增加 scale-aware 训练机制后的结果。在小目标检测上性能有所提升，但是在大目标检测上 $AP_s$ 值掉了。我们猜测，scale-sware 训练机制虽然能阻止分支去训练极端尺寸的目标，但也可能引入过拟合问题，因为每个分支上训练的有效样本数量减少。</li>\n<li><strong>Weight-sharing</strong><br>Table 2(c) 为在 多分支版本 Table 2(b) 基础上增加权重共享这一设计，Table 2(e) TridentNet 为在 Baseline 上应用以上三个设计。这两个网络的性能均得到提升，这证实权重共享是有效的。由于分支共享权重参数，所以参数的训练利用了所有尺度的目标，从而降低了 scale-aware 训练中的过拟合问题。</li>\n</ol>\n<p><strong>分支数量</strong> Table 3 显示了使用 1-4 个分支时的评估结果。这里没有增加 scale-aware 训练，这是为了避免精心地调整不同分支的有效范围。Table 3 说明 TridentNet 比单分支结构（baseline）方法的评估指标高。可以注意到，四分支结构比三分支结构没有带来提升效果，所以我们选择三分支结构作为默认 TridentNet。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Branches</th>\n<th align=\"center\">AP</th>\n<th align=\"center\">AP<sub>50</sub></th>\n<th align=\"center\">AP<sub>s</sub></th>\n<th align=\"center\">AP<sub>m</sub></th>\n<th align=\"center\">AP<sub>l</sub></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">33.2</td>\n<td align=\"center\">53.8</td>\n<td align=\"center\">17.4</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">46.4</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">56.7</td>\n<td align=\"center\"><strong>19.0</strong></td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">51.2</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\"><strong>36.6</strong></td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.3</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\"><strong>52.3</strong></td>\n</tr>\n<tr>\n<td align=\"center\">4</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\"><strong>57.3</strong></td>\n<td align=\"center\">18.8</td>\n<td align=\"center\"><strong>41.4</strong></td>\n<td align=\"center\">51.9</td>\n</tr>\n</tbody></table>\n<p><font size=\"2\"> Table 3 COCO <em>minival</em> 目标检测结果。ResNet-50，使用不同分支数量</font></p>\n<p>其他的消融学习，如在哪个 conv stage 上使用 trident 块，和 trident 块的数量等等，以及 TridentNet 与其他 SOTA 目标检测器的结果对比，可参考原文的实验结果及说明。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>提出了 TridentNet 网络，可以生成具有相同表征能力的 scale 相关的 feature maps。提出 scale-aware 训练机制，使得不同的分支善于处理不同尺度范围的目标。快速 inference 方法使用一个主分支来近似 TridentNet，提高了检测效果（相比于 baseline），并且不引入额外的参数和计算量。</p>\n"},{"title":"mAP","date":"2019-06-16T03:43:57.000Z","mathjax":true,"_content":"# mAP\n目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。\n-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。\n## 相关概念\n0. Positive 表示检测结果\n1. True Positive (TP): IoU 大于等于阈值的检测 box\n2. False Positive (FP): IoU 小于阈值的检测 box\n3. Precision = TP/(TP+FP) = TP/(所有检测)\n4. Recall = TP/(TP+FN) = TP/(所有gt)\n\n由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在[项目](https://github.io/shajian/shajian.github.io)提 issue）：\n1. TP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP\n2. FP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n   如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN\n4. TN\n   \n   目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。\n\nVOC 使用阈值 `0.5`。\n## 指标\n### PR 曲线\n每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R' >= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自[stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)）。\n\n给定目标分类 \"Aeroplane\"，假设检测结果如下,\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\n（BB 表示检测结果所匹配 \"match\" 的 GT box）\n\n以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，\n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n稍作解释：\n\n1. rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14\n2. rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29\n3. rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29\n4. ...\n\n### AP\nVOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,...,1}，然后对 R' >= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R' >= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 __PR 曲线__ 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：\n\n#### 11-点插值\n取11个 R 值的 [0,1] 区间等分点计算平均正确率：\n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n其中，$\\rho(\\tilde r)$ 为计算得到的正确率。\n举个例子如图（完整例子请参考[这里](https://github.com/rafaelpadilla/Object-Detection-Metrics)），\n![](/images/mAP_fig1.png)\n\n蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### 所有点插值\nAP 计算式为，\n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。\n如下图，\n![](/images/mAP_fig2.png)\n\n蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，\n![](/images/mAP_fig3.png)\n\n于是计算 AP 为，\n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC 曲线\n## 相关概念\n1. TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)\n2. TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC 曲线\n\nROC 是常见的评价分类器的指标。\n\nROC 全称 [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)（以下很多内容均来自于这个维基百科词条）。\n\n根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。\n如下图所示，\n![](/images/mAP_fig4.png)\n\n图中 (0,0) 和 (1,1) 两点分别对应：\n1. 当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0\n2. 当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1\n\n实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。\n\n一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。\n\n## ROC 空间\n二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X>T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n两者均为阈值 T 的函数。\n\n1. TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率\n2. FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。\n\n下图表示某分类器的分类情况，\n![图 5](/images/mAP_fig5.png)\n\n横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。\n## AUC\n通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。\n\nAUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有\n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n于是，\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。\n\n最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n概率密度分别为 $f_1,f_0$。\n\n由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 < X_1$ 的概率为：\n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n与上面的计算式形式完全一样，证毕。","source":"_posts/mAP.md","raw":"---\ntitle: mAP\ndate: 2019-06-16 11:43:57\ntags: object detection\nmathjax: true\n---\n# mAP\n目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。\n-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。\n## 相关概念\n0. Positive 表示检测结果\n1. True Positive (TP): IoU 大于等于阈值的检测 box\n2. False Positive (FP): IoU 小于阈值的检测 box\n3. Precision = TP/(TP+FP) = TP/(所有检测)\n4. Recall = TP/(TP+FN) = TP/(所有gt)\n\n由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在[项目](https://github.io/shajian/shajian.github.io)提 issue）：\n1. TP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP\n2. FP\n   \n   检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：\n\n   $$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\\\\\\n   GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\\\\\\n   \\text{Conf}_a > \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$\n3. FN\n   \n   如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN\n4. TN\n   \n   目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。\n\nVOC 使用阈值 `0.5`。\n## 指标\n### PR 曲线\n每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R' >= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自[stackexchange](https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge)）。\n\n给定目标分类 \"Aeroplane\"，假设检测结果如下,\n```\nBB  | confidence | GT\n----------------------\nBB1 |  0.9       | 1\n----------------------\nBB2 |  0.9       | 1\n----------------------\nBB3 |  0.7       | 0\n----------------------\nBB4 |  0.7       | 0\n----------------------\nBB5 |  0.7       | 1\n----------------------\nBB6 |  0.7       | 0\n----------------------\nBB7 |  0.7       | 0\n----------------------\nBB8 |  0.7       | 1\n----------------------\nBB9 |  0.7       | 1\n----------------------\n```\n（BB 表示检测结果所匹配 \"match\" 的 GT box）\n\n以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 [Evaluation](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000) 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，\n```\nrank=1  precision=1.00 and recall=0.14\n----------\nrank=2  precision=1.00 and recall=0.29\n----------\nrank=3  precision=0.66 and recall=0.29\n----------\nrank=4  precision=0.50 and recall=0.29\n----------\nrank=5  precision=0.40 and recall=0.29\n----------\nrank=6  precision=0.50 and recall=0.43\n----------\nrank=7  precision=0.43 and recall=0.43\n----------\nrank=8  precision=0.38 and recall=0.43\n----------\nrank=9  precision=0.44 and recall=0.57\n----------\nrank=10 precision=0.50 and recall=0.71\n----------\n```\n稍作解释：\n\n1. rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14\n2. rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29\n3. rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29\n4. ...\n\n### AP\nVOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,...,1}，然后对 R' >= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R' >= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 __PR 曲线__ 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：\n\n#### 11-点插值\n取11个 R 值的 [0,1] 区间等分点计算平均正确率：\n$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)} \\qquad(1) \\\\\\\\\n\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$\n\n其中，$\\rho(\\tilde r)$ 为计算得到的正确率。\n举个例子如图（完整例子请参考[这里](https://github.com/rafaelpadilla/Object-Detection-Metrics)），\n![](/images/mAP_fig1.png)\n\n蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：\n\n$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,...,1}} \\rho_{interp(r)}$\n\n$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$\n\n$AP=26.84\\%$\n\n#### 所有点插值\nAP 计算式为，\n$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\\\\\\n\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$\n其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。\n如下图，\n![](/images/mAP_fig2.png)\n\n蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，\n![](/images/mAP_fig3.png)\n\n于是计算 AP 为，\n\n$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\\\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56\\%$\n\n# ROC 曲线\n## 相关概念\n1. TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)\n2. TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)\n3. FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)\n4. FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)\n5. LR+ (positive likelihood ratio):\n   \n   $LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$\n6. LR- (negative likelihood ratio):\n   \n   $LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$\n7. Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR\n\n## ROC 曲线\n\nROC 是常见的评价分类器的指标。\n\nROC 全称 [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)（以下很多内容均来自于这个维基百科词条）。\n\n根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。\n如下图所示，\n![](/images/mAP_fig4.png)\n\n图中 (0,0) 和 (1,1) 两点分别对应：\n1. 当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0\n2. 当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1\n\n实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。\n\n一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。\n\n## ROC 空间\n二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X>T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,\n$$TPR=\\int_T^{\\infty} f_1(x)dx \\\\\nFPR = \\int_T^{\\infty} f_0(x)dx$$\n两者均为阈值 T 的函数。\n\n1. TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率\n2. FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。\n\n下图表示某分类器的分类情况，\n![图 5](/images/mAP_fig5.png)\n\n横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。\n## AUC\n通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。\n\nAUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有\n$$TPR(T): T \\rightarrow y(x) \\\\\\\\\nFPR(T): T \\rightarrow x$$\n于是，\n$$\nA =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\\\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ '(T) \\ dT \\\\\\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T') \\ dT' \\right) f_0(T) \\ dT \\\\\\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T')f_0(T) \\ dT' dT \\\\\\\\ = P(X_1>X_0)\n$$\n其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。\n\n最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:\n$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\\\\\\nF_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$\n概率密度分别为 $f_1,f_0$。\n\n由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 < X_1$ 的概率为：\n$$P(X_1>X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$\n与上面的计算式形式完全一样，证毕。","slug":"mAP","published":1,"updated":"2019-06-21T07:02:21.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptt000xxgvc6yc0orym","content":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p>目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。<br>-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。</p>\n<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol start=\"0\">\n<li>Positive 表示检测结果</li>\n<li>True Positive (TP): IoU 大于等于阈值的检测 box</li>\n<li>False Positive (FP): IoU 小于阈值的检测 box</li>\n<li>Precision = TP/(TP+FP) = TP/(所有检测)</li>\n<li>Recall = TP/(TP+FN) = TP/(所有gt)</li>\n</ol>\n<p>由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\">项目</a>提 issue）：</p>\n<ol>\n<li><p>TP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP</p>\n</li>\n<li><p>FP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p>如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN</p>\n</li>\n<li><p>TN</p>\n<p>目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。</p>\n</li>\n</ol>\n<p>VOC 使用阈值 <code>0.5</code>。</p>\n<h2 id=\"指标\"><a href=\"#指标\" class=\"headerlink\" title=\"指标\"></a>指标</h2><h3 id=\"PR-曲线\"><a href=\"#PR-曲线\" class=\"headerlink\" title=\"PR 曲线\"></a>PR 曲线</h3><p>每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R’ &gt;= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自<a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a>）。</p>\n<p>给定目标分类 “Aeroplane”，假设检测结果如下,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n\n<p>（BB 表示检测结果所匹配 “match” 的 GT box）</p>\n<p>以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank=1  precision=1.00 and recall=0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=2  precision=1.00 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=3  precision=0.66 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=4  precision=0.50 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=5  precision=0.40 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=6  precision=0.50 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=7  precision=0.43 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=8  precision=0.38 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=9  precision=0.44 and recall=0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=10 precision=0.50 and recall=0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n\n<p>稍作解释：</p>\n<ol>\n<li>rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14</li>\n<li>rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29</li>\n<li>rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29</li>\n<li>…</li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,…,1}，然后对 R’ &gt;= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R’ &gt;= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 <strong>PR 曲线</strong> 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：</p>\n<h4 id=\"11-点插值\"><a href=\"#11-点插值\" class=\"headerlink\" title=\"11-点插值\"></a>11-点插值</h4><p>取11个 R 值的 [0,1] 区间等分点计算平均正确率：<br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>其中，$\\rho(\\tilde r)$ 为计算得到的正确率。<br>举个例子如图（完整例子请参考<a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\">这里</a>），<br><img src=\"/images/mAP_fig1.png\" alt></p>\n<p>蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"所有点插值\"><a href=\"#所有点插值\" class=\"headerlink\" title=\"所有点插值\"></a>所有点插值</h4><p>AP 计算式为，<br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。<br>如下图，<br><img src=\"/images/mAP_fig2.png\" alt></p>\n<p>蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，<br><img src=\"/images/mAP_fig3.png\" alt></p>\n<p>于是计算 AP 为，</p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-曲线\"><a href=\"#ROC-曲线\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h1><h2 id=\"相关概念-1\"><a href=\"#相关概念-1\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol>\n<li><p>TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC-曲线-1\"><a href=\"#ROC-曲线-1\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h2><p>ROC 是常见的评价分类器的指标。</p>\n<p>ROC 全称 <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a>（以下很多内容均来自于这个维基百科词条）。</p>\n<p>根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。<br>如下图所示，<br><img src=\"/images/mAP_fig4.png\" alt></p>\n<p>图中 (0,0) 和 (1,1) 两点分别对应：</p>\n<ol>\n<li>当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0</li>\n<li>当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1</li>\n</ol>\n<p>实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。</p>\n<p>一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。</p>\n<h2 id=\"ROC-空间\"><a href=\"#ROC-空间\" class=\"headerlink\" title=\"ROC 空间\"></a>ROC 空间</h2><p>二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X&gt;T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br>两者均为阈值 T 的函数。</p>\n<ol>\n<li>TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率</li>\n<li>FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。</li>\n</ol>\n<p>下图表示某分类器的分类情况，<br><img src=\"/images/mAP_fig5.png\" alt=\"图 5\"></p>\n<p>横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。</p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p>通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。</p>\n<p>AUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有<br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br>于是，<br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ ‘(T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T’) \\ dT’ \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T’)f_0(T) \\ dT’ dT \\\\ = P(X_1&gt;X_0)<br>$$<br>其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。</p>\n<p>最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br>概率密度分别为 $f_1,f_0$。</p>\n<p>由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 &lt; X_1$ 的概率为：<br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br>与上面的计算式形式完全一样，证毕。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"mAP\"><a href=\"#mAP\" class=\"headerlink\" title=\"mAP\"></a>mAP</h1><p>目标检测中，不同比赛的评估指标通常也不相同，我们先以 PASCAL VOC 为例进行说明。<br>-目标检测中常用的评价标准是 mAP（mean Average Precision），入坑目标检测的应该都知道 mAP 是 AP 的平均，即每个分类单独计算出一个 AP 值，然后对所有分类的 AP 值求平均就得到 mAP。</p>\n<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol start=\"0\">\n<li>Positive 表示检测结果</li>\n<li>True Positive (TP): IoU 大于等于阈值的检测 box</li>\n<li>False Positive (FP): IoU 小于阈值的检测 box</li>\n<li>Precision = TP/(TP+FP) = TP/(所有检测)</li>\n<li>Recall = TP/(TP+FN) = TP/(所有gt)</li>\n</ol>\n<p>由于现在我们专注于目标检测这个场景，所以首先需要弄清楚目标检测中 TP,FP,TN,FN 这四个基本概念。（以下4点均基于个人理解，如有错误，请及时通知本人修改，若博客不支持评论，可在<a href=\"https://github.io/shajian/shajian.github.io\" target=\"_blank\" rel=\"noopener\">项目</a>提 issue）：</p>\n<ol>\n<li><p>TP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 超过阈值（$Threshold_{VOC}=0.5$）的检测为 TP</p>\n</li>\n<li><p>FP</p>\n<p>检测结果为P (Positive)，其中与 gt box 最大 IoU 低于阈值的检测为 FP。如果某个检测与某 gt box 有最大 IoU 且超过阈值，但是这个 gt box 已被另一个检测匹配（match），且另一个检测的 confidence 更高，则当前检测也被认为是 FP。用数学语言描述为：</p>\n<p>$$\\left. \\begin{array}{} GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_a, GT_i) \\\\<br>GT_1=\\underset{GT_i} {\\text{argmax}} \\quad \\text{IoU}(Det_b, GT_i) \\\\<br>\\text{Conf}_a &gt; \\text{Conf}_b \\end{array} \\right] \\Rightarrow Det_b \\in FP$$</p>\n</li>\n<li><p>FN</p>\n<p>如果某个 gt box 未被检测到，即没有检测结果与这个 gt box 的 IoU 大于0，则认为这个 gt box 为 FN</p>\n</li>\n<li><p>TN</p>\n<p>目标检测中没有阴性预测，TN = 0。以二分类问题为例，则分类判断不是 Positive 就是 Negative，TN 表示判断为 Negative，而实际是 Positive。</p>\n</li>\n</ol>\n<p>VOC 使用阈值 <code>0.5</code>。</p>\n<h2 id=\"指标\"><a href=\"#指标\" class=\"headerlink\" title=\"指标\"></a>指标</h2><h3 id=\"PR-曲线\"><a href=\"#PR-曲线\" class=\"headerlink\" title=\"PR 曲线\"></a>PR 曲线</h3><p>每个预测 box 均有一个 score 表示 confidence，对这个 confidence 设置阈值，仅考虑大于等于这个阈值的预测 box，小于这个阈值的检测结果则忽略，于是每个不同的 confidence 阈值均对应一对 PR（Precision x Recall）值。实际计算中，按 confidence 降序排列，将预测数量从 1 增加到全部预测数量（从 rank=1 到全部预测数量），每次计算一对 PR 值，于是得到原始的 PR 曲线，对于召回率 R’ &gt;= R 选取最大的 P 值则得到插值 PR 曲线。我们使用一个例子予以说明（搬运自<a href=\"https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\" target=\"_blank\" rel=\"noopener\">stackexchange</a>）。</p>\n<p>给定目标分类 “Aeroplane”，假设检测结果如下,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BB  | confidence | GT</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB1 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB2 |  0.9       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB3 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB4 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB5 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB6 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB7 |  0.7       | 0</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB8 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br><span class=\"line\">BB9 |  0.7       | 1</span><br><span class=\"line\">----------------------</span><br></pre></td></tr></table></figure>\n\n<p>（BB 表示检测结果所匹配 “match” 的 GT box）</p>\n<p>以上表格中已经按 confidence 降序排列，GT=1 表示 TP，GT=0 表示 FP，此外还有两个未检测到的 BBox，即 FN=2。TP=5 (BB1,BB2,BB5,BB8,BB9)，FP=5，其中有一个检测为 BB1，但是其 confidence 小于 0.9 而被抑制，故认为此检测是 FP，对应如下的 rank=3 这个 case，舍弃这个检测。这一点在 PASCAL VOC 主页的 Detection Task 的 <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" target=\"_blank\" rel=\"noopener\">Evaluation</a> 一节也进行了说明。GT box 数量为 TP+FN=5+2=7。计算所有点的 PR 值如下，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rank=1  precision=1.00 and recall=0.14</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=2  precision=1.00 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=3  precision=0.66 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=4  precision=0.50 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=5  precision=0.40 and recall=0.29</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=6  precision=0.50 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=7  precision=0.43 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=8  precision=0.38 and recall=0.43</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=9  precision=0.44 and recall=0.57</span><br><span class=\"line\">----------</span><br><span class=\"line\">rank=10 precision=0.50 and recall=0.71</span><br><span class=\"line\">----------</span><br></pre></td></tr></table></figure>\n\n<p>稍作解释：</p>\n<ol>\n<li>rank=1，检测数量为 1（此时其他检测结果均被舍弃），TP 仅 BB1 一个，没有 FP，故 P=1，R=1/7=0.14</li>\n<li>rank=2，检测数量为 2，TP 包括 BB1,BB2，没有 FP，故 P=1，R=2/7=0.29</li>\n<li>rank=3，检测数量为 3，TP 包括 BB1,BB2，FP 为 BB1，故 P=2/3=0.66，R=2/7=0.29</li>\n<li>…</li>\n</ol>\n<h3 id=\"AP\"><a href=\"#AP\" class=\"headerlink\" title=\"AP\"></a>AP</h3><p>VOC 在 2010 之前，选择固定的 11 个 R 值 等分点，即 R={0,0.1,…,1}，然后对 R’ &gt;= R 选择最大 P 值得到插值 PR 曲线。 AP 则是每个 R 阈值处的平均正确率（average precision）。VOC 2010 之后，仍然是对 R’ &gt;= R 选择最大 P 值，但是 R 是 [0,1] 之间的所有值（参考上一节内容 <strong>PR 曲线</strong> 中的计算过程），此时 AP 为 PR 曲线下方的面积 AUC （area under the curve）。两种计算方法如下：</p>\n<h4 id=\"11-点插值\"><a href=\"#11-点插值\" class=\"headerlink\" title=\"11-点插值\"></a>11-点插值</h4><p>取11个 R 值的 [0,1] 区间等分点计算平均正确率：<br>$$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)} \\qquad(1) \\\\<br>\\rho_{interp(r)}=\\max_{\\tilde r:\\tilde r \\ge r} \\rho(\\tilde r) \\qquad(2) $$</p>\n<p>其中，$\\rho(\\tilde r)$ 为计算得到的正确率。<br>举个例子如图（完整例子请参考<a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\" target=\"_blank\" rel=\"noopener\">这里</a>），<br><img src=\"/images/mAP_fig1.png\" alt></p>\n<p>蓝色折线的顶点为根据预测结果计算得到的 PR 值，红色点则是根据11个固定的 R 值进行插值得到的 PR 值，比如计算阈值 R=0.2 处的插值，根据式 (2)，大于等于 0.2 的 $\\tilde r$ 值可取 {0.2,0.2666,0.3333,0.4,0.4666}，当 $\\tilde r=0.4$ 时，显然 P 有最大值为 0.4285。根据 11-点插值，计算 AP：</p>\n<p>$AP=\\frac 1 {11} \\sum_{r \\in {0,0.1,…,1}} \\rho_{interp(r)}$</p>\n<p>$AP=\\frac 1 {11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)$</p>\n<p>$AP=26.84%$</p>\n<h4 id=\"所有点插值\"><a href=\"#所有点插值\" class=\"headerlink\" title=\"所有点插值\"></a>所有点插值</h4><p>AP 计算式为，<br>$$AP=\\sum_{r=0}^1(r_{n+1}-r_n) \\rho_{interp}(r_{n+1}) \\qquad(3) \\\\<br>\\rho_{interp}(r_{n+1})=\\max_{\\tilde r: \\tilde r \\ge r_{n+1}} \\rho(\\tilde r) \\qquad(4)$$<br>其中，$\\rho (\\tilde r)$ 为 Recall $\\tilde r$ 处的正确率。这种 AP 计算方法首先插值得到每个召回率值的正确率，然后计算插值后 PR 曲线下的面积 AUC。<br>如下图，<br><img src=\"/images/mAP_fig2.png\" alt></p>\n<p>蓝色折线顶点表示根据检测结果计算出来的 PR 值，红色虚线表示插值后的 RP 值，可将 AUC 划为 4 个区域，如下图，<br><img src=\"/images/mAP_fig3.png\" alt></p>\n<p>于是计算 AP 为，</p>\n<p>$AP=A_1+A_2+A_3+A_4=(0.0666-0) \\times 1+(0.1333-0.0666) \\times 0.6666 \\\\ +(0.4-0.1333) \\times 0.4285+(0.4666-0.4) \\times 0.3043=24.56%$</p>\n<h1 id=\"ROC-曲线\"><a href=\"#ROC-曲线\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h1><h2 id=\"相关概念-1\"><a href=\"#相关概念-1\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><ol>\n<li><p>TPR (true positive rate)，又称灵敏度 (sensitivity)、召回率 (recall)：TPR = TP/(TP+FN)</p>\n</li>\n<li><p>TNR (true negative rate)，又称特异度 (specificity): TNR = TN/(FP+TN)</p>\n</li>\n<li><p>FNR (false negative rate)，又称漏诊率: FNR = 1 - TPR = FN/(TP+FN)</p>\n</li>\n<li><p>FPR (false positive rate)，又称误诊率: FPR = 1 - TNR = FP/(FP+TN)</p>\n</li>\n<li><p>LR+ (positive likelihood ratio):</p>\n<p>$LR^+=\\frac {TPR} {FPR} = \\frac {Sensitivily} {1-Specificity}$</p>\n</li>\n<li><p>LR- (negative likelihood ratio):</p>\n<p>$LR^-=\\frac {FNR} {TNR} = \\frac {1-Sensitivity} {Specificity}$</p>\n</li>\n<li><p>Youden index: Youden index = Sensitivity + Specificity - 1 = TPR - FPR</p>\n</li>\n</ol>\n<h2 id=\"ROC-曲线-1\"><a href=\"#ROC-曲线-1\" class=\"headerlink\" title=\"ROC 曲线\"></a>ROC 曲线</h2><p>ROC 是常见的评价分类器的指标。</p>\n<p>ROC 全称 <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener\">receiver operating characteristic</a>（以下很多内容均来自于这个维基百科词条）。</p>\n<p>根据不同的判别阈值（大于等于阈值为正，否则为负），得到一组 TPR-FPR 值，所画曲线就是 ROC 曲线。<br>如下图所示，<br><img src=\"/images/mAP_fig4.png\" alt></p>\n<p>图中 (0,0) 和 (1,1) 两点分别对应：</p>\n<ol>\n<li>当阈值为 1 时，全部判断为 Negative，故 TP=FP=0，所以 TPR=FPR=0</li>\n<li>当阈值为 0 时，全部判断为 Positive，故 TN=FN=0，所以 TPR=FPR=1</li>\n</ol>\n<p>实际上，阈值可以位于范围 $(-\\infty,0) \\cup (1,+\\infty)$，位于 $(-\\infty,0)$ 是与第 2 点相同，位于 $(1,+\\infty)$ 是与第 1 点相同。</p>\n<p>一个好的分类器其 ROC 曲线应该位于直线 y=x 的上方，直线 y=x 对应随机猜测的分类器，也就是说，不管选择什么阈值，都应该让真阳性率大于误诊率。理想情况下，TPR 接近 1，FPR 接近 0，故 ROC 曲线越接近 (0,1)，越偏离直线 y=x，就越好。</p>\n<h2 id=\"ROC-空间\"><a href=\"#ROC-空间\" class=\"headerlink\" title=\"ROC 空间\"></a>ROC 空间</h2><p>二分类中，每个实例的分类预测均基于一个连续随机变量 X，即实例对应的得分 score，例如逻辑回归中的概率。给定阈值 T，如果 X&gt;T，为正例，否则为负例。如果实例属于正例，那么 X 的概率密度为 $f_1(x)$，如果实例属于负例，那么 X 的概率密度为 $f_0(x)$。因此,<br>$$TPR=\\int_T^{\\infty} f_1(x)dx \\<br>FPR = \\int_T^{\\infty} f_0(x)dx$$<br>两者均为阈值 T 的函数。</p>\n<ol>\n<li>TPR(T) 表示在该阈值下随机选择一个正例，判断该正例为正例的概率</li>\n<li>FPR(T) 表示在该阈值下随机选择一个负例，判断该负例为正例的概率。</li>\n</ol>\n<p>下图表示某分类器的分类情况，<br><img src=\"/images/mAP_fig5.png\" alt=\"图 5\"></p>\n<p>横轴为随机变量 X 的取值（表示计算得分 score），与纵轴的交点处为判断阈值，纵轴表示概率密度，越大则表示此 score 对应的实例越多。两个曲线相聚越远，则表示越容易区分正负例。</p>\n<h2 id=\"AUC\"><a href=\"#AUC\" class=\"headerlink\" title=\"AUC\"></a>AUC</h2><p>通常使用 ROC 曲线下方的面积 AUC 来评价一个分类器的好坏。</p>\n<p>AUC 等于一个概率值：当随机选择一个正例和随机选择一个负例时，分类器计算正例的 Score 大于计算负例的 Score 的概率。根据ROC 曲线，可以将 TPR 看作是 FPR 的函数，而实际上这两者均是判断阈值 T 的函数，所以有<br>$$TPR(T): T \\rightarrow y(x) \\\\<br>FPR(T): T \\rightarrow x$$<br>于是，<br>$$<br>A =\\int_0^1 y(x) \\ dx  =\\int_0^1 TPR[FPR^{-1}(x)] \\ dx \\\\ \\stackrel{x=FPR(T)} =\\int_{-\\infty}^{+\\infty} TPR(T) \\ d[FPR(T)] =\\int_{-\\infty}^{+\\infty} TPR(T) \\cdot FPR \\ ‘(T) \\ dT \\\\ = \\int_{-\\infty}^{+\\infty} \\left( \\int_T^{+\\infty}  f_1(T’) \\ dT’ \\right) f_0(T) \\ dT \\\\ =\\int_{-\\infty}^{+\\infty}\\int_T^{+\\infty}  f_1(T’)f_0(T) \\ dT’ dT \\\\ = P(X_1&gt;X_0)<br>$$<br>其中，$X_1$ 表示正例的得分，$X_0$表示负例的得分。</p>\n<p>最后一个等号可能不容易理解，我们将 $X_1$ 和 $X_0$ 均看作随机变量，其分布函数为:<br>$$F_1(x)=\\int_{-\\infty}^{x} f_1(x) dx \\\\<br>F_0(x)=\\int_{-\\infty}^{x} f_1(x) dx$$<br>概率密度分别为 $f_1,f_0$。</p>\n<p>由于$X_1, X_0$ 互相独立，二维随机变量 $(X_1,X_0)$ 的联合概率密度为 $f(x_1,x_0)=f_1(x_1) f_0(x_0)$，于是 $X_0 &lt; X_1$ 的概率为：<br>$$P(X_1&gt;X_0)=\\iint_{G} f(x_1,x_0) dx_1 dx_0=\\int_{-\\infty}^{+\\infty}\\int_{x_0}^{+\\infty}f_1(x_1) f_0(x_0) \\ dx_1 dx_0$$<br>与上面的计算式形式完全一样，证毕。</p>\n"},{"title":"mask-rcnn","date":"2019-07-08T09:39:57.000Z","mathjax":true,"_content":"论文 [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n\n# Introduction\n这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  \n\nMask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。  \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  \n\n另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。\n\n# Mask R-CNN\nFaster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。\n\n__Faster R-CNN:__ 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。\n\n__Mask R-CNN:__ 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。\n\n训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，\n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。\n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。  \nmask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，\n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n其中 $f(\\cdot)$ 表示 sigmoid。\n\n__Mask Representation:__ 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。\n\n__RoIAlign:__ RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。\n\n可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，\n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI 的大小 和 空间 bin 的大小分别为\n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n对于第 (i,j) 个 bin，其位置为\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n其中 $0 \\le i<7, \\ 0\\le j<7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）\n\n两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，\n![](/images/mask-rcnn_fig3.png)\n\n特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。\n\n__Network Architecture:__ Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。\n\nBackbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。\n\n我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。\n\n对于 Network head，如图 4，\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。  \n图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。\n# Experiments\n实验部分略，请阅读原文。\n\n# Appendix\n有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i < R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，\n1. 计算第 i 个 box 的宽高  \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2. 将 mask map resize 到 box 宽高的大小  \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3. 将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4. 将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","source":"_posts/mask-rcnn.md","raw":"---\ntitle: mask-rcnn\ndate: 2019-07-08 17:39:57\ntags: object detection\nmathjax: true\n---\n论文 [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n\n# Introduction\n这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  \n\nMask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。  \n![](/images/mask-rcnn_fig1.png)\n\nFaster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  \n\n另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。\n\n# Mask R-CNN\nFaster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。\n\n__Faster R-CNN:__ 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。\n\n__Mask R-CNN:__ 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。\n\n训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，\n$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$\n上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。\n$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u,v_i)$$\n上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。  \nmask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，\n$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$\n其中 $f(\\cdot)$ 表示 sigmoid。\n\n__Mask Representation:__ 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。\n\n__RoIAlign:__ RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。\n\n可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，\n$$x_1'=[x_1/16] \\quad y_1'=[y_1/16]\n\\quad x_2'=[x_2/16]\n\\quad y_2'=[y_2/16]$$\nRoI 的大小 和 空间 bin 的大小分别为\n$$w'=x_2'-x_1'+1\n\\quad h'=y_2'-y_1'+1\n\\\\\\\\ w^b=w'/7 \\quad h^b=h'/7$$\n对于第 (i,j) 个 bin，其位置为\n$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$\n\n其中 $0 \\le i<7, \\ 0\\le j<7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）\n\n两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，\n![](/images/mask-rcnn_fig3.png)\n\n特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。\n\n__Network Architecture:__ Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。\n\nBackbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。\n\n我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。\n\n对于 Network head，如图 4，\n![](/images/mask-rcnn_fig4.png)\n\nResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。  \n图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。\n# Experiments\n实验部分略，请阅读原文。\n\n# Appendix\n有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i < R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，\n1. 计算第 i 个 box 的宽高  \n   $w=x_2-x_1, \\ h=y_2-y_1$\n2. 将 mask map resize 到 box 宽高的大小  \n   ```python\n   mask=cv2.resize(M_i_k, (w,h))\n   ```\n3. 将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  \n   ```python\n   mask=np.array(mask>0.5)\n   ```\n4. 将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  \n   ```python\n   im_mask=np.zero((H,W), dtype=np.uint8)\n   im_mask[y1:y2,x1:x2]=mask\n   ```\n","slug":"mask-rcnn","published":1,"updated":"2019-07-09T10:57:27.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptu000zxgvcwgb99gsn","content":"<p>论文 <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  </p>\n<p>Mask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。<br><img src=\"/images/mask-rcnn_fig1.png\" alt></p>\n<p>Faster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  </p>\n<p>另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。</p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。</p>\n<p><strong>Faster R-CNN:</strong> 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。</p>\n<p><strong>Mask R-CNN:</strong> 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。</p>\n<p>训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，<br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br>上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。<br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br>上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。<br>mask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，<br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br>其中 $f(\\cdot)$ 表示 sigmoid。</p>\n<p><strong>Mask Representation:</strong> 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。</p>\n<p><strong>RoIAlign:</strong> RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。</p>\n<p>可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，<br>$$x_1’=[x_1/16] \\quad y_1’=[y_1/16]<br>\\quad x_2’=[x_2/16]<br>\\quad y_2’=[y_2/16]$$<br>RoI 的大小 和 空间 bin 的大小分别为<br>$$w’=x_2’-x_1’+1<br>\\quad h’=y_2’-y_1’+1<br>\\\\ w^b=w’/7 \\quad h^b=h’/7$$<br>对于第 (i,j) 个 bin，其位置为<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p>其中 $0 \\le i&lt;7, \\ 0\\le j&lt;7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）</p>\n<p>两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，<br><img src=\"/images/mask-rcnn_fig3.png\" alt></p>\n<p>特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。</p>\n<p><strong>Network Architecture:</strong> Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。</p>\n<p>Backbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。</p>\n<p>我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。</p>\n<p>对于 Network head，如图 4，<br><img src=\"/images/mask-rcnn_fig4.png\" alt></p>\n<p>ResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。<br>图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，请阅读原文。</p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p>有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i &lt; R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，</p>\n<ol>\n<li><p>计算第 i 个 box 的宽高<br>$w=x_2-x_1, \\ h=y_2-y_1$</p>\n</li>\n<li><p>将 mask map resize 到 box 宽高的大小  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1703.06870\" target=\"_blank\" rel=\"noopener\">Mask R-CNN</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>这是一篇实例分割的文章。实例分割结合了目标检测和语义分割，这看似是需要一个复杂的模型才能完成的任务，实际上本文提出的 Mask R-CNN 出奇的简单灵活且高效。  </p>\n<p>Mask R-CNN 是对 Faster R-CNN 的扩展，增加了一个分支用于预测每个 RoI 的分割掩模（segmentation masks），如图 1，这个分支与原先的分类和回归分支并列。mask 分支在 RoI 上以 pixel-to-pixel 方式预测得到一个 segmentation mask，有语义分割背景的话，不难想象 mask 分支应是一个全卷积网络 FCN。如何构建这个 mask 分支则至关重要。<br><img src=\"/images/mask-rcnn_fig1.png\" alt></p>\n<p>Faster R-CNN 的网络输入和输出之间不是点与点对齐，这是由于 RoIPool 层在抽取特征时使用了离散化空间坐标（坐标值必须为整数，RoI 坐标从输入 image 平面映射到 feature map 时，坐标变为原来的 1/16，并四舍五入取整），而 mask 分支是 pixel-wise 的，所以必须要解决这个不对齐问题，为此我们提出了 RoIAlign 层以保持准确的空间位置，这个改动虽小，但效果却十分明显：提高了大约 10%~50% 的 mask 准确度。  </p>\n<p>另外，有必要将分类预测和 binary mask 预测解耦，每个分类独立进行 binary mask 预测，并根据 RoI 分类分支来确定目标分类。相反在语义分割 FCN 方法中，每个像素位置均进行多分类，这就耦合了分割和分类，如果用在实例分割任务中则表现较差。</p>\n<h1 id=\"Mask-R-CNN\"><a href=\"#Mask-R-CNN\" class=\"headerlink\" title=\"Mask R-CNN\"></a>Mask R-CNN</h1><p>Faster R-CNN 中每个候选区域均对应两个输出：分类标签和坐标偏差。Mask R-CNN 则在此基础上增加第三个输出：目标 binary mask，与前两个输出不同的是，此输出需要非常精确的目标空间位置，这是 Mask R-CNN 的关键点之一。</p>\n<p><strong>Faster R-CNN:</strong> 简单的回顾一下 Faster R-CNN，这是一个 two-stage 目标检测器，其中第一个 stage 为 RPN，用于生成 proposals，第二个 stage 其本质就是 Fast R-CNN，使用 RoIPooling 从每个 proposal 中提取固定长度的特征并进行分类和 bbox 回归。</p>\n<p><strong>Mask R-CNN:</strong> 在 Faster R-CNN 基础上增加第二个 stage 的输出，即为每个 RoI 生成 binary mask。</p>\n<p>训练时，每个 RoI 的损失为 $L=L_{cls}+L_{box}+L_{mask}$，其中分类损失 $L_{cls}$ 和回归损失 $L_{box}$ 均与 Fast/Faster R-CNN 中相同，<br>$$L_{cls}=L_{cls}(p,u)=-\\log p_u$$<br>上式为 log loss，proposal 对应的 gt 分类为 u，$p_u$ 为 proposal 分类为 u 对应的置信度（分类得分）。<br>$$L_{loc}=L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}} smooth_{L_1}(t_i^u,v_i)$$<br>上式为 smooth L1 loss，$t_u$ 为在分类 u 下的 bbox 的四个偏移值，v 表示 gt box 相对 proposal 的偏移 target。<br>mask 分支会为每个 RoI 生成 $Km^2$ 维输出向量，然后对这个输出向量应用 pixel-wise sigmoid，表示 K 个 binary mask，每个 mask 分辨率为 $m \\times m$，这里 K 表示所有分类数量，定义 $L_{mark}$ 为平均二值交叉熵损失，记 RoI 的 gt 分类为 k，$L_{mark}$ 仅由第 k 个 binary mask 计算得到，其他 K-1 个 binary mask 均不参与 $L_{mark}$ 的计算，<br>$$L_{mark}=-\\frac 1{m^2} \\sum_{i=1}^{m^2} \\sum_{j=0}^1 [t_i=j] \\cdot \\log f(s_i^j)=-\\frac 1{m^2} \\sum_{i=1}^{m^2} [t_i \\cdot \\log f(s_i) + (1-t_i) \\cdot \\log (1-f(s_i))]$$<br>其中 $f(\\cdot)$ 表示 sigmoid。</p>\n<p><strong>Mask Representation:</strong> 对单个 RoI 而言，无论其大小，对应的分类和 bbox 偏移这两个输出都是固定长度，可由 fc 层输出得到，而 mask 则以 pixel-to-pixel 方式表征 RoI 中目标的空间布局，所以适合使用卷积。事实上，我们正是使用了全卷积网络 FCN 来为每个 RoI 生成 $m \\times m$ 空间尺寸的 mask。然而需要注意的是，pixel-to-pixel 的方式要求 RoI 特征能如实地保留每个像素的空间对应关系，于是我们提出 RoIAlign 来解决这个问题。</p>\n<p><strong>RoIAlign:</strong> RoIPool 是从 RoI 中抽取固定长度特征（例如 7x7）的标准方法，首先将浮点数 RoI 量化成整数粒度的 feature map，然后将量化后的 RoI 切分得到一系列空间 bins，每个空间 bin 的大小也是浮点数，所以每个空间 bin 的位置也需要量化，然后将其中的像素值聚合得到这个空间 bin 的值，一般使用最大值池化进行聚合。</p>\n<p>可见前后有两次量化过程，第一次量化是在将 RoI 的坐标 x 从输入 image 平面上映射到特征平面上，在 Faster R-CNN 中，这个特征的 stride 为 16，所以 RoI 在特征平面上的坐标为 $[x/16]$，其中 $[\\cdot]$ 表示四舍五入成整数；第二次量化是在计算空间 bin 位置时，假设 RoI 为 $(x_1,y_1,x_2,y_2)$（通常均为浮点数，因为 RPN 中对 anchor 位置进行偏移得到 RoI），一共将 RoI 划分为 7x7 个空间 bins，经过第一次量化后特征平面上 RoI 表示为，<br>$$x_1’=[x_1/16] \\quad y_1’=[y_1/16]<br>\\quad x_2’=[x_2/16]<br>\\quad y_2’=[y_2/16]$$<br>RoI 的大小 和 空间 bin 的大小分别为<br>$$w’=x_2’-x_1’+1<br>\\quad h’=y_2’-y_1’+1<br>\\\\ w^b=w’/7 \\quad h^b=h’/7$$<br>对于第 (i,j) 个 bin，其位置为<br>$$x_1^b=\\lfloor j \\cdot w^b\\rfloor \\quad y_1^b=\\lfloor i \\cdot h^b\\rfloor \\quad x_2^b=\\lceil (j+1) \\cdot w^b\\rceil \\quad y_1^b=\\lceil (i+1) \\cdot h^b\\rceil$$</p>\n<p>其中 $0 \\le i&lt;7, \\ 0\\le j&lt;7$。（当然还需要对 bin 的位置十分越界进行检查，这里略）</p>\n<p>两次量化使得 RoI 与抽取到的特征不对齐，这对 pixel-to-pixel 的 mask 而言是非常不利的，所幸 RoIAlign 可以解决这个问题。使用 RoIAlign 代替 RoIPool，避免量化操作，如图 3，<br><img src=\"/images/mask-rcnn_fig3.png\" alt></p>\n<p>特征平面上的 RoI 的位置为 $x/16$，其中每个 bin 采样 4 个位置点，采样位置处的值通过双线性插值计算得到，然后每个 bin 的值使用这四个采样位置的值进行聚合得到（max 或者 average 聚合）。整个过程没有任何量化操作。实验的最终结果对采样位置不敏感，对采样位置的数量也不敏感。</p>\n<p><strong>Network Architecture:</strong> Mask R-CNN 网络组成包括 1. 用于抽取特征的 backbone，2. network head，用于 bbox 分类和回归，以及 mask 预测。</p>\n<p>Backbone 网络的命名法：我们使用了 ResNet 和 ResNeXt（深度为 50 或101）。Faster R-CNN 中使用 ResNet 的 4-th stage 的最后一个 conv 的输出作为特征，这里记为 C4。于是，当 ResNet 为 ResNet-50 时，我们称 backbone 为 ResNet-50-C4。</p>\n<p>我们也研究了其他的 backbone 例如 FPN，FPN 使用 top-down 结构以及横向连接生成 feature pyramid。使用 ResNet-FPN 作为 backbone 时，Mask R-CNN 的准确率以及响应速度均有提升。</p>\n<p>对于 Network head，如图 4，<br><img src=\"/images/mask-rcnn_fig4.png\" alt></p>\n<p>ResNet-C4 作为 backbone 时，后面的 head 结构包含 ResNet 的 5-th stage（即，具有 9 个 conv 的 res5）。ResNet-FPN 作为 backbone 时，由于 backbone 已经包含了 res5，故后面的 head 结构较为简单高效。<br>图 4 左边部分，res5 表示 ResNet 的 5-th stage，为简单起见，作用到 7x7 的 RoI feature maps 上的第一个 conv 的 stride 为 1，而原始 ResNet 中对应的这个 conv 由于作用在（conv4_x 输出的）14x14 feature maps 上，这个 conv 的 stride 为 2，这一点有所不同。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略，请阅读原文。</p>\n<h1 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h1><p>有关 mask 分支，这里详细说明一下处理过程。如图 4，mask 分支输出大小为 $(R,K,m,m)$，根据 bbox 回归得到预测 box 的坐标数据，数据块大小为 $(R,4)$，其中 R 为检测到的所有预测 box 的数量，K 为目标分类数量，mxm 为 mask 的空间大小。对于第 i 个 目标，$0 \\le i &lt; R$，记预测 box 位置为 $(x_1,y_1,x_2,y_2)$，对于第 k 个分类，记对应的 mask map 为 $M_i^k$，</p>\n<ol>\n<li><p>计算第 i 个 box 的宽高<br>$w=x_2-x_1, \\ h=y_2-y_1$</p>\n</li>\n<li><p>将 mask map resize 到 box 宽高的大小  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=cv2.resize(M_i_k, (w,h))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将 mask map 二值化，因为 mask 是 pixel-wise sigmoid 之后的值，介于 (0,1) 之间，所以需要二值化处理  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mask=np.array(mask&gt;<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将 binary mask 映射到原始输入 image 平面。记原始输入 image 的宽高为 (W,H)，于是得到分割 mask  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">im_mask=np.zero((H,W), dtype=np.uint8)</span><br><span class=\"line\">im_mask[y1:y2,x1:x2]=mask</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n"},{"title":"libra-rcnn","date":"2019-07-03T12:07:44.000Z","mathjax":true,"_content":"论文 [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n\n# Introduction\n当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：\n1. 所选 region 是否具有代表性\n2. 抽取的特征是否被充分利用\n3. 设计的目标函数是否最优\n\n如图 1，很多训练过程均存在以上三点问题，\n![](/images/libra-rcnn_fig1.png) <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center>\n\n由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：\n\n## 样本不平衡\n为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。\n\n## 特征不平衡\n深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，\n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n从上图可知，融合后的 a' 特征，其来自于 b,c 层的特征信息不是均衡的。\n\n## 优化目标不平衡\n目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。\n\n我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：\n1. IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例\n2. 均衡的 feature pyramid，使用 __相同深度__ 合并到一起的均衡语义特征进行强化\n3. 均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者\n\n# Methodology\nLibra R-CNN 结构如图 2，\n![](/images/libra-rcnn_fig2.png)\n\n其中所有组件的详细介绍如下。\n\n## IoU-balanced Sampling\n首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，\n![](/images/libra-rcnn_fig3.png)\n\n我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。\n\n受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为\n$$p=\\frac N M$$\n\n为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为\n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。\n\nIoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。\n\n### SOURCE CODE\n经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：\n1. 获取所有 proposals 的最大 IoU，记为 `max_overlaps`\n2. 获取所有 proposals 的最大 IoU 的最大值，`max_iou=max_overlaps.max()`\n3. 设置阈值下限 `floor_thr`，对 `(floor_thr, max_iou)` 范围内的 proposals 进行 IoU balanced sampling\n4. 设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 `(max_iou-floor_thr)/K`\n5. 对于第 k 个桶，计算对应的 IoU 范围，记为 `[sk,ek)`\n6. 获取第 k 个桶内的 proposals 的 index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7. 获取第 k 个桶内的负例的 index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set 为 floor_thr<iou<0.5  proposals 的 index\n   ```\n8. 从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例\n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling 正例采样过程：\n1. 获取正例 proposals 所对应的 gt 的 index，记为 `gt_inds`\n2. 将第 1 步的结果去重，`unique_gt_inds=gt_inds.unique()`，得到所有 gt 的 index\n   \n   为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中\n3. 所有 gt 的数量为 `num_gts=len(unique_gt_indx)`，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 `num_per_gt=N/num_gts` 个正例\n4. 对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 `num_per_gt` 个正例\n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码\n\n## Balanced Feature Pyramid\n使用 __相同深度__ 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。\n![](/images/libra-rcnn_fig4.png)\n\n### 获取均衡语义特征\n记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 $\\{C_2,C_3,C_4,C_5\\}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，\n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。\n\n### 精修均衡语义特征\n进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$ 可用于目标检测，检测网络结构与 FPN 的一致。\n\n## Balanced L1 Loss\n遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，\n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。\n\n由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,\n![](/images/libra-rcnn_fig5.png)<center>横坐标 regression error 为 |x|，参见下文中的说明</center>\n\n均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，\n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n相关的梯度满足，\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\n上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，\n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n其中，\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n于是 smooth L1 损失对应的梯度为，\n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|<1  的梯度（因为 |x|<1 表示样本损失较小），首先对于 smooth L1 损失在 |x|<1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 __上凹__ 的，满足这些特性的一组曲线其函数为，\n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，\n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n根据损失在 |x|=1 处连续，得到\n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有\n$$\\alpha \\ln (b+1)=\\gamma$$\n解得，\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n损失函数曲线如图 5(b) 所示。\n\n# Experiments\n实验部分略\n\n# Conclusion\n提出了 Libra R-CNN，包含三点：\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","source":"_posts/libra-rcnn.md","raw":"---\ntitle: libra-rcnn\ndate: 2019-07-03 20:07:44\ntags: object detection\nmathjax: true\n---\n论文 [Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)\n\n# Introduction\n当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：\n1. 所选 region 是否具有代表性\n2. 抽取的特征是否被充分利用\n3. 设计的目标函数是否最优\n\n如图 1，很多训练过程均存在以上三点问题，\n![](/images/libra-rcnn_fig1.png) <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center>\n\n由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：\n\n## 样本不平衡\n为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。\n\n## 特征不平衡\n深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，\n![](/images/libra-rcnn_figa.png)<center>FPN</center>\n\n从上图可知，融合后的 a' 特征，其来自于 b,c 层的特征信息不是均衡的。\n\n## 优化目标不平衡\n目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。\n\n我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：\n1. IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例\n2. 均衡的 feature pyramid，使用 __相同深度__ 合并到一起的均衡语义特征进行强化\n3. 均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者\n\n# Methodology\nLibra R-CNN 结构如图 2，\n![](/images/libra-rcnn_fig2.png)\n\n其中所有组件的详细介绍如下。\n\n## IoU-balanced Sampling\n首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，\n![](/images/libra-rcnn_fig3.png)\n\n我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。\n\n受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为\n$$p=\\frac N M$$\n\n为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为\n$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$\n其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。\n\nIoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。\n\n### SOURCE CODE\n经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：\n1. 获取所有 proposals 的最大 IoU，记为 `max_overlaps`\n2. 获取所有 proposals 的最大 IoU 的最大值，`max_iou=max_overlaps.max()`\n3. 设置阈值下限 `floor_thr`，对 `(floor_thr, max_iou)` 范围内的 proposals 进行 IoU balanced sampling\n4. 设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 `(max_iou-floor_thr)/K`\n5. 对于第 k 个桶，计算对应的 IoU 范围，记为 `[sk,ek)`\n6. 获取第 k 个桶内的 proposals 的 index\n   ```python\n   tmp_set = np.where(np.logical_and(max_overlaps>=sk, max_overlaps<ek))[0]\n   ```\n7. 获取第 k 个桶内的负例的 index\n   ```python\n   tmp_inds = list(tmp_set & full_set) # full_set 为 floor_thr<iou<0.5  proposals 的 index\n   ```\n8. 从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例\n   ```python\n   random_choice(tmp_inds, N/K)\n   ```\nIoU balanced sampling 正例采样过程：\n1. 获取正例 proposals 所对应的 gt 的 index，记为 `gt_inds`\n2. 将第 1 步的结果去重，`unique_gt_inds=gt_inds.unique()`，得到所有 gt 的 index\n   \n   为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中\n3. 所有 gt 的数量为 `num_gts=len(unique_gt_indx)`，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 `num_per_gt=N/num_gts` 个正例\n4. 对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 `num_per_gt` 个正例\n   ```python\n   inds = torch.nonzero(assign_result.gt_inds == i.item())\n   inds = random_choice(inds, num_per_gt)\n   ```\n以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码\n\n## Balanced Feature Pyramid\n使用 __相同深度__ 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。\n![](/images/libra-rcnn_fig4.png)\n\n### 获取均衡语义特征\n记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 $\\{C_2,C_3,C_4,C_5\\}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，\n$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$\n\n这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。\n\n### 精修均衡语义特征\n进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。\n\nBalanced feature pyramid $\\{P_2,P_3,P_4,P_5\\}$ 可用于目标检测，检测网络结构与 FPN 的一致。\n\n## Balanced L1 Loss\n遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，\n$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$\n上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。\n\n由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,\n![](/images/libra-rcnn_fig5.png)<center>横坐标 regression error 为 |x|，参见下文中的说明</center>\n\n均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，\n$$L_{loc}=\\sum_{i \\in \\{x,y,w,h\\}} L_b (t_i^u-v_i)$$\n相关的梯度满足，\n$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$\n上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，\n$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$\n其中，\n$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 & |x|<1\n\\\\\\\\ |x|-0.5 & otherwise \\end{cases}$$\n于是 smooth L1 损失对应的梯度为，\n$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| & |x|<1\n\\\\\\\\ 1 & |x| \\ge 1 \\end{cases}$$\n我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|<1  的梯度（因为 |x|<1 表示样本损失较小），首先对于 smooth L1 损失在 |x|<1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 __上凹__ 的，满足这些特性的一组曲线其函数为，\n$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) & |x|<1\n\\\\\\\\ \\gamma & otherwise \\end{cases}$$\n其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，\n$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| & |x| < 1\n\\\\\\\\ \\gamma |x| + C & otherwise \\end{cases}$$\n根据损失在 |x|=1 处连续，得到\n$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$\n由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有\n$$\\alpha \\ln (b+1)=\\gamma$$\n解得，\n$$b=e^{\\gamma / \\alpha} -1\n\\\\\\\\ C=\\gamma/b-\\alpha$$\n\n损失函数曲线如图 5(b) 所示。\n\n# Experiments\n实验部分略\n\n# Conclusion\n提出了 Libra R-CNN，包含三点：\n1. IoU balanced sampling\n2. balanced feature pyramid\n3. balanced L1 loss\n\n# Reference\n1. Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava\n2. Non-local neural networks. Xiaolong Wang","slug":"libra-rcnn","published":1,"updated":"2019-07-05T12:35:22.631Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5ptv0011xgvc3ir0fgvx","content":"<p>论文 <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：</p>\n<ol>\n<li>所选 region 是否具有代表性</li>\n<li>抽取的特征是否被充分利用</li>\n<li>设计的目标函数是否最优</li>\n</ol>\n<p>如图 1，很多训练过程均存在以上三点问题，<br><img src=\"/images/libra-rcnn_fig1.png\" alt> <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center></p>\n<p>由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：</p>\n<h2 id=\"样本不平衡\"><a href=\"#样本不平衡\" class=\"headerlink\" title=\"样本不平衡\"></a>样本不平衡</h2><p>为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。</p>\n<h2 id=\"特征不平衡\"><a href=\"#特征不平衡\" class=\"headerlink\" title=\"特征不平衡\"></a>特征不平衡</h2><p>深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，<br><img src=\"/images/libra-rcnn_figa.png\" alt><center>FPN</center></p>\n<p>从上图可知，融合后的 a’ 特征，其来自于 b,c 层的特征信息不是均衡的。</p>\n<h2 id=\"优化目标不平衡\"><a href=\"#优化目标不平衡\" class=\"headerlink\" title=\"优化目标不平衡\"></a>优化目标不平衡</h2><p>目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。</p>\n<p>我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：</p>\n<ol>\n<li>IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例</li>\n<li>均衡的 feature pyramid，使用 <strong>相同深度</strong> 合并到一起的均衡语义特征进行强化</li>\n<li>均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者</li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN 结构如图 2，<br><img src=\"/images/libra-rcnn_fig2.png\" alt></p>\n<p>其中所有组件的详细介绍如下。</p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p>首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，<br><img src=\"/images/libra-rcnn_fig3.png\" alt></p>\n<p>我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。</p>\n<p>受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为<br>$$p=\\frac N M$$</p>\n<p>为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为<br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。</p>\n<p>IoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。</p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p>经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：</p>\n<ol>\n<li><p>获取所有 proposals 的最大 IoU，记为 <code>max_overlaps</code></p>\n</li>\n<li><p>获取所有 proposals 的最大 IoU 的最大值，<code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p>设置阈值下限 <code>floor_thr</code>，对 <code>(floor_thr, max_iou)</code> 范围内的 proposals 进行 IoU balanced sampling</p>\n</li>\n<li><p>设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p>对于第 k 个桶，计算对应的 IoU 范围，记为 <code>[sk,ek)</code></p>\n</li>\n<li><p>获取第 k 个桶内的 proposals 的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>获取第 k 个桶内的负例的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set 为 floor_thr&lt;iou&lt;0.5  proposals 的 index</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>IoU balanced sampling 正例采样过程：</p>\n<ol>\n<li><p>获取正例 proposals 所对应的 gt 的 index，记为 <code>gt_inds</code></p>\n</li>\n<li><p>将第 1 步的结果去重，<code>unique_gt_inds=gt_inds.unique()</code>，得到所有 gt 的 index</p>\n<p>为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中</p>\n</li>\n<li><p>所有 gt 的数量为 <code>num_gts=len(unique_gt_indx)</code>，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 <code>num_per_gt=N/num_gts</code> 个正例</p>\n</li>\n<li><p>对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 <code>num_per_gt</code> 个正例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码</p>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p>使用 <strong>相同深度</strong> 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。<br><img src=\"/images/libra-rcnn_fig4.png\" alt></p>\n<h3 id=\"获取均衡语义特征\"><a href=\"#获取均衡语义特征\" class=\"headerlink\" title=\"获取均衡语义特征\"></a>获取均衡语义特征</h3><p>记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 ${C_2,C_3,C_4,C_5}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，<br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p>这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。</p>\n<h3 id=\"精修均衡语义特征\"><a href=\"#精修均衡语义特征\" class=\"headerlink\" title=\"精修均衡语义特征\"></a>精修均衡语义特征</h3><p>进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$ 可用于目标检测，检测网络结构与 FPN 的一致。</p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p>遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，<br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br>上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。</p>\n<p>由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,<br><img src=\"/images/libra-rcnn_fig5.png\" alt><center>横坐标 regression error 为 |x|，参见下文中的说明</center></p>\n<p>均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，<br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br>相关的梯度满足，<br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，<br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br>其中，<br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br>于是 smooth L1 损失对应的梯度为，<br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br>我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|&lt;1  的梯度（因为 |x|&lt;1 表示样本损失较小），首先对于 smooth L1 损失在 |x|&lt;1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 <strong>上凹</strong> 的，满足这些特性的一组曲线其函数为，<br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br>其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，<br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br>根据损失在 |x|=1 处连续，得到<br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br>由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有<br>$$\\alpha \\ln (b+1)=\\gamma$$<br>解得，<br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p>损失函数曲线如图 5(b) 所示。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出了 Libra R-CNN，包含三点：</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1904.02701\" target=\"_blank\" rel=\"noopener\">Libra R-CNN: Towards Balanced Learning for Object Detection</a></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>当前大多数目标检测器无论是 one-stage 还是 two-stage，其训练范式都是 image 上区域选择（以下使用原英文单词 Region 表示），从 region 抽取特征，以及联合目标分类和定位的多任务目标函数优化。基于此训练范式，有如下三点关于训练是否成功：</p>\n<ol>\n<li>所选 region 是否具有代表性</li>\n<li>抽取的特征是否被充分利用</li>\n<li>设计的目标函数是否最优</li>\n</ol>\n<p>如图 1，很多训练过程均存在以上三点问题，<br><img src=\"/images/libra-rcnn_fig1.png\" alt> <center> 不平衡包括 a. 样本级别；b. 特征级别；c. 目标函数级别 </center></p>\n<p>由于以上三个不平衡问题的存在，即使一个设计良好的模型，也可能最终训练出来的性能不佳。以下我们具体讨论这三个不平衡问题：</p>\n<h2 id=\"样本不平衡\"><a href=\"#样本不平衡\" class=\"headerlink\" title=\"样本不平衡\"></a>样本不平衡</h2><p>为了避免模型倾向于预测为负，很多训练过程设置了一个训练批次中正负样本的比例（如 1:3）。训练目标检测器时，困难样例更具有价值，可以加快训练收敛速度，有效提高检测性能，然而事实上随机选择的 region 主要都是简单负样例，这些简单负样例贡献不了什么有用的特征，于是有了在线难负例发掘方法 OHEM [1]，这个 OHEM 对于噪声标签非常敏感，因为噪声标签会使得误分类为负也就是难负例筛选不准确，此外 OHEM 显然提高了内存占用并增加了计算量。通过进一步降低分类正确的那部分损失，Focal loss 也可以缓和样本不平衡这个问题，但是 Focal loss 通常用在 one-stage 模型中，在 two-stage 模型中则作用不明显，因为大部分的简单负例在 first stage 已经被过滤掉了（正如前面所说的正负样本比例为 1:3），此时若再使用 Focal loss 则会使得正负例样本所产生的梯度不平衡，较小的梯度淹没在较大的梯度里，难以起到梯度优化指导作用。</p>\n<h2 id=\"特征不平衡\"><a href=\"#特征不平衡\" class=\"headerlink\" title=\"特征不平衡\"></a>特征不平衡</h2><p>深度高层的特征具有更丰富的语义信息，而浅层特征则保留了更多的视觉内容描述（局部细节信息）。近年来，FPN 和 PANet 则通过 top-down 结构和 横向连接来进行特征整合，提高了目标检测性能，这说明高底层的特征对于目标检测的作用确实是互补的。但是，如何最佳地整合特征？前面提高的特征整合方法，feature pyramid 中每一层的特征整合更多的是关注邻近的特征（直接相连），而很少关注其他非邻近特征（非直接相连），非邻近特征需要经过一个或多个中间层才能到达本层特征，显然非近邻特征的语义信息被稀释的非常淡。如下图所示，<br><img src=\"/images/libra-rcnn_figa.png\" alt><center>FPN</center></p>\n<p>从上图可知，融合后的 a’ 特征，其来自于 b,c 层的特征信息不是均衡的。</p>\n<h2 id=\"优化目标不平衡\"><a href=\"#优化目标不平衡\" class=\"headerlink\" title=\"优化目标不平衡\"></a>优化目标不平衡</h2><p>目标检测器是多任务的：分类和定位。两者的目标函数加起来作为最终的优化目标，如果这两者之间不均衡，会导致次优的结果，较小的梯度会淹没在较大的梯度里，起不到优化指导作用。这个情况与训练中样本导致的梯度不平衡的情况是相同的，均会限制模型进一步的性能调优。</p>\n<p>我们提出 Libra R-CNN（天秤 R-CNN），以平衡以上三个问题，Libra R-CNN 框架包含三个创新组件：</p>\n<ol>\n<li>IoU 均衡采样，根据与 gt box 的 IoU 来挖掘难例</li>\n<li>均衡的 feature pyramid，使用 <strong>相同深度</strong> 合并到一起的均衡语义特征进行强化</li>\n<li>均衡的 L1 loss，提升关键的梯度从而平衡 1)分类 2) 大致定位 3) 准确定位 这三者</li>\n</ol>\n<h1 id=\"Methodology\"><a href=\"#Methodology\" class=\"headerlink\" title=\"Methodology\"></a>Methodology</h1><p>Libra R-CNN 结构如图 2，<br><img src=\"/images/libra-rcnn_fig2.png\" alt></p>\n<p>其中所有组件的详细介绍如下。</p>\n<h2 id=\"IoU-balanced-Sampling\"><a href=\"#IoU-balanced-Sampling\" class=\"headerlink\" title=\"IoU-balanced Sampling\"></a>IoU-balanced Sampling</h2><p>首先一个基本问题是：训练样本 region 和其对应的 gt box 之间的重合与此样本的难易程度是否有关联？图 3 显示了三种 region 采样的 IoU 分布，<br><img src=\"/images/libra-rcnn_fig3.png\" alt></p>\n<p>我们仅考虑难负例，因为难负例是上文我们分析的关键三点之一。从图 3 中可见超过 60% 的难负例有大于 0.05 的 IoU（因为图 3 中橙色部分 IoU 低于 0.05 的占比大约为 37%），而随机采样时仅仅有大约 30% 的训练样本其 IoU 大于 0.05，这意味着如果随机采样会得到很多 IoU 位于 [0,0.05) 区间的样本，而分布在这个区间的难负例样本较少，所以随机采样会得到很多简单样本。</p>\n<p>受以上结论启发，我们提出 IoU-balanced 采样：既然难负例分布在各个 IoU 区间，那么我们就对各个 IoU 区间分别采样。假设从 M 个候选中选出 N 个负样本，随机采样下每个样本被选择的概率为<br>$$p=\\frac N M$$</p>\n<p>为了提高选择难负例的概率，根据 IoU 将采样区间等分成 K 个桶，从每个桶中选择 N/K 个负样本，于是在 IoU-balanced sampling 下，第 k 个桶中每个样本被选择的概率为<br>$$p_k=\\frac N K \\cdot \\frac 1 {M_k}, \\ k \\in [0,K)$$<br>其中，$M_k$ 是第 k 个桶中的样例候选数量。实验中 K=3。</p>\n<p>IoU-balanced sampling 结果如图 3，可以看到使用这种采样方式得到的训练样本分布与难负例的分布非常接近。采样候选数量不足，这种采样方法难以扩展到正例采样，为了得到均衡采样过程，使用一种替换方案：对每个 gt box 我们进行数量相等的采样。</p>\n<h3 id=\"SOURCE-CODE\"><a href=\"#SOURCE-CODE\" class=\"headerlink\" title=\"SOURCE CODE\"></a>SOURCE CODE</h3><p>经过阅读源码，本人总结 IoU balanced sampling 负例采样过程为：</p>\n<ol>\n<li><p>获取所有 proposals 的最大 IoU，记为 <code>max_overlaps</code></p>\n</li>\n<li><p>获取所有 proposals 的最大 IoU 的最大值，<code>max_iou=max_overlaps.max()</code></p>\n</li>\n<li><p>设置阈值下限 <code>floor_thr</code>，对 <code>(floor_thr, max_iou)</code> 范围内的 proposals 进行 IoU balanced sampling</p>\n</li>\n<li><p>设置桶（bin）数量 K，假设所需要的负例数量为 N，对每个桶采样数量为 N/K，每个桶的 IoU 范围跨度为 <code>(max_iou-floor_thr)/K</code></p>\n</li>\n<li><p>对于第 k 个桶，计算对应的 IoU 范围，记为 <code>[sk,ek)</code></p>\n</li>\n<li><p>获取第 k 个桶内的 proposals 的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_set = np.where(np.logical_and(max_overlaps&gt;=sk, max_overlaps&lt;ek))[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>获取第 k 个桶内的负例的 index</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmp_inds = list(tmp_set &amp; full_set) <span class=\"comment\"># full_set 为 floor_thr&lt;iou&lt;0.5  proposals 的 index</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>从第 7 步中得到的第 k 个桶中所有负例的 index，在随机抽取 N/K 个负例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random_choice(tmp_inds, N/K)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>IoU balanced sampling 正例采样过程：</p>\n<ol>\n<li><p>获取正例 proposals 所对应的 gt 的 index，记为 <code>gt_inds</code></p>\n</li>\n<li><p>将第 1 步的结果去重，<code>unique_gt_inds=gt_inds.unique()</code>，得到所有 gt 的 index</p>\n<p>为什么说是所有 gt 呢？因为所有 gt 均作为正例被添加到正例 proposals 中</p>\n</li>\n<li><p>所有 gt 的数量为 <code>num_gts=len(unique_gt_indx)</code>，假设总共要采样 N 个正例，以每个 gt 为中心，均需要采样 <code>num_per_gt=N/num_gts</code> 个正例</p>\n</li>\n<li><p>对于第 i 个 gt，获取与其匹配的所有正例，并从中随机选择 <code>num_per_gt</code> 个正例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inds = torch.nonzero(assign_result.gt_inds == i.item())</span><br><span class=\"line\">inds = random_choice(inds, num_per_gt)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>以上采样过程均经过简化，如需彻底理解细节问题则直接阅读源码</p>\n<h2 id=\"Balanced-Feature-Pyramid\"><a href=\"#Balanced-Feature-Pyramid\" class=\"headerlink\" title=\"Balanced Feature Pyramid\"></a>Balanced Feature Pyramid</h2><p>使用 <strong>相同深度</strong> 合并到一起的均衡语义特征来加强 multi-level features，如图 4，特征经过四个步骤：尺度缩放、整合、精修和加强。<br><img src=\"/images/libra-rcnn_fig4.png\" alt></p>\n<h3 id=\"获取均衡语义特征\"><a href=\"#获取均衡语义特征\" class=\"headerlink\" title=\"获取均衡语义特征\"></a>获取均衡语义特征</h3><p>记 l 层级的特征为 $C_l$，层级数量为 L。最低层和最高层分别记为 $l_{min}, l_{max}$。如图 4，将所有层级的特征 ${C_2,C_3,C_4,C_5}$ resize 到一个中间大小即 $C_4$ 的大小，resize 操作使用插值或者最大值池化实现，所有特征经过尺度缩放后，均衡语义特征可由下式获得，<br>$$C=\\frac 1 L \\sum_{l_{min}}^{l_{max}} C_l$$</p>\n<p>这个均衡语义特征可经过相反的 rescale 操作来加强各自原始特征（图 4 中的 Identity）。</p>\n<h3 id=\"精修均衡语义特征\"><a href=\"#精修均衡语义特征\" class=\"headerlink\" title=\"精修均衡语义特征\"></a>精修均衡语义特征</h3><p>进一步精修均衡语义特征使其更具判别力。使用嵌入的高斯 non-local 注意力机制[2]精修均衡语义特征。</p>\n<p>Balanced feature pyramid ${P_2,P_3,P_4,P_5}$ 可用于目标检测，检测网络结构与 FPN 的一致。</p>\n<h2 id=\"Balanced-L1-Loss\"><a href=\"#Balanced-L1-Loss\" class=\"headerlink\" title=\"Balanced L1 Loss\"></a>Balanced L1 Loss</h2><p>遵循 Fast R-CNN 中的分类和目标定位的损失，定义如下，<br>$$L_{p,u,t^u,v}=L_{cls}(p,u) + \\lambda [u\\ge 1] L_{loc}(t^u,v)$$<br>上式为单个样本的损失，其中，预测和 target 分别记为 p 和 u。t<sup>u</sup> 表示回归预测，v 表示回归 target。$\\lambda$ 为平衡系数。我们称损失大于等于 1.0 的样本为外点 outliers，损失小于 1.0 的样本为内点 inliers。</p>\n<p>由于回归 target 值是无界的，如果直接增大 $\\lambda$ 会使得模型对 outliers 更为敏感，outliers 可以视作困难样本（困难样本可以认为是误差较大的样本），由于较大的损失使得梯度也较大，这对训练过程是不利的。Inliers 可以看作简单样本，与 outliers 相比，其梯度贡献较小，具体而言，inliers 平均每个样本仅贡献了 30% 的梯度，基于这些考虑，我们提出均衡 L1 损失，从传统的 smooth L1 损失演变而来，记为 $L_b$。设置一个拐点分离 inliers 和 outliers，并使用最大值 1.0 来剃平 outliers 的较大梯度，如图 5(a) 所示,<br><img src=\"/images/libra-rcnn_fig5.png\" alt><center>横坐标 regression error 为 |x|，参见下文中的说明</center></p>\n<p>均衡 L1 损失的核心思想是提升关键的回归梯度，也就是来自 inliers 的梯度，使得所有样本的所有任务的梯度达到平衡。使用均衡 L1 损失的定位损失为，<br>$$L_{loc}=\\sum_{i \\in {x,y,w,h}} L_b (t_i^u-v_i)$$<br>相关的梯度满足，<br>$$\\frac {\\partial L_{loc}} {\\partial w} \\propto \\frac {\\partial L_b} {\\partial t_i^u} \\propto \\frac {L_b} x$$<br>上式中，w 表示网络权重参数（我是这么认为的），x 表示 $t_i^u - v_i$，因为 smooth L1 损失就是这么表示的，回顾一下 smooth L1 损失，其定义如下，<br>$$L_{loc}(t^u, v) = \\sum_{x,y,w,h} smooth_{L_1} (t_i^u-v_i)$$<br>其中，<br>$$smooth_{L_1}(x)=\\begin{cases} 0.5 x^2 &amp; |x|&lt;1<br>\\\\ |x|-0.5 &amp; otherwise \\end{cases}$$<br>于是 smooth L1 损失对应的梯度为，<br>$$\\frac {\\partial L_1} {\\partial |x|} = \\begin{cases} |x| &amp; |x|&lt;1<br>\\\\ 1 &amp; |x| \\ge 1 \\end{cases}$$<br>我们将 |x| 看作是回归误差（regression error），显然误差总是非负的。现在，我们要想提升 inliers 的梯度，也就是 |x|&lt;1  的梯度（因为 |x|&lt;1 表示样本损失较小），首先对于 smooth L1 损失在 |x|&lt;1 范围内的梯度为 $\\nabla_{|x|} L = |x|$ 也就是一条经过 (0,0) 和 (1,1) 的线段，要提高这个范围内的梯度，很自然的想法是位于直线 y=x 上方的曲线，当然曲线必须要经过原点(0,0)，表示预测与 target 相等即误差为零时损失也为零，为了与 $|x| \\ge 1$ 的梯度保持连续，梯度曲线仍然经过 (1,1) 点，同时还要保持单调递增，这说明曲线是 <strong>上凹</strong> 的，满足这些特性的一组曲线其函数为，<br>$$\\frac {\\partial L_b} {\\partial x} = \\begin{cases} \\alpha \\ln (b|x|+1) &amp; |x|&lt;1<br>\\\\ \\gamma &amp; otherwise \\end{cases}$$<br>其中 $\\alpha$ 越小，对 inliers 的梯度提升越大，$\\gamma$ 控制 outliers 的梯度，或者说整个梯度的上限，$\\gamma$ 参数用于平衡回归损失和分类损失，平衡后的梯度曲线如图 5(a)所示。参数 b 则用于确保损失在 |x|=1 处连续，对梯度积分得到损失函数为，<br>$$L_b(x)=\\begin{cases} \\frac \\alpha b (b|x|+1) \\ln (b|x|+1) - \\alpha |x| &amp; |x| &lt; 1<br>\\\\ \\gamma |x| + C &amp; otherwise \\end{cases}$$<br>根据损失在 |x|=1 处连续，得到<br>$$\\frac \\alpha b (b+1) \\ln (b+1) - \\alpha=(\\alpha + \\frac \\alpha b) \\ln(b+1) -\\alpha = \\gamma + C$$<br>由于 C 可以是任意常数，所以可令 $C=\\frac \\alpha b \\ln(b+1) -\\alpha$，于是有<br>$$\\alpha \\ln (b+1)=\\gamma$$<br>解得，<br>$$b=e^{\\gamma / \\alpha} -1<br>\\\\ C=\\gamma/b-\\alpha$$</p>\n<p>损失函数曲线如图 5(b) 所示。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验部分略</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>提出了 Libra R-CNN，包含三点：</p>\n<ol>\n<li>IoU balanced sampling</li>\n<li>balanced feature pyramid</li>\n<li>balanced L1 loss</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li>Training Region-based Object Detectors with Online Hard Example Mining. Abhinav Shrivastava</li>\n<li>Non-local neural networks. Xiaolong Wang</li>\n</ol>\n"},{"title":"GIoU","date":"2019-06-13T07:00:48.000Z","mathjax":true,"_content":"论文 [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n# 摘要\nIoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。\n# 简介\n在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。\n\nIoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。\n\n然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\n预测box（黑矩形）和gt box（绿矩形）均由左上角和右下角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。\n\n直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。\n\n为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。\n\n本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。\n\n我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。\n\n主要贡献如下：\n\n- 介绍了GIoU，作为比较两个任意形状差距的指标\n- 以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解\n- 将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升\n\n# 相关工作\n__目标检测准确性测量：__ IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。\n\n__bbox表示和损失：__ 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：\n1. YOLO v1\n   \n   YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。\n2. R-CNN\n   \n   R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。\n3. Fast R-CNN\n   \n   Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。\n4. Faster R-CNN\n   \n   Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。\n\n大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 \n\n__使用近似或替代函数优化IoU：__ 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。\n\n# 泛化IoU\n用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：\n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：\n- IoU作为距离同时也作为评估指标。\n  \n  IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可分的同一性，对称性和三角不等式\n- IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关\n   \n但是，IoU的主要问题是：\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远\n\n为了解决这个问题，我们提出了泛化版IoU，即 GIoU。\n\n两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）\n\n整个计算过程总结如下算法1：\n___\n算法1：GIoU\n___\n输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$\n\n输出： GIoU\n- 在 S 空间中寻找包含 A B 的最小凸形 C\n- 计算 IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n- 计算 GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\n作为新的指标，GIoU 具有性质：\n\n- 与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可分的同一性，对称性和三角不等式\n  \n  IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。\n- 与 IoU 类似，GIoU 具有尺度不变性。\n- GIoU 上限为 IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。\n- IoU 和 GIoU 的值域\n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。\n\n  我们看下如何获得边界值：\n   * 与 IoU 相同，只有当 A B 完成重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$\n   * 当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}} GIoU(A,B)=-1$\n\n综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。\n\n## GIoU用作BBox回归损失\n我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。\n\n好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} < x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}<x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}<x_A^{br}$。\n\n反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。\n_________\n算法2：IoU和GIoU用作BBox回归损失\n_________\n输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1. 因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，\n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$，故进行如下转换：\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2. 计算 GT box 面积：\n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3. 计算预测 box 面积：\n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. 计算交：\n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5. 计算最小包含凸形 c：\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6. 计算 c 的面积：\n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7. 计算 IoU：\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$\n8. 计算 GIoU：\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9. 计算 GIoU 损失：\n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\n根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，\n\n![](/images/GIoU_fig2.png)\n\n图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。\n\n### 损失稳定性\n我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。\n\n假设 GT box 是矩形，且面积大于0即，$A^g > 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U > 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$\n\n检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U > 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} <1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 < GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。\n\n### IoU=0时$\\mathcal L_{GIoU}$的行为\nGIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。\n\n# 实验结果\n引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）\n\n__数据集__ 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。\n\n__评估方案__ 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU=\\{.5,.55,...,.95\\}$，计算这些 IoU 阈值下 mAP 值的平均，记为 __AP__，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU=\\{.5,.55,...,.95\\}$，计算这些阈值下 mAP 的平均，__AP__，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 __AP75__。\n\n## YOLO v3\n__训练方案__ 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。\n\n## Faster R-CNN 和 Mask R-CNN\n__训练方案__ 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。\n\n# 结论\n介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。\n\n我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。\n\n# 后记\n这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。","source":"_posts/GIoU.md","raw":"---\ntitle: GIoU\ndate: 2019-06-13 15:00:48\ntags: object detection\nmathjax: true\n---\n论文 [Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression](https://arxiv.org/abs/1902.09630)\n# 摘要\nIoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。\n# 简介\n在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。\n\nIoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。\n\n然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，\n![](/images/GIoU_fig1.png)\n\n<!-- {% asset_img fig1.png This figure 1 %} -->\n\n预测box（黑矩形）和gt box（绿矩形）均由左上角和右下角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。\n\n直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。\n\n为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。\n\n本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。\n\n我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。\n\n主要贡献如下：\n\n- 介绍了GIoU，作为比较两个任意形状差距的指标\n- 以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解\n- 将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升\n\n# 相关工作\n__目标检测准确性测量：__ IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。\n\n__bbox表示和损失：__ 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：\n1. YOLO v1\n   \n   YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。\n2. R-CNN\n   \n   R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。\n3. Fast R-CNN\n   \n   Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。\n4. Faster R-CNN\n   \n   Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。\n\n大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 \n\n__使用近似或替代函数优化IoU：__ 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。\n\n# 泛化IoU\n用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：\n$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$\n两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：\n- IoU作为距离同时也作为评估指标。\n  \n  IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可分的同一性，对称性和三角不等式\n- IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关\n   \n但是，IoU的主要问题是：\n- $|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远\n\n为了解决这个问题，我们提出了泛化版IoU，即 GIoU。\n\n两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）\n\n整个计算过程总结如下算法1：\n___\n算法1：GIoU\n___\n输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$\n\n输出： GIoU\n- 在 S 空间中寻找包含 A B 的最小凸形 C\n- 计算 IoU\n  \n  $IoU=\\frac {|A \\cap B|} {|A \\cup B|}$\n- 计算 GIoU\n  \n  $GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$\n___\n\n作为新的指标，GIoU 具有性质：\n\n- 与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可分的同一性，对称性和三角不等式\n  \n  IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。\n- 与 IoU 类似，GIoU 具有尺度不变性。\n- GIoU 上限为 IoU\n  \n  $\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。\n- IoU 和 GIoU 的值域\n  \n  $\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。\n\n  我们看下如何获得边界值：\n   * 与 IoU 相同，只有当 A B 完成重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$\n   * 当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}} GIoU(A,B)=-1$\n\n综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。\n\n## GIoU用作BBox回归损失\n我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。\n\n好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} < x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}<x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}<x_A^{br}$。\n\n反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。\n_________\n算法2：IoU和GIoU用作BBox回归损失\n_________\n输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$\n\n输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$\n\n1. 因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，\n   \n   $x_2^p>x_1^p, \\ y_2^p>y_1^p$，故进行如下转换：\n   - $\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$\n   - $\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$\n2. 计算 GT box 面积：\n   \n   $A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$\n3. 计算预测 box 面积：\n   \n   $A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$\n4. 计算交：\n   \n   - $x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$\n   - $y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$\n   - $\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) & x_2^{\\mathcal I} > x_1^{\\mathcal I}, y_2^{\\mathcal I} > y_1^{\\mathcal I} \\\\ 0 & \\text{otherwise} \\end{cases}$\n5. 计算最小包含凸形 c：\n   \n   - $x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$\n   - $y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$\n6. 计算 c 的面积：\n   \n   $A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$\n7. 计算 IoU：\n   \n   $IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$\n8. 计算 GIoU：\n   \n   $GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$\n9. 计算 GIoU 损失：\n    \n   $\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$\n_________\n根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，\n\n![](/images/GIoU_fig2.png)\n\n图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。\n\n### 损失稳定性\n我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。\n\n假设 GT box 是矩形，且面积大于0即，$A^g > 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U > 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$\n\n检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U > 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} <1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 < GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。\n\n### IoU=0时$\\mathcal L_{GIoU}$的行为\nGIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。\n\n# 实验结果\n引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）\n\n__数据集__ 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。\n\n__评估方案__ 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU=\\{.5,.55,...,.95\\}$，计算这些 IoU 阈值下 mAP 值的平均，记为 __AP__，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU=\\{.5,.55,...,.95\\}$，计算这些阈值下 mAP 的平均，__AP__，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 __AP75__。\n\n## YOLO v3\n__训练方案__ 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。\n\n## Faster R-CNN 和 Mask R-CNN\n__训练方案__ 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。\n\n# 结论\n介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。\n\n我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。\n\n# 后记\n这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。","slug":"GIoU","published":1,"updated":"2019-06-25T09:10:09.665Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5pz10014xgvc96likv8m","content":"<p>论文 <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>IoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。</p>\n<p>IoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。</p>\n<p>然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，<br><img src=\"/images/GIoU_fig1.png\" alt></p>\n<!--  -->\n\n<p>预测box（黑矩形）和gt box（绿矩形）均由左上角和右下角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。</p>\n<p>直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。</p>\n<p>为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。</p>\n<p>本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。</p>\n<p>我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。</p>\n<p>主要贡献如下：</p>\n<ul>\n<li>介绍了GIoU，作为比较两个任意形状差距的指标</li>\n<li>以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解</li>\n<li>将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p><strong>目标检测准确性测量：</strong> IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。</p>\n<p><strong>bbox表示和损失：</strong> 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。</p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。</p>\n</li>\n</ol>\n<p>大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 </p>\n<p><strong>使用近似或替代函数优化IoU：</strong> 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。</p>\n<h1 id=\"泛化IoU\"><a href=\"#泛化IoU\" class=\"headerlink\" title=\"泛化IoU\"></a>泛化IoU</h1><p>用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：<br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：</p>\n<ul>\n<li><p>IoU作为距离同时也作为评估指标。</p>\n<p>IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可分的同一性，对称性和三角不等式</p>\n</li>\n<li><p>IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关</p>\n</li>\n</ul>\n<p>但是，IoU的主要问题是：</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远</li>\n</ul>\n<p>为了解决这个问题，我们提出了泛化版IoU，即 GIoU。</p>\n<p>两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）</p>\n<p>整个计算过程总结如下算法1：</p>\n<hr>\n<p>算法1：GIoU</p>\n<hr>\n<p>输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p>输出： GIoU</p>\n<ul>\n<li><p>在 S 空间中寻找包含 A B 的最小凸形 C</p>\n</li>\n<li><p>计算 IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p>计算 GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>作为新的指标，GIoU 具有性质：</p>\n<ul>\n<li><p>与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可分的同一性，对称性和三角不等式</p>\n<p>IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。</p>\n</li>\n<li><p>与 IoU 类似，GIoU 具有尺度不变性。</p>\n</li>\n<li><p>GIoU 上限为 IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。</p>\n</li>\n<li><p>IoU 和 GIoU 的值域</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。</p>\n<p>我们看下如何获得边界值：</p>\n<ul>\n<li>与 IoU 相同，只有当 A B 完成重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$</li>\n<li>当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。</p>\n<h2 id=\"GIoU用作BBox回归损失\"><a href=\"#GIoU用作BBox回归损失\" class=\"headerlink\" title=\"GIoU用作BBox回归损失\"></a>GIoU用作BBox回归损失</h2><p>我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。</p>\n<p>好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} &lt; x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$。</p>\n<p>反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。</p>\n<hr>\n<p>算法2：IoU和GIoU用作BBox回归损失</p>\n<hr>\n<p>输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，</p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$，故进行如下转换：</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p>计算 GT box 面积：</p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p>计算预测 box 面积：</p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p>计算交：</p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p>计算最小包含凸形 c：</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p>计算 c 的面积：</p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p>计算 IoU：</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p>计算 GIoU：</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p>计算 GIoU 损失：</p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，</p>\n<p><img src=\"/images/GIoU_fig2.png\" alt></p>\n<p>图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。</p>\n<h3 id=\"损失稳定性\"><a href=\"#损失稳定性\" class=\"headerlink\" title=\"损失稳定性\"></a>损失稳定性</h3><p>我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。</p>\n<p>假设 GT box 是矩形，且面积大于0即，$A^g &gt; 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U &gt; 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p>检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U &gt; 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} &lt;1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 &lt; GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。</p>\n<h3 id=\"IoU-0时-mathcal-L-GIoU-的行为\"><a href=\"#IoU-0时-mathcal-L-GIoU-的行为\" class=\"headerlink\" title=\"IoU=0时$\\mathcal L_{GIoU}$的行为\"></a>IoU=0时$\\mathcal L_{GIoU}$的行为</h3><p>GIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）</p>\n<p><strong>数据集</strong> 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。</p>\n<p><strong>评估方案</strong> 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU={.5,.55,…,.95}$，计算这些 IoU 阈值下 mAP 值的平均，记为 <strong>AP</strong>，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU={.5,.55,…,.95}$，计算这些阈值下 mAP 的平均，<strong>AP</strong>，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 <strong>AP75</strong>。</p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong>训练方案</strong> 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。</p>\n<h2 id=\"Faster-R-CNN-和-Mask-R-CNN\"><a href=\"#Faster-R-CNN-和-Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN 和 Mask R-CNN\"></a>Faster R-CNN 和 Mask R-CNN</h2><p><strong>训练方案</strong> 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。</p>\n<p>我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文 <a href=\"https://arxiv.org/abs/1902.09630\" target=\"_blank\" rel=\"noopener\">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>IoU 是目标检测benchmarks中使用最广的评估指标，然而，优化回归bbox参数的距离损失并不等价于最大化IoU指标。对于轴对齐的2D bbox，IoU 可直接用作回归损失，但是 IoU 无法优化无重叠的bbox，所以本文提出一种泛化版的Iou，名为 GIou。结合 GIoU 和 sota 目标检测框架，在流行的目标检测benchmarks例如 PASCAL VOC 和 MS COCO中，分别使用标准 IoU 和 GIoU 损失，我们发现性能一致得到提升。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>在很多2D/3D 计算机视觉任务中，bbox 回归是最基本的组成之一。例如目标定位，多目标检测，目标跟踪以及实例分割，均依赖准确的bbox回归。提升性能的主流趋势是使用深度神经网络，然而，还有一种提升方法被广泛忽略，那就是使用基于 IoU 的指标损失来代替传统的回归损失如 $l_1, l_2$ 等。</p>\n<p>IoU 将要比较的目标的形状属性例如 bbox 的宽、高和位置等信息编码成区域属性，并基于面积（体积）计算出一个归一化的值。语义分割、目标检测和跟踪等任务中的性能测量均采用IoU作为测量指标。</p>\n<p>然而，最小化损失如$l_n$范数与提高IoU并不是强相关。考虑一个2D场景，如图1(a)，<br><img src=\"/images/GIoU_fig1.png\" alt></p>\n<!--  -->\n\n<p>预测box（黑矩形）和gt box（绿矩形）均由左上角和右下角坐标表示$(x_1,y_1,x_2,y_2)$，为简单起见，我们令两个box的其中一个corner的距离（例如$l_2$范数）固定，于是，以 gt box 另一个 corner 为圆心，某半径长的圆，无论预测 box 的另一个 corner 的坐标，只要其位于这个圆上，其 $l_2$ 距离均保持不变；然而IoU却不同。这个问题可以延伸到其他损失和bbox表示上，例如图1(b)。</p>\n<p>直觉而言，这些类型的损失的一个较好的局部最优解可能并非 IoU 的局部最优解。而且，与 IoU 不同的是，$l_n$ 不具有尺度不变性，相同重叠程度的几对 bbox，其损失值各不相同。另外，一些 bbox 的表示方法，由于没有对不同类型的表示参数进行正则处理，使得这个问题更加严重。例如在 center+size 表示法中，$(x_c,y_c)$ 是中心坐标，$(w,h)$ 是 box size。当表示参数变多时，如旋转度，复杂度继续上升。</p>\n<p>为了解决这些问题，sota 检测器引入了 anchor 的概念，并使用了非线性表示方法简单地处理尺度问题（例如，faster-rcnn中坐标偏差的计算）。但即使使用了这些手工设计，优化回归损失和IoU值依然存在偏差。</p>\n<p>本文探索了轴对齐矩形之间的IoU计算，以及轴对齐超矩形（$ndim \\ge 2$）之间的IoU计算，此时IoU 有解析解，并且可反向传播，也就是说，IoU 可以直接用作目标函数进行优化，而优化Iou目标函数与优化某个损失函数之间，显然选择优化IoU目标函数，能与提高Iou指标强相关，但是这也导致一个问题：如果两个目标没有重叠，IoU则为0，无法知道两个目标距离有多远，IoU为0，其梯度也将为0，导致无法优化。</p>\n<p>我们将IoU这一概念延伸到无重叠情况下来解决上述问题。这种泛化：(a) 沿袭 IoU 能将被比较的目标的形状属性编码进区域属性；(b) 维持 IoU 的尺度不变性；(c) 在目标有重叠情况下与IoU强相关。这个泛化版的IoU，我们称为GIoU，将GIoU引入到 sota 目标检测框架，在流行的目标检测的benchmarks上，比较标准的IoU和GIoU，发现性能一致均得到提升。</p>\n<p>主要贡献如下：</p>\n<ul>\n<li>介绍了GIoU，作为比较两个任意形状差距的指标</li>\n<li>以GIoU作为轴对齐矩形或超矩形的损失时，使用解析解</li>\n<li>将GIoU引入sota 检测器如Faster R-CNN，Mask R-CNN和YOLO v3，并在标准目标检测benchmark上验证性能得到提升</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p><strong>目标检测准确性测量：</strong> IoU作为评估指标在目标检测任务中广为使用，常用于确定预测box是真阳性还是假阳性。使用IoU时需要选择一个阈值。在PASCAL VOC上，计算mAP时选择 IoU阈值=0.5，但是随意选择的IoU阈值不能完全反映定位性能，所有定位准确性大于这个阈值的检测结果认为是真阳性，从而参与mAP的计算，IoU阈值的选择将直接影响mAP值。为了是性能测量对IoU阈值不敏感，MS COCO benchmark 则选择不同的IoU阈值计算多个mAP然后取平均。</p>\n<p><strong>bbox表示和损失：</strong> 2D目标检测中，bbox参数非常重要。最近的文献提出多种不同bbox表示和损失：</p>\n<ol>\n<li><p>YOLO v1</p>\n<p>YOLO v1 直接回归 bbox 参数$(x_c,y_c,w,h)$，坐标损失使用平方差。计算损失时，为了降低目标scale对(w,h)损失项的影响，将这一损失项由$(w-\\hat w)^2+(h-\\hat h)^2$ 改为 $(\\sqrt w - \\sqrt {\\hat w})^2+(\\sqrt h - \\sqrt {\\hat h})^2$。</p>\n</li>\n<li><p>R-CNN</p>\n<p>R-CNN使用selective search先获得候选boxes，然后回归bbox中心点偏差（求差）和size的偏差（求商），为了降低scale敏感度，将size 偏差转换到对数空间（求log），然会对偏差使用$l_2$范数（最小均方差MSE）作为目标函数进行优化。</p>\n</li>\n<li><p>Fast R-CNN</p>\n<p>Fast R-CNN对坐标偏差使用 $l_1$-smooth 损失，使得模型即使在异常值情况下也具有较好的鲁棒性（异常值情况一般指偏差非常大的情况，此时若使用 $l_2$ 范数的目标函数，其梯度比较大，使得模型训练初期非常不稳定）。</p>\n</li>\n<li><p>Faster R-CNN</p>\n<p>Faster R-CNN使用密集anchor boxes，然后对其中心坐标和size的偏差进行回归，根据anchor boxes的得分（分类置信度）按正负例的一定比例（1:3）得到一个batch （数量为128）的proposals，然后再使用Fast R-CNN的分类和回归两个分支进行最终的预测。为了进一步解决正负例不平衡问题，RetinaNet 使用 focal loss。</p>\n</li>\n</ol>\n<p>大部分目标检测器都是结合以上某种bbox表示和某种损失。这些努力推动目标检测有了明显的发展。我们的工作表明，使用GIoU 损失可以进一步提高目标定位，因为如前面所分析的那样 bbox 回归损失并不能够直接反映检测评估指标IoU。 </p>\n<p><strong>使用近似或替代函数优化IoU：</strong> 在语义分割任务种，曾使用近似函数或替代损失优化IoU。类似地，目标检测任务中，最近的一些研究工作也尝试直接或间接利用IoU以更好地进行bbox回归，然而却在非重叠情况下优化IoU时遇到近似或梯度平坦问题。本文我们通过引入GIoU解决IoU在非重叠情况下的问题。</p>\n<h1 id=\"泛化IoU\"><a href=\"#泛化IoU\" class=\"headerlink\" title=\"泛化IoU\"></a>泛化IoU</h1><p>用于比较两个任意形状 $A,B \\subseteq S \\in \\mathbb{R}^n$ 的 IoU 计算方法为：<br>$$IoU = \\frac {|A \\cap B|} {|A \\cup B|} $$<br>两个显著特性使得这种相似性测量方法流行于评估2D/3D计算机视觉任务中：</p>\n<ul>\n<li><p>IoU作为距离同时也作为评估指标。</p>\n<p>IoU距离即 $\\mathcal L_{IoU}=1-IoU$，这意味着 $\\mathcal L_{IoU}$ 满足 IoU 指标的所有性质，例如非负性，不可分的同一性，对称性和三角不等式</p>\n</li>\n<li><p>IoU具有尺度不变性。这意味着，两个任意形状A B的相似度与它们在 S 空间的尺度无关</p>\n</li>\n</ul>\n<p>但是，IoU的主要问题是：</p>\n<ul>\n<li>$|A \\cap B|=0 \\Rightarrow IoU(A,B)=0$，此时，IoU无法分辨两个形状A B是靠的非常近还是非常远</li>\n</ul>\n<p>为了解决这个问题，我们提出了泛化版IoU，即 GIoU。</p>\n<p>两个任意的凸形 $A, B \\subseteq S \\in \\mathbb S^n$，首先在 S 空间中寻找包含 A 和 B 的最小凸形 C。如果比较两个具体类型的几何图形，C 可以也是这个具体类型，例如比较两个椭圆形，C 则是包含这两个椭圆形的最小椭圆形。然后我们计算 C 中扣掉 A 和 B 剩余部分的面积（体积）与 C 自身的面积（体积）的比例，这个比例代表了一种归一化的且注重 A 和 B 之间的空白部分面积（体积）的测量方法，然后，从 IoU 中减去这个比例就得到 GIoU。（面积/体积对应 2D/3D）</p>\n<p>整个计算过程总结如下算法1：</p>\n<hr>\n<p>算法1：GIoU</p>\n<hr>\n<p>输入： 两个任意凸形 $A,B \\subseteq S \\in \\mathbb S^n$</p>\n<p>输出： GIoU</p>\n<ul>\n<li><p>在 S 空间中寻找包含 A B 的最小凸形 C</p>\n</li>\n<li><p>计算 IoU</p>\n<p>$IoU=\\frac {|A \\cap B|} {|A \\cup B|}$</p>\n</li>\n<li><p>计算 GIoU</p>\n<p>$GIoU = IoU - \\frac {|C \\setminus (A \\cup B)|} {|C|}$</p>\n</li>\n</ul>\n<hr>\n<p>作为新的指标，GIoU 具有性质：</p>\n<ul>\n<li><p>与 IoU 类似，GIoU 作为距离具有指标的所有性质：非负性，不可分的同一性，对称性和三角不等式</p>\n<p>IoU 距离即 $\\mathcal L_{GIoU} = 1-GIoU$。</p>\n</li>\n<li><p>与 IoU 类似，GIoU 具有尺度不变性。</p>\n</li>\n<li><p>GIoU 上限为 IoU</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, GIoU(A,B) \\le IoU(A,B)$，当 A B越靠近且形状越相似，则 GIoU 越接近 IoU，即 $\\lim_{A \\rightarrow B} GIoU(A,B)=IoU(A,B)$。</p>\n</li>\n<li><p>IoU 和 GIoU 的值域</p>\n<p>$\\forall A,B \\subseteq \\mathbb S, 0 \\le IoU(A,B) \\le 1$，但是 GIoU 的值域则关于零点对称，$-1 \\le GIoU(A,B) \\le 1$。</p>\n<p>我们看下如何获得边界值：</p>\n<ul>\n<li>与 IoU 相同，只有当 A B 完成重合的时候，即$|A \\cup B|=|A \\cap B|$，此时$GIoU =IoU=1$</li>\n<li>当 A B 两个形所占面积（体积）与 C 所在面积（体积）之比趋于 0，GIoU 趋于 -1，即$\\lim_{\\frac{|A \\cup B|}{|C|}} GIoU(A,B)=-1$</li>\n</ul>\n</li>\n</ul>\n<p>综上，GIoU 保持了 IoU 的主要性质并避免了 IoU 的缺点，所以在2D/3D计算机视觉任务的性能测试中可以使用 GIoU 代替 IoU。本文我们侧重于 2D 目标检测，推导 GIoU 的解析解，GIoU 同时担当性能指标和损失。在非轴对齐 3D 场景下的 GIoU 则待以后的工作去研究。</p>\n<h2 id=\"GIoU用作BBox回归损失\"><a href=\"#GIoU用作BBox回归损失\" class=\"headerlink\" title=\"GIoU用作BBox回归损失\"></a>GIoU用作BBox回归损失</h2><p>我们已经介绍了 GIoU 可以作为任意两个形状的距离测量指标，但是与 IoU 一样，没有解析解计算两个任意形状的交，也没有解析解可以计算包含这俩形状的最小凸形。</p>\n<p>好在2D 目标检测任务中，bbox 是轴对齐的，此时 GIoU 有解析解。两个形状 A B 的交，以及包含 A B 的最小凸形均具为矩形，对 A B 的顶点坐标使用 min 或 max 操作可以求得它们的顶点坐标。为了确定 A B 是否重叠，还需要进行条件检查，比如 A B 的交，作为矩形，其左上顶点的 x 坐标必然比右下顶点的 x 坐标小即 $x^{tl} &lt; x^{br}$，而 $x^{tl}=\\max (x_A^{tl}, x_B^{tl}), \\ x^{br}=\\min (x_A^{br},x_B^{br})$，所以有 $x_B^{tl} \\le x_A^{tl}&lt;x_B^{br}$ 或 $x_A^{tl} \\le x_B^{tl}&lt;x_A^{br}$。</p>\n<p>反向传播中，min、max和按位计算的线性函数如 ReLU 的梯度计算均是可行的，算法2 中每个部分均可以求导，故 IoU 和 GIoU 均可以直接用作损失即 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 来优化基于深度神经网络的目标检测器。</p>\n<hr>\n<p>算法2：IoU和GIoU用作BBox回归损失</p>\n<hr>\n<p>输入：预测框 $B^p$ 和 GT 框 $B^g$ 的坐标，$B^p=(x_1^p,y_1^p,x_2^p,y_2^p), \\quad B^g=(x_1^g,y_1^g,x_2^g,y_2^g)$</p>\n<p>输出：$\\mathcal L_{IoU}, \\ \\mathcal L_{GIoU}$</p>\n<ol>\n<li><p>因为预测框各个坐标是独立预测出来的，所以需要确保 预测 box 坐标有效即，</p>\n<p>$x_2^p&gt;x_1^p, \\ y_2^p&gt;y_1^p$，故进行如下转换：</p>\n<ul>\n<li>$\\hat x_1^p=\\min(x_1^p,x_2^p), \\ \\hat x_2^p=\\max(x_1^p,x_2^p)$</li>\n<li>$\\hat y_1^p=\\min(y_1^p,y_2^p), \\ \\hat y_2^p=\\max(y_1^p,y_2^p)$</li>\n</ul>\n</li>\n<li><p>计算 GT box 面积：</p>\n<p>$A^g=(x_2^g-x_1^g)\\times (y_2^g-y_1^g)$</p>\n</li>\n<li><p>计算预测 box 面积：</p>\n<p>$A^p=(x_2^p-x_1^p)\\times (y_2^p-y_1^p)$</p>\n</li>\n<li><p>计算交：</p>\n<ul>\n<li>$x_1^{\\mathcal I}=\\max(\\hat x_1^p, x_1^g), \\ x_2^{\\mathcal I}=\\min(\\hat x_2^p,x_2^p)$</li>\n<li>$y_1^{\\mathcal I}=\\max(\\hat y_1^p, y_1^g), \\ y_2^{\\mathcal I}=\\min(\\hat y_2^p,y_2^p)$</li>\n<li>$\\mathcal I=\\begin{cases} (x_2^{\\mathcal I}-x_1^{\\mathcal I})\\times (y_2^{\\mathcal I}-y_1^{\\mathcal I}) &amp; x_2^{\\mathcal I} &gt; x_1^{\\mathcal I}, y_2^{\\mathcal I} &gt; y_1^{\\mathcal I} \\ 0 &amp; \\text{otherwise} \\end{cases}$</li>\n</ul>\n</li>\n<li><p>计算最小包含凸形 c：</p>\n<ul>\n<li>$x_1^c=\\min(\\hat x_1^p, x_1^g), \\ \\max(\\hat x_2^p, x_2^g)$</li>\n<li>$y_1^c=\\min(\\hat y_1^p, y_1^g), \\ \\max(\\hat y_2^p, y_2^g)$</li>\n</ul>\n</li>\n<li><p>计算 c 的面积：</p>\n<p>$A^c=(x_2^c-x_1^c)\\times (y_2^c-y_1^c)$</p>\n</li>\n<li><p>计算 IoU：</p>\n<p>$IoU = \\frac {\\mathcal I}{\\mathcal U}$，其中 $\\mathcal U = A^p+A^g-\\mathcal I$</p>\n</li>\n<li><p>计算 GIoU：</p>\n<p>$GIoU = IoU - \\frac {A^c-\\mathcal U} {A^c}$</p>\n</li>\n<li><p>计算 GIoU 损失：</p>\n<p>$\\mathcal L_{IoU}=1-IoU, \\ \\mathcal L_{GIoU}=1-GIoU$</p>\n</li>\n</ol>\n<hr>\n<p>根据指标检测性能时，以指标本身作为损失来优化显然是最佳选择，但是在bbox非重叠场景下，IoU=0，其梯度也为0，影响训练质量和收敛速度，相反，GIoU 则一直有有效梯度指导如何优化模型。另外，根据性质3，GIoU 与 IoU 强相关，在 IoU 较大时，这种强相关更加显著。图2 定性的分析了这种相关性，</p>\n<p><img src=\"/images/GIoU_fig2.png\" alt></p>\n<p>图2中，随机选择了1万组 2D 矩形pair，计算其 IoU 和 GIoU，观察发现，在重叠较小时，例如 $IoU \\le 0.2, \\ GIoU \\le 0.2$，GIoU 可以比 IoU 变化更显著，而且在任何情况下，GIoU 的梯度都可以很陡，所以将 GIoU 作为损失$\\mathcal L_{GIoU}$，比使用 IoU 作为损失$\\mathcal L_{IoU}$，更有利于优化，并且最终的性能测量指标只要是基于IoU，无论使用哪种指标均可。</p>\n<h3 id=\"损失稳定性\"><a href=\"#损失稳定性\" class=\"headerlink\" title=\"损失稳定性\"></a>损失稳定性</h3><p>我们也考察了预测值为任意的情况下，损失是否会不稳定或者出现未定义情况（比如除数为0）。</p>\n<p>假设 GT box 是矩形，且面积大于0即，$A^g &gt; 0$，算法2中第1点和第4点分别确保了预测框和两个bbox的交均非负即，$A^p \\ge 0, \\ \\mathcal I \\ge 0, \\forall B^p \\in \\mathbb R^4$，又根据$\\mathcal U \\ge A^g$，故 $\\mathcal U &gt; 0$，所以 IoU 的分母为正非零。又 $\\mathcal U \\ge \\mathcal I$，故 $0 \\le IoU \\le 1$，于是 IoU 损失范围为 $0 \\le \\mathcal L_{IoU} \\le 1$</p>\n<p>检查 GIoU 的稳定性，需要考察项 $\\frac {A^c-\\mathcal U} {A^c}$，显然包含 A B 的最小凸形不小于 A B 的并，即 $A^c \\ge \\mathcal U &gt; 0$，所以 $\\frac {A^c-\\mathcal U} {A^c} \\ge 0$。理论上来讲，$\\frac {A^c-\\mathcal U} {A^c} &lt;1$，且当 A B 中心点的几何距离比 A B 的 size 大很多时，即 A B 离得很远，此时 $\\frac {A^c-\\mathcal U} {A^c} \\rightarrow 1$，故 $-1 &lt; GIoU \\le 1$，为了对称，改写为 $-1 \\le GIoU \\le 1$。</p>\n<h3 id=\"IoU-0时-mathcal-L-GIoU-的行为\"><a href=\"#IoU-0时-mathcal-L-GIoU-的行为\" class=\"headerlink\" title=\"IoU=0时$\\mathcal L_{GIoU}$的行为\"></a>IoU=0时$\\mathcal L_{GIoU}$的行为</h3><p>GIoU 损失 $\\mathcal L_{GIoU}=1-GIoU=1+\\frac {A^c-\\mathcal U} {A^c} - IoU$，当 $B^p$ 和 $B^g$ 不相交，即 $\\mathcal I=0, IoU=0$，此时 GIoU 损失简化为 $\\mathcal L_{GIoU}=1+\\frac{A^c-\\mathcal U}{A^c}=2-\\frac {\\mathcal U}{A^c}$，最小化 GIoU 损失则需要最大化 $\\frac {\\mathcal U}{A^c}$，这一项已经是归一化的，即 $0\\le \\frac {\\mathcal U}{A^c} \\le 1$，并且最大化这一项则需要最小化 $A^c$，同时最大化 $\\mathcal U$，因为 $\\mathcal I=0$，故此时 $\\mathcal U=A^p+A^g$，由于 $A^g$ 已知且固定，所以需要最大化 $A^p$，也就是说，最小化 $A^c$ 且同时最大化 $A^p$，显然，这就使得 $B^p$ 趋于与 $B^g$ 重合。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>引入 bbox 回归损失 $\\mathcal L_{GIoU}$ 到2D目标检测器中如 Faster R-CNN、Mask R-CNN 和 YOLO v3，即，将原来 Faster R-CNN/Mask R-CNN 中的 $l_1$-smooth 损失和 YOLO v3 中的 MSE 损失替换为 $\\mathcal L_{GIoU}$，并且我们还对比了 baseline 和使用 $\\mathcal L_{IoU}$ 损失时的结果。记使用目标检测器原先的损失为 baseline。（具体实验数据和结果分析请参考原论文，这里省略）</p>\n<p><strong>数据集</strong> 使用PASCAL VOC 和 MS COCO 数据集，训练方案的细节和对应的评估见下文。</p>\n<p><strong>评估方案</strong> 本文采取 MS COCO 的性能测试方法，在所有分类上计算 mAP，计算在不同 IoU 阈值下的 mAP 值，IoU 阈值用于判断正负例（因为计算mAP需要知道正例数量），取阈值 $IoU={.5,.55,…,.95}$，计算这些 IoU 阈值下 mAP 值的平均，记为 <strong>AP</strong>，然后使用 GIoU 代替 IoU 来判断正负例，同样取阈值 $GIoU={.5,.55,…,.95}$，计算这些阈值下 mAP 的平均，<strong>AP</strong>，特别地，文中还报导了当 IoU 和 GIoU 阈值为 0.75 时的 mAP，记为 <strong>AP75</strong>。</p>\n<h2 id=\"YOLO-v3\"><a href=\"#YOLO-v3\" class=\"headerlink\" title=\"YOLO v3\"></a>YOLO v3</h2><p><strong>训练方案</strong> 使用 YOLO v3 的 Darknet 使用版本。为了得到 Baseline 结果（使用 MSE 损失），我们使用 DarkNet-608 作为 backbone，训练所使用的参数与 YOLO v3 中一致。使用 IoU 和 GIoU 损失训练 YOLO v3 时，我们仅仅将原先的 MSE 损失替换为 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，考虑到 MSE 损失无边界，我们的新损失则是有边界的，而 YOLO v3 损失还包含了分类损失，所以需要针对分类损失将 bbox 回归损失进行正则处理。当然，我们做了一个极小的努力来进行正则处理。</p>\n<h2 id=\"Faster-R-CNN-和-Mask-R-CNN\"><a href=\"#Faster-R-CNN-和-Mask-R-CNN\" class=\"headerlink\" title=\"Faster R-CNN 和 Mask R-CNN\"></a>Faster R-CNN 和 Mask R-CNN</h2><p><strong>训练方案</strong> 使用最新的 Faster R-CNN/Mask R-CNN 的 PyTorch 实现。为了得到baseline结果（使用$l_1$-smooth损失），我们使用 ResNet-50 作为 backbone，其他训练所使用的参数与原先保持一致。当使用 IoU 和 GIoU 损失时，在最后的坐标改进阶段（而不是RPN阶段）使用 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$，与 YOLO v3 的情况一样，我们进行了极小的努力来进行正则处理。所有的实验中，简单的将 $\\mathcal L_{IoU}, \\mathcal L_{GIoU}$ 均乘以 10。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>介绍了 GIoU 作为新的指标来测量两个任意形状的距离。GIoU 继承了 IoU 的优秀特性且避免了 IoU 的缺点（非重叠情况），所以在基于 IoU 作为指标的 2D/3D 的计算机视觉任务中，GIoU 是一个很好的选择。</p>\n<p>我们也提供了轴对齐的两个矩形之间 GIoU 的解析解。 GIoU 作为距离其导数/梯度可计算，故可以使用 GIoU 作为 bbox 回归损失。将 GIoU 损失结合进 sota 目标检测器，其检测性能在多个数据集上均一致得到提升。我们认为，指标自身就是针对指标的最优损失，GIoU 可以作为最佳 bbox 回归损失用于需要 2D bbox 回归的所有计算机视觉任务中。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>这篇文章主要是提出了一个新的损失来优化模型，文章通俗易懂，实在没什么可分析的，于是就写成了翻译，也算是一种阅读记录吧。</p>\n"},{"title":"PyTorch-1","date":"2019-06-12T11:17:11.000Z","_content":"# 安装\n一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。\n\n当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。\n\n下载源码\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n由于使用了子模块所以增加--recursive选项，记pytorch的root dir为$ROOT_DIR。\n\n根据安装步骤进行自上而下的阅读。Linux下安装使用命令\n```\ncd pytorch\npython setup.py install\n```\npytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库\n\n### build_deps()\n这个方法内部关键的一步为\n```\nbuild_caffe2(...)\n```\n查看这个方法的定义，发现build_caffe2做了如下几件事：\n1. run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为`$ROOD_DIR/build`， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt\n2. 在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）\n3. 将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成\n\n这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\n这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。\n\n另外，top level 中CMakeLists.txt中有这么一行\n```\ninclude(cmake/Dependencies.cmake)\n```\n这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如`$ROOT_DIR/third_party`或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：\n1. 非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\n这个find_package告诉我们去查看`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n这就相当于能生成-lopenblas这样的链接选项。\n\n我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\n其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中\n\n以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。\n\n### setup()\nsetup方法（可以参考[setup()](https://docs.python.org/3/distutils/apiref.html)），其中几个值得说明的参数：\n1. ext_modules 有5个扩展库分别如下：\n- torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir\n- torch._dl 非WINDOWS平台下才有，指定了C源文件\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\n后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。\n\n2. cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含\n\n- create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据`$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)`这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n其中每个{...}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。\n- run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑\n- build_extensions 生成由ext_modules指定的python扩展库所用的方法\n\next_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。\n\n然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为`$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是.../miniconda3/lib/python3.7/site-packages/caffe2/python/目录。\n\n3. packages 指定安装到python 的site-packages下的包\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\n由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含__init__.py，所以将caffe2和torch两个包安装到site-packages下。\n\n现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。\n\n### 整理\n\n以上就是pytorch安装过程，主要分为两部分:\n\n1. 使用CMake生成c++库，对应build_deps()这个方法执行\n2. 使用python的setup方法生成扩展库，主要是build_ext。\n\n根据上面两点，重新整理一遍。\n```\ntop-level的CMakeLists.txt中\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\n于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，\n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\n安装目录则寻找对应的install语句。此外，文件中还有一句\n```\nadd_subdirectory(../torch torch)\n```\n（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）\n\n于是查看torch目录下的CMakeLists.txt， 其中生成的库为\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n然后根据其中的\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\n有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：\n- 预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 `-I<include dir>flag`。\n- 本项目内包含的库。包括：\n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # 添加tbb库\n```\n(2) qnnpack\n```\n# 添加 qnnpack 库\n# source directory为${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory为${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。\n(3) nnpack\n```\n# 添加 nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\n跳至nnpack.cmake文件，发现其中包含\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。\n\n(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置\n\n(5) LMDB。使用如下语句\n```\nfind_package(LMDB)\n```\n所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\n类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。\n\n(6) pybind11。在Dependencies.cmake添加pybind11依赖，\n```\nfind_package(pybind11 CONFIG)# 配置模式下寻找，然而没有${pybind11_DIR}，也没有pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # 继续module模式下寻找\nendif()\n```\n虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\n如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDA。在Dependencies.cmake中有\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\n在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\n其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\n保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。\n\n(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。\n\nDependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. 生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。\n\n- torch._C 链接的两个库为\n```\nmain_libraries=['shm', 'torch_python']\n```\n显然前面已经生成了这两个库。而使用的源文件则为\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了<dlfcn.h>中的三个常量到torch._dl库中，如下\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。\n\n### 还有...\n可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。","source":"_posts/PyTorch-1.md","raw":"---\ntitle: PyTorch-1\ndate: 2019-06-12 19:17:11\ntags: PyTorch\ncategories: DL Framework\n---\n# 安装\n一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。\n\n当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。\n\n下载源码\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n由于使用了子模块所以增加--recursive选项，记pytorch的root dir为$ROOT_DIR。\n\n根据安装步骤进行自上而下的阅读。Linux下安装使用命令\n```\ncd pytorch\npython setup.py install\n```\npytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库\n\n### build_deps()\n这个方法内部关键的一步为\n```\nbuild_caffe2(...)\n```\n查看这个方法的定义，发现build_caffe2做了如下几件事：\n1. run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为`$ROOD_DIR/build`， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt\n2. 在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）\n3. 将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成\n\n这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：\n```\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\nadd_subdirectory(modules)\n```\n这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。\n\n另外，top level 中CMakeLists.txt中有这么一行\n```\ninclude(cmake/Dependencies.cmake)\n```\n这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如`$ROOT_DIR/third_party`或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：\n1. 非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为\n```\n...\nelseif(BLAS STREQUAL \"OpenBLAS\")\n  find_package(OpenBLAS REQUIRED)\n  include_directories(SYSTEM ${OpenBLAS_INCLUDE_DIR})\n  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS ${OpenBLAS_LIB})\n```\n这个find_package告诉我们去查看`$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake`，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有\n```\ntarget_link_libraries(caffe2 PUBLIC ${Caffe2_PUBLIC_DEPENDENCY_LIBS})\n```\n这就相当于能生成-lopenblas这样的链接选项。\n\n我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现\n```\nadd_subdirectory(python)\n...\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\ninstall(TARGETS caffe2_pybind11_state DESTINATION \"${PYTHON_LIB_REL_PATH}/caffe2/python\")\n```\n其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中\n\n以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。\n\n### setup()\nsetup方法（可以参考[setup()](https://docs.python.org/3/distutils/apiref.html)），其中几个值得说明的参数：\n1. ext_modules 有5个扩展库分别如下：\n- torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir\n- torch._dl 非WINDOWS平台下才有，指定了C源文件\n- caffe2.python.caffe2_pybind11_state\n- caffe2.python.caffe2_pybind11_state_gpu\n- caffe2.python.caffe2_pybind11_state_hip\n\n后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。\n\n2. cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含\n\n- create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据`$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)`这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下\n```\n[\n{\n  \"directory\":\"<path/to/root>build/third_party/protobuf/cmake\",\n  \"command\": \"/usr/bin/c++ ... -I<path/to/root>/third_party/protobuf/src ... \n                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...\",\n  \"file\": \"<path/to/root>/third_party/protobuf/src/google/protobuf/arena.cc\"\n},\n...\n]\n```\n其中每个{...}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。\n- run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑\n- build_extensions 生成由ext_modules指定的python扩展库所用的方法\n\next_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。\n\n然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为`$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/`，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/`下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为`$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/`，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是.../miniconda3/lib/python3.7/site-packages/caffe2/python/目录。\n\n3. packages 指定安装到python 的site-packages下的包\n```\npackages = find_packages(exclude=['tools', 'tools.*'])\n```\n\n由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含__init__.py，所以将caffe2和torch两个包安装到site-packages下。\n\n现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。\n\n### 整理\n\n以上就是pytorch安装过程，主要分为两部分:\n\n1. 使用CMake生成c++库，对应build_deps()这个方法执行\n2. 使用python的setup方法生成扩展库，主要是build_ext。\n\n根据上面两点，重新整理一遍。\n```\ntop-level的CMakeLists.txt中\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n```\n于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，\n```\nadd_library(caffe2_proto STATIC $<TARGET_OBJECTS:Caffe2_PROTO>\nadd_library(thnvrtc SHARED ${TORCH_SRC_DIR}/csrc/jit/fuser/cuda/thnvrtc.cpp>\nadd_library(caffe2 ${Caffe2_CPU_SRCS})\nif (TORCH_STATIC)\n  add_library(torch STATIC ${DUMMY_EMPTY_FILE})\nelse()\n  add_library(torch SHARED ${DUMMY_EMPTY_FILE})\nendif()\ntorch_cuda_based_add_library(caffe2_gpu ${Caffe2_GPU_SRCS})\nhip_add_library(caffe2_hip ${Caffe2_HIP_SRCS})\nadd_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_gpu MODULE ${Caffe2_GPU_PYTHON_SRCS})\nadd_library(caffe2_pybind11_state_hip MODULE ${Caffe2_HIP_PYTHON_SRCS})\n```\n安装目录则寻找对应的install语句。此外，文件中还有一句\n```\nadd_subdirectory(../torch torch)\n```\n（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）\n\n于是查看torch目录下的CMakeLists.txt， 其中生成的库为\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\n然后根据其中的\n```\nset(LIBSHM libshm)\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为\n```\nADD_LIBRARY(shm SHARED core.cpp)\n```\n有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：\n- 预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 `-I<include dir>flag`。\n- 本项目内包含的库。包括：\n(1) tbb\n```\nadd_subdirectory(${CMAKE_SOURCE_DIR}/aten/src/ATen/cpu/tbb)    # 添加tbb库\n```\n(2) qnnpack\n```\n# 添加 qnnpack 库\n# source directory为${PROJECT_SOURCE_DIR}/third_party/QNNPACK\n# output directory为${PROJECT_BINARY_DIR}/confu-deps/QNNPACK\nadd_subdirectory(\"${QNNPACK_SOURCE_DIR}\" \"${CONFU_DEPENDENCIES_BINARY_DIR}/QNNPACK\")\nlist(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)\n```\n最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。\n(3) nnpack\n```\n# 添加 nnpack\ninclude(${CMAKE_CURRENT_LIST_DIR}/External/nnpack.cmake)\n```\n跳至nnpack.cmake文件，发现其中包含\n```\nadd_subdirectory(${NNPACK_SOURCE_DIR} ${CONFU_DEPENDENCIES_BINARY_DIR}/NNPACK)\n```\n找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。\n\n(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置\n\n(5) LMDB。使用如下语句\n```\nfind_package(LMDB)\n```\n所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行\n```\ninclude_directories(SYSTEM ${LMDB_INCLUDE_DIR})\nlist(APPEND Caffe2_DEPENDENCY_LIBS ${LMDB_LIBRARIES})\n```\n类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。\n\n(6) pybind11。在Dependencies.cmake添加pybind11依赖，\n```\nfind_package(pybind11 CONFIG)# 配置模式下寻找，然而没有${pybind11_DIR}，也没有pybind11Config.cmake\nif(NOT pybind11_FOUND)\n  find_package(pybind11)     # 继续module模式下寻找\nendif()\n```\n虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加\n```\ninclude_directories(SYSTEM ${CMAKE_CURRENT_LIST_DIR}/../third_party/pybind11/include)\n\n```\n(7) OPENMP\n```\nFIND_PACKAGE(OpenMP QUIET)\n```\n如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，\n```\ntarget_compile_options(caffe2 INTERFACE ${OpenMP_CXX_FLAGS})\ntarget_link_libraries(caffe2 PRIVATE ${OpenMP_CXX_LIBRARIES})\n```\n(8) CUDA。在Dependencies.cmake中有\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/public/cuda.cmake)\n```\n在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，\n```\nadd_library(caffe2::cuda UNKNOWN IMPORTED)\n```\n其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，\n```\nlist(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)\n```\n保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。\n\n(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。\n\nDependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag\n```\ntarget_link_libraries(caffe2 PRIVATE ${Caffe2_DEPENDENCY_LIBS})\n```\n2. 生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。\n\n- torch._C 链接的两个库为\n```\nmain_libraries=['shm', 'torch_python']\n```\n显然前面已经生成了这两个库。而使用的源文件则为\n```\nmain_sources=[\"torch/csrc/stub.cpp\"]\n```\n- torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了<dlfcn.h>中的三个常量到torch._dl库中，如下\n```\nRTLD_GLOBAL=0x100\nRTLD_NOW   =0x2\nRTLD_LAZY  =0x1\n```\n这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。\n\n### 还有...\n可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。","slug":"PyTorch-1","published":1,"updated":"2019-06-20T10:07:49.801Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5pz20015xgvcdt114x05","content":"<h1 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h1><p>一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。</p>\n<p>当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。</p>\n<p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/pytorch/pytorch</span><br></pre></td></tr></table></figure>\n\n<p>由于使用了子模块所以增加–recursive选项，记pytorch的root dir为$ROOT_DIR。</p>\n<p>根据安装步骤进行自上而下的阅读。Linux下安装使用命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n\n<p>pytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库</p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p>这个方法内部关键的一步为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n\n<p>查看这个方法的定义，发现build_caffe2做了如下几件事：</p>\n<ol>\n<li>run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为<code>$ROOD_DIR/build</code>， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt</li>\n<li>在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）</li>\n<li>将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成</li>\n</ol>\n<p>这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n\n<p>这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。</p>\n<p>另外，top level 中CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake/Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如<code>$ROOT_DIR/third_party</code>或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：</p>\n<ol>\n<li>非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>这个find_package告诉我们去查看<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>这就相当于能生成-lopenblas这样的链接选项。</p>\n<p>我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;/caffe2/python&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中</p>\n<p>以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup方法（可以参考<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a>），其中几个值得说明的参数：</p>\n<ol>\n<li>ext_modules 有5个扩展库分别如下：</li>\n</ol>\n<ul>\n<li>torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir</li>\n<li>torch._dl 非WINDOWS平台下才有，指定了C源文件</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。</p>\n<ol start=\"2\">\n<li>cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含</action></li>\n</ol>\n<ul>\n<li>create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据<code>$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path/to/root&gt;build/third_party/protobuf/cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;/usr/bin/c++ ... -I&lt;path/to/root&gt;/third_party/protobuf/src ... </span><br><span class=\"line\">                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path/to/root&gt;/third_party/protobuf/src/google/protobuf/arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>其中每个{…}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。</p>\n<ul>\n<li>run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑</li>\n<li>build_extensions 生成由ext_modules指定的python扩展库所用的方法</li>\n</ul>\n<p>ext_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。</p>\n<p>然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为<code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是…/miniconda3/lib/python3.7/site-packages/caffe2/python/目录。</p>\n<ol start=\"3\">\n<li>packages 指定安装到python 的site-packages下的包<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages = find_packages(exclude=[&apos;tools&apos;, &apos;tools.*&apos;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含<strong>init</strong>.py，所以将caffe2和torch两个包安装到site-packages下。</p>\n<p>现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。</p>\n<h3 id=\"整理\"><a href=\"#整理\" class=\"headerlink\" title=\"整理\"></a>整理</h3><p>以上就是pytorch安装过程，主要分为两部分:</p>\n<ol>\n<li>使用CMake生成c++库，对应build_deps()这个方法执行</li>\n<li>使用python的setup方法生成扩展库，主要是build_ext。</li>\n</ol>\n<p>根据上面两点，重新整理一遍。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-level的CMakeLists.txt中</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n\n<p>于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;/csrc/jit/fuser/cuda/thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>安装目录则寻找对应的install语句。此外，文件中还有一句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(../torch torch)</span><br></pre></td></tr></table></figure>\n\n<p>（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）</p>\n<p>于是查看torch目录下的CMakeLists.txt， 其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>然后根据其中的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n\n<p>有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：</p>\n<ul>\n<li>预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 <code>-I&lt;include dir&gt;flag</code>。</li>\n<li>本项目内包含的库。包括：<br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;/aten/src/ATen/cpu/tbb)    # 添加tbb库</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>(2) qnnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 qnnpack 库</span><br><span class=\"line\"># source directory为$&#123;PROJECT_SOURCE_DIR&#125;/third_party/QNNPACK</span><br><span class=\"line\"># output directory为$&#123;PROJECT_BINARY_DIR&#125;/confu-deps/QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n\n<p>最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。<br>(3) nnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/External/nnpack.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>跳至nnpack.cmake文件，发现其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/NNPACK)</span><br></pre></td></tr></table></figure>\n\n<p>找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。</p>\n<p>(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置</p>\n<p>(5) LMDB。使用如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n\n<p>所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。</p>\n<p>(6) pybind11。在Dependencies.cmake添加pybind11依赖，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# 配置模式下寻找，然而没有$&#123;pybind11_DIR&#125;，也没有pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # 继续module模式下寻找</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n\n<p>虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../third_party/pybind11/include)</span><br></pre></td></tr></table></figure>\n\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n\n<p>如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>(8) CUDA。在Dependencies.cmake中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/public/cuda.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n\n<p>其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n\n<p>保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。</p>\n<p>(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。</p>\n<p>Dependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li>生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。</li>\n</ol>\n<ul>\n<li>torch._C 链接的两个库为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries=[&apos;shm&apos;, &apos;torch_python&apos;]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>显然前面已经生成了这两个库。而使用的源文件则为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources=[&quot;torch/csrc/stub.cpp&quot;]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了&lt;dlfcn.h&gt;中的三个常量到torch._dl库中，如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL=0x100</span><br><span class=\"line\">RTLD_NOW   =0x2</span><br><span class=\"line\">RTLD_LAZY  =0x1</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。</p>\n<h3 id=\"还有…\"><a href=\"#还有…\" class=\"headerlink\" title=\"还有…\"></a>还有…</h3><p>可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h1><p>一直以来就对深度学习的框架源码有着浓厚兴趣，但是由于涉及到的领域较多，C++，python，CUDA，数学等，加上时间也比较零碎，就耽搁至今，后来意识到我不可能等完全弄明白之后再来写博客记录，毕竟能力不足，所以还是边看源码边记录，不求完全搞明白，但求能从整体上有个大致的理解，如果还能整明白一些数学计算上的代码实现，那就再好不过了。</p>\n<p>当前最流行的深度学习框架就是tensorflow和pytorch了，但是tensorflow据说代码工业化程度非常高，我等菜鸡先避其锋芒，来分析pytorch，希望能给自己带来点信心。</p>\n<p>下载源码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/pytorch/pytorch</span><br></pre></td></tr></table></figure>\n\n<p>由于使用了子模块所以增加–recursive选项，记pytorch的root dir为$ROOT_DIR。</p>\n<p>根据安装步骤进行自上而下的阅读。Linux下安装使用命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd pytorch</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n\n<p>pytorch底层计算使用C++实现，并提供了python调用接口，所以这一命令就是使用setuptools安装python包，安装依赖库及修改配置项这里均跳过，故直接看$ROOT_DIR/setup.py中的setup()方法，但是在这个方法之前先执行了build_deps()用于生成有关 caffe2 的依赖库</p>\n<h3 id=\"build-deps\"><a href=\"#build-deps\" class=\"headerlink\" title=\"build_deps()\"></a>build_deps()</h3><p>这个方法内部关键的一步为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">build_caffe2(...)</span><br></pre></td></tr></table></figure>\n\n<p>查看这个方法的定义，发现build_caffe2做了如下几件事：</p>\n<ol>\n<li>run_cmake。执行cmake，这个命令的选项这里省略不展开，注意执行cmake这个命令的工作目录为<code>$ROOD_DIR/build</code>， cmake的Source Tree为$ROOD_DIR，这个 目录下存在top level的CMakeLists.txt</li>\n<li>在$ROOT_DIR/build下编译并安装，使用make install或者 ninja install（cmake生成的Makefile中install这个target包含了build这个步骤）</li>\n<li>将build/caffe2/proto下的所有.py文件 拷贝到caffe2/proto/下，这些.py文件是根据caffe2/proto/下的.proto文件生成</li>\n</ol>\n<p>这其中最复杂的部分就是run_cmake了，先是使用cmake的-D option设置一些cmake的变量，然后对source tree应用cmake， 查看top level的CMakeLists.txt，这个文件看着好像特别庞大，实际上做的事情也就那么几种：1)设置变量，根据不同操作系统设置或修改变量；2)设置include dir以及lib dir；3）加载.cmake文件以使用其中自定义的cmake函数；4）设置C++文件编译选项；5）安装配置文件/目录到指定位置等；我们注意比较关键的语句如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br><span class=\"line\">add_subdirectory(modules)</span><br></pre></td></tr></table></figure>\n\n<p>这表明将c10,caffe2,modules等目录添加进build tree，这些目录下必定也有相应的CMakeLists.txt， 所以需要继续查看这些CMakeLists.txt中定义了哪些生成规则。</p>\n<p>另外，top level 中CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(cmake/Dependencies.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>这个Dependencies.cmake指明安装Caffe2所依赖的各种库，其中一些库位于本项目中如<code>$ROOT_DIR/third_party</code>或$ROOT_DIR/caffe2，还有一些库则是需要预先手动安装的，举个例子：</p>\n<ol>\n<li>非本项目的公共库，比如添加BLAS库依赖，假设最开始设置了环境变量BLAS=OpenBLAS（环境变量的设置可参考setup.py文件头部注释）, 那么选择添加OpenBLAS库依赖，在Dependencies.cmake中代码为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">elseif(BLAS STREQUAL &quot;OpenBLAS&quot;)</span><br><span class=\"line\">  find_package(OpenBLAS REQUIRED)</span><br><span class=\"line\">  include_directories(SYSTEM $&#123;OpenBLAS_INCLUDE_DIR&#125;)</span><br><span class=\"line\">  list(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS $&#123;OpenBLAS_LIB&#125;)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>这个find_package告诉我们去查看<code>$ROOT_DIR/cmake/Modules/FindOpenBLAS.cmake</code>，好的我们跳过去看一下这个.cmake文件，发现其定义了OpenBLAS的头文件和库文件的搜索路径，然后根据这些搜索路径分别搜索头文件cblas.h所在目录以及库名openblas， 分别使用变量OpenBLAS_INCLUDE_DIR和OpenBLAS_LIB保存，从上面的代码片段，我们知道搜索到的库名被添加到Caffe2_PUBLIC_DEPENDENCY_LIBS中，而我们再跳至$ROOT_DIR/caffe2/CMakeLists.txt发现其中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PUBLIC $&#123;Caffe2_PUBLIC_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>这就相当于能生成-lopenblas这样的链接选项。</p>\n<p>我们直接再看另一个库caffe2_pybind11_state的生成，因为下文会提到它，查看$ROOT_DIR/caffe2/CMakeLists.txt发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(python)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">install(TARGETS caffe2_pybind11_state DESTINATION &quot;$&#123;PYTHON_LIB_REL_PATH&#125;/caffe2/python&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>其中Caffe2_CPU_PYTHON_SRCS在$ROOT_DIR/caffe2/python/CMakeLists.txt中设置， 类似地，还根据是否使用CUDA或者ROCM , 生成caffe2_pybind11_state_gpu或caffe2_pybind11_state_hip。生成这些库文件后，直接install到python的site-packages目录下的caffe2/python目录中</p>\n<p>以上就是build_dep()这个方法介绍，接着看$ROOT_DIR/setup.py中的setup方法。</p>\n<h3 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup()\"></a>setup()</h3><p>setup方法（可以参考<a href=\"https://docs.python.org/3/distutils/apiref.html\" target=\"_blank\" rel=\"noopener\">setup()</a>），其中几个值得说明的参数：</p>\n<ol>\n<li>ext_modules 有5个扩展库分别如下：</li>\n</ol>\n<ul>\n<li>torch._C 指定了C++源文件，链接库，编译选项，链接选项和头文件/库dir</li>\n<li>torch._dl 非WINDOWS平台下才有，指定了C源文件</li>\n<li>caffe2.python.caffe2_pybind11_state</li>\n<li>caffe2.python.caffe2_pybind11_state_gpu</li>\n<li>caffe2.python.caffe2_pybind11_state_hip</li>\n</ul>\n<p>后三个库在上一步中其实已经生成好了，其中caffe2.python前缀表示两级目录（package），可以在$ROOT_DIR/build/caffe2/python目录下查看。扩展模块ext_modules在build_ext这个动作中生成。</p>\n<ol start=\"2\">\n<li>cmdclass，重写了build_ext, clean, install这几个action，这个action用在python setup.py <action> 命令中。install动作跟默认一致。 clean是清除编译过程中产生的临时文件，这些临时文件的pattern在.gitignore中给定。我们重点看一下build_ext这个动作对应的类build_ext，其中方法包含</action></li>\n</ol>\n<ul>\n<li>create_compile_commands这是一个自定义方法，用于将compile_commands.json中的gcc编译器改为g++，修改原因代码注释写的很清楚，使用gcc编译s时不会include c++的头文件目录。 文件compile_commands.json是根据<code>$ROOT_DIR/CMakeLists.txt中的set(CMAKE_EXPORT_COMPILE_COMMAND ON)</code>这句代码而生成，所以位于$ROOT_DIR/build目录下，这个json文件中指明了编译各个文件时的工作路径（working directory），编译指令（command）以及被编译的原文件，格式如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;directory&quot;:&quot;&lt;path/to/root&gt;build/third_party/protobuf/cmake&quot;,</span><br><span class=\"line\">  &quot;command&quot;: &quot;/usr/bin/c++ ... -I&lt;path/to/root&gt;/third_party/protobuf/src ... </span><br><span class=\"line\">                -o CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o ...&quot;,</span><br><span class=\"line\">  &quot;file&quot;: &quot;&lt;path/to/root&gt;/third_party/protobuf/src/google/protobuf/arena.cc&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>其中每个{…}块表示编译一个源文件到目标文件 .o。 将文件中gcc改为g++后重新保存为$ROOT_DIR/compile_commands.json。</p>\n<ul>\n<li>run打印各library（比如 CUDA, CUDNN, NUMPY等）的使用情况，然后执行基类同名方法的逻辑</li>\n<li>build_extensions 生成由ext_modules指定的python扩展库所用的方法</li>\n</ul>\n<p>ext_modules中添加了5个扩展，后三个扩展在build_deps()中已经生成并安装，当然，caffe2_pybind11_state_gpu和caffe2_pybind11_state_hip是根据配置决定是否生成，配置了CUDA则生成前者，配置了ROCM则生成后者，如果均未配置，则这两个扩展均不生成。既然在build_deps()中已经生成并安装，所以这里将其从ext_modules中删除，于是build_extensions实际上只生成torch._C, torch._dl这两个扩展库。</p>\n<p>然而，除了build_deps()方法还有其他方法可用于生成ext_modules中 的后三个扩展库，生成路径为<code>$ROOT_DIR/torch/lib/python3.7/site-packages/caffe2/python/</code>，所以需要判断在这个路径下是否存在后三个扩展库，若不在（此时就是前面所说的使用build_deps()生成），则将扩展库名称从ext_modules中予以删除， 若存在，则还需则将其拷贝到生成目录<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/</code>下，并修改拷贝后的文件名称，以caffe2.python.caffe2_pybind11_state为例说明，两级前缀表示目录所以最终的目录为<code>$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/</code>，剩余的caffe2_pybind11_state表示扩展库的文件名，还需要添加后缀名，这个后缀名由系统平台和python版本，我这里是.cpython-37m-x86_64-linux-gnu.so，于是拷贝后得到文件$ROOT_DIR/torch/build/lib.linux-x86_64-3.7/caffe2/python/caffe2_pybind11_state.cpython-37m-x86_64-linux-gnu.so ，这样使用基类的build_extensions()方法才能将其进一步安装到 python的site-packages目录下，我这里是…/miniconda3/lib/python3.7/site-packages/caffe2/python/目录。</p>\n<ol start=\"3\">\n<li>packages 指定安装到python 的site-packages下的包<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">packages = find_packages(exclude=[&apos;tools&apos;, &apos;tools.*&apos;])</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>由于PyTorch项目中除tools之外，只有caffe2和torch两个目录包含<strong>init</strong>.py，所以将caffe2和torch两个包安装到site-packages下。</p>\n<p>现在再回头看看ext_modules中指定的5个扩展，不难得知，其中torch._C, torch._dl这两个扩展安装到site-packages/torch下，扩展包名称分别为_C, _dl（省略了文件ext后缀），而另外三个caffe2有关的扩展则根据名称（.号切分，前面都是目录名，最后一个是文件名）知道其安装在site-packages/caffe2/python下。</p>\n<h3 id=\"整理\"><a href=\"#整理\" class=\"headerlink\" title=\"整理\"></a>整理</h3><p>以上就是pytorch安装过程，主要分为两部分:</p>\n<ol>\n<li>使用CMake生成c++库，对应build_deps()这个方法执行</li>\n<li>使用python的setup方法生成扩展库，主要是build_ext。</li>\n</ol>\n<p>根据上面两点，重新整理一遍。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top-level的CMakeLists.txt中</span><br><span class=\"line\">add_subdirectory(c10)</span><br><span class=\"line\">add_subdirectory(caffe2)</span><br></pre></td></tr></table></figure>\n\n<p>于是先看caffe2这个目录下的CMakeLists.txt， 寻找其中的关键语句，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2_proto STATIC $&lt;TARGET_OBJECTS:Caffe2_PROTO&gt;</span><br><span class=\"line\">add_library(thnvrtc SHARED $&#123;TORCH_SRC_DIR&#125;/csrc/jit/fuser/cuda/thnvrtc.cpp&gt;</span><br><span class=\"line\">add_library(caffe2 $&#123;Caffe2_CPU_SRCS&#125;)</span><br><span class=\"line\">if (TORCH_STATIC)</span><br><span class=\"line\">  add_library(torch STATIC $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">else()</span><br><span class=\"line\">  add_library(torch SHARED $&#123;DUMMY_EMPTY_FILE&#125;)</span><br><span class=\"line\">endif()</span><br><span class=\"line\">torch_cuda_based_add_library(caffe2_gpu $&#123;Caffe2_GPU_SRCS&#125;)</span><br><span class=\"line\">hip_add_library(caffe2_hip $&#123;Caffe2_HIP_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state MODULE $&#123;Caffe2_CPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_gpu MODULE $&#123;Caffe2_GPU_PYTHON_SRCS&#125;)</span><br><span class=\"line\">add_library(caffe2_pybind11_state_hip MODULE $&#123;Caffe2_HIP_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>安装目录则寻找对应的install语句。此外，文件中还有一句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory(../torch torch)</span><br></pre></td></tr></table></figure>\n\n<p>（实际上caffe2目录下CMakeLists.txt中存在很多add_subdirectory，但是都是类似的处理过程，所以不一一说明，仅以torch这个目录进行说明）</p>\n<p>于是查看torch目录下的CMakeLists.txt， 其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>然后根据其中的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;TORCH_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>继续查看torch/lib/libshm下的CMakeLists.txt，其中生成的库为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD_LIBRARY(shm SHARED core.cpp)</span><br></pre></td></tr></table></figure>\n\n<p>有关的库依赖，分为预装库和本项目（pytorch）内包含的库，CMake生成规则位于cmake/Dependencies.cmake文件中，仔细查看该文件发现：</p>\n<ul>\n<li>预先装的库依赖，这些库名存在Caffe2_PUBLIC_DEPENDENCY_LIBS中。如上文所举例子OpenBLAS 那样添加g++的链接flag和 <code>-I&lt;include dir&gt;flag</code>。</li>\n<li>本项目内包含的库。包括：<br>(1) tbb<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;CMAKE_SOURCE_DIR&#125;/aten/src/ATen/cpu/tbb)    # 添加tbb库</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>(2) qnnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 qnnpack 库</span><br><span class=\"line\"># source directory为$&#123;PROJECT_SOURCE_DIR&#125;/third_party/QNNPACK</span><br><span class=\"line\"># output directory为$&#123;PROJECT_BINARY_DIR&#125;/confu-deps/QNNPACK</span><br><span class=\"line\">add_subdirectory(&quot;$&#123;QNNPACK_SOURCE_DIR&#125;&quot; &quot;$&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/QNNPACK&quot;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS qnnpack)</span><br></pre></td></tr></table></figure>\n\n<p>最后一行指引CMake去QNNPACK的目录（位于third_party下）去生成qnnpack库，然后回到Dependencies.cmake中添加到Caffe2_DEPENDENCY_LIBS中。<br>(3) nnpack</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加 nnpack</span><br><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/External/nnpack.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>跳至nnpack.cmake文件，发现其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_subdirectory($&#123;NNPACK_SOURCE_DIR&#125; $&#123;CONFU_DEPENDENCIES_BINARY_DIR&#125;/NNPACK)</span><br></pre></td></tr></table></figure>\n\n<p>找到包含NNPACK的代码目录位于third_party下，显然这个NNPACK也应该包含CMakeLists.txt文件指示CMake 生成nnpack库，然后回到Dependencies.cmake中将nnpack添加到Caffe2_DEPENDENCY_LIBS。</p>\n<p>(4) 类似地，还添加了 cpuinfo，gflag，glog::glog，googletest，fbgemm，fp16等。这些也不一定全部使用，是否使用还得看相应配置</p>\n<p>(5) LMDB。使用如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(LMDB)</span><br></pre></td></tr></table></figure>\n\n<p>所以去cmake/Modules目录下寻找FindLMDB.cmake， 在这个.cmake文件中寻找lmdb库以及lmdb.h头文件（linux中已经安装，分别位于/usr/lib/x86_64-linux-gnu和/usr/include）, 将库名称和头文件目录分别保存于变量LMDB_LIBRARIES和LMDB_INCLUDE_DIR，然后回到Dependencies.cmake，照例执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;LMDB_INCLUDE_DIR&#125;)</span><br><span class=\"line\">list(APPEND Caffe2_DEPENDENCY_LIBS $&#123;LMDB_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>类似的，还可以添加OPENCL，LEVELDB，NUMA，ZMQ，REDIS，OPENCV，FFMPEG，Python，MPI等。</p>\n<p>(6) pybind11。在Dependencies.cmake添加pybind11依赖，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find_package(pybind11 CONFIG)# 配置模式下寻找，然而没有$&#123;pybind11_DIR&#125;，也没有pybind11Config.cmake</span><br><span class=\"line\">if(NOT pybind11_FOUND)</span><br><span class=\"line\">  find_package(pybind11)     # 继续module模式下寻找</span><br><span class=\"line\">endif()</span><br></pre></td></tr></table></figure>\n\n<p>虽然存在cmake/Modules/Findpybind11.cmake，然而其中find_path并没有找到pybind11/pybind11.h这个头文件，因为我没有预先安装pybind11，CMake自然是找不到的，于是在Dependencies.cmake中直接添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(SYSTEM $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../third_party/pybind11/include)</span><br></pre></td></tr></table></figure>\n\n<p>(7) OPENMP</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FIND_PACKAGE(OpenMP QUIET)</span><br></pre></td></tr></table></figure>\n\n<p>如果找到OpenMP，那么${OpenMP_CXX_FLAGS} 和 ${OpenMP_CXX_LIBRARIES}分别存储头文件搜索路径和库文件链接flag，生成caffe2时可以用到OpenMP，用法是在caffe2/CMakeLists.txt中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_compile_options(caffe2 INTERFACE $&#123;OpenMP_CXX_FLAGS&#125;)</span><br><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;OpenMP_CXX_LIBRARIES&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>(8) CUDA。在Dependencies.cmake中有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include($&#123;CMAKE_CURRENT_LIST_DIR&#125;/public/cuda.cmake)</span><br></pre></td></tr></table></figure>\n\n<p>在这个cuda.cmake中，使用 find_library寻找cuda相关的库，找到后作为IMPORTED target进行库的添加，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(caffe2::cuda UNKNOWN IMPORTED)</span><br></pre></td></tr></table></figure>\n\n<p>其他cuda有关的库类似的进行添加，包括caffe2::cudart，caffe2::cudnn，caffe2::curand，caffe2::cufft，caffe2::tensorrt， caffe2::cublas，caffe2::nvrtc，当然这些库不一定全部添加，根据配置决定添加哪些库，然后回到Dependencies.cmake中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(APPEND Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS caffe2::cuda caffe2::nvrtc)</span><br></pre></td></tr></table></figure>\n\n<p>保存到Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS，将来在caffe2/CMakeLists.txt用于链接。</p>\n<p>(9) 其他的依赖库如NCCL，CUB，GLOO等与上述某一点说明类似，不再一一罗列。</p>\n<p>Dependencies.cmake中有很多库是作为生成caffe2库的依赖，比如QNNPACK，对这部分库添加到Caffe2_DEPENDENCY_LIBS（或Caffe2_PUBLIC_DEPENDENCY_LIBS，Caffe2_PUBLIC_CUDA_DEPENDENCY_LIBS），这个使用下面语句（位于caffe2/CMakeLists.txt）得到链接flag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2 PRIVATE $&#123;Caffe2_DEPENDENCY_LIBS&#125;)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li>生成python的扩展库。首先后三个有关caffe2的扩展已经在上一步中生成并安装，所以对于剩下的两个扩展予以说明。</li>\n</ol>\n<ul>\n<li>torch._C 链接的两个库为<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_libraries=[&apos;shm&apos;, &apos;torch_python&apos;]</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>显然前面已经生成了这两个库。而使用的源文件则为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main_sources=[&quot;torch/csrc/stub.cpp&quot;]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>torch._dl此扩展使用源文件torch/csrc/dl.c生成 。查看这个文件，发现就是添加了&lt;dlfcn.h&gt;中的三个常量到torch._dl库中，如下<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RTLD_GLOBAL=0x100</span><br><span class=\"line\">RTLD_NOW   =0x2</span><br><span class=\"line\">RTLD_LAZY  =0x1</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>这三个常量指示动态加载（比如加载torch._C）的模式，用于dlopen()方法中，增加这三个常量是为了防止python 的os 模块中没有这些flag，并且也没有python的DLFCN模块，此时可以从torch._dl中得到这些flag。相当于把torch._dl当作备胎。</p>\n<h3 id=\"还有…\"><a href=\"#还有…\" class=\"headerlink\" title=\"还有…\"></a>还有…</h3><p>可能，大概了解清楚PyTorch的安装过程了，毕竟安装过程我也没试过（只试过较老版本的安装），没有看到最终生成的各种文件，仅供参考吧。</p>\n"},{"title":"PyTorch-3","date":"2019-06-18T08:44:44.000Z","_content":"在 [PyTorch-2](PyTorch-2) 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的[官方文档](https://pytorch.org/docs/stable/torch.html)，了解其中各个函数的具体实现。\n# torch 包\n从 `torch/__init__.py` 中可以查看所有的 torch 包的所有字段，包括：\n1. 直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等\n2. 从 torch 包的模块中导入的函数/类，如\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3. 从 torch._C 中导入的字段/函数/类\n4. 从 torch._C._VariableFunctions 导入的字段/函数\n   \nPyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。\n## torch.empty\n这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：\n1. caffe2/CMakeLists.txt 中的文件生成语句为\n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2. 执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，\n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：\n1. 在 caffe2/CMakeLists.txt 中有语句\n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2. 在文件 cmake/Codegen.cmake 中调用 `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n   （在 aten/src/ATen/native/native_functions.yaml 找到 `empty` 的函数签名）\n3. aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件\n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4. 根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件\n   - CMakeLists.txt 中的 add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt 中有，\n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n   事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。\n   \n找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，\n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。\n\n## empty 定义\n我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // 函数签名在vector中的下标\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：\n1. `out` 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor\n2. `out` 参数不为None, 则检查 `out` 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 `out` 的 requires_grad 属性重置为参数 requires_grad\n\n然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，\n```\n// empty 函数调用者提供了 Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty 函数调用者未提供 Tensor 'out'，需要根据参数 options 创建\ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n### 有输出 Tensor\n我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），\n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，\n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n显然，根据输出 Tensor 是否是稀疏的进行不同的处理。\n1. 输出 Tensor 是稀疏的\n   \n   对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n   先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n   其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n   根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。\n2. 输出 Tensor 是密集的\n   \n   Tensor 的 resize_ 方法定义见 TensorMethods.h，为\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n   调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下\n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n   可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，\n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         // 获取 Tensor 的底层实现类对象\n       // 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块\n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，\n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           // 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存\n           // size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等\n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // 未指定步幅，则数据布局是近邻的，连续的，即，stride=1\n           self->set_sizes_contiguous(size);    // 设置当前 size 为新的 size\n           storage_size = self->numel();        // 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize 操作\n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset() 通常返回 0\n           // 只有需要更多的元素数量时，才重新分配内存\n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n   我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，\n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           // 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存\n           // 设置 Tensor 内部存储指向新数据，同时返回旧数据\n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   // 旧数据 size，元素数量\n           storage->set_numel(size);                // 设置新的元素熟路\n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // 内存数据考虑\n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n   从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么\n   1. N1 >= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中\n   2. N1 < N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。\n\n实验验证上述 torch.empty 过程，代码如下，\n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize 到一个较大的 size\nprint(x)\ntorch.empty(1,2,out=x)  # resize 到一个较小的 size\nprint(x)\ntorch.empty(4,4,out=x)  # 再次 resize 到一个较大的 size\n```\n本次输出如下，从以下结果可以看出是符合上述过程的。\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n### 无输出 Tensor\n直接按给定的 size 参数新建一个 Tensor，具体过程略。\n\n# PS\n好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。","source":"_posts/PyTorch-3.md","raw":"---\ntitle: PyTorch-3\ndate: 2019-06-18 16:44:44\ntags: PyTorch\ncategories: DL Framework\n---\n在 [PyTorch-2](PyTorch-2) 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的[官方文档](https://pytorch.org/docs/stable/torch.html)，了解其中各个函数的具体实现。\n# torch 包\n从 `torch/__init__.py` 中可以查看所有的 torch 包的所有字段，包括：\n1. 直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等\n2. 从 torch 包的模块中导入的函数/类，如\n   ```\n   from .random import set_rng_state, get_rng_state, manual_seed, initial_seed\n   ...\n   ```\n3. 从 torch._C 中导入的字段/函数/类\n4. 从 torch._C._VariableFunctions 导入的字段/函数\n   \nPyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。\n## torch.empty\n这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：\n1. caffe2/CMakeLists.txt 中的文件生成语句为\n   ```\n   set(GENERATED_CXX_PYTHON\n     ...\n     \"${TORCH_SRC_DIR}/csrc/autograd/generated/python_torch_functions.cpp\"\n     ...)\n   ...\n   add_custom_command(\n       OUTPUT\n       ${TORCH_GENERATED_CODE}\n       COMMAND\n       \"${PYTHON_EXECUTABLE}\" tools/setup_helpers/generate_code.py\n        ...\n       DEPENDS\n       ...)\n   ```\n2. 执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，\n   ```\n   generate_nn_wrappers\n   gen_autograd_python\n   gen_autograd\n   gen_jit_dispatch\n   ```\n这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：\n1. 在 caffe2/CMakeLists.txt 中有语句\n   ```\n   include(../cmake/Codegen.cmake)\n   ```\n2. 在文件 cmake/Codegen.cmake 中调用 `gen.py`\n   ```\n   SET(GEN_COMMAND\n       \"${PYTHON_EXECUTABLE}\" ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/gen.py\n       --source-path ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen\n       --install_dir ${CMAKE_BINARY_DIR}/aten/src/ATen\n       ${GEN_ROCM_FLAG}\n       ${cwrap_files})\n   ```\n   （在 aten/src/ATen/native/native_functions.yaml 找到 `empty` 的函数签名）\n3. aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件\n   ```\n   file_manager.write(\"Declarations.yaml\", format_yaml(output_declarations))\n   ```\n4. 根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件\n   - CMakeLists.txt 中的 add_subdirectory(caffe2)\n   - caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)\n   - aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)\n   - aten/src/ATen/CMakeLists.txt 中有，\n     ```\n     INSTALL(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n       DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n     ```\n   事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。\n   \n找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，\n```\nPY_VARIABLE_METHOD_VARARGS = CodeTemplate(\"\"\"\\\nstatic PyObject * ${pycname}(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgsParser parser({\n        ${signatures}\n    }, /*traceable=*/${traceable});\n    ${unpack_self}\n    ParserArgs<${max_args}> parsed_args;\n    auto r = parser.parse(args, kwargs, parsed_args);\n    ${declare_namedtuple_return_types}\n    ${dispatch}\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n\"\"\")\n...\ndef create_python_bindings(python_functions, has_self, is_module=False):\n    def process_function(name, declarations):\n        ...\n        env = {\n            'name': name,\n            'dispatch_name': 'dispatch_{}'.format(name),\n            'pycname': 'THPVariable_{}'.format(name),\n            'signature': [],\n            'max_args': max(len(o['arguments'])+len(o['python_binding_arguments']) for o in declarations),\n            'unpack_self': [],\n            'dispatch': [],\n            'declare_namedtuple_return_types': '',\n        }\n        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value\n        if len(declarations) == 1 and len(declarations[0]['args']) == 1 and has_self:\n            ...\n        else:\n            tmpl = PY_VARIABLE_METHOD_VARARGS\n            env['flags'] = 'METH_VARARGS | METH_KEYWORDS'\n        if not is_module and not has_self:\n            env['flags'] += ' | METH_STATIC'\n        \n        py_methods.append(tmpl.substitute(env))\n        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))\n```\n通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。\n\n## empty 定义\n我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）\n```\nstatic PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)\n{\n    HANDLE_TH_ERRORS\n    static PythonArgParser parser({\n        \"empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n    }, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector\n    ParseArgs<6> parsed_args;\n    auto r = parser.parse(args, kwargs, parseed_args);\n    if (r.idx == 0) {       // 函数签名在vector中的下标\n        if (r.isNone(1)) {  // parameter 'out' is None\n            auto size = r.intlist(0);\n            auto dtype = r.scalartype(2);\n            auto device = r.device(4);\n            const auto options = TensorOptions()\n                .dtype(dtype)\n                .device(device)\n                .layout(r.layout(3).layout)\n                .requires_grad(r.toBool(5));\n            return wrap(dispatch_empty(size, options));\n        } else {\n            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),\n                                   r.layout(3), r.isNone(3),\n                                   r.device(4), r.isNone(4));\n            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));\n        }\n    }\n    Py_RETURN_NONE;\n    END_HANDLE_TH_ERRORS\n}\n```\n从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：\n1. `out` 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor\n2. `out` 参数不为None, 则检查 `out` 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 `out` 的 requires_grad 属性重置为参数 requires_grad\n\n然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，\n```\n// empty 函数调用者提供了 Tensor 'out'\ninline Tensor dispatch_empty(IntList size, Tensor result) {\n    AutoNoGIL no_gil;\n    return at::empty_out(result, size);\n}\n// empty 函数调用者未提供 Tensor 'out'，需要根据参数 options 创建\ninline Tensor dispatch_empty(IntList size, const TensorOptions & options) {\n    maybe_initialize_cuda(options);\n    AutoNoGIL no_gil;\n    return torch::empty(size, options);\n}\n```\n### 有输出 Tensor\n我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为\n```\nAutoNoGIL() : save(PyEval_SaveThread()) {}\n```\n可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，\n```\n~AutoNoGIL() {\n    PyEval_RestoreThread(save);\n}\n```\n然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，\n```\nstatic inline Tensor & empty_out(Tensor & result, IntList size) {\n    return detail::infer_type(result).empty_out(result, size);\n}\n```\n在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），\n```\nfile_manager.write('Functions.h', FUNCTIONS_H, top_env)\n```\n现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，\n```\nTensor & TypeDefault::empty_out(Tensor & result, IntList size) const {\n    return at::native::empty_out(/* native_actuals */ result, size);\n}\n```\n首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，\n```\nnamespace at {\nnamespace native {\n...\nTensor& empty_out(Tensor& result, IntList size) {\n    if (result.is_sparse()) {\n        result.sparse_resize_and_clear_(size, size.size(), 0);\n    } else {\n        result.resize_(size);\n    }\n    return result;\n}\n...\n}\n}\n```\n显然，根据输出 Tensor 是否是稀疏的进行不同的处理。\n1. 输出 Tensor 是稀疏的\n   \n   对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，\n   ```\n   inline Tensor & Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) {\n       return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);\n   }\n   ```\n   先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为\n   ```\n   Tensor & SparseCPUByteType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {\n       const OptionalDeviceGuard device_guard(device_of(self));\n       return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);\n   }\n   ```\n   其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，\n   ```\n   SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparse_dim, int64_t dense_dim) {\n       get_sparse_impl(self)->resize_and_clear_(sparse_dim, dense_dim, size);\n       return self;\n   }\n   ```\n   根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。\n2. 输出 Tensor 是密集的\n   \n   Tensor 的 resize_ 方法定义见 TensorMethods.h，为\n   ```\n   inline Tensor & Tensor::resize_(IntList size) {\n       return type().resize_(*this, size);\n   }\n   ```\n   调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下\n   ```c++\n   Tensor & CPUByteType::resize_(Tensor & self, IntList size) const {\n       return at::native::resize_cpu_(/* actuals */ self, size);\n   }\n   ```\n   可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，\n   ```c++\n   Tensor& resize_cpu_(Tensor& self, IntList size) {\n       auto* self = self.unsafeGetTensorImpl();         // 获取 Tensor 的底层实现类对象\n       // 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块\n       resize_impl_cpu_(self_, size, c10::nullopt);     \n       self_->maybe_zero_dim(size.size()==0);\n       return self;\n   }\n   ```\n   上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，\n   ```c++\n   inline TensorImpl* resize_impl_cpu_(\n       TensorImpl* self,\n       IntList size,\n       c10::optional<IntList> stride) {\n       if (self->sizes() == size && (!stride || self->strides() == stride)) {\n           // 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存\n           // size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等\n           return self;\n       }\n       int64_t storage_size = 1;\n       ...\n       if(!stride){     // 未指定步幅，则数据布局是近邻的，连续的，即，stride=1\n           self->set_sizes_contiguous(size);    // 设置当前 size 为新的 size\n           storage_size = self->numel();        // 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3\n       }\n       maybe_resize_storage_cpu(self, storage_size);    // resize 操作\n   }\n   \n   static inline void maybe_resize_storage_cpu(TensorImpl* self, int64_t new_size) {\n       ...\n       if (new_size+self->storage_offset() > self->storage().numel()) {\n           // self->storage_offset() 通常返回 0\n           // 只有需要更多的元素数量时，才重新分配内存\n           THStorage_resize(THTensor_getStoragePtr(self), new_size+self->storage_offset());\n       }\n   }\n   ```\n   我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，\n   ```c++\n   void THStorage_resize(THStorage* storage, ptrdiff_t size) {\n       if (storage->resizable()) {\n           at::DataPtr new_data;\n           if (size != 0) {\n               new_data = storage->allocator()->allocate(storage->itemsize()*size);\n           }\n           // 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存\n           // 设置 Tensor 内部存储指向新数据，同时返回旧数据\n           at::DataPtr old_data = storage->set_data_ptr(std::move(new_data));\n           ptrdiff_t old_size = storage->numel();   // 旧数据 size，元素数量\n           storage->set_numel(size);                // 设置新的元素熟路\n           if (old_data != nullptr) {\n               ptrdiff_t copy_size = old_size;\n               if (storage->numel() < copy_size) {\n                   copy_size = storage_numel();\n               }\n               if (copy_size > 0) {                 // 内存数据考虑\n                   memcpy(\n                       storage->data(),\n                       old_data.get(),\n                       storage->itemsize() * copy_size);\n               }\n           }\n       }\n       ...\n   }\n   ```\n   从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么\n   1. N1 >= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中\n   2. N1 < N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。\n\n实验验证上述 torch.empty 过程，代码如下，\n```python\nimport torch\n\nx=torch.rand(3,4)\nprint(x)\ntorch.empty(4,5,out=x)  # resize 到一个较大的 size\nprint(x)\ntorch.empty(1,2,out=x)  # resize 到一个较小的 size\nprint(x)\ntorch.empty(4,4,out=x)  # 再次 resize 到一个较大的 size\n```\n本次输出如下，从以下结果可以看出是符合上述过程的。\n```\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694]])\ntensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],\n        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],\n        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\ntensor([[0.0446, 0.1545]])\ntensor([[0.0446, 0.1545, 0.5059, 0.6027],\n        [0.4872, 0.4557, 0.1010, 0.2962],\n        [0.0576, 0.1087, 0.3033, 0.4694],\n        [0.0000, 0.0000,    nan, 0.0000]])\n```\n### 无输出 Tensor\n直接按给定的 size 参数新建一个 Tensor，具体过程略。\n\n# PS\n好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。","slug":"PyTorch-3","published":1,"updated":"2019-07-10T08:03:21.154Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5pz30017xgvcuhn4ej3y","content":"<p>在 <a href=\"PyTorch-2\">PyTorch-2</a> 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的<a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>，了解其中各个函数的具体实现。</p>\n<h1 id=\"torch-包\"><a href=\"#torch-包\" class=\"headerlink\" title=\"torch 包\"></a>torch 包</h1><p>从 <code>torch/__init__.py</code> 中可以查看所有的 torch 包的所有字段，包括：</p>\n<ol>\n<li><p>直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等</p>\n</li>\n<li><p>从 torch 包的模块中导入的函数/类，如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>从 torch._C 中导入的字段/函数/类</p>\n</li>\n<li><p>从 torch._C._VariableFunctions 导入的字段/函数</p>\n</li>\n</ol>\n<p>PyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。</p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p>这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：</p>\n<ol>\n<li><p>caffe2/CMakeLists.txt 中的文件生成语句为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;/csrc/autograd/generated/python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools/setup_helpers/generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：</p>\n<ol>\n<li><p>在 caffe2/CMakeLists.txt 中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(../cmake/Codegen.cmake)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在文件 cmake/Codegen.cmake 中调用 <code>gen.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen/gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>（在 aten/src/ATen/native/native_functions.yaml 找到 <code>empty</code> 的函数签名）</p>\n</li>\n<li><p>aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件</p>\n<ul>\n<li>CMakeLists.txt 中的 add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt 中有，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen/Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;/ATen)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。</p>\n</li>\n</ol>\n<p>找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS = CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, /*traceable=*/$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module=False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env = &#123;</span><br><span class=\"line\">            &apos;name&apos;: name,</span><br><span class=\"line\">            &apos;dispatch_name&apos;: &apos;dispatch_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;pycname&apos;: &apos;THPVariable_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;signature&apos;: [],</span><br><span class=\"line\">            &apos;max_args&apos;: max(len(o[&apos;arguments&apos;])+len(o[&apos;python_binding_arguments&apos;]) for o in declarations),</span><br><span class=\"line\">            &apos;unpack_self&apos;: [],</span><br><span class=\"line\">            &apos;dispatch&apos;: [],</span><br><span class=\"line\">            &apos;declare_namedtuple_return_types&apos;: &apos;&apos;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value</span><br><span class=\"line\">        if len(declarations) == 1 and len(declarations[0][&apos;args&apos;]) == 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl = PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&apos;flags&apos;] = &apos;METH_VARARGS | METH_KEYWORDS&apos;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&apos;flags&apos;] += &apos; | METH_STATIC&apos;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n\n<p>通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。</p>\n<h2 id=\"empty-定义\"><a href=\"#empty-定义\" class=\"headerlink\" title=\"empty 定义\"></a>empty 定义</h2><p>我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)&quot;,</span><br><span class=\"line\">    &#125;, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx == 0) &#123;       // 函数签名在vector中的下标</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  // parameter &apos;out&apos; is None</span><br><span class=\"line\">            auto size = r.intlist(0);</span><br><span class=\"line\">            auto dtype = r.scalartype(2);</span><br><span class=\"line\">            auto device = r.device(4);</span><br><span class=\"line\">            const auto options = TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：</p>\n<ol>\n<li><code>out</code> 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor</li>\n<li><code>out</code> 参数不为None, 则检查 <code>out</code> 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 <code>out</code> 的 requires_grad 属性重置为参数 requires_grad</li>\n</ol>\n<p>然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// empty 函数调用者提供了 Tensor &apos;out&apos;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">// empty 函数调用者未提供 Tensor &apos;out&apos;，需要根据参数 options 创建</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"有输出-Tensor\"><a href=\"#有输出-Tensor\" class=\"headerlink\" title=\"有输出 Tensor\"></a>有输出 Tensor</h3><p>我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&apos;Functions.h&apos;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n\n<p>现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(/* native_actuals */ result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>显然，根据输出 Tensor 是否是稀疏的进行不同的处理。</p>\n<ol>\n<li><p>输出 Tensor 是稀疏的</p>\n<p>对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。</p>\n</li>\n<li><p>输出 Tensor 是密集的</p>\n<p>Tensor 的 resize_ 方法定义见 TensorMethods.h，为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; CPUByteType::resize_(Tensor &amp; self, IntList size) <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">// 获取 Tensor 的底层实现类对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块</span></span><br><span class=\"line\">    resize_impl_cpu_(self_, size, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(size.size()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == size &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存</span></span><br><span class=\"line\">        <span class=\"comment\">// size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// 未指定步幅，则数据布局是近邻的，连续的，即，stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(size);    <span class=\"comment\">// 设置当前 size 为新的 size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">// 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize 操作</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset() 通常返回 0</span></span><br><span class=\"line\">        <span class=\"comment\">// 只有需要更多的元素数量时，才重新分配内存</span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*size);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存</span></span><br><span class=\"line\">        <span class=\"comment\">// 设置 Tensor 内部存储指向新数据，同时返回旧数据</span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::move(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">// 旧数据 size，元素数量</span></span><br><span class=\"line\">        storage-&gt;set_numel(size);                <span class=\"comment\">// 设置新的元素熟路</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// 内存数据考虑</span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.get(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么</p>\n<ol>\n<li>N1 &gt;= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中</li>\n<li>N1 &lt; N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。</li>\n</ol>\n</li>\n</ol>\n<p>实验验证上述 torch.empty 过程，代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize 到一个较大的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize 到一个较小的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\"># 再次 resize 到一个较大的 size</span></span><br></pre></td></tr></table></figure>\n\n<p>本次输出如下，从以下结果可以看出是符合上述过程的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"无输出-Tensor\"><a href=\"#无输出-Tensor\" class=\"headerlink\" title=\"无输出 Tensor\"></a>无输出 Tensor</h3><p>直接按给定的 size 参数新建一个 Tensor，具体过程略。</p>\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p>好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在 <a href=\"PyTorch-2\">PyTorch-2</a> 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的<a href=\"https://pytorch.org/docs/stable/torch.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>，了解其中各个函数的具体实现。</p>\n<h1 id=\"torch-包\"><a href=\"#torch-包\" class=\"headerlink\" title=\"torch 包\"></a>torch 包</h1><p>从 <code>torch/__init__.py</code> 中可以查看所有的 torch 包的所有字段，包括：</p>\n<ol>\n<li><p>直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等</p>\n</li>\n<li><p>从 torch 包的模块中导入的函数/类，如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>从 torch._C 中导入的字段/函数/类</p>\n</li>\n<li><p>从 torch._C._VariableFunctions 导入的字段/函数</p>\n</li>\n</ol>\n<p>PyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。</p>\n<h2 id=\"torch-empty\"><a href=\"#torch-empty\" class=\"headerlink\" title=\"torch.empty\"></a>torch.empty</h2><p>这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：</p>\n<ol>\n<li><p>caffe2/CMakeLists.txt 中的文件生成语句为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(GENERATED_CXX_PYTHON</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &quot;$&#123;TORCH_SRC_DIR&#125;/csrc/autograd/generated/python_torch_functions.cpp&quot;</span><br><span class=\"line\">  ...)</span><br><span class=\"line\">...</span><br><span class=\"line\">add_custom_command(</span><br><span class=\"line\">    OUTPUT</span><br><span class=\"line\">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class=\"line\">    COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools/setup_helpers/generate_code.py</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    DEPENDS</span><br><span class=\"line\">    ...)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">generate_nn_wrappers</span><br><span class=\"line\">gen_autograd_python</span><br><span class=\"line\">gen_autograd</span><br><span class=\"line\">gen_jit_dispatch</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<p>这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：</p>\n<ol>\n<li><p>在 caffe2/CMakeLists.txt 中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include(../cmake/Codegen.cmake)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在文件 cmake/Codegen.cmake 中调用 <code>gen.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET(GEN_COMMAND</span><br><span class=\"line\">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen/gen.py</span><br><span class=\"line\">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;/../aten/src/ATen</span><br><span class=\"line\">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen</span><br><span class=\"line\">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class=\"line\">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>（在 aten/src/ATen/native/native_functions.yaml 找到 <code>empty</code> 的函数签名）</p>\n</li>\n<li><p>aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件</p>\n<ul>\n<li>CMakeLists.txt 中的 add_subdirectory(caffe2)</li>\n<li>caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)</li>\n<li>aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)</li>\n<li>aten/src/ATen/CMakeLists.txt 中有，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;/aten/src/ATen/Declarations.yaml</span><br><span class=\"line\">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;/ATen)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。</p>\n</li>\n</ol>\n<p>找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PY_VARIABLE_METHOD_VARARGS = CodeTemplate(&quot;&quot;&quot;\\</span><br><span class=\"line\">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgsParser parser(&#123;</span><br><span class=\"line\">        $&#123;signatures&#125;</span><br><span class=\"line\">    &#125;, /*traceable=*/$&#123;traceable&#125;);</span><br><span class=\"line\">    $&#123;unpack_self&#125;</span><br><span class=\"line\">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parsed_args);</span><br><span class=\"line\">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class=\"line\">    $&#123;dispatch&#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&quot;&quot;&quot;)</span><br><span class=\"line\">...</span><br><span class=\"line\">def create_python_bindings(python_functions, has_self, is_module=False):</span><br><span class=\"line\">    def process_function(name, declarations):</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        env = &#123;</span><br><span class=\"line\">            &apos;name&apos;: name,</span><br><span class=\"line\">            &apos;dispatch_name&apos;: &apos;dispatch_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;pycname&apos;: &apos;THPVariable_&#123;&#125;&apos;.format(name),</span><br><span class=\"line\">            &apos;signature&apos;: [],</span><br><span class=\"line\">            &apos;max_args&apos;: max(len(o[&apos;arguments&apos;])+len(o[&apos;python_binding_arguments&apos;]) for o in declarations),</span><br><span class=\"line\">            &apos;unpack_self&apos;: [],</span><br><span class=\"line\">            &apos;dispatch&apos;: [],</span><br><span class=\"line\">            &apos;declare_namedtuple_return_types&apos;: &apos;&apos;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ... // 向 env 增加 key-value pair or 更新 env 中已有 key 的 value</span><br><span class=\"line\">        if len(declarations) == 1 and len(declarations[0][&apos;args&apos;]) == 1 and has_self:</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            tmpl = PY_VARIABLE_METHOD_VARARGS</span><br><span class=\"line\">            env[&apos;flags&apos;] = &apos;METH_VARARGS | METH_KEYWORDS&apos;</span><br><span class=\"line\">        if not is_module and not has_self:</span><br><span class=\"line\">            env[&apos;flags&apos;] += &apos; | METH_STATIC&apos;</span><br><span class=\"line\">        </span><br><span class=\"line\">        py_methods.append(tmpl.substitute(env))</span><br><span class=\"line\">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>\n\n<p>通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。</p>\n<h2 id=\"empty-定义\"><a href=\"#empty-定义\" class=\"headerlink\" title=\"empty 定义\"></a>empty 定义</h2><p>我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    HANDLE_TH_ERRORS</span><br><span class=\"line\">    static PythonArgParser parser(&#123;</span><br><span class=\"line\">        &quot;empty(IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)&quot;,</span><br><span class=\"line\">    &#125;, /*tracebalbe*/true); // 大括号初始化器，得到函数签名的vector</span><br><span class=\"line\">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class=\"line\">    auto r = parser.parse(args, kwargs, parseed_args);</span><br><span class=\"line\">    if (r.idx == 0) &#123;       // 函数签名在vector中的下标</span><br><span class=\"line\">        if (r.isNone(1)) &#123;  // parameter &apos;out&apos; is None</span><br><span class=\"line\">            auto size = r.intlist(0);</span><br><span class=\"line\">            auto dtype = r.scalartype(2);</span><br><span class=\"line\">            auto device = r.device(4);</span><br><span class=\"line\">            const auto options = TensorOptions()</span><br><span class=\"line\">                .dtype(dtype)</span><br><span class=\"line\">                .device(device)</span><br><span class=\"line\">                .layout(r.layout(3).layout)</span><br><span class=\"line\">                .requires_grad(r.toBool(5));</span><br><span class=\"line\">            return wrap(dispatch_empty(size, options));</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class=\"line\">                                   r.layout(3), r.isNone(3),</span><br><span class=\"line\">                                   r.device(4), r.isNone(4));</span><br><span class=\"line\">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_RETURN_NONE;</span><br><span class=\"line\">    END_HANDLE_TH_ERRORS</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：</p>\n<ol>\n<li><code>out</code> 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor</li>\n<li><code>out</code> 参数不为None, 则检查 <code>out</code> 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 <code>out</code> 的 requires_grad 属性重置为参数 requires_grad</li>\n</ol>\n<p>然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// empty 函数调用者提供了 Tensor &apos;out&apos;</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return at::empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">// empty 函数调用者未提供 Tensor &apos;out&apos;，需要根据参数 options 创建</span><br><span class=\"line\">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class=\"line\">    maybe_initialize_cuda(options);</span><br><span class=\"line\">    AutoNoGIL no_gil;</span><br><span class=\"line\">    return torch::empty(size, options);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"有输出-Tensor\"><a href=\"#有输出-Tensor\" class=\"headerlink\" title=\"有输出 Tensor\"></a>有输出 Tensor</h3><p>我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~AutoNoGIL() &#123;</span><br><span class=\"line\">    PyEval_RestoreThread(save);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class=\"line\">    return detail::infer_type(result).empty_out(result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file_manager.write(&apos;Functions.h&apos;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>\n\n<p>现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class=\"line\">    return at::native::empty_out(/* native_actuals */ result, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace at &#123;</span><br><span class=\"line\">namespace native &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class=\"line\">    if (result.is_sparse()) &#123;</span><br><span class=\"line\">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        result.resize_(size);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>显然，根据输出 Tensor 是否是稀疏的进行不同的处理。</p>\n<ol>\n<li><p>输出 Tensor 是稀疏的</p>\n<p>对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class=\"line\">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class=\"line\">    return at::native::sparse_resize_and_clear_(/* actuals */ self, size, sparse_dim, dense_dim);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class=\"line\">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class=\"line\">    return self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。</p>\n</li>\n<li><p>输出 Tensor 是密集的</p>\n<p>Tensor 的 resize_ 方法定义见 TensorMethods.h，为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class=\"line\">    return type().resize_(*this, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor &amp; CPUByteType::resize_(Tensor &amp; self, IntList size) <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> at::native::resize_cpu_(<span class=\"comment\">/* actuals */</span> self, size);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Tensor&amp; <span class=\"title\">resize_cpu_</span><span class=\"params\">(Tensor&amp; self, IntList size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">auto</span>* self = self.unsafeGetTensorImpl();         <span class=\"comment\">// 获取 Tensor 的底层实现类对象</span></span><br><span class=\"line\">    <span class=\"comment\">// 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块</span></span><br><span class=\"line\">    resize_impl_cpu_(self_, size, c10::nullopt);     </span><br><span class=\"line\">    self_-&gt;maybe_zero_dim(size.size()==<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">inline</span> TensorImpl* <span class=\"title\">resize_impl_cpu_</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    TensorImpl* self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    IntList size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (self-&gt;sizes() == size &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存</span></span><br><span class=\"line\">        <span class=\"comment\">// size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int64_t</span> storage_size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!stride)&#123;     <span class=\"comment\">// 未指定步幅，则数据布局是近邻的，连续的，即，stride=1</span></span><br><span class=\"line\">        self-&gt;set_sizes_contiguous(size);    <span class=\"comment\">// 设置当前 size 为新的 size</span></span><br><span class=\"line\">        storage_size = self-&gt;numel();        <span class=\"comment\">// 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    maybe_resize_storage_cpu(self, storage_size);    <span class=\"comment\">// resize 操作</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> <span class=\"title\">maybe_resize_storage_cpu</span><span class=\"params\">(TensorImpl* self, <span class=\"keyword\">int64_t</span> new_size)</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// self-&gt;storage_offset() 通常返回 0</span></span><br><span class=\"line\">        <span class=\"comment\">// 只有需要更多的元素数量时，才重新分配内存</span></span><br><span class=\"line\">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">THStorage_resize</span><span class=\"params\">(THStorage* storage, <span class=\"keyword\">ptrdiff_t</span> size)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storage-&gt;resizable()) &#123;</span><br><span class=\"line\">        at::DataPtr new_data;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*size);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存</span></span><br><span class=\"line\">        <span class=\"comment\">// 设置 Tensor 内部存储指向新数据，同时返回旧数据</span></span><br><span class=\"line\">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class=\"built_in\">std</span>::move(new_data));</span><br><span class=\"line\">        <span class=\"keyword\">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class=\"comment\">// 旧数据 size，元素数量</span></span><br><span class=\"line\">        storage-&gt;set_numel(size);                <span class=\"comment\">// 设置新的元素熟路</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (old_data != <span class=\"literal\">nullptr</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">ptrdiff_t</span> copy_size = old_size;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class=\"line\">                copy_size = storage_numel();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (copy_size &gt; <span class=\"number\">0</span>) &#123;                 <span class=\"comment\">// 内存数据考虑</span></span><br><span class=\"line\">                <span class=\"built_in\">memcpy</span>(</span><br><span class=\"line\">                    storage-&gt;data(),</span><br><span class=\"line\">                    old_data.get(),</span><br><span class=\"line\">                    storage-&gt;itemsize() * copy_size);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么</p>\n<ol>\n<li>N1 &gt;= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中</li>\n<li>N1 &lt; N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。</li>\n</ol>\n</li>\n</ol>\n<p>实验验证上述 torch.empty 过程，代码如下，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">5</span>,out=x)  <span class=\"comment\"># resize 到一个较大的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">1</span>,<span class=\"number\">2</span>,out=x)  <span class=\"comment\"># resize 到一个较小的 size</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\">torch.empty(<span class=\"number\">4</span>,<span class=\"number\">4</span>,out=x)  <span class=\"comment\"># 再次 resize 到一个较大的 size</span></span><br></pre></td></tr></table></figure>\n\n<p>本次输出如下，从以下结果可以看出是符合上述过程的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class=\"line\">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class=\"line\">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class=\"line\">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class=\"line\">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545]])</span><br><span class=\"line\">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class=\"line\">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class=\"line\">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class=\"line\">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"无输出-Tensor\"><a href=\"#无输出-Tensor\" class=\"headerlink\" title=\"无输出 Tensor\"></a>无输出 Tensor</h3><p>直接按给定的 size 参数新建一个 Tensor，具体过程略。</p>\n<h1 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h1><p>好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。</p>\n"},{"title":"PyTorch-2","date":"2019-06-13T02:19:52.000Z","_content":"# torch installization\n依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是\n```\nimport torch\n```\n于是阅读torch/__init__.py，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。__\n\n__init__.py除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用\n{\n  return initModule();\n}\n```\n根据python3扩展库的规则可知，`import torch._C` ，调用PyInit__C函数（调用名为PyInit_&lt;package>的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。\n\nshm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。\n\ninitModule方法定义在torch/csrc/Module.cpp，\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // 声明并定义模块初始化函数\n  // 向methods中添加方法定义\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // 真正的扩展模块定义\n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // 扩展模块名\n    nullptr,                           \n    -1,\n    methods.data()                       // 模块中的方法定义\n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // 创建模块并确保创建成功\n  // 对模块进行各种初始化\n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // 执行cuda相关的初始化\n#endif\n  ...\n  // 定义模块的属性设置函数，setter\n  // 属性名为name，值为v，incref表示是否对值对象增加引用计数\n  // 设置成功返回1，否则返回0\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // 设置模块属性\n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // 向模块添加方法\n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // 设置模块其他属性\n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  // 增加 _THNN 属性\n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\n从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。\n# methods/members in torch._C\n- 使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括\n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  同上类似\n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- 生成模块torch._C 后再向其添加如下成员：\n\n    - 向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。\n\n        torch._C._TensorBase这个类型具有属性\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        并且具有以下方法\n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        类型torch._C._FunctionBase， 这个类型具有方法和属性为\n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n        不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。\n\n    - 向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。\n\n    - 向torch._C添加模块， _nn，cpp，_onnx。\n\n    - 向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。\n\n注意到从Module.cpp文件中头文件引用：\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\n可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。\n\n一方面，头文件 TH/TH.h 中引用了#include <TH/THGeneral.h>，在aten/src/TH目录下的CMakeLists.txt中有这么一行\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\n在THGeneral.h中有如下宏定义\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND\n```\n另一方面，torch/csrc/THP.h 中引用了#include <torch/src/Storage.h>，在这个Storage.h中有如下语句\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\n而文件TH/THGenerateAllType.h则包含语句\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有\n```\n// 此时 TH_GENERIC_FILE是已定义的\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义\n```\n以TH/THGenerateFloatType.h为例说明，此文件中有语句\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换\n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n类似地，#include <TH/THGenerateDoubleType.h>，则得到THPDoubleStorage_init，\n\n#include <TH/THGenerateIntTypes.h> 得到\n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n对4组include中的其他三组，则得到\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\n以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init 函数声明\n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\n上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\n在TH/THGeneral.h中存在宏定义\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\n由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为\"FloatStorageBase\"，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。\n\n以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\n即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。\n\n其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。\n\ntorch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。\n\n现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\n可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\n（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）\n\n然后查看类创建 THPStorage_(pynew) 的定义\n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数\n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // 分配内存，让self指向这个内存块\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // 获取参数allocator的值\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      // 转为 c10::Allocator 指针\n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // 提供了cdata值\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // 返回THPStorage指针\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // 未提供cdata值，则需要创建THWStorage类型实例\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     // 使用其他方法设置 self->cdata\n}   \n```\n从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义\n```\n#define THWStorage THStorage\n```\n转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n在 TH/generic/THStorage.h 中找到宏定义\n```\n#define THStorage at::StorageImpl\n```\n在 c10/core/StorageImpl.h 中找到结构定义\n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // 数据类型\n  DataPtr data_ptr_;             // 数据指针\n  int64_t numel_;                // 数据数量\n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // 数据的内存分配器\n};\n}\n```\n所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为\n```\nnew                // 新建THStorage，未指定 size，即size=0，使用默认Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // 新建THStorage，指定 size，使用默认Allocator\nnewWithAllocator   // 新建THStorage，指定 size 和 Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n此外有宏定义\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\n函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。\n\n（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）\n\n以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // 新建scalar_t 类型\n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\n从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\n我们看上上个代码片段中StorageImpl构造函数的实参，\n\n首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）\n```\n#define scalar_t float\n```\n于是，\n```\ncaffe2::TypeMeta::Make<scalar_t>()    // 假设 THQUANTIZED 未定义\n```\ncaffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n经过宏替换，得到 _typeMetaDataInstance的模板函数定义\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n还有一组宏，用于生成模板特例化，\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// 调用\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n对于系统内部变量如 float，得到函数模板特例化的定义\n```\n// 函数声明\nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\n另外，在c10/util/typeid.cpp中有如下调用\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n经过宏替换得到\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n于是函数模板特例化最终形式为，\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，\n```\nstruct TypeMetaData final {\n// 函数类型的别名\nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // 占位new\nusing Copy = void(const void*, void*, size_t);  // 类型数组拷贝\nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //构造函数\n\nsize_t itemsize_;  // 类型占多少字节\nNew* new_;\nPlacementNew* placementNew_;   // 定位放置 new\nCopy* copy_;        // 类型拷贝\nDelete* delete_;    // 类型析构\nTypeIdentifier id_; // 类型全局唯一id\nconst char* name_;  // 类型名称\n};\n```\n我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义\n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // 类型T占多少字节\n          _PickNew<T>(),             // 通过 new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // 获取类型的全局唯一id，\n          typeName};                 // 类型名称，例如float的名称为\"float\"\n```\n构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用\n```\nTypeIdentifier::Get<T>()\n```\n在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。\n\n对于其他类型如 int，double，int64_t等类似处理。\n\nPyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。\n\n至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为\n```\nsize,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）\ngetTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配\ntrue                      // 被构造的StorageImpl可以resize\n```\n创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase\n\n记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。\n\nFloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。\n\n类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\n函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。\n\n`torch._C`模块初始化过程到这里就完成了。回到 `torch/__init__.py`，继续看看 import torch时接下来做了哪些事情：\n\n1. 定义了模块函数 typename，is_tensor，is_storage等\n2. 导入torch下其他子模块\n3. 调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理\n4. 调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：\n    - 初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；\n    - 初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；\n    - 初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；\n    - 初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor\n    - 共享内存管理初始化，设置文件路径；\n    - 执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n        其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。\n\n到这里，import torch 的工作全部完成。\n\n# 后记：\n初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。","source":"_posts/PyTorch-2.md","raw":"---\ntitle: PyTorch-2\ndate: 2019-06-13 10:19:52\ntags: PyTorch\ncategories: DL Framework\n---\n# torch installization\n依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是\n```\nimport torch\n```\n于是阅读torch/__init__.py，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，\n```\nold_flags=sys.getdlopenflags()\nsys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)\nfrom torch._C import *\n__all__ += [name for name in dir(_C)\n            if name[0] != '_' and\n            not name.endswith('Base')]\nsys.setdlopenflags(old_flags)\n```\n__将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。__\n\n__init__.py除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，\n```\nextern PyObject* initModule();\nPyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用\n{\n  return initModule();\n}\n```\n根据python3扩展库的规则可知，`import torch._C` ，调用PyInit__C函数（调用名为PyInit_&lt;package>的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。\n\nshm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，\n```\nset(LIBSHM_SUBDIR libshm)\nset(LIBSHM_SRCDIR ${LIBSHM_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\n```\nTORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。\n\ninitModule方法定义在torch/csrc/Module.cpp，\n```\n#ifdef USE_CUDA\nnamespace torch { namespace cuda {\nvoid initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明\n}}\n#endif\n\nstatic std::vector<PyMethodDef> methods;\n\nPyObject* module;\nPyObject* initModule() {                 // 声明并定义模块初始化函数\n  // 向methods中添加方法定义\n  THPUtils_addPyMethodDefs(methods, TorchMethods);\n  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);\n  ...\n  // 真正的扩展模块定义\n  static struct PyModuleDef torchmodule = {\n    PyModuleDef_HEAD_INIT,\n    \"torch._C\",                          // 扩展模块名\n    nullptr,                           \n    -1,\n    methods.data()                       // 模块中的方法定义\n  };\n  ASSERT_TRUE(module = PyModule_Create(&torchmodule)); // 创建模块并确保创建成功\n  // 对模块进行各种初始化\n#ifdef USE_CUDA\n  torch::cuda::initModule(module);       // 执行cuda相关的初始化\n#endif\n  ...\n  // 定义模块的属性设置函数，setter\n  // 属性名为name，值为v，incref表示是否对值对象增加引用计数\n  // 设置成功返回1，否则返回0\n  auto set_module_attr = [&](const char* name, PyObject* v, bool incref = true) \n  {\n    if(incref) {\n      Py_INCREF(v);\n    }\n    return PyModule_AddObject(module, name, v) == 0;\n  }\n  // 设置模块属性\n  ...\n  ASSERT_TRUE(set_module_attr(\"has_cudnn\", has_cudnn));\n  // 向模块添加方法\n  auto py_module = py::reinterpret_borrow<py::module>(module);\n  py_module.def(\"_demangle\", &c10::demangle);\n  py_module.def(\"_log_api_usage_once\", &LogAPIUsageOnceFromPython);\n  ...    // 设置模块其他属性\n  ASSERT_TRUE(set_module_attr(\"default_generator\", \n        (PyObject*)THPDefaultGenerator, false));\n  torch::nn::init__THNN(module);  // 增加 _THNN 属性\n#ifdef USE_CUDA\n  torch::nn::init_THCUDD(module);\n#endif\n  return module;\n  ...\n}\n```\n从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。\n# methods/members in torch._C\n- 使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括\n```\n# TorchMethods \n_initExtension\n_autograd_init\n...\n# DataLoaderMethods \n_set_worker_signal_handlers\n_set_worker_pids\n...\n# torch::autograd::python_functions(), torch/csrc/autograd/init.cpp\nset_grad_enabled\nis_grad_enabled\nset_anomaly_enabled\nis_anomaly_enabled\n# torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp\n_multiprocessing_init\n# torch::distributed::c10d::python_functions()  同上类似\n...\n# THCPModule_method(), torch/csrc/cuda/Module.cpp\n_cuda_init\n_cuda_setDevice\n...\n_nccl_version\n...\n# THCUDNN_method()\n_cudnn_version\n# THDPModule_methods(), torch/csrc/distributed/Module.cpp\n_dist_init_extension\n_dist_init_process_group\n...\n```\n- 生成模块torch._C 后再向其添加如下成员：\n\n    - 向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。\n\n        torch._C._TensorBase这个类型具有属性\n        ```\n        _cdata\n        _version\n        grad_fn\n        _grad_fn\n        is_leaf\n        data\n        _grad\n        grad\n        ...\n        device\n        ndim\n        ```\n        并且具有以下方法\n        ```\n        # variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp\n        __add__\n        __radd__\n        ...\n        apply_\n        byte\n        char\n        contiguous\n        ...\n        where\n        zero_\n        # extra_method\n        _make_subclass\n        ```\n        类型torch._C._FunctionBase， 这个类型具有方法和属性为\n        ```\n        # method\n        apply\n        _do_forward\n        _do_backward\n        _register_hook_dict\n        register_hook\n        # property\n        saved_tensors\n        saved_variables\n        ...\n        requires_grad\n        metadata\n        ```\n        不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。\n\n    - 向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。\n\n    - 向torch._C添加模块， _nn，cpp，_onnx。\n\n    - 向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。\n\n# some installization w.r.t. torch._C\n### THPxxxStorage_init\ntorch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。\n\n注意到从Module.cpp文件中头文件引用：\n```\n#include <TH/TH.h>               // TH=TorcH\n#include <c10/util/Logging.h>\n#include <ATen/ATen.h>\n...\n#include <torch/csrc/THP.h>      // THP=TorcH Python\n...\n```\n可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。\n\n一方面，头文件 TH/TH.h 中引用了#include <TH/THGeneral.h>，在aten/src/TH目录下的CMakeLists.txt中有这么一行\n```\nCONFIGURE_FILE(THGeneral.h.in \"${CMAKE_CURRENT_BINARY_DIR}/THGeneral.h\")\n```\n在THGeneral.h中有如下宏定义\n```\n#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w\n#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND\n```\n另一方面，torch/csrc/THP.h 中引用了#include <torch/src/Storage.h>，在这个Storage.h中有如下语句\n```\n#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)\n...\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateAllType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.h>\n#include <TH/THGenerateQTypes.h>\n```\n上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.h\"         // (0)\n#else\n...\nbool THPStorage_(init)(PyObject *module);                      // (1)\n...\n#endif\n```\n而文件TH/THGenerateAllType.h则包含语句\n```\n#include <TH/THGenerateFloatTypes.h>\n#include <TH/THGenerateIntTypes.h>\n...\n#undef TH_GENERIC_FILE\n```\n4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有\n```\n// 此时 TH_GENERIC_FILE是已定义的\n#include <TH/THGenerateFloatType.h>\n#include <TH/THGenerateDoubleType.h>\n#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义\n```\n以TH/THGenerateFloatType.h为例说明，此文件中有语句\n```\n#define Real Float\n...\n#line 1 TH_GENERIC_FILE\n#include TH_GENERIC_FILE         // (2)\n...\n#undef Real\n```\n注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换\n```\nbool THPStorage_(init)(PyObject *module);  ->\nbool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    ->\nbool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   ->\nbool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); ->\nbool THPFloatStorage_init(PyObject *module);\n```\n类似地，#include <TH/THGenerateDoubleType.h>，则得到THPDoubleStorage_init，\n\n#include <TH/THGenerateIntTypes.h> 得到\n```\nTHPByteStorage_init\nTHPCharStorage_init\nTHPShortStorage_init\nTHPIntStorage_init\nTHPLongStorage_init\n```\n对4组include中的其他三组，则得到\n```\nTHPHalfStorage_init\nTHPBoolStorage_init\nTHPQUInt8Storage_init\nTHPQInt8Storage_init\nTHPQInt32Storage_init\n```\n以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含\n```\n#include <TH/THStorageFunctions.hpp>\n#include <torch/csrc/THP.h>                   // include THPxxxStorage_init 函数声明\n...\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateAllTypes.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateHalfType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateBoolType.h>\n\n#include <torch/csrc/generic/Storage.cpp>\n#include <TH/THGenerateQTypes.h>\n```\n又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，\n```\n#ifndef TH_GENERIC_FILE\n#define TH_GENERIC_FILE \"torch/csrc/generic/Storage.cpp\"              // (11)\n#else\n...                                                                   // (12)\nbool THPStorage_(init)(PyObject *module)\n{\n  static std::vector<PyMethodDef> methods;\n  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));\n#ifndef THD_GENERIC_FILE\n  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);\n#endif\n  \n  THPStorageType.tp_methods = methods.data();\n  THPStorageType.tp_members = THPStorage_(members);\n  THPStorageType.tp_getset = THPStorage_(properties);\n  if (PyType_Ready(&THPStorageType) < 0)\n    return false;\n  Py_INCREF(&THPStorageType);\n  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&THPStorageType);\n  THPStorage_(initCopyMethods)();\n  return true;\n}\n```\n上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏\n```\n#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)\n```\n在TH/THGeneral.h中存在宏定义\n```\n#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)\n#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y\n```\n由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为\"FloatStorageBase\"，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。\n\n以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据\n```\n#define Real Float\n...\n#include TH_GENERIC_FILE\n```\n即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。\n\n其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。\n\ntorch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。\n\n现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，\n```\nPyTypeObject THPStorageType = {\n  PyVarObject_HEAD_INIT(nullptr, 0)\n  \"torch._C.\" THPStorageBaseStr,               /* tp_name */\n  sizeof(THPStorage),                          /* tp_basicsize */\n  ...\n  THPStorage_(pynew),                          /* tp_new */\n}\n```\n可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义\n```\nstruct THPStorage {\n  PyObject_HEAD\n  THWStorage *cdata;\n};\n```\n（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）\n\n然后查看类创建 THPStorage_(pynew) 的定义\n```\nstatic PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n{\n  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数\n\n  THPStoragePtr self((THPStorage *)type->tp_alloc(type, 0); // 分配内存，让self指向这个内存块\n  ...\n  c10::Allocator * allocator = nullptr;\n\n  if (kwargs != nullptr) {                               // named arguments\n    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, \"allocator\"); // 获取参数allocator的值\n    if (allocator_ptr) {\n      THPUtils_assert(THPUtils_checkLong(allocator_ptr), \"invalid allocator\");\n      // 转为 c10::Allocator 指针\n      allocator = static_cast<c10::Allocator*>(PyLong_AsVoidPtr(allocator_ptr));\n      PyDict_DelItemString(kwargs, \"allocator\");\n    }\n    Py_ssize_t num_kwargs = PyDict_Size(kwargs);\n    if (num_args == 0) {\n      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, \"cdata\");\n      if (num_kwargs==1 && cdata_ptr && THPUtils_checkLong(cdata_ptr)) {   // 提供了cdata值\n        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);\n        self->cdata = ptr;\n        return (PyObject*)self.release();       // 返回THPStorage指针\n      }\n    }\n    THPUtils_assert(num_kwargs == 0, THPStoragePtr \"(): invalid keyword arguments\");\n  }\n\n  if (num_args == 0) {\n    if (allocator) {                            // 未提供cdata值，则需要创建THWStorage类型实例\n      self->cdata = THPStorage_(newWithAllocator)(0, allocator);\n    } else {\n      self->cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);\n    }\n    return (PyObject*)self.release();\n  }\n  ...     // 使用其他方法设置 self->cdata\n}   \n```\n从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义\n```\n#define THWStorage THStorage\n```\n转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，\n```\nStorage.cpp                 ->\n#include <TH/TH.h>          ->\n#include <TH/THStorageFunction.h>   ->\n#include <TH/generic/THStorage.h>   ->\n#include <c10/core/StorageImpl.h>\n```\n在 TH/generic/THStorage.h 中找到宏定义\n```\n#define THStorage at::StorageImpl\n```\n在 c10/core/StorageImpl.h 中找到结构定义\n```\nnamespace c10 {\nstruct C10_API StorageImpl final : public c10::intrusive_ptr_target {\n...\nprivate:\n  caffe2::TypeMeta  data_type_;  // 数据类型\n  DataPtr data_ptr_;             // 数据指针\n  int64_t numel_;                // 数据数量\n  bool resizable_;\n  bool received_cuda_;\n  Allocator* allocator_;         // 数据的内存分配器\n};\n}\n```\n所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为\n```\nnew                // 新建THStorage，未指定 size，即size=0，使用默认Allocator\nfree\nsize\nget\nset\ndata\nnewWithSize        // 新建THStorage，指定 size，使用默认Allocator\nnewWithAllocator   // 新建THStorage，指定 size 和 Allocator\ncopy_functions\ncopyByte\n...\ncopyCudaByte\n...\n```\n此外有宏定义\n```\n#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h\n#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h\n```\n函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。\n\n（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）\n\n以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义\n```\nTHStorage* THStorage_(newWithSize)(ptrdiff_t size)\n{\n  THStorage* storage = c10::make_instrusive<at::StorageImpl>(\n#ifdef THQUANTIZED\n    caffe2::TypeMeta::Make<quantized_t>(),\n#else\n    caffe2::TypeMeta::Make<scalar_t>(),        // 新建scalar_t 类型\n#endif\n    size,\n    getTHDefaultAllocator(),\n    true).release();\n  return storage;\n}\n```\n从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，\n```\nStorageImpl(\n    caffe2::TypeMeta data_type,\n    int64_4 numel,\n    at::Allocator* allocator,\n    bool resizable)\n...\n```\n我们看上上个代码片段中StorageImpl构造函数的实参，\n\n首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）\n```\n#define scalar_t float\n```\n于是，\n```\ncaffe2::TypeMeta::Make<scalar_t>()    // 假设 THQUANTIZED 未定义\n```\ncaffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，\n```\n#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\\n  namespace detail {                                                       \\\n  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\\n    _makeTypeMetaDataInstance<T>(_typeName<T>(#T));                        \\\n  }                                                                        \\\n  template<>                                                               \\\n  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                          \\\n    return &C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\\n  }\n  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)\n\n#define C10_CONCATENATE_IMPL(s1,s2) s1##s2\n#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)\n```\n经过宏替换，得到 _typeMetaDataInstance的模板函数定义\n```\ntemplate<>\nconst detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<T>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<T>(_typeName<T>(#T));\n}\n```\n还有一组宏，用于生成模板特例化，\n```\n#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\\n  template<>                                                           \\\n  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get<T>() {          \\\n    return TypeIdentifier(PreallocatedId);                             \\\n  }                                                                    \\\n  namespace detail {                                                   \\\n  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\\n    _typeMetaDataInstance_preallocated_,                               \\\n    PreallocatedId);                                                   \\\n  }                                                                    \\\n  template<>                                                           \\\n  inline const detail::TypeMetaData*                                   \\\n  TypeMeta::_typeMetaDataInstance<T>() noexcept {                      \\\n    return &C10_CONCATENATE(                                           \\\n      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\\n  }                                                                    \\\n#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\\n  namespace detail {                                                 \\\n  const TypeMetaData C10_CONCATENATE(                                \\\n    _typeMetaDataInstance_preallocated_,                             \\\n    PreallocatedId) = _makeTypeMetaDataInstance<T>(_typeName<T>(#T));\\\n  }                                                                  \n// 调用\nCAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)\n```\n对于系统内部变量如 float，得到函数模板特例化的定义\n```\n// 函数声明\nnamespace detail {\n__attrubyte((__visibility(\"default\"))) extern const TypeMetaData\n_typeMetaDataInstance_preallocated_Preallocated;\n}\n\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_typeMetaDataInstance_preallocated_Preallocated;\n}\n```\n另外，在c10/util/typeid.cpp中有如下调用\n```\nCAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)\n```\n经过宏替换得到\n```\nnamespace detail {                                                 \n  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId\n    = _makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}   \n```\n于是函数模板特例化最终形式为，\n```\ntemplate<>\ninline const detail::TypeMetaData*\nTypeMeta::_typeMetaDataInstance<float>() noexcept {\n  return &detail::_makeTypeMetaDataInstance<float>(_typeName<float>(\"float\"));\n}\n```\ndetail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，\n```\nstruct TypeMetaData final {\n// 函数类型的别名\nusing New = void*();                            // new\nusing PlacementNew = void(void*, size_t);       // 占位new\nusing Copy = void(const void*, void*, size_t);  // 类型数组拷贝\nusing PlacementDelete = void(void*, size_t);\nusing Delete = void(void*);\n... //构造函数\n\nsize_t itemsize_;  // 类型占多少字节\nNew* new_;\nPlacementNew* placementNew_;   // 定位放置 new\nCopy* copy_;        // 类型拷贝\nDelete* delete_;    // 类型析构\nTypeIdentifier id_; // 类型全局唯一id\nconst char* name_;  // 类型名称\n};\n```\n我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义\n```\ntemplate <class T>\ninline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) {\n  return {sizeof(T),                 // 类型T占多少字节\n          _PickNew<T>(),             // 通过 new T\n          _PickPlacementNew<T>(),\n          _PickCopy<T>(),      \n          _PickPlacementDelete<T>(),\n          _PickDelete<T>(),\n          TypeIdentifier::Get<T>(),  // 获取类型的全局唯一id，\n          typeName};                 // 类型名称，例如float的名称为\"float\"\n```\n构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用\n```\nTypeIdentifier::Get<T>()\n```\n在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。\n\n对于其他类型如 int，double，int64_t等类似处理。\n\nPyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。\n\n至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为\n```\nsize,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）\ngetTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配\ntrue                      // 被构造的StorageImpl可以resize\n```\n创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase\n\n记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。\n\nFloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。\n\n类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，\n```\n  torch::nn::init_THNN(module);\n#ifdef USE_CUDA\n  torch::nn::init_THCUNN(module);\n#endif\n```\n函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。\n\n`torch._C`模块初始化过程到这里就完成了。回到 `torch/__init__.py`，继续看看 import torch时接下来做了哪些事情：\n\n1. 定义了模块函数 typename，is_tensor，is_storage等\n2. 导入torch下其他子模块\n3. 调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理\n4. 调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：\n    - 初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；\n    - 初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；\n    - 初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；\n    - 初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor\n    - 共享内存管理初始化，设置文件路径；\n    - 执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），\n        ```\n        torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, \n        TH_CONCAT_2(at::k, Real));\n        ```\n        其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，\n        ```\n        AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`\n        ```\n        这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。\n\n到这里，import torch 的工作全部完成。\n\n# 后记：\n初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。","slug":"PyTorch-2","published":1,"updated":"2019-06-20T10:25:17.453Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5q19001gxgvcgbo3hat6","content":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1><p>依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n\n<p>于是阅读torch/<strong>init</strong>.py，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags=sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ += [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] != &apos;_&apos; and</span><br><span class=\"line\">            not name.endswith(&apos;Base&apos;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n\n<p><strong>将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。</strong></p>\n<p><strong>init</strong>.py除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据python3扩展库的规则可知，<code>import torch._C</code> ，调用PyInit__C函数（调用名为PyInit_&lt;package&gt;的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。</p>\n<p>shm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>TORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。</p>\n<p>initModule方法定义在torch/csrc/Module.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 // 声明并定义模块初始化函数</span><br><span class=\"line\">  // 向methods中添加方法定义</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // 真正的扩展模块定义</span><br><span class=\"line\">  static struct PyModuleDef torchmodule = &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          // 扩展模块名</span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       // 模块中的方法定义</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module = PyModule_Create(&amp;torchmodule)); // 创建模块并确保创建成功</span><br><span class=\"line\">  // 对模块进行各种初始化</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       // 执行cuda相关的初始化</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // 定义模块的属性设置函数，setter</span><br><span class=\"line\">  // 属性名为name，值为v，incref表示是否对值对象增加引用计数</span><br><span class=\"line\">  // 设置成功返回1，否则返回0</span><br><span class=\"line\">  auto set_module_attr = [&amp;](const char* name, PyObject* v, bool incref = true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) == 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // 设置模块属性</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  // 向模块添加方法</span><br><span class=\"line\">  auto py_module = py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    // 设置模块其他属性</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  // 增加 _THNN 属性</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p>使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch/csrc/autograd/init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  同上类似</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch/csrc/cuda/Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch/csrc/distributed/Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>生成模块torch._C 后再向其添加如下成员：</p>\n<ul>\n<li><p>向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。</p>\n<p>  torch._C._TensorBase这个类型具有属性</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n\n<p>  并且具有以下方法</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n\n<p>  类型torch._C._FunctionBase， 这个类型具有方法和属性为</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n\n<p>  不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。</p>\n</li>\n<li><p>向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。</p>\n</li>\n<li><p>向torch._C添加模块， _nn，cpp，_onnx。</p>\n</li>\n<li><p>向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。</p>\n<p>注意到从Module.cpp文件中头文件引用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/TH.h&gt;               // TH=TorcH</span><br><span class=\"line\">#include &lt;c10/util/Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen/ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;      // THP=TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。</p>\n<p>一方面，头文件 TH/TH.h 中引用了#include &lt;TH/THGeneral.h&gt;，在aten/src/TH目录下的CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>在THGeneral.h中有如下宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND</span><br></pre></td></tr></table></figure>\n\n<p>另一方面，torch/csrc/THP.h 中引用了#include &lt;torch/src/Storage.h&gt;，在这个Storage.h中有如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.h&quot;         // (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      // (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>而文件TH/THGenerateAllType.h则包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 此时 TH_GENERIC_FILE是已定义的</span><br><span class=\"line\">#include &lt;TH/THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义</span><br></pre></td></tr></table></figure>\n\n<p>以TH/THGenerateFloatType.h为例说明，此文件中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         // (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n\n<p>注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n\n<p>类似地，#include &lt;TH/THGenerateDoubleType.h&gt;，则得到THPDoubleStorage_init，</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; 得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n\n<p>对4组include中的其他三组，则得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n\n<p>以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;                   // include THPxxxStorage_init 函数声明</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.cpp&quot;              // (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   // (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods = methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members = THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset = THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n\n<p>在TH/THGeneral.h中存在宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n\n<p>由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为”FloatStorageBase”，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。</p>\n<p>以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。</p>\n<p>其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。</p>\n<p>torch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。</p>\n<p>现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType = &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               /* tp_name */</span><br><span class=\"line\">  sizeof(THPStorage),                          /* tp_basicsize */</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          /* tp_new */</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）</p>\n<p>然后查看类创建 THPStorage_(pynew) 的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数</span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); // 分配内存，让self指向这个内存块</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator = nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs != nullptr) &#123;                               // named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, &quot;allocator&quot;); // 获取参数allocator的值</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      // 转为 c10::Allocator 指针</span><br><span class=\"line\">      allocator = static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs = PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args == 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs==1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   // 提供了cdata值</span><br><span class=\"line\">        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata = ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       // 返回THPStorage指针</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs == 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args == 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            // 未提供cdata值，则需要创建THWStorage类型实例</span><br><span class=\"line\">      self-&gt;cdata = THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     // 使用其他方法设置 self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n\n<p>转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH/TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH/THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH/generic/THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10/core/StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>在 TH/generic/THStorage.h 中找到宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n\n<p>在 c10/core/StorageImpl.h 中找到结构定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  // 数据类型</span><br><span class=\"line\">  DataPtr data_ptr_;             // 数据指针</span><br><span class=\"line\">  int64_t numel_;                // 数据数量</span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         // 数据的内存分配器</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                // 新建THStorage，未指定 size，即size=0，使用默认Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        // 新建THStorage，指定 size，使用默认Allocator</span><br><span class=\"line\">newWithAllocator   // 新建THStorage，指定 size 和 Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>此外有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n\n<p>函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。</p>\n<p>（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）</p>\n<p>以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage = c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        // 新建scalar_t 类型</span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>我们看上上个代码片段中StorageImpl构造函数的实参，</p>\n<p>首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n\n<p>于是，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    // 假设 THQUANTIZED 未定义</span><br></pre></td></tr></table></figure>\n\n<p>caffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n\n<p>经过宏替换，得到 _typeMetaDataInstance的模板函数定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>还有一组宏，用于生成模板特例化，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) = _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">// 调用</span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n\n<p>对于系统内部变量如 float，得到函数模板特例化的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 函数声明</span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>另外，在c10/util/typeid.cpp中有如下调用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n\n<p>经过宏替换得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    = _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>于是函数模板特例化最终形式为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>detail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">// 函数类型的别名</span><br><span class=\"line\">using New = void*();                            // new</span><br><span class=\"line\">using PlacementNew = void(void*, size_t);       // 占位new</span><br><span class=\"line\">using Copy = void(const void*, void*, size_t);  // 类型数组拷贝</span><br><span class=\"line\">using PlacementDelete = void(void*, size_t);</span><br><span class=\"line\">using Delete = void(void*);</span><br><span class=\"line\">... //构造函数</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  // 类型占多少字节</span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   // 定位放置 new</span><br><span class=\"line\">Copy* copy_;        // 类型拷贝</span><br><span class=\"line\">Delete* delete_;    // 类型析构</span><br><span class=\"line\">TypeIdentifier id_; // 类型全局唯一id</span><br><span class=\"line\">const char* name_;  // 类型名称</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 // 类型T占多少字节</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             // 通过 new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  // 获取类型的全局唯一id，</span><br><span class=\"line\">          typeName&#125;;                 // 类型名称，例如float的名称为&quot;float&quot;</span><br></pre></td></tr></table></figure>\n\n<p>构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n\n<p>在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。</p>\n<p>对于其他类型如 int，double，int64_t等类似处理。</p>\n<p>PyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。</p>\n<p>至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）</span><br><span class=\"line\">getTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配</span><br><span class=\"line\">true                      // 被构造的StorageImpl可以resize</span><br></pre></td></tr></table></figure>\n\n<p>创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase</p>\n<p>记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。</p>\n<p>FloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。</p>\n<p>类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。</p>\n<p><code>torch._C</code>模块初始化过程到这里就完成了。回到 <code>torch/__init__.py</code>，继续看看 import torch时接下来做了哪些事情：</p>\n<ol>\n<li><p>定义了模块函数 typename，is_tensor，is_storage等</p>\n</li>\n<li><p>导入torch下其他子模块</p>\n</li>\n<li><p>调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理</p>\n</li>\n<li><p>调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：</p>\n<ul>\n<li><p>初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；</p>\n</li>\n<li><p>初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；</p>\n</li>\n<li><p>初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；</p>\n</li>\n<li><p>初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor</p>\n</li>\n<li><p>共享内存管理初始化，设置文件路径；</p>\n</li>\n<li><p>执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n\n<p>  其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`</span><br></pre></td></tr></table></figure>\n\n<p>  这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>到这里，import torch 的工作全部完成。</p>\n<h1 id=\"后记：\"><a href=\"#后记：\" class=\"headerlink\" title=\"后记：\"></a>后记：</h1><p>初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"torch-installization\"><a href=\"#torch-installization\" class=\"headerlink\" title=\"torch installization\"></a>torch installization</h1><p>依然采取自顶向下的原则剖析，借助PyTorch的python接口。我们知道使用PyTorch第一步都是</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure>\n\n<p>于是阅读torch/<strong>init</strong>.py，发现需要加载torch._C这个库，但是需要以（RTLD_GLOBAL|RTLD_LAZY）这个模式动态加载，于是先将动态加载模式设置到（RTLD_GLOBAL|RTLD_LAZY）之后加载torch._C然后再恢复动态加载模式，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">old_flags=sys.getdlopenflags()</span><br><span class=\"line\">sys.setdlopenflags(_dl_flags.RTDL_GLOBAL | _dl_flags.RTLD_LAZY)</span><br><span class=\"line\">from torch._C import *</span><br><span class=\"line\">__all__ += [name for name in dir(_C)</span><br><span class=\"line\">            if name[0] != &apos;_&apos; and</span><br><span class=\"line\">            not name.endswith(&apos;Base&apos;)]</span><br><span class=\"line\">sys.setdlopenflags(old_flags)</span><br></pre></td></tr></table></figure>\n\n<p><strong>将torch._C中（不包括_开头和Base结尾）的属性导出到当前域。</strong></p>\n<p><strong>init</strong>.py除了import torch._C，还import了同目录下其他module，以及同目录下的package。首先看torch._C导入时做了什么， torch._C的源文件只有torch/csrc/stub.cpp，链接库为shm和torch_python，stub.cpp中仅仅是初始化模块，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">extern PyObject* initModule();</span><br><span class=\"line\">PyMODINIT_FUNC PyInit__C()   // 在python脚本中，import _C 时调用</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  return initModule();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据python3扩展库的规则可知，<code>import torch._C</code> ，调用PyInit__C函数（调用名为PyInit_&lt;package&gt;的函数），这个函数内部调用initModule，也就是说，具体的模块定义由initModule实现。看到extern知道initModule方法定义在外部，所以只能从shm和torch_python对应的源文件中寻找方法定义。</p>\n<p>shm库实现Domain Socket通信获得共享内存的句柄，解决多进程的内存分配问题，查看torch/CMakeLists.txt，发现生成shm相关语句为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(LIBSHM_SUBDIR libshm)</span><br><span class=\"line\">set(LIBSHM_SRCDIR $&#123;LIBSHM_SRC_DIR&#125;/lib/$&#123;LIBSHM_SUBDIR&#125;)</span><br><span class=\"line\">add_subdirectory($&#123;LIBSHM_SRCDIR&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>从上面语句得知shm库的源码位于torch/lib/libshm目录下，这个跟torch._C模块定义没有关系，暂且不细展开，继续查看torch_python的源码以寻求initModule方法定义。在torch/CMakeLists.txt中发现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(torch_python SHARED $&#123;TORCH_PYTHON_SRCS&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>TORCH_PYTHON_SRCS是一个列表，存储了torch_python库的源文件，生成torch_python库所需要的源文件以及依赖库直接查看torch/CMakeLists.txt，这里不再展开一一说明。</p>\n<p>initModule方法定义在torch/csrc/Module.cpp，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">namespace torch &#123; namespace cuda &#123;</span><br><span class=\"line\">void initModule(PyObject* module);       // 模块中有关cuda部分的初始化函数声明</span><br><span class=\"line\">&#125;&#125;</span><br><span class=\"line\">#endif</span><br><span class=\"line\"></span><br><span class=\"line\">static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\"></span><br><span class=\"line\">PyObject* module;</span><br><span class=\"line\">PyObject* initModule() &#123;                 // 声明并定义模块初始化函数</span><br><span class=\"line\">  // 向methods中添加方法定义</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // 真正的扩展模块定义</span><br><span class=\"line\">  static struct PyModuleDef torchmodule = &#123;</span><br><span class=\"line\">    PyModuleDef_HEAD_INIT,</span><br><span class=\"line\">    &quot;torch._C&quot;,                          // 扩展模块名</span><br><span class=\"line\">    nullptr,                           </span><br><span class=\"line\">    -1,</span><br><span class=\"line\">    methods.data()                       // 模块中的方法定义</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">  ASSERT_TRUE(module = PyModule_Create(&amp;torchmodule)); // 创建模块并确保创建成功</span><br><span class=\"line\">  // 对模块进行各种初始化</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::cuda::initModule(module);       // 执行cuda相关的初始化</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // 定义模块的属性设置函数，setter</span><br><span class=\"line\">  // 属性名为name，值为v，incref表示是否对值对象增加引用计数</span><br><span class=\"line\">  // 设置成功返回1，否则返回0</span><br><span class=\"line\">  auto set_module_attr = [&amp;](const char* name, PyObject* v, bool incref = true) </span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    if(incref) &#123;</span><br><span class=\"line\">      Py_INCREF(v);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return PyModule_AddObject(module, name, v) == 0;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // 设置模块属性</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;has_cudnn&quot;, has_cudnn));</span><br><span class=\"line\">  // 向模块添加方法</span><br><span class=\"line\">  auto py_module = py::reinterpret_borrow&lt;py::module&gt;(module);</span><br><span class=\"line\">  py_module.def(&quot;_demangle&quot;, &amp;c10::demangle);</span><br><span class=\"line\">  py_module.def(&quot;_log_api_usage_once&quot;, &amp;LogAPIUsageOnceFromPython);</span><br><span class=\"line\">  ...    // 设置模块其他属性</span><br><span class=\"line\">  ASSERT_TRUE(set_module_attr(&quot;default_generator&quot;, </span><br><span class=\"line\">        (PyObject*)THPDefaultGenerator, false));</span><br><span class=\"line\">  torch::nn::init__THNN(module);  // 增加 _THNN 属性</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUDD(module);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  return module;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码中可见，定义并生成名为torch._C的模块，然后对这个模块设置attr，添加方法，添加子模块等。</p>\n<h1 id=\"methods-members-in-torch-C\"><a href=\"#methods-members-in-torch-C\" class=\"headerlink\" title=\"methods/members in torch._C\"></a>methods/members in torch._C</h1><ul>\n<li><p>使用 THPUtils_addPyMethodDefs 向torch._C 添加模块方法。包括</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># TorchMethods </span><br><span class=\"line\">_initExtension</span><br><span class=\"line\">_autograd_init</span><br><span class=\"line\">...</span><br><span class=\"line\"># DataLoaderMethods </span><br><span class=\"line\">_set_worker_signal_handlers</span><br><span class=\"line\">_set_worker_pids</span><br><span class=\"line\">...</span><br><span class=\"line\"># torch::autograd::python_functions(), torch/csrc/autograd/init.cpp</span><br><span class=\"line\">set_grad_enabled</span><br><span class=\"line\">is_grad_enabled</span><br><span class=\"line\">set_anomaly_enabled</span><br><span class=\"line\">is_anomaly_enabled</span><br><span class=\"line\"># torch::multiprocessing::python_functions(), torch/csrc/multiprocessing/init.cpp</span><br><span class=\"line\">_multiprocessing_init</span><br><span class=\"line\"># torch::distributed::c10d::python_functions()  同上类似</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCPModule_method(), torch/csrc/cuda/Module.cpp</span><br><span class=\"line\">_cuda_init</span><br><span class=\"line\">_cuda_setDevice</span><br><span class=\"line\">...</span><br><span class=\"line\">_nccl_version</span><br><span class=\"line\">...</span><br><span class=\"line\"># THCUDNN_method()</span><br><span class=\"line\">_cudnn_version</span><br><span class=\"line\"># THDPModule_methods(), torch/csrc/distributed/Module.cpp</span><br><span class=\"line\">_dist_init_extension</span><br><span class=\"line\">_dist_init_process_group</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>生成模块torch._C 后再向其添加如下成员：</p>\n<ul>\n<li><p>向torch._C添加类型_PtrWrapper，Generator，FatalError，Size，dtype，iinfo，layout，memory_format，device，_LegacyVariableBase，_TensorBase，_VariableFunctions，_FunctionBase，_EngineBase，JITException，IODescriptor，_THNN，_THCUNN。</p>\n<p>  torch._C._TensorBase这个类型具有属性</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_cdata</span><br><span class=\"line\">_version</span><br><span class=\"line\">grad_fn</span><br><span class=\"line\">_grad_fn</span><br><span class=\"line\">is_leaf</span><br><span class=\"line\">data</span><br><span class=\"line\">_grad</span><br><span class=\"line\">grad</span><br><span class=\"line\">...</span><br><span class=\"line\">device</span><br><span class=\"line\">ndim</span><br></pre></td></tr></table></figure>\n\n<p>  并且具有以下方法</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># variable_methods, torch/csrc/autograd/generated/python_variable_methods.cpp</span><br><span class=\"line\">__add__</span><br><span class=\"line\">__radd__</span><br><span class=\"line\">...</span><br><span class=\"line\">apply_</span><br><span class=\"line\">byte</span><br><span class=\"line\">char</span><br><span class=\"line\">contiguous</span><br><span class=\"line\">...</span><br><span class=\"line\">where</span><br><span class=\"line\">zero_</span><br><span class=\"line\"># extra_method</span><br><span class=\"line\">_make_subclass</span><br></pre></td></tr></table></figure>\n\n<p>  类型torch._C._FunctionBase， 这个类型具有方法和属性为</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># method</span><br><span class=\"line\">apply</span><br><span class=\"line\">_do_forward</span><br><span class=\"line\">_do_backward</span><br><span class=\"line\">_register_hook_dict</span><br><span class=\"line\">register_hook</span><br><span class=\"line\"># property</span><br><span class=\"line\">saved_tensors</span><br><span class=\"line\">saved_variables</span><br><span class=\"line\">...</span><br><span class=\"line\">requires_grad</span><br><span class=\"line\">metadata</span><br></pre></td></tr></table></figure>\n\n<p>  不难知道_TensorBase是Tensor的基类，包含了Tensor的各种操作，_FunctionBase则包括了前后向传播方法，从这里能将深度学习中的一些概念与代码实现建立一点点联系了。</p>\n</li>\n<li><p>向torch._C中添加函数 _wrap_tensor_impl，_tensor_impl_raw_handle，_demangle，_log_api_usage_once，以_jit开头的一系列函数。</p>\n</li>\n<li><p>向torch._C添加模块， _nn，cpp，_onnx。</p>\n</li>\n<li><p>向torch._C添加属性 has_cudnn，has_openmp，has_mkl，has_lapack，has_cuda，has_mkldnn，_GLIBCXX_USE_CXX11_API，default_generator。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"some-installization-w-r-t-torch-C\"><a href=\"#some-installization-w-r-t-torch-C\" class=\"headerlink\" title=\"some installization w.r.t. torch._C\"></a>some installization w.r.t. torch._C</h1><h3 id=\"THPxxxStorage-init\"><a href=\"#THPxxxStorage-init\" class=\"headerlink\" title=\"THPxxxStorage_init\"></a>THPxxxStorage_init</h3><p>torch._C模块中各种Tensor的定义通过 THPxxxStorage_init 和 THCPxxxStorage_init 完成，在项目中是无法直接搜索到这两种函数定义的，下面讲解这两个函数的定义。</p>\n<p>注意到从Module.cpp文件中头文件引用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/TH.h&gt;               // TH=TorcH</span><br><span class=\"line\">#include &lt;c10/util/Logging.h&gt;</span><br><span class=\"line\">#include &lt;ATen/ATen.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;      // THP=TorcH Python</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>可以看出先引用ATen和c10库的头文件，然后再引用torch中的头文件，这是因为ATen [A Tensor Library的缩写] 实现了Tensor的运算等，c10 [表示caffe2和ATen] 实现了Tensor存储等，这两个库作为基础。</p>\n<p>一方面，头文件 TH/TH.h 中引用了#include &lt;TH/THGeneral.h&gt;，在aten/src/TH目录下的CMakeLists.txt中有这么一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CONFIGURE_FILE(THGeneral.h.in &quot;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/THGeneral.h&quot;)</span><br></pre></td></tr></table></figure>\n\n<p>在THGeneral.h中有如下宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_4_EXPAND(x,y,z,w) x ## y ## z ## w</span><br><span class=\"line\">#define TH_CONCAT_4(x,y,z,w) TH_CONCAT_4_EXPAND</span><br></pre></td></tr></table></figure>\n\n<p>另一方面，torch/csrc/THP.h 中引用了#include &lt;torch/src/Storage.h&gt;，在这个Storage.h中有如下语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorage_(NAME) TH_CONCAT_4(THP, Real, Storage_, NAME)</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>上面是4组include操作（根据不同类型生成对应的方法声明/定义，这种策略，后面还会用到很多次），可以看到每组include一次 torch/csrc/generic/Storage.h，这是为什么呢？查看文件torch/csrc/generic/Storage.h 发现其包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.h&quot;         // (0)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module);                      // (1)</span><br><span class=\"line\">...</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>而文件TH/THGenerateAllType.h则包含语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THGenerateFloatTypes.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateIntTypes.h&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>4组include操作中，每组的第二个被include的文件均包含#undef TH_GENERIC_FILE，这使得每组include操作中，include torch/csrc/generic/Storage.h时均执行语句 (0)，而非语句 (1)，继续进一步查看TH/THGenerateFloatTypes.h，发现有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 此时 TH_GENERIC_FILE是已定义的</span><br><span class=\"line\">#include &lt;TH/THGenerateFloatType.h&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateDoubleType.h&gt;</span><br><span class=\"line\">#undef TH_GENERIC_FILE     // 这里将TH_GENERIC_FILE 设为未定义</span><br></pre></td></tr></table></figure>\n\n<p>以TH/THGenerateFloatType.h为例说明，此文件中有语句</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#line 1 TH_GENERIC_FILE</span><br><span class=\"line\">#include TH_GENERIC_FILE         // (2)</span><br><span class=\"line\">...</span><br><span class=\"line\">#undef Real</span><br></pre></td></tr></table></figure>\n\n<p>注意语句 (2) 是include torch/csrc/generic/Storate.h，而此时TH_GENERIC_FILE是已定义的，所以执行 语句 (1)， 于是按如下过程进行宏替换</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool THPStorage_(init)(PyObject *module);  -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Real, Storage_, init)(PyObject *module);    -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4(THP, Float, Storage_, init)(PyObject *module);   -&gt;</span><br><span class=\"line\">bool TH_CONCAT_4_EXPAND(THP, Float, Storage_, init)(PyObject *module); -&gt;</span><br><span class=\"line\">bool THPFloatStorage_init(PyObject *module);</span><br></pre></td></tr></table></figure>\n\n<p>类似地，#include &lt;TH/THGenerateDoubleType.h&gt;，则得到THPDoubleStorage_init，</p>\n<p>#include &lt;TH/THGenerateIntTypes.h&gt; 得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPByteStorage_init</span><br><span class=\"line\">THPCharStorage_init</span><br><span class=\"line\">THPShortStorage_init</span><br><span class=\"line\">THPIntStorage_init</span><br><span class=\"line\">THPLongStorage_init</span><br></pre></td></tr></table></figure>\n\n<p>对4组include中的其他三组，则得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THPHalfStorage_init</span><br><span class=\"line\">THPBoolStorage_init</span><br><span class=\"line\">THPQUInt8Storage_init</span><br><span class=\"line\">THPQInt8Storage_init</span><br><span class=\"line\">THPQInt32Storage_init</span><br></pre></td></tr></table></figure>\n\n<p>以上仅得到函数的声明，我们还需要弄清楚其定义，定义部分的构造与声明类似，首先查看torch/csrc/Storage.cpp，其中包含</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;TH/THStorageFunctions.hpp&gt;</span><br><span class=\"line\">#include &lt;torch/csrc/THP.h&gt;                   // include THPxxxStorage_init 函数声明</span><br><span class=\"line\">...</span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateAllTypes.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateHalfType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateBoolType.h&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;torch/csrc/generic/Storage.cpp&gt;</span><br><span class=\"line\">#include &lt;TH/THGenerateQTypes.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>又是4组include 操作，还是熟悉的配方，torch/csrc/generic/Storage.cpp中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ifndef TH_GENERIC_FILE</span><br><span class=\"line\">#define TH_GENERIC_FILE &quot;torch/csrc/generic/Storage.cpp&quot;              // (11)</span><br><span class=\"line\">#else</span><br><span class=\"line\">...                                                                   // (12)</span><br><span class=\"line\">bool THPStorage_(init)(PyObject *module)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  static std::vector&lt;PyMethodDef&gt; methods;</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class=\"line\">#ifndef THD_GENERIC_FILE</span><br><span class=\"line\">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  </span><br><span class=\"line\">  THPStorageType.tp_methods = methods.data();</span><br><span class=\"line\">  THPStorageType.tp_members = THPStorage_(members);</span><br><span class=\"line\">  THPStorageType.tp_getset = THPStorage_(properties);</span><br><span class=\"line\">  if (PyType_Ready(&amp;THPStorageType) &lt; 0)</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  Py_INCREF(&amp;THPStorageType);</span><br><span class=\"line\">  PyModule_AddObject(module, THPStorageBaseStr, (PyObject*)&amp;THPStorageType);</span><br><span class=\"line\">  THPStorage_(initCopyMethods)();</span><br><span class=\"line\">  return true;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>上述代码容易看出是向模块module添加字段THPStorageBaseStr， 在torch/csrc/Storage.h中有宏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THPStorageBaseStr TH_CONCAT_STRING_2(Real, StorageBase)</span><br></pre></td></tr></table></figure>\n\n<p>在TH/THGeneral.h中存在宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define TH_CONCAT_STRING_2(x,y) TH_CONCAT_STRING_2_EXPAND(x,y)</span><br><span class=\"line\">#define TH_CONCAT_STRING_2_EXPAND(x,y) #x #y</span><br></pre></td></tr></table></figure>\n\n<p>由于StorageBase没有宏定义，Real则可以是 Int, Float, Double, Short, Char等（见前面THPxxxStorage_init的声明分析部分），以Real=Float为例，THPStorageBaseStr此时变为”FloatStorageBase”，所以实际上是向torch._C添加字段 FloatStorageBase， 此字段类型为python class torch._C.FloatStorageBase。</p>\n<p>以4组include操作的第一组为例说明，首次include torch/csrc/generic/Storage.cpp时，TH_GENERIC_FILE未定义，所以执行 (11)，然后include TH/THGenerateAllTypes.h，同样的，在TH/THGenerateFloatType.h中根据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define Real Float</span><br><span class=\"line\">...</span><br><span class=\"line\">#include TH_GENERIC_FILE</span><br></pre></td></tr></table></figure>\n\n<p>即，再一次include torch/csrc/generic/Storage.cpp，此时TH_GENERIC_FILE已定义，所以从 (12) 处开始执行，得到THPFloatStorage_init的函数定义，前面已经分析过，此函数用于向torch._C 模块添加类 FloatStorageBase。</p>\n<p>其他如Int，Char，Byte，Double，Half，QUInt8等类似处理。</p>\n<p>torch/csrc/Module.cpp中模块初始化initModule函数中还有一些 THCPxxxStorage_init 的函数，这些函数的声明和定义与 THPxxxStorage_init 的声明和定义 的生成方式一样，不再展开细讲，直接阅读torch/csrc/cuda/Storage.h 和 torch/csrc/cuda/Storage.cpp 两个文件。</p>\n<p>现在我们来看一下上面所述的torch._C模块中新增类到底是什么。以FloatStorageBase为例，查看torch/csrc/generic/Storage.cpp中 THPStorageType的定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PyTypeObject THPStorageType = &#123;</span><br><span class=\"line\">  PyVarObject_HEAD_INIT(nullptr, 0)</span><br><span class=\"line\">  &quot;torch._C.&quot; THPStorageBaseStr,               /* tp_name */</span><br><span class=\"line\">  sizeof(THPStorage),                          /* tp_basicsize */</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  THPStorage_(pynew),                          /* tp_new */</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见python中的类型FloatStorageBase对应在C++中的类型为THPStorage，在 torch/csrc/StorageDef.h中查看THPStorage定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct THPStorage &#123;</span><br><span class=\"line\">  PyObject_HEAD</span><br><span class=\"line\">  THWStorage *cdata;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>（插播一下，torch/csrc/generic/Storage.cpp 这里如何找到 THPStorage的定义？首先，torch/csrc/Storage.cpp中include了文件 torch/csrc/THP.h，torch/csrc/generic/Storage.cpp，然后 torch/csrc/THP.h 中include 了文件torch/csrc/Storage.h，torch/csrc/Storage.h又include了torch/csrc/generic/Storage.h，最后在这个generic/Storage.h中include了 torch/csrc/StorageDef.h）</p>\n<p>然后查看类创建 THPStorage_(pynew) 的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static PyObject* THPStorage_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  Py_ssize_t num_args = args ? PyTuple_Size(args) : 0;   // 可变长度参数的个数</span><br><span class=\"line\"></span><br><span class=\"line\">  THPStoragePtr self((THPStorage *)type-&gt;tp_alloc(type, 0); // 分配内存，让self指向这个内存块</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  c10::Allocator * allocator = nullptr;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (kwargs != nullptr) &#123;                               // named arguments</span><br><span class=\"line\">    PyObject *allocator_ptr = PyDict_GetItemString(kwargs, &quot;allocator&quot;); // 获取参数allocator的值</span><br><span class=\"line\">    if (allocator_ptr) &#123;</span><br><span class=\"line\">      THPUtils_assert(THPUtils_checkLong(allocator_ptr), &quot;invalid allocator&quot;);</span><br><span class=\"line\">      // 转为 c10::Allocator 指针</span><br><span class=\"line\">      allocator = static_cast&lt;c10::Allocator*&gt;(PyLong_AsVoidPtr(allocator_ptr));</span><br><span class=\"line\">      PyDict_DelItemString(kwargs, &quot;allocator&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Py_ssize_t num_kwargs = PyDict_Size(kwargs);</span><br><span class=\"line\">    if (num_args == 0) &#123;</span><br><span class=\"line\">      PyObject *cdata_ptr = PyDict_GetItemString(kwargs, &quot;cdata&quot;);</span><br><span class=\"line\">      if (num_kwargs==1 &amp;&amp; cdata_ptr &amp;&amp; THPUtils_checkLong(cdata_ptr)) &#123;   // 提供了cdata值</span><br><span class=\"line\">        THWStorage *ptr = (THWStorage*)PyLong_AsVoidPtr(cdata_ptr);</span><br><span class=\"line\">        self-&gt;cdata = ptr;</span><br><span class=\"line\">        return (PyObject*)self.release();       // 返回THPStorage指针</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    THPUtils_assert(num_kwargs == 0, THPStoragePtr &quot;(): invalid keyword arguments&quot;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (num_args == 0) &#123;</span><br><span class=\"line\">    if (allocator) &#123;                            // 未提供cdata值，则需要创建THWStorage类型实例</span><br><span class=\"line\">      self-&gt;cdata = THPStorage_(newWithAllocator)(0, allocator);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      self-&gt;cdata = THWStorage_(new)(LIBRARY_STATE_NOARGS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (PyObject*)self.release();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...     // 使用其他方法设置 self-&gt;cdata</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从上面的代码中可见，创建FloatStorageBase实例时，核心是设置 THPStorage.cdata的值，其指向一个THWStorage类型对象，在torch/csrc/THP.h中有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage THStorage</span><br></pre></td></tr></table></figure>\n\n<p>转而去寻找 THStorage 的定义，我们从torch/csrc/Storage.cpp出发，逐级查看被include的文件，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Storage.cpp                 -&gt;</span><br><span class=\"line\">#include &lt;TH/TH.h&gt;          -&gt;</span><br><span class=\"line\">#include &lt;TH/THStorageFunction.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;TH/generic/THStorage.h&gt;   -&gt;</span><br><span class=\"line\">#include &lt;c10/core/StorageImpl.h&gt;</span><br></pre></td></tr></table></figure>\n\n<p>在 TH/generic/THStorage.h 中找到宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THStorage at::StorageImpl</span><br></pre></td></tr></table></figure>\n\n<p>在 c10/core/StorageImpl.h 中找到结构定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace c10 &#123;</span><br><span class=\"line\">struct C10_API StorageImpl final : public c10::intrusive_ptr_target &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">private:</span><br><span class=\"line\">  caffe2::TypeMeta  data_type_;  // 数据类型</span><br><span class=\"line\">  DataPtr data_ptr_;             // 数据指针</span><br><span class=\"line\">  int64_t numel_;                // 数据数量</span><br><span class=\"line\">  bool resizable_;</span><br><span class=\"line\">  bool received_cuda_;</span><br><span class=\"line\">  Allocator* allocator_;         // 数据的内存分配器</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>所以，THWStorage实际上是类型 at::StorageImpl，这个结构是数据存储实现，我们先不去深挖这个结构，转而继续 THPStorage_(pynew) 的定义，当未提供 cdata变量值时，需要创建 THWStorage 类型实例，使用THWStorage_(NAME)函数，NAME可能的值为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new                // 新建THStorage，未指定 size，即size=0，使用默认Allocator</span><br><span class=\"line\">free</span><br><span class=\"line\">size</span><br><span class=\"line\">get</span><br><span class=\"line\">set</span><br><span class=\"line\">data</span><br><span class=\"line\">newWithSize        // 新建THStorage，指定 size，使用默认Allocator</span><br><span class=\"line\">newWithAllocator   // 新建THStorage，指定 size 和 Allocator</span><br><span class=\"line\">copy_functions</span><br><span class=\"line\">copyByte</span><br><span class=\"line\">...</span><br><span class=\"line\">copyCudaByte</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>此外有宏定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define THWStorage_(NAME) THStorage_(NAME)     // torch/csrc/THP.h</span><br><span class=\"line\">#define THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   // TH/THStorageFunctions.h</span><br></pre></td></tr></table></figure>\n\n<p>函数THStorage_(NAME) 声明分布在文件 TH/generic/THStorage.h，TH/generic/THStorageCopy.h，实现部分则位于相应的 cpp文件。</p>\n<p>（插播：在使用cuda的情况下，#define THWStorage_(NAME) THCStorage_(NAME)，后者的声明则分布在THC/generic/THCStorage.h，THC/generic/THCStorageCopy.h）</p>\n<p>以 THStorage_(newWithSize)函数为例说明，查看 TH/generic/THStorage.cpp，有定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THStorage* THStorage_(newWithSize)(ptrdiff_t size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  THStorage* storage = c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class=\"line\">#ifdef THQUANTIZED</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;quantized_t&gt;(),</span><br><span class=\"line\">#else</span><br><span class=\"line\">    caffe2::TypeMeta::Make&lt;scalar_t&gt;(),        // 新建scalar_t 类型</span><br><span class=\"line\">#endif</span><br><span class=\"line\">    size,</span><br><span class=\"line\">    getTHDefaultAllocator(),</span><br><span class=\"line\">    true).release();</span><br><span class=\"line\">  return storage;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>从这段代码中不难看出，创建StorageImpl对象，以及指向其的一个intrusive_ptr类型的指针，返回一个新的普通指针，指向这个StorageImpl，并销毁intrusive_ptr 内部指针，上文讲过有宏定义 THStorage 就是 at::StorageImpl，所以这个方法就是新建一个StorageImpl对象，并返回指向它的指针。根据c10::make_instrusive的函数定义，实际上是调用StorageImpl的构造函数完成这项工作，此构造函数为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">StorageImpl(</span><br><span class=\"line\">    caffe2::TypeMeta data_type,</span><br><span class=\"line\">    int64_4 numel,</span><br><span class=\"line\">    at::Allocator* allocator,</span><br><span class=\"line\">    bool resizable)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>我们看上上个代码片段中StorageImpl构造函数的实参，</p>\n<p>首先回顾一下我们是从FloatStorageBase出发走到现在这里，所以在TH/THGenerateFloatType.h 文件中找到（如果理解上文所说的 4组include操作，就能理解为什么是在这个文件中）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define scalar_t float</span><br></pre></td></tr></table></figure>\n\n<p>于是，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caffe2::TypeMeta::Make&lt;scalar_t&gt;()    // 假设 THQUANTIZED 未定义</span><br></pre></td></tr></table></figure>\n\n<p>caffe2::TypeMeta::Make 这个方法是创建caffe2::TypeMeta 对象，其内部维护一个detail::TypeMetaData* 变量data_，如何new 一个TypeMetaData对象暂且不表，我们先看一组宏，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, Counter)         \\</span><br><span class=\"line\">  namespace detail &#123;                                                       \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(_typeMetaDataInstance_, Counter) =    \\</span><br><span class=\"line\">    _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));                        \\</span><br><span class=\"line\">  &#125;                                                                        \\</span><br><span class=\"line\">  template&lt;&gt;                                                               \\</span><br><span class=\"line\">  EXPORT_IF_NOT_GCC const detail::TypeMetaData*                            \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                          \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(detail::_typeMetaDataInstance_, Counter);      \\</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE(T, __COUNTER__)</span><br><span class=\"line\"></span><br><span class=\"line\">#define C10_CONCATENATE_IMPL(s1,s2) s1##s2</span><br><span class=\"line\">#define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)</span><br></pre></td></tr></table></figure>\n\n<p>经过宏替换，得到 _typeMetaDataInstance的模板函数定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>还有一组宏，用于生成模板特例化，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)       \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline C10_EXPORT TypeIdentifier TypeIdentifier::Get&lt;T&gt;() &#123;          \\</span><br><span class=\"line\">    return TypeIdentifier(PreallocatedId);                             \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  namespace detail &#123;                                                   \\</span><br><span class=\"line\">  C10_EXPORT extern const TypeMetaData C10_CONCATENATE(                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                               \\</span><br><span class=\"line\">    PreallocatedId);                                                   \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">  template&lt;&gt;                                                           \\</span><br><span class=\"line\">  inline const detail::TypeMetaData*                                   \\</span><br><span class=\"line\">  TypeMeta::_typeMetaDataInstance&lt;T&gt;() noexcept &#123;                      \\</span><br><span class=\"line\">    return &amp;C10_CONCATENATE(                                           \\</span><br><span class=\"line\">      detail::_typeMetaDataInstance_preallocated_, PreallocatedId);    \\</span><br><span class=\"line\">  &#125;                                                                    \\</span><br><span class=\"line\">#define CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(PreallocatedId, T)      \\</span><br><span class=\"line\">  namespace detail &#123;                                                 \\</span><br><span class=\"line\">  const TypeMetaData C10_CONCATENATE(                                \\</span><br><span class=\"line\">    _typeMetaDataInstance_preallocated_,                             \\</span><br><span class=\"line\">    PreallocatedId) = _makeTypeMetaDataInstance&lt;T&gt;(_typeName&lt;T&gt;(#T));\\</span><br><span class=\"line\">  &#125;                                                                  </span><br><span class=\"line\">// 调用</span><br><span class=\"line\">CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE(0, uint8_t)</span><br></pre></td></tr></table></figure>\n\n<p>对于系统内部变量如 float，得到函数模板特例化的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 函数声明</span><br><span class=\"line\">namespace detail &#123;</span><br><span class=\"line\">__attrubyte((__visibility(&quot;default&quot;))) extern const TypeMetaData</span><br><span class=\"line\">_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_typeMetaDataInstance_preallocated_Preallocated;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>另外，在c10/util/typeid.cpp中有如下调用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CAFFE_DEFINE_PREALLOCATED_KNOWN_TYPE(0, float)</span><br></pre></td></tr></table></figure>\n\n<p>经过宏替换得到</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace detail &#123;                                                 </span><br><span class=\"line\">  const TypeMetaData _typeMetaDataInstance_preallocated_PreallocatedId</span><br><span class=\"line\">    = _makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>于是函数模板特例化最终形式为，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;&gt;</span><br><span class=\"line\">inline const detail::TypeMetaData*</span><br><span class=\"line\">TypeMeta::_typeMetaDataInstance&lt;float&gt;() noexcept &#123;</span><br><span class=\"line\">  return &amp;detail::_makeTypeMetaDataInstance&lt;float&gt;(_typeName&lt;float&gt;(&quot;float&quot;));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>detail::_makeTypeMetaDataInstance是一个模板函数，根据模板参数提供的类型创建相应类型的TypeMetaData实例，TypeMetaData是类型元数据，指定了类型在内存占多少字节空间（比如 float四个字节），类型名称，类型的构造函数、析构函数和拷贝函数等，以及类型的全局id，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">struct TypeMetaData final &#123;</span><br><span class=\"line\">// 函数类型的别名</span><br><span class=\"line\">using New = void*();                            // new</span><br><span class=\"line\">using PlacementNew = void(void*, size_t);       // 占位new</span><br><span class=\"line\">using Copy = void(const void*, void*, size_t);  // 类型数组拷贝</span><br><span class=\"line\">using PlacementDelete = void(void*, size_t);</span><br><span class=\"line\">using Delete = void(void*);</span><br><span class=\"line\">... //构造函数</span><br><span class=\"line\"></span><br><span class=\"line\">size_t itemsize_;  // 类型占多少字节</span><br><span class=\"line\">New* new_;</span><br><span class=\"line\">PlacementNew* placementNew_;   // 定位放置 new</span><br><span class=\"line\">Copy* copy_;        // 类型拷贝</span><br><span class=\"line\">Delete* delete_;    // 类型析构</span><br><span class=\"line\">TypeIdentifier id_; // 类型全局唯一id</span><br><span class=\"line\">const char* name_;  // 类型名称</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>我们还以float为例，看看如何构造这个类型元数据的实例，根据以上分析查看detail::_makeTypeMetaDataInstance 模板函数的定义</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class T&gt;</span><br><span class=\"line\">inline TypeMetaData _makeTypeMetaDataInstance(const char* typeName) &#123;</span><br><span class=\"line\">  return &#123;sizeof(T),                 // 类型T占多少字节</span><br><span class=\"line\">          _PickNew&lt;T&gt;(),             // 通过 new T</span><br><span class=\"line\">          _PickPlacementNew&lt;T&gt;(),</span><br><span class=\"line\">          _PickCopy&lt;T&gt;(),      </span><br><span class=\"line\">          _PickPlacementDelete&lt;T&gt;(),</span><br><span class=\"line\">          _PickDelete&lt;T&gt;(),</span><br><span class=\"line\">          TypeIdentifier::Get&lt;T&gt;(),  // 获取类型的全局唯一id，</span><br><span class=\"line\">          typeName&#125;;                 // 类型名称，例如float的名称为&quot;float&quot;</span><br></pre></td></tr></table></figure>\n\n<p>构造struct结构实例，按照struct内字段顺序传入字段的值直接{}构造，类型的全局唯一id的获取使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TypeIdentifier::Get&lt;T&gt;()</span><br></pre></td></tr></table></figure>\n\n<p>在上述宏定义CAFFE_DECLARE_PREALLOCATED_KNOWN_TYPE中给出这个函数（模板特例化）定义 ，其是通过调用TypeIdentifer(PreallocatedId)获取，对于float，PreallocatedId的实参值为6。</p>\n<p>对于其他类型如 int，double，int64_t等类似处理。</p>\n<p>PyTorch源码中给定了一些预定义好的类型及其全局唯一id值，如果是自定义变量，那么其全局唯一id则通过宏_CAFFE_KNOWN_TYPE_DEFINE_TYPEMETADATA_INSTANCE得到，具体而言是通过TypeIdentifier::createTypeId()得到，这个函数从PyTorch中预定义好的类型全局唯一id最大值（为32，对应类型为虚构的一个类型_CaffeHighestPreallocatedTypeId）开始，每次对一个自定义类型，id值增1。</p>\n<p>至此完成TypeMetaData实例的创建，从而完成TypeMeta（其内部维护TypeMetaData指针）创建，得到构造StorageImpl的第一个实参，回到前面的THStorage_(newWithSize)(ptrdiff_t size)的函数体部分，构造StorageImpl后面的实参分别为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">size,             // 被构造的StorageImpl包含多少类型变量（类型在TypeMeta中指定，例如float）</span><br><span class=\"line\">getTHDefaultAllocator(),  // 使用默认内存分配器，最终是使用posix_memalign函数实现内存分配</span><br><span class=\"line\">true                      // 被构造的StorageImpl可以resize</span><br></pre></td></tr></table></figure>\n\n<p>创建了StorageImpl实例后，就完成了THPStorage实例构造（其内部维护StorageImpl的指针），而THPStorage就对应 torch._C 模块中新增的类型FloatStorageBase</p>\n<p>记住，这里仅以float为例说明，THPStorage还可以对应其他类型如IntStorageBase等。</p>\n<p>FloatStorageBase的methods, members, properties 参考generic/Storage.cpp中THPStorage_(int)(PyObject* module)函数定义。</p>\n<p>类型 _THNN 和 _THCUNN 分别通过如下函数调用添加到模型 torch._C中，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  torch::nn::init_THNN(module);</span><br><span class=\"line\">#ifdef USE_CUDA</span><br><span class=\"line\">  torch::nn::init_THCUNN(module);</span><br><span class=\"line\">#endif</span><br></pre></td></tr></table></figure>\n\n<p>函数定义位于文件torch/csrc/nn目录下的THNN.cpp和THCUNN.cpp文件中，这两个文件是生成 torch_python 这个TARGET时使用 tools/setup_helpers/generate_code.py这个脚本生成的，具体参见 torch/CMakeLists.txt。</p>\n<p><code>torch._C</code>模块初始化过程到这里就完成了。回到 <code>torch/__init__.py</code>，继续看看 import torch时接下来做了哪些事情：</p>\n<ol>\n<li><p>定义了模块函数 typename，is_tensor，is_storage等</p>\n</li>\n<li><p>导入torch下其他子模块</p>\n</li>\n<li><p>调用_C._init_name，这个函数在文件torch/csrc/Module.cpp 中实现，用于将torch模块中的DoubleStorage名称改为 torch.DoubleStorage，其他类型如FloatStorage，HalfStorage则同样这么处理</p>\n</li>\n<li><p>调用_C._initExtension，这个函数同样在文件torch/csrc/Module.cpp 中实现，（阅读源码其实不难理解）所做的事情如下：</p>\n<ul>\n<li><p>初始化布局layout，向torch模块添加strided、sparse_coo和_mkldnn布局；</p>\n</li>\n<li><p>初始化内存格式，向torch模块添加any_format、preserve_format、contiguous_format和channels_last内存格式；</p>\n</li>\n<li><p>初始化类型，向torch模块添加uint8、int8、float64、float32、int32、int64、int16、float16、complex32、complex64、complex128、bool、qint8、quint8、qint32等类型，其中部分类型有旧名称，所以将旧名称类型也添加进torch模块；</p>\n</li>\n<li><p>初始化python绑定：1）初始化PyTensorType 类型实例，每个PyTensorType实例对应一组Backend和ScalarType；2）初始化torch.tensortype类型，表示torch.FloatTensor等Tensor的metaclass；3）初始化python的各个Tensor类，如torch.FloatTensor等；4）将各个Tensor类添加到模块 torch 中；5）设置FloatTensor为默认Tensor</p>\n</li>\n<li><p>共享内存管理初始化，设置文件路径；</p>\n</li>\n<li><p>执行 THPxxxStorage_postInit(module)，其中xxx是类型名称，这些函数的定义可与THPxxxStorage_Init 类似地得到，其中module是torch（而非torch._C），调用这个函数注册类型相关的Python storage类（比如Float对应torch.FloatStorage），</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch::registerStoragePyTypeObject((PyTypeObject*)THPStorageClass, backend, </span><br><span class=\"line\">TH_CONCAT_2(at::k, Real));</span><br></pre></td></tr></table></figure>\n\n<p>  其中 TH_CONCAT_2(at::k, Real)，即at::kReal由以下宏展开得到，是一个常量，当Real=Float时，其值为at::ScalarType::Float，</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_CONSTANT)`</span><br></pre></td></tr></table></figure>\n\n<p>  这个注册调用其实就是添加THPStorageClass与back+at::kReal之间的映射。</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>到这里，import torch 的工作全部完成。</p>\n<h1 id=\"后记：\"><a href=\"#后记：\" class=\"headerlink\" title=\"后记：\"></a>后记：</h1><p>初次阅读PyTorch源码，语言组织可能比较乱，加上鄙人还有很多东西没看懂，看懂的部分仅仅是零散分布的点，不一定能连成线，更加没有形成（知识）面，所以如果有错误，请直接指正，多谢。</p>\n"},{"title":"gcc-src","date":"2019-07-02T05:49:38.000Z","_content":"多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。\n\n以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去\n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone 这个位于 github 的远程仓库\n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html) 选择一个镜像网址，直接下载 gcc 源码\n\nC++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。\n\n我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。\n\n# Allocator\n我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，\n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    // 定义一系列 _Tp 内部类型的别名\nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：\n1. 提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 \n2. 提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：\n    - 前两个模板参数满足 _Tp::template rebind<_Up>::other 为有效定义，那么匹配第二个 __rebind 定义\n    - 否则，匹配第一个 __rebind 定义\n\n接着此文件定义了\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n根据前面的分析，只有 _Alloc::template rebind<_Up>::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 __rebind 模板定义，当 _Alloc 是模板类时，__alloc_rebind 为 _Alloc<_Up, ...>::type。\n\n然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，\n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type*。当然通常情况下，_Alloc::pointer 其实也就是 value_type* 类型。__pointer 来自基类成员类型，是一个模板类。  \n在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，\n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n定义了模板类 _Ptr，具有内部类型 type 为 _Func<_Alloc>，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 `pointer_traits<pointer>::template rebind<_Tp>`，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，\n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，\n```c++\n// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits<pointer>::difference_type，此时一般为（有符号长整型）\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型\nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。\n\n我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，\n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     // 假设 __cplusplus >= 201103L，其他情况这里不考虑\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits 就是上面刚讨论的那个特性类\n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域\n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n于是回到 std::allocator_traits 中查看例如 allocate 的定义，\n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏\nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。\n\n由于 std::allocator_traits::construct 的方法参数为，\n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 __gnu_cxx::__alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，\n```c++\n// 是否是自定义指针的判断\ntemplate<typename _Ptr>\nusing __is_custom_pointer\n// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 => __is_custom_pointer 为真\n// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假\n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n// 重载非标准指针类型的 构造函数\ntemplate<typename _Ptr, typename... _Args>\n// 条件判断，当 __is_custom_pointer<_Ptr> 为真时，enable_if<xx>::type 才存在\nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\n我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。\n\n接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。\n\nstd::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。\n\n# Iterator\n开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，\n```c++\n// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器\n// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法\ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。\n\n在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，\n\n1. input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。  \n   支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）\n2. output 迭代器。与 input 迭代器相反，依次向容器写入元素。  \n   支持的操作：自增（向前），解引用（左值，赋值）。\n3. forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。\n4. bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。\n5. random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。\n\n也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器\n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到\npublic:\n    // 构造函数略\n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值\n                        // 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变\n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // 反向迭代器表示从右往左，故自增表示正常迭代器的自减\n        return *this;\n    }\n}\n```\n记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如\n1. 解引用: *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\n还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）\n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // 指定迭代器标签，其他相关类型则由容器决定\n{\nprotected:\n    _Container* container;  // 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素\npublic:\n    back_insert_iterator&   // 给此迭代器赋值就等于向容器末端插入元素\n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       // 解引用不是取所指元素的值，因为是 output 迭代器\n    back_insert_iterator&\n    operator++() {return *this; }       // 自增也不是指向下一个元素，因为只能向容器末端插入值\n    back_insert_iterator&\n}\n```\n与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。\n\n## __normal_iterator\n这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，\n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator 的迭代操作实际上由 _M_current 完成\n    ...\npublic:\n    // 构造函数略\n    ...\n    // 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成\n    // 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成\n    // 确实是再 normal 不过了\n}\n```\n\n## move_iterator\n顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator 的特性类\n    typedef typename __traits_type::reference   __base_ref;     // 萃取 _Iterator 相关的元素引用类型\npublic:\n    ...\n    // 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型\n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // 将内部迭代器解引用得到的值转为右值引用\n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // 随机访问取值，也转为右值引用\n}\n```\n\n\n# Container\n## Vector\n以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor<_Tp>），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp*，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp*，所以 _Vector_base::pointer 就是 _Tp*。\n\n接着 _Vector_base 中又定义了几个内部结构\n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   // 指向 vector 中第一个元素的内存位置\n    pointer _M_finish;  // 指向 vector 中 past-the-last-element 的内存位置\n    pointer _M_end_of_storage;  // vector 分配 past-the-max-element 内存位置\n\n    // 构造函数，拷贝函数，交换数据函数。比较简单，略\n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // 构造函数，略\n    // vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer\n}\n```\n回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // 分配器变量\n    ...\n    // 构造/析构 函数\n\n    pointer _M_allocator(size_t __n) {      // 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中\n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        // 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存\n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // 分配内存，并保存内存起始位置和截止位置\n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，\n```c++\n// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，\n//  默认 _Alloc 为 std::allocator<_Tp>，显然是于 _Tp 匹配的，\n//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 __gnu::cxx::__normal_iterator 的模板参数 _Iterator。  \n然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 _M_impl._M_finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，\n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。\n\n其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size > old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size<=old_size，则重置 _M_impl._M_finish 所指位置（[_M_start, _M_finish) 范围内的元素有效），_M_finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。\n\n来看 vector 的 push_back 函数实现，\n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        // 当前分配的内存空间还足以存储新的元素 __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  // 重新分配内存，并在 _M_finish 位置插入元素 __x，然后\n                                        //  将 _M_finish 所指位置向前移动一个元素单位\n}\n```\n在 bits/vector.tcc 中找到 _M_realloc_insert 的实现，\n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    // 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; // 原来的起始元素指针和 past-the-last 元素指针\n    const size_type __elems_before = __position - begin();// 插入位置之前的元素数量\n    pointer __new_start(this->_M_allocate(__len));   // 重新分配内存，使得能容纳 __len 个元素\n    pointer __new_finish(__new_start);  // 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等\n    __try\n    {\n        // 在指定位置处插入目标对象\n        _Alloc_traits::construct(this->_M_impl,                     // 使用此分配器\n                                 __new_start + __elems_before,      // 在指定位置处\n                                 std::forward<_Args>(__args)...);   // 根据此参数构造对象\n        // 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // 如果元素类型支持移动插入\n            // 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置\n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            // 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   // 此时 __new_finish 就是新的 past-of-last 元素位置了\n        }\n        ...\n        // 失败处理，略\n        // 析构原先内存上的对象，并释放内存，略\n        // 更新元素起始和截止位置等，略\n    }\n}\n```\nvector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。\n\n本文结束","source":"_posts/gcc-src.md","raw":"---\ntitle: gcc-src\ndate: 2019-07-02 13:49:38\ntags: c++\n---\n多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。\n\n以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去\n1. [gcc-mirror/gcc](https://github.com/gcc-mirror/gcc) clone 这个位于 github 的远程仓库\n2. [GNU Mirror List](http://www.gnu.org/prep/ftp.html) 选择一个镜像网址，直接下载 gcc 源码\n\nC++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。\n\n我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。\n\n# Allocator\n我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，\n```c++\nstruct __allocator_traits_base\n{\n    template<typename _Tp, typename _Up, typename = void>\n    struct __rebind : __replace_first_arg<_Tp, _Up> { };\n    template<typename _Tp, typename _Up>\n    struct __rebind<_Tp, _Up, __void_t<typename _Tp::template rebind<_Up>::other>>\n    { using type = typename _Tp::template rebind<_Up>::other; };\n\n    // 定义一系列 _Tp 内部类型的别名\nprotected:\n    template<typename _Tp>\n    using __pointer = typename _Tp::pointer;\n    ...\n};\n```\n类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：\n1. 提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 \n2. 提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：\n    - 前两个模板参数满足 _Tp::template rebind<_Up>::other 为有效定义，那么匹配第二个 __rebind 定义\n    - 否则，匹配第一个 __rebind 定义\n\n接着此文件定义了\n```c++\ntemplate<typename _Alloc, typename _Up>\nusing __alloc_rebind = typename __allocator_traits_base::template __rebind<_Alloc, _Up>::type;\n```\n根据前面的分析，只有 _Alloc::template rebind<_Up>::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 __rebind 模板定义，当 _Alloc 是模板类时，__alloc_rebind 为 _Alloc<_Up, ...>::type。\n\n然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，\n```c++\ntemplate<typename _Alloc>\nstruct allocator_traits: _allocator_traits_base\n{\n    typedef _Alloc allocator_type;\n    typedef type _Alloc::value_type value_type;\n\n    using pointer = __detected_or_t<value_type*, __pointer, _Alloc>;\n    ...\n};\n```\n类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type*。当然通常情况下，_Alloc::pointer 其实也就是 value_type* 类型。__pointer 来自基类成员类型，是一个模板类。  \n在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，\n```c++\ntemplate<template<typename> class _Func, typename _Tp, typename = void>\nstruct _Ptr\n{\n    using type = typename pointer_traits<pointer>::template rebind<_Tp>;\n};\ntemplate<template<typename> class _Func, typename _Tp>\nstruct _Ptr<_Func, _Tp, __void_t<_Func<_Alloc>>>\n{\n    using type = _Func<_Alloc>;\n};\n```\n定义了模板类 _Ptr，具有内部类型 type 为 _Func<_Alloc>，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 `pointer_traits<pointer>::template rebind<_Tp>`，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，\n```\ntemplate<typename _Tp>\nstruct pointer_traits<_Tp*>\n{\n    ...\n    template<typename _Up>\n    using rebind = _Up*;\n    ...\n};\n```\n可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，\n```c++\n// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*\nusing const_pointer = typename _Ptr<__c_pointer, const value_type>::type;\n// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*\nusing void_pointer = typename _Ptr<__v_pointer, void>::type;\n// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits<pointer>::difference_type，此时一般为（有符号长整型）\nusing difference_type = typename _Diff<_Alloc, pointer>::type;\n// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型\nusing size_type = typename _Size<_Alloc, difference_type>::type;\n```\n篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。\n\n我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，\n```c++\ntemplate<typename _Alloc, typename = typename _Alloc::value_type>\nstruct __alloc_traits\n    : std::allocator_traits<_Alloc>     // 假设 __cplusplus >= 201103L，其他情况这里不考虑\n{\n    typedef _Alloc allocator_type;\n    // std::allocator_traits 就是上面刚讨论的那个特性类\n    typedef std::allocator_traits<_Alloc>               _Base_type;\n    typedef typename _Base_type::value_type             value_type;\n    // _Alloc::pointer or value_type*\n    typedef typename _Base_type::pointer                pointer;\n    typedef typename _Base_type::const_pointer          const_pointer;\n    typedef typename _Base_type::size_type              size_type;\n    typedef typename _Base_type::difference_type        difference_type;\n    typedef value_type&                                 reference;\n    typedef const value_type&                           const_reference;\n    // 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域\n    using _Base_type::allocate;\n    using _Base_type::deallocate;\n    using _Base_type::construct;\n    using _Base_type::destroy;\n    using _Base_type::max_size;\n    ...\n}\n```\n于是回到 std::allocator_traits 中查看例如 allocate 的定义，\n```c++\n_GLIBCXX_NODISCARD static pointer       // _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏\nallocate(_Alloc& __a, size_type __n)\n{ return __a.allocate(__n); }\n\n_GLIBCXX_NODISCARD static pointer\nallocate(_Alloc& __a, size_type __n, const_void_pointer __hint) \n{ return _S_allocate(__a, __n, __hint, 0); }\n```\n上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。\n\n由于 std::allocator_traits::construct 的方法参数为，\n```c++\ntemplate<typename _Tp, typename... _Args>\nstatic auto construct(_Alloc& __a, _Tp* __p, _Args&&... __args)\n...\n```\n发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 __gnu_cxx::__alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，\n```c++\n// 是否是自定义指针的判断\ntemplate<typename _Ptr>\nusing __is_custom_pointer\n// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 => __is_custom_pointer 为真\n// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假\n= std::__and_<std::is_same<pointer, _Ptr>,\n        std::__not_<std::is_pointer<_Ptr>>>;\n\n// 重载非标准指针类型的 构造函数\ntemplate<typename _Ptr, typename... _Args>\n// 条件判断，当 __is_custom_pointer<_Ptr> 为真时，enable_if<xx>::type 才存在\nstatic typename std::enable_if<__is_custom_pointer<_Ptr>::value>::type\nconstruct(_Alloc& __a, _Ptr __p, _Args&&... __args)\n...\n```\n我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。\n\n接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。\n\nstd::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。\n\n# Iterator\n开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，\n```c++\n// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器\n// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法\ninput_iterator_tag\noutput_iterator_tag\nforward_iterator_tag\nbidirectional_iterator_tag\nrandom_access_iterator_tag\n```\n对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。\n\n在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，\n\n1. input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。  \n   支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）\n2. output 迭代器。与 input 迭代器相反，依次向容器写入元素。  \n   支持的操作：自增（向前），解引用（左值，赋值）。\n3. forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。\n4. bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。\n5. random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。\n\n也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。\n## reverse_iterator\n```c++\ntemplate<typename _Iterator>\nclass reverse_iterator      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器\n: public iterator<typename iterator_traits<_Iterator>::iterator_category,\n                typename iterator_traits<_Iterator>::value_type,\n                typename iterator_traits<_Iterator>::difference_type,\n                typename iterator_traits<_Iterator>::pointer,\n                typename iterator_traits<_Iterator>::reference>\n{\nprotected:\n    _Iterator current;  // 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到\npublic:\n    // 构造函数略\n    ...\n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const {\n        _Iterator __tmp = current;\n        return *--__tmp;// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值\n                        // 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变\n    }\n\n    _GLIBCXX17_CONSTEXPR reverse_iterator&\n    operator++() {\n        --current;      // 反向迭代器表示从右往左，故自增表示正常迭代器的自减\n        return *this;\n    }\n}\n```\n记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如\n1. 解引用: *r = *(i-1)\n2. ++r = --i, --r = ++i\n3. r+n = i-n, r-n = i+n\n\n还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）\n\n## back_insert_iterator\n```c++\ntemplate<typename _Container>\nclass back_insert_iterator\n:public iterator<output_iterator_tag, void, void, void, void> // 指定迭代器标签，其他相关类型则由容器决定\n{\nprotected:\n    _Container* container;  // 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素\npublic:\n    back_insert_iterator&   // 给此迭代器赋值就等于向容器末端插入元素\n    operator=(const typename _Container::value_type& __value)\n    {\n        container->push_back(__value);\n        return *this;\n    }\n    ...\n\n    back_insert_iterator&\n    operator*() { return *this; }       // 解引用不是取所指元素的值，因为是 output 迭代器\n    back_insert_iterator&\n    operator++() {return *this; }       // 自增也不是指向下一个元素，因为只能向容器末端插入值\n    back_insert_iterator&\n}\n```\n与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。\n\n## __normal_iterator\n这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，\n```c++\ntemplate<typename _Iterator, typename _Container>\nclass __normal_iterator\n{\nprotected:\n    _Iterator _M_current;   // _normal_iterator 的迭代操作实际上由 _M_current 完成\n    ...\npublic:\n    // 构造函数略\n    ...\n    // 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成\n    // 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成\n    // 确实是再 normal 不过了\n}\n```\n\n## move_iterator\n顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。\n```c++\ntemplate<typename _Iterator>\nclass move_iterator\n{\nprotected:\n    _Iterator _M_current;\n    typedef iterator_traits<_Iterator>          __traits_type;  // _Iterator 的特性类\n    typedef typename __traits_type::reference   __base_ref;     // 萃取 _Iterator 相关的元素引用类型\npublic:\n    ...\n    // 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型\n    typedef typename conditional<is_reference<__base_ref>::value,\n        typename remove_reference<__base_ref>::type&&,\n        __base_ref>::type               reference;\n    \n    _GLIBCXX17_CONSTEXPR reference\n    operator*() const\n    { return static_cast<reference>(*_M_current); } // 将内部迭代器解引用得到的值转为右值引用\n\n    _GLIBCXX17_CONSTEXPR reference\n    operator[](difference_type __n) const\n    { return std::move(_M_current[__n]); }  // 随机访问取值，也转为右值引用\n}\n```\n\n\n# Container\n## Vector\n以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template rebind<_Tp>::other _Tp_alloc_type;\n    typedef typename __gnu_cxx::__alloc_traits<_Tp_alloc_type>::pointer pointer;\n    ...\n}\n```\n\n模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor<_Tp>），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp*，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp*，所以 _Vector_base::pointer 就是 _Tp*。\n\n接着 _Vector_base 中又定义了几个内部结构\n```c++\nstruct _Vector_impl_data\n{\n    pointer _M_start;   // 指向 vector 中第一个元素的内存位置\n    pointer _M_finish;  // 指向 vector 中 past-the-last-element 的内存位置\n    pointer _M_end_of_storage;  // vector 分配 past-the-max-element 内存位置\n\n    // 构造函数，拷贝函数，交换数据函数。比较简单，略\n    ...\n}\n\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data\n{\n    // 构造函数，略\n    // vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer\n}\n```\n回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，\n```c++\ntemplate<typename _Tp, typename _Alloc>\nstruct _Vector_base\n{\n    ...\npublic:\n    typedef _Alloc allocator_type;\n    _Vector_impl _M_impl;           // 分配器变量\n    ...\n    // 构造/析构 函数\n\n    pointer _M_allocator(size_t __n) {      // 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中\n        typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;\n        // 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存\n        return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();\n    }\n\nprotected:\n    void _M_create_storage(size_t __n) {    // 分配内存，并保存内存起始位置和截止位置\n        this->_M_impl._M_start = this->_M_allocate(__n);\n        this->_M_impl._M_finish = this->_M_impl._M_start;\n        this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;\n    }\n}\n```\n基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，\n```c++\n// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，\n//  默认 _Alloc 为 std::allocator<_Tp>，显然是于 _Tp 匹配的，\n//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor\ntemplate<typename _Tp, typename _Alloc = std::allocator<_Tp>>\nclass vector : protected _Vector_base<_Tp, _Alloc>\n{\n    typedef _Vector_base<_Tp, _Alloc>               _Base;\n    typedef typename _Base::_Tp_alloc_type          _Tp_alloc_type;\n    typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type>   _Alloc_traits;\npublic:\n    typedef _Tp                             value_type;\n    typedef typename _Base::pointer         pointer;\n    typedef __gnu_cxx::__normal_iterator<pointer, vector>   iterator;\n    typedef std::reverse_iterator<iterator>                 reverse_iterator;\n    ...\n}\n```\n事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 __gnu::cxx::__normal_iterator 的模板参数 _Iterator。  \n然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 _M_impl._M_finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，\n```c++\niterator\nbegin() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_start); }\n\niterator\nend() _GLIBCXX_NOEXCEPT\n{ return iterator(this->_M_impl._M_finish); }\n\nreverse_iterator\nrbegin() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(end()); }\n\nreverse_iterator\nrend() _GLIBCXX_NOEXCEPT\n{ return reverse_iterator(begin()); }\n```\n可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。\n\n其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size > old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size<=old_size，则重置 _M_impl._M_finish 所指位置（[_M_start, _M_finish) 范围内的元素有效），_M_finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。\n\n来看 vector 的 push_back 函数实现，\n```c++\nvoid\npush_back(const value_type& __x)\n{\n    if(this->_M_impl._M_finish != this->_M_impl._M_end_of_storage) {\n        // 当前分配的内存空间还足以存储新的元素 __x\n        ...\n    } else\n        _M_realloc_insert(end(), __x);  // 重新分配内存，并在 _M_finish 位置插入元素 __x，然后\n                                        //  将 _M_finish 所指位置向前移动一个元素单位\n}\n```\n在 bits/vector.tcc 中找到 _M_realloc_insert 的实现，\n```c++\ntemplate<typename _Tp, typename _Alloc>\ntemplate<typename ..._Arg>\nvoid\nvector<_Tp, _Alloc>::_M_realloc_insert(iterator __position, _Args&&... _args)\n{\n    // 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len\n    const size_type __len = _M_check_len(size_type(1), \"vector::_M_realloc_insert\");\n    pointer __old_start = this->_M_impl._M_start;\n    pointer __old_finish = this->_M_impl._M_finish; // 原来的起始元素指针和 past-the-last 元素指针\n    const size_type __elems_before = __position - begin();// 插入位置之前的元素数量\n    pointer __new_start(this->_M_allocate(__len));   // 重新分配内存，使得能容纳 __len 个元素\n    pointer __new_finish(__new_start);  // 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等\n    __try\n    {\n        // 在指定位置处插入目标对象\n        _Alloc_traits::construct(this->_M_impl,                     // 使用此分配器\n                                 __new_start + __elems_before,      // 在指定位置处\n                                 std::forward<_Args>(__args)...);   // 根据此参数构造对象\n        // 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值\n        __new_finish = pointer();\n\n        if _GLIBCXX17_CONSTEXPR (_S_use_relocate()) {   // 如果元素类型支持移动插入\n            // 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置\n            __new_finish = _S_relocate(__old_start, __position.base()\n                __new_start, _M_get_Tp_allocator());\n            // 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处\n            ++__new_finish;\n            __new_finish = _S_relocate(__position.base(), __old_finish,\n                __new_finish, _M_get_Tp_allocator());   // 此时 __new_finish 就是新的 past-of-last 元素位置了\n        }\n        ...\n        // 失败处理，略\n        // 析构原先内存上的对象，并释放内存，略\n        // 更新元素起始和截止位置等，略\n    }\n}\n```\nvector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。\n\n本文结束","slug":"gcc-src","published":1,"updated":"2019-07-13T10:32:47.398Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy8l5q1b001hxgvc2g60t7fr","content":"<p>多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。</p>\n<p>以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去</p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone 这个位于 github 的远程仓库</li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a> 选择一个镜像网址，直接下载 gcc 源码</li>\n</ol>\n<p>C++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。</p>\n<p>我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。</p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p>我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义一系列 _Tp 内部类型的别名</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：</p>\n<ol>\n<li>提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 </li>\n<li>提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：<ul>\n<li>前两个模板参数满足 _Tp::template rebind&lt;_Up&gt;::other 为有效定义，那么匹配第二个 __rebind 定义</li>\n<li>否则，匹配第一个 __rebind 定义</li>\n</ul>\n</li>\n</ol>\n<p>接着此文件定义了</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p>根据前面的分析，只有 _Alloc::template rebind&lt;_Up&gt;::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 <strong>rebind 模板定义，当 _Alloc 是模板类时，</strong>alloc_rebind 为 _Alloc&lt;_Up, …&gt;::type。</p>\n<p>然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type<em>。当然通常情况下，_Alloc::pointer 其实也就是 value_type</em> 类型。__pointer 来自基类成员类型，是一个模板类。<br>在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>定义了模板类 _Ptr，具有内部类型 type 为 _Func&lt;_Alloc&gt;，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code>，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind = _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits&lt;pointer&gt;::difference_type，此时一般为（有符号长整型）</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p>篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。</p>\n<p>我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">// 假设 __cplusplus &gt;= 201103L，其他情况这里不考虑</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits 就是上面刚讨论的那个特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>于是回到 std::allocator_traits 中查看例如 allocate 的定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏</span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。</p>\n<p>由于 std::allocator_traits::construct 的方法参数为，</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 <strong>gnu_cxx::</strong>alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 是否是自定义指针的判断</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 =&gt; __is_custom_pointer 为真</span></span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假</span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 重载非标准指针类型的 构造函数</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">// 条件判断，当 __is_custom_pointer&lt;_Ptr&gt; 为真时，enable_if&lt;xx&gt;::type 才存在</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。</p>\n<p>接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。</p>\n<p>std::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。</p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p>开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器</span></span><br><span class=\"line\"><span class=\"comment\">// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法</span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n\n<p>对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。</p>\n<p>在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，</p>\n<ol>\n<li>input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。<br>支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）</li>\n<li>output 迭代器。与 input 迭代器相反，依次向容器写入元素。<br>支持的操作：自增（向前），解引用（左值，赋值）。</li>\n<li>forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。</li>\n<li>bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。</li>\n<li>random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。</li>\n</ol>\n<p>也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。</p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器</span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值</span></span><br><span class=\"line\">                        <span class=\"comment\">// 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// 反向迭代器表示从右往左，故自增表示正常迭代器的自减</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如</p>\n<ol>\n<li>解引用: *r = *(i-1)</li>\n<li>++r = –i, –r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）</p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// 指定迭代器标签，其他相关类型则由容器决定</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// 给此迭代器赋值就等于向容器末端插入元素</span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 解引用不是取所指元素的值，因为是 output 迭代器</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 自增也不是指向下一个元素，因为只能向容器末端插入值</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。</p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p>这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator 的迭代操作实际上由 _M_current 完成</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 确实是再 normal 不过了</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p>顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator 的特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">// 萃取 _Iterator 相关的元素引用类型</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// 将内部迭代器解引用得到的值转为右值引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::move(_M_current[__n]); &#125;  <span class=\"comment\">// 随机访问取值，也转为右值引用</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p>以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor&lt;_Tp&gt;），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp<em>，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp</em>，所以 _Vector_base::pointer 就是 _Tp*。</p>\n<p>接着 _Vector_base 中又定义了几个内部结构</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">// 指向 vector 中第一个元素的内存位置</span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">// 指向 vector 中 past-the-last-element 的内存位置</span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector 分配 past-the-max-element 内存位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，拷贝函数，交换数据函数。比较简单，略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，略</span></span><br><span class=\"line\">    <span class=\"comment\">// vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// 分配器变量</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 构造/析构 函数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">// 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中</span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">// 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// 分配内存，并保存内存起始位置和截止位置</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，</span></span><br><span class=\"line\"><span class=\"comment\">//  默认 _Alloc 为 std::allocator&lt;_Tp&gt;，显然是于 _Tp 匹配的，</span></span><br><span class=\"line\"><span class=\"comment\">//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 <strong>gnu::cxx::</strong>normal_iterator 的模板参数 _Iterator。<br>然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 <em>M</em>impl.<em>M</em>finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\">begin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\">end() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(end()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(begin()); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。</p>\n<p>其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size &gt; old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size&lt;=old_size，则重置 <em>M</em>impl.<em>M</em>finish 所指位置（[<em>M</em>start, <em>M</em>finish) 范围内的元素有效），<em>M</em>finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。</p>\n<p>来看 vector 的 push_back 函数实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 当前分配的内存空间还足以存储新的元素 __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(end(), __x);  <span class=\"comment\">// 重新分配内存，并在 _M_finish 位置插入元素 __x，然后</span></span><br><span class=\"line\">                                        <span class=\"comment\">//  将 _M_finish 所指位置向前移动一个元素单位</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在 bits/vector.tcc 中找到 <em>M</em>realloc_insert 的实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">// 原来的起始元素指针和 past-the-last 元素指针</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - begin();<span class=\"comment\">// 插入位置之前的元素数量</span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">// 重新分配内存，使得能容纳 __len 个元素</span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">// 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等</span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 在指定位置处插入目标对象</span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// 使用此分配器</span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// 在指定位置处</span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// 根据此参数构造对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// 如果元素类型支持移动插入</span></span><br><span class=\"line\">            <span class=\"comment\">// 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置</span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">// 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">// 此时 __new_finish 就是新的 past-of-last 元素位置了</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// 失败处理，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 析构原先内存上的对象，并释放内存，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 更新元素起始和截止位置等，略</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>vector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。</p>\n<p>本文结束</p>\n","site":{"data":{}},"excerpt":"","more":"<p>多阅读 c++ 标准库源码，才能更好的理解 c++ 标准库。</p>\n<p>以 ubuntu 为例，gcc 版本为 7.3.0，目录 /usr/include/c++/7/ 包含了大多数标准库（头）文件，标准库的大多数的实现逻辑也在这些头文件中，如要获取完整的源码，则可以去</p>\n<ol>\n<li><a href=\"https://github.com/gcc-mirror/gcc\" target=\"_blank\" rel=\"noopener\">gcc-mirror/gcc</a> clone 这个位于 github 的远程仓库</li>\n<li><a href=\"http://www.gnu.org/prep/ftp.html\" target=\"_blank\" rel=\"noopener\">GNU Mirror List</a> 选择一个镜像网址，直接下载 gcc 源码</li>\n</ol>\n<p>C++ 标准模板库包含容器，以及容器相关的算法，涉及到的概念包括容器，算法，迭代器以及分配器等，各自功能从名称可窥见一二。</p>\n<p>我们采用自底向上的方式来分析各类的实现，虽然自顶向下才是阅读这些源码的最自然的方式，不过阅读方式并不影响什么，待熟悉了这些类的各自功能后，反过来梳理一遍正好可以加深印象。</p>\n<h1 id=\"Allocator\"><a href=\"#Allocator\" class=\"headerlink\" title=\"Allocator\"></a>Allocator</h1><p>我们以 __allocator_traits_base 为例开始分析，此类位于 libstdc++-v3/include/bits/alloc_traits.h 头文件中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">allocator_traits_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up, <span class=\"keyword\">typename</span> = <span class=\"keyword\">void</span>&gt;</span><br><span class=\"line\">    struct __rebind : __replace_first_arg&lt;_Tp, _Up&gt; &#123; &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">rebind</span>&lt;_Tp, _Up, __void_t&lt;typename _Tp::template rebind&lt;_Up&gt;::other&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">    &#123;</span> <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> _Tp::<span class=\"keyword\">template</span> rebind&lt;_Up&gt;::other; &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义一系列 _Tp 内部类型的别名</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp&gt;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> __pointer = <span class=\"keyword\">typename</span> _Tp::pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>类内部定义了模板类 __rebind，对于类 __rebind，分以下几种情况讨论：</p>\n<ol>\n<li>提供三个模板参数，并且第三个模板参数不为 void，此时匹配最泛型模板类，即第一个 __rebind 定义 </li>\n<li>提供三个模板参数并且第三个模板参数为 void，或者仅提供两个模板参数，此时再分两种情况：<ul>\n<li>前两个模板参数满足 _Tp::template rebind&lt;_Up&gt;::other 为有效定义，那么匹配第二个 __rebind 定义</li>\n<li>否则，匹配第一个 __rebind 定义</li>\n</ul>\n</li>\n</ol>\n<p>接着此文件定义了</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> _Up&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __alloc_rebind = <span class=\"keyword\">typename</span> __allocator_traits_base::<span class=\"keyword\">template</span> __rebind&lt;_Alloc, _Up&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p>根据前面的分析，只有 _Alloc::template rebind&lt;_Up&gt;::other 这个类型存在时，这个别名才存在，并且就是这个类型的别名，否则的话，根据第一个 <strong>rebind 模板定义，当 _Alloc 是模板类时，</strong>alloc_rebind 为 _Alloc&lt;_Up, …&gt;::type。</p>\n<p>然后就是 allocator_traits 类，这个特性用于萃取分配器相关的类型，定义如下，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">allocator_traits</span>:</span> _allocator_traits_base</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> type _Alloc::value_type value_type;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">using</span> pointer = <span class=\"keyword\">__detected_or_t</span>&lt;value_type*, __pointer, _Alloc&gt;;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>类中 pointer 这个别名指的是 _Alloc::pointer 类型，如果这个类型存在的话，否则就是类型 value_type<em>。当然通常情况下，_Alloc::pointer 其实也就是 value_type</em> 类型。__pointer 来自基类成员类型，是一个模板类。<br>在 std/type_traits 文件中可查看 __detected_or_t 定义，与前文 __rebind 匹配规则类似，不再赘述。我们再继续看 allocator_traits 其他内部类，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>, <span class=\"title\">typename</span> = <span class=\"title\">void</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = <span class=\"keyword\">typename</span> pointer_traits&lt;pointer&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span>&gt; <span class=\"class\"><span class=\"keyword\">class</span> _<span class=\"title\">Func</span>, <span class=\"title\">typename</span> _<span class=\"title\">Tp</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">struct</span> _<span class=\"title\">Ptr</span>&lt;_Func, _Tp, __void_t&lt;_Func&lt;_Alloc&gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> type = _Func&lt;_Alloc&gt;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>定义了模板类 _Ptr，具有内部类型 type 为 _Func&lt;_Alloc&gt;，当这个类型存在时，也就是说 _Func 是模板类型，否则 type 就是 <code>pointer_traits&lt;pointer&gt;::template rebind&lt;_Tp&gt;</code>，此时，假设 pointer 为 value_type* 类型（参见上文介绍），根据 bits/ptr_traits.h 中的 pointer_traits 定义，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;typename _Tp&gt;</span><br><span class=\"line\">struct pointer_traits&lt;_Tp*&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    template&lt;typename _Up&gt;</span><br><span class=\"line\">    using rebind = _Up*;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>可知此时 _Ptr::type 为 _Tp* 类型。allocator_traits 内部还定义了很多模板类，比如 _Diff，其类型成员 type 表示指针位移类型（一般是有符号长整型），_Size 的类型成员 type 表示数量类型（一般是无符号长整型），对 _Ptr::type, _Diff::type 和 _Size::type 设置类型别名，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果 _Alloc::const_pointer 存在，则为 _Alloc::const_pointer，否则为 const value_type*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> const_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__c_pointer, <span class=\"keyword\">const</span> value_type&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::void_pointer 类型，如果这个类型存在的话，否则为 void*</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> void_pointer = <span class=\"keyword\">typename</span> _Ptr&lt;__v_pointer, <span class=\"keyword\">void</span>&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::difference_type 类型，如果它存在的话，否则为 pointer_traits&lt;pointer&gt;::difference_type，此时一般为（有符号长整型）</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> difference_type = <span class=\"keyword\">typename</span> _Diff&lt;_Alloc, pointer&gt;::type;</span><br><span class=\"line\"><span class=\"comment\">// 为 _Alloc::size_type 类型，如果它存在的话，否则为 difference_type 的无符号版本类型</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> size_type = <span class=\"keyword\">typename</span> _Size&lt;_Alloc, difference_type&gt;::type;</span><br></pre></td></tr></table></figure>\n\n<p>篇幅有限，不一一介绍，后面的讨论中如果遇到，则根据需要再进行展开讨论。</p>\n<p>我们再看一个类 __alloc_traits，位于 ext/alloc_traits.h 中，看看它提供了哪些类型萃取，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Alloc, <span class=\"keyword\">typename</span> = <span class=\"keyword\">typename</span> _Alloc::value_type&gt;</span><br><span class=\"line\">struct __alloc_traits</span><br><span class=\"line\">    : <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;     <span class=\"comment\">// 假设 __cplusplus &gt;= 201103L，其他情况这里不考虑</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    <span class=\"comment\">// std::allocator_traits 就是上面刚讨论的那个特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::allocator_traits&lt;_Alloc&gt;               _Base_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::value_type             value_type;</span><br><span class=\"line\">    <span class=\"comment\">// _Alloc::pointer or value_type*</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::pointer                pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::const_pointer          const_pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::size_type              size_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base_type::difference_type        difference_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> value_type&amp;                                 reference;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">const</span> value_type&amp;                           const_reference;</span><br><span class=\"line\">    <span class=\"comment\">// 以上各类型含义已经非常明显易懂了，不再赘述。以下引入一组方法到当前域</span></span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::allocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::deallocate;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::construct;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::destroy;</span><br><span class=\"line\">    <span class=\"keyword\">using</span> _Base_type::max_size;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>于是回到 std::allocator_traits 中查看例如 allocate 的定义，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer       <span class=\"comment\">// _GLIBCXX_NODISCARD 指示编译器，如果返回结果被抛弃，则编译器发出警告。显然这么做是应该的，否则动态申请的内存，将无法被释放，造成内存泄漏</span></span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n)</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> __a.allocate(__n); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">_GLIBCXX_NODISCARD <span class=\"keyword\">static</span> pointer</span><br><span class=\"line\">allocate(_Alloc&amp; __a, size_type __n, const_void_pointer __hint) </span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> _S_allocate(__a, __n, __hint, <span class=\"number\">0</span>); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>上面代码片段中，_Alloc 表示分配器类型，第一个 allocate 模板直接调用分配器 __a 分配 __n 个元素的内存，第二个 allocate 模板增加了一个参数 __hint 指向临近内存位置的指针，分配器会尝试尽可能分配靠近 __hint 的内存块。易知，分配器特性类的 allocate 方法实际上依赖具体分配器的 allocate 方法实现。实际上，不光是 allocate，deallocate, construct, destroy, max_size 也可能依赖于分配器的同名方法实现（当然，如果目标类型 _Tp 有相应方法实现，则依赖于 _Tp 的同名方法实现）。</p>\n<p>由于 std::allocator_traits::construct 的方法参数为，</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">auto</span> <span class=\"title\">construct</span><span class=\"params\">(_Alloc&amp; __a, _Tp* __p, _Args&amp;&amp;... __args)</span></span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>发现参数类型为 _Tp*，这是 _Tp 类型的标准内存指针，在 <strong>gnu_cxx::</strong>alloc_traits 中还实现了使用自定义指针类型作为参数的 construct 方法，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 是否是自定义指针的判断</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr&gt;</span><br><span class=\"line\"><span class=\"keyword\">using</span> __is_custom_pointer</span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 相同，那么 _Ptr 不是指针时 =&gt; __is_custom_pointer 为真</span></span><br><span class=\"line\"><span class=\"comment\">// 如果 _Ptr 与 pointer 不同，那么 __is_custom_pointer 为假</span></span><br><span class=\"line\">= <span class=\"built_in\">std</span>::__and_&lt;<span class=\"built_in\">std</span>::is_same&lt;pointer, _Ptr&gt;,</span><br><span class=\"line\">        <span class=\"built_in\">std</span>::__not_&lt;<span class=\"built_in\">std</span>::is_pointer&lt;_Ptr&gt;&gt;&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 重载非标准指针类型的 构造函数</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Ptr, <span class=\"keyword\">typename</span>... _Args&gt;</span><br><span class=\"line\"><span class=\"comment\">// 条件判断，当 __is_custom_pointer&lt;_Ptr&gt; 为真时，enable_if&lt;xx&gt;::type 才存在</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;__is_custom_pointer&lt;_Ptr&gt;::value&gt;::type</span><br><span class=\"line\">construct(_Alloc&amp; __a, _Ptr __p, _Args&amp;&amp;... __args)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>我们阶段性地总结一下以上分配器特性类，主要提供了与分配器有关的类型萃取，如分配器类型，分配的目标对象类型（包含值类型，const 型，指针型），分配的目标对象数量类型（因为是用于容器/序列的分配，涉及到元素对象的数量），位移类型（常用于序列的迭代器）等。另外还提供了一些方法，比如 rebind，由于模板参数 _Alloc 可由调用者传入，假如传入的 _Alloc 其用于分配的模板对象类型 value_type 与当前分配器特性类 allocator_traits 的 value_type 不一致，那么 rebind 将重新绑定得到与 allocator_traits::value_type 一致的分配器类型 Allocator。</p>\n<p>接下来则是分配器类，注意与分配器特性类区别开来，后者更注重与分配器有关的类型萃取，前者更注重完成分配器的如分配，反分配，对象构造/析构等实际工作。可能是故意分开成两个类，这种设计能提高自由度，当然，这是我个人理解。</p>\n<p>std::allocator 类位于文件 bits/allocator.h 中，包含了分配器模板类定义和偏特化模板类定义，现在理解这些代码应该比较容易了，其中最泛化的模板类继承了 __allocator_base，这个类为 __gnu_cxx::new_allocator 的类型别名，在 new_allocator 中我们可以看到 allocate, deallocate, max_size, construct, destroy 等方法实现。读者可仔细阅读这些源码，这里不再一一分析。</p>\n<h1 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h1><p>开门见山不绕弯子，位于 bit/stl_iterator_base_types.h 中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 定义一组 Iterator 标记，它们都是空类型，仅仅用于区分不同的迭代器</span></span><br><span class=\"line\"><span class=\"comment\">// 迭代器底层的算法会根据迭代器本身的类型标记来选择最优算法</span></span><br><span class=\"line\">input_iterator_tag</span><br><span class=\"line\">output_iterator_tag</span><br><span class=\"line\">forward_iterator_tag</span><br><span class=\"line\">bidirectional_iterator_tag</span><br><span class=\"line\">random_access_iterator_tag</span><br></pre></td></tr></table></figure>\n\n<p>对于一个迭代器，需要指定迭代器自身的类型（上述某 iterator_tag 之一），迭代目标对象的值/指针/引用类型，迭代位移类型等，这正是 std::iterator 的定义，然后还需要一个相关的特性模板 iterator_traits 用于萃取其相关的类型。</p>\n<p>在 bits/stl_iterator.h 中还提供了几个迭代器适配器，其本质也是一个迭代器，只不过是提供某些专有功能的迭代器。我们先阐述上面五种迭代器类型，然后再结合迭代器适配器理解更有心得，</p>\n<ol>\n<li>input 迭代器。看到 input 可以将容器类比标准输入，比如从屏幕读取输入，这里 input 迭代器类似，从容器读取元素，迭代器迭代器，说明是依次向前读取容器内的元素。<br>支持的操作：自增（向前），解引用（右值，取值），判断两个迭代器是否相等（是否迭代到头）</li>\n<li>output 迭代器。与 input 迭代器相反，依次向容器写入元素。<br>支持的操作：自增（向前），解引用（左值，赋值）。</li>\n<li>forward 迭代器。结合了 input 和 output 迭代器，解引用，既可作左值也可作右值。自增指向下一个元素。与 input/output 迭代器不同的是，forward 迭代器支持 multipass 算法。</li>\n<li>bidirectional 迭代器。在 forword 迭代器的基础上增加了自减操作，指向上一个元素。</li>\n<li>random-access 迭代器。在 bidirectional 迭代器基础上增加关系比较，算术运算等。</li>\n</ol>\n<p>也许上面的总结还不能完全理解，没关系，现在结合迭代器适配器的代码来综合理解。</p>\n<h2 id=\"reverse-iterator\"><a href=\"#reverse-iterator\" class=\"headerlink\" title=\"reverse_iterator\"></a>reverse_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iterator</span>      // 首先是一个迭代器，其次是提供某些特殊功能的迭代器</span></span><br><span class=\"line\"><span class=\"class\">:</span> <span class=\"keyword\">public</span> iterator&lt;<span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::iterator_category,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::value_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::difference_type,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::pointer,</span><br><span class=\"line\">                <span class=\"keyword\">typename</span> iterator_traits&lt;_Iterator&gt;::reference&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator current;  <span class=\"comment\">// 声明所用迭代器类型的一个变量，反向迭代器正是在此迭代器之上进行构造得到</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span> &#123;</span><br><span class=\"line\">        _Iterator __tmp = current;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *--__tmp;<span class=\"comment\">// 先自减，然后解引用，返回的是前一个元素值的引用，返回值只能用作右值</span></span><br><span class=\"line\">                        <span class=\"comment\">// 由于是在临时变量临时变量上自减，故当前迭代器所指位置不变</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reverse_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;</span><br><span class=\"line\">        --current;      <span class=\"comment\">// 反向迭代器表示从右往左，故自增表示正常迭代器的自减</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>记反向迭代器为 r，其内部迭代器为 i，那么 r 的所有操作均转换为 i 上的操作，并由 i 完成，如</p>\n<ol>\n<li>解引用: *r = *(i-1)</li>\n<li>++r = –i, –r = ++i</li>\n<li>r+n = i-n, r-n = i+n</li>\n</ol>\n<p>还有其他一些操作如关系比较，基本上，r 的操作与 i 的操作相反（除了等于，不等于操作）</p>\n<h2 id=\"back-insert-iterator\"><a href=\"#back-insert-iterator\" class=\"headerlink\" title=\"back_insert_iterator\"></a>back_insert_iterator</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">back_insert_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">:</span><span class=\"keyword\">public</span> iterator&lt;output_iterator_tag, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>, <span class=\"keyword\">void</span>&gt; <span class=\"comment\">// 指定迭代器标签，其他相关类型则由容器决定</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Container* container;  <span class=\"comment\">// 构造此迭代器时，需要传入容器变量，此迭代器用于向这个容器末端插入元素</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    back_insert_iterator&amp;   <span class=\"comment\">// 给此迭代器赋值就等于向容器末端插入元素</span></span><br><span class=\"line\">    <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> <span class=\"keyword\">typename</span> _Container::value_type&amp; __value)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        container-&gt;push_back(__value);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() &#123; <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 解引用不是取所指元素的值，因为是 output 迭代器</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>++() &#123;<span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; &#125;       <span class=\"comment\">// 自增也不是指向下一个元素，因为只能向容器末端插入值</span></span><br><span class=\"line\">    back_insert_iterator&amp;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>与此类似还有 front_insert_iterator, insert_iterator 分别表示像容器首端插入值和像容器插入值，插入操作的实现均依赖于容器自身的插入操作，你所能做的，就是给这些迭代赋值，除了赋值还是赋值。。。</p>\n<h2 id=\"normal-iterator\"><a href=\"#normal-iterator\" class=\"headerlink\" title=\"__normal_iterator\"></a>__normal_iterator</h2><p>这是一个正常的迭代器模板，有两个模板参数 _Iterator, _Container，内部维持了一个迭代器对象，用于迭代操作，_Container 作用仅仅是用于生成不同的 __normal_iterator 类型，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator, <span class=\"keyword\">typename</span> _Container&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> __<span class=\"title\">normal_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;   <span class=\"comment\">// _normal_iterator 的迭代操作实际上由 _M_current 完成</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 迭代器的解引用，自增，自减，指针访问成员，位移等均由 _M_current 完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 两个 __normal_iterator 的关系比较也由对应的两个 _M_current 的关系比较完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 确实是再 normal 不过了</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"move-iterator\"><a href=\"#move-iterator\" class=\"headerlink\" title=\"move_iterator\"></a>move_iterator</h2><p>顾名思义就是提供 move 操作，其内部也有一个迭代器，move_iterator 的解引用就是将其内部解引用得到的值进行 move 从而转为右值引用，这用于某些泛型方法中，move 代替了 copy，提高了效率。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Iterator&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">move_iterator</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    _Iterator _M_current;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> iterator_traits&lt;_Iterator&gt;          __traits_type;  <span class=\"comment\">// _Iterator 的特性类</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __traits_type::reference   __base_ref;     <span class=\"comment\">// 萃取 _Iterator 相关的元素引用类型</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 如果__base_ref 是引用类型，将其转为右值引用，否则保持不变。通常来讲，__base_ref 都是引用类型</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> conditional&lt;is_reference&lt;__base_ref&gt;::value,</span><br><span class=\"line\">        <span class=\"keyword\">typename</span> remove_reference&lt;__base_ref&gt;::type&amp;&amp;,</span><br><span class=\"line\">        __base_ref&gt;::type               reference;</span><br><span class=\"line\">    </span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>*() <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"keyword\">static_cast</span>&lt;reference&gt;(*_M_current); &#125; <span class=\"comment\">// 将内部迭代器解引用得到的值转为右值引用</span></span><br><span class=\"line\"></span><br><span class=\"line\">    _GLIBCXX17_CONSTEXPR reference</span><br><span class=\"line\">    <span class=\"keyword\">operator</span>[](difference_type __n) <span class=\"keyword\">const</span></span><br><span class=\"line\">    &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">std</span>::move(_M_current[__n]); &#125;  <span class=\"comment\">// 随机访问取值，也转为右值引用</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h1><h2 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h2><p>以 vector 为例，代码位于 bits/stl_vector.h 中，首先是基类 _Vector_base，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Alloc&gt;::<span class=\"keyword\">template</span> rebind&lt;_Tp&gt;::other _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;::pointer pointer;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>模板参数 _Alloc 的 value_type 不一定是 _Tp，所以通过 rebind 得到 value_type 为 _Tp 的 alloctor（即 alloctor&lt;_Tp&gt;），设置其别名为 _Tp_alloc_type，然后设置其关联的 pointer，即 _Tp_alloc_type::pointer，如果它存在的话，否则为 _Tp<em>，然后根据 std::allocator 模板定义不难知道 _Tp_alloc_type::pointer 其实就是 _Tp</em>，所以 _Vector_base::pointer 就是 _Tp*。</p>\n<p>接着 _Vector_base 中又定义了几个内部结构</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl_data</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    pointer _M_start;   <span class=\"comment\">// 指向 vector 中第一个元素的内存位置</span></span><br><span class=\"line\">    pointer _M_finish;  <span class=\"comment\">// 指向 vector 中 past-the-last-element 的内存位置</span></span><br><span class=\"line\">    pointer _M_end_of_storage;  <span class=\"comment\">// vector 分配 past-the-max-element 内存位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，拷贝函数，交换数据函数。比较简单，略</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_impl</span> :</span> <span class=\"keyword\">public</span> _Tp_alloc_type, <span class=\"keyword\">public</span> _Vector_impl_data</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，略</span></span><br><span class=\"line\">    <span class=\"comment\">// vector 内存 overflow 检测，需要指定 _GLIBCXX_SANITIZE_VECTOR。参考 AddressSanitizer</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>回到 _Vector_base 中来，_Vector_base 定义了类型别名和一个变量，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> _<span class=\"title\">Vector_base</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Alloc allocator_type;</span><br><span class=\"line\">    _Vector_impl _M_impl;           <span class=\"comment\">// 分配器变量</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// 构造/析构 函数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pointer _M_allocator(<span class=\"keyword\">size_t</span> __n) &#123;      <span class=\"comment\">// 分配 n 个元素的内存，起始位置保存到 pointer 类型变量中</span></span><br><span class=\"line\">        <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt; _Tr;</span><br><span class=\"line\">        <span class=\"comment\">// 如 __n=0，则返回 nullptr，否则使用分配器 _M_impl 分配内存</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> __n != <span class=\"number\">0</span> ? _Tr::allocate(_M_impl, __n) : pointer();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span>:</span><br><span class=\"line\">    <span class=\"keyword\">void</span> _M_create_storage(<span class=\"keyword\">size_t</span> __n) &#123;    <span class=\"comment\">// 分配内存，并保存内存起始位置和截止位置</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_start = <span class=\"keyword\">this</span>-&gt;_M_allocate(__n);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start + __n;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>基类 _Vector_base 中仅仅做了内存分配和记录内存块位置的事情，其他 vector 相关的操作则放在 vector 类中，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector 模板参数指明了 vector 关联的元素类型 _Tp，以及 vector 的内存分配器类型 _Alloc，</span></span><br><span class=\"line\"><span class=\"comment\">//  默认 _Alloc 为 std::allocator&lt;_Tp&gt;，显然是于 _Tp 匹配的，</span></span><br><span class=\"line\"><span class=\"comment\">//  若提供的模板参数 _Alloc 与 _Tp 不匹配，那么也由 _Alloc::rebind 获取匹配的 alloctor</span></span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc = <span class=\"built_in\">std</span>::allocator&lt;_Tp&gt;&gt;</span><br><span class=\"line\">class <span class=\"built_in\">vector</span> : <span class=\"keyword\">protected</span> _Vector_base&lt;_Tp, _Alloc&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Vector_base&lt;_Tp, _Alloc&gt;               _Base;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::_Tp_alloc_type          _Tp_alloc_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__alloc_traits&lt;_Tp_alloc_type&gt;   _Alloc_traits;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> _Tp                             value_type;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> _Base::pointer         pointer;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> __gnu_cxx::__normal_iterator&lt;pointer, <span class=\"built_in\">vector</span>&gt;   iterator;</span><br><span class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::reverse_iterator&lt;iterator&gt;                 reverse_iterator;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>事实上，指针包含解引用，自增和自减等操作，也可看作是一种特殊的迭代器，所以这里 vector 内部类型 iterator，使用 pointer 作为 <strong>gnu::cxx::</strong>normal_iterator 的模板参数 _Iterator。<br>然后是 vector 的各种构造函数，需要注意到 vector 在实际分配内存后，都会更新 <em>M</em>impl.<em>M</em>finish 使其指向 past-the-last-element 的位置。我们来看一下 vector 获取迭代器的函数，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iterator</span><br><span class=\"line\">begin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_start); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">iterator</span><br><span class=\"line\">end() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> iterator(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rbegin() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(end()); &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">reverse_iterator</span><br><span class=\"line\">rend() _GLIBCXX_NOEXCEPT</span><br><span class=\"line\">&#123; <span class=\"keyword\">return</span> reverse_iterator(begin()); &#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见迭代器的自增自减解引用均转为指针的自增自减解引用操作。</p>\n<p>其他的 vector 操作，resize 表示重置 vector 中有效元素的数量，重置后 new_size &gt; old_size，那么末尾多出来的元素使用默认值填充（如果 resize 提供了指定值，那么使用指定值填充），如果 new_size&lt;=old_size，则重置 <em>M</em>impl.<em>M</em>finish 所指位置（[<em>M</em>start, <em>M</em>finish) 范围内的元素有效），<em>M</em>finish 之后的元素则根据元素类型决定是调用元素的析构函数还是放任不理，注意这一过程中内存占用没有改变。</p>\n<p>来看 vector 的 push_back 函数实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">push_back(<span class=\"keyword\">const</span> value_type&amp; __x)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>-&gt;_M_impl._M_finish != <span class=\"keyword\">this</span>-&gt;_M_impl._M_end_of_storage) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 当前分配的内存空间还足以存储新的元素 __x</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">        _M_realloc_insert(end(), __x);  <span class=\"comment\">// 重新分配内存，并在 _M_finish 位置插入元素 __x，然后</span></span><br><span class=\"line\">                                        <span class=\"comment\">//  将 _M_finish 所指位置向前移动一个元素单位</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在 bits/vector.tcc 中找到 <em>M</em>realloc_insert 的实现，</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> _Tp, <span class=\"keyword\">typename</span> _Alloc&gt;</span><br><span class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ..._Arg&gt;</span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;_Tp, _Alloc&gt;::_M_realloc_insert(iterator __position, _Args&amp;&amp;... _args)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 计算即将重新分配元素数量，这里重新分配的元素数量是原来元素数量的 2 倍，参见 _M_check_len</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __len = _M_check_len(size_type(<span class=\"number\">1</span>), <span class=\"string\">\"vector::_M_realloc_insert\"</span>);</span><br><span class=\"line\">    pointer __old_start = <span class=\"keyword\">this</span>-&gt;_M_impl._M_start;</span><br><span class=\"line\">    pointer __old_finish = <span class=\"keyword\">this</span>-&gt;_M_impl._M_finish; <span class=\"comment\">// 原来的起始元素指针和 past-the-last 元素指针</span></span><br><span class=\"line\">    <span class=\"keyword\">const</span> size_type __elems_before = __position - begin();<span class=\"comment\">// 插入位置之前的元素数量</span></span><br><span class=\"line\">    pointer __new_start(<span class=\"keyword\">this</span>-&gt;_M_allocate(__len));   <span class=\"comment\">// 重新分配内存，使得能容纳 __len 个元素</span></span><br><span class=\"line\">    pointer __new_finish(__new_start);  <span class=\"comment\">// 由于尚未填充元素，故此时 past-the-last 指针与起始指针相等</span></span><br><span class=\"line\">    __try</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 在指定位置处插入目标对象</span></span><br><span class=\"line\">        _Alloc_traits::construct(<span class=\"keyword\">this</span>-&gt;_M_impl,                     <span class=\"comment\">// 使用此分配器</span></span><br><span class=\"line\">                                 __new_start + __elems_before,      <span class=\"comment\">// 在指定位置处</span></span><br><span class=\"line\">                                 <span class=\"built_in\">std</span>::forward&lt;_Args&gt;(__args)...);   <span class=\"comment\">// 根据此参数构造对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 此时 vector 中有了元素，将 __new_finish 先置为 nullptr，等元素全部填充完毕，再更新其值</span></span><br><span class=\"line\">        __new_finish = pointer();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> _GLIBCXX17_CONSTEXPR (_S_use_relocate()) &#123;   <span class=\"comment\">// 如果元素类型支持移动插入</span></span><br><span class=\"line\">            <span class=\"comment\">// 将原来起始位置到插入位置截止，之间的元素重定位到新的起始位置</span></span><br><span class=\"line\">            __new_finish = _S_relocate(__old_start, __position.base()</span><br><span class=\"line\">                __new_start, _M_get_Tp_allocator());</span><br><span class=\"line\">            <span class=\"comment\">// 此时 __new_finish 所指位置就是新插入的元素，自增 1，移动新插入元素之后，将原来剩余的元素重定位到此位置处</span></span><br><span class=\"line\">            ++__new_finish;</span><br><span class=\"line\">            __new_finish = _S_relocate(__position.base(), __old_finish,</span><br><span class=\"line\">                __new_finish, _M_get_Tp_allocator());   <span class=\"comment\">// 此时 __new_finish 就是新的 past-of-last 元素位置了</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// 失败处理，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 析构原先内存上的对象，并释放内存，略</span></span><br><span class=\"line\">        <span class=\"comment\">// 更新元素起始和截止位置等，略</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>vector 类中还有很多其他方法，但是到了这一步，相信这些方法的代码实现不难理解了，由于篇幅有限，不对这些方法进行分析。</p>\n<p>本文结束</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjy8l5pz20015xgvcdt114x05","category_id":"cjy8l5pz50018xgvcu51xvdx6","_id":"cjy8l5pz7001dxgvcrwezetas"},{"post_id":"cjy8l5pz30017xgvcuhn4ej3y","category_id":"cjy8l5pz50018xgvcu51xvdx6","_id":"cjy8l5pz8001fxgvcxz11uzyi"},{"post_id":"cjy8l5q19001gxgvcgbo3hat6","category_id":"cjy8l5pz50018xgvcu51xvdx6","_id":"cjy8l5q1d001kxgvc6fx87tbh"}],"PostTag":[{"post_id":"cjy8l5prw0000xgvcwe1v6lrj","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5psc0008xgvc1yhggma9"},{"post_id":"cjy8l5ps40002xgvc4fodk46g","tag_id":"cjy8l5psb0007xgvc4ml86bcf","_id":"cjy8l5psg000bxgvckhi7cm9e"},{"post_id":"cjy8l5ps70004xgvc0xvgiymg","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5psi000dxgvcsv1xruu7"},{"post_id":"cjy8l5ps90005xgvcnt75sjvd","tag_id":"cjy8l5psg000cxgvcrb1d5son","_id":"cjy8l5psi000fxgvc39t4cgcr"},{"post_id":"cjy8l5psa0006xgvcs9cm2yqo","tag_id":"cjy8l5psi000exgvcrciqshpu","_id":"cjy8l5psk000hxgvcxxxuhbhd"},{"post_id":"cjy8l5psc0009xgvc7egaresy","tag_id":"cjy8l5psi000exgvcrciqshpu","_id":"cjy8l5psk000ixgvcho9vidv5"},{"post_id":"cjy8l5ptj000kxgvcmf2hc3ek","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptm000mxgvcyx0lbnj5"},{"post_id":"cjy8l5ptl000lxgvcz7c717a6","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptn000oxgvcgeuqhqcs"},{"post_id":"cjy8l5ptm000nxgvcca3398j3","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5pto000qxgvcldqje5am"},{"post_id":"cjy8l5ptn000pxgvcxllat9gl","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptq000sxgvc84u9d3dg"},{"post_id":"cjy8l5ptp000rxgvc349pv0gm","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5pts000uxgvcw9znb7fm"},{"post_id":"cjy8l5ptq000txgvcet4kqeiz","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptt000wxgvc02dg8y3b"},{"post_id":"cjy8l5pts000vxgvc346f74ae","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptu000yxgvc6aq5w5n7"},{"post_id":"cjy8l5ptt000xxgvc6yc0orym","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptv0010xgvczrx3llrd"},{"post_id":"cjy8l5ptu000zxgvcwgb99gsn","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptw0012xgvc2ejrajao"},{"post_id":"cjy8l5ptv0011xgvc3ir0fgvx","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5ptw0013xgvcm3i849br"},{"post_id":"cjy8l5pz10014xgvc96likv8m","tag_id":"cjy8l5ps60003xgvcsip22e4g","_id":"cjy8l5pz30016xgvcklz3clsn"},{"post_id":"cjy8l5pz20015xgvcdt114x05","tag_id":"cjy8l5pz50019xgvcl96snyh1","_id":"cjy8l5pz7001cxgvc5a9uilfw"},{"post_id":"cjy8l5pz30017xgvcuhn4ej3y","tag_id":"cjy8l5pz50019xgvcl96snyh1","_id":"cjy8l5pz8001exgvcly6vf9h2"},{"post_id":"cjy8l5q19001gxgvcgbo3hat6","tag_id":"cjy8l5pz50019xgvcl96snyh1","_id":"cjy8l5q1c001ixgvcrcw8b52o"},{"post_id":"cjy8l5q1b001hxgvc2g60t7fr","tag_id":"cjy8l5psg000cxgvcrb1d5son","_id":"cjy8l5q1c001jxgvcwdozgj6u"}],"Tag":[{"name":"object detection","_id":"cjy8l5ps60003xgvcsip22e4g"},{"name":"tool","_id":"cjy8l5psb0007xgvc4ml86bcf"},{"name":"c++","_id":"cjy8l5psg000cxgvcrb1d5son"},{"name":"CV","_id":"cjy8l5psi000exgvcrciqshpu"},{"name":"PyTorch","_id":"cjy8l5pz50019xgvcl96snyh1"}]}}