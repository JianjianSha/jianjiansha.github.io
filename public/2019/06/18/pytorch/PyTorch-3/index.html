<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shajianjian.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="在 PyTorch-2 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的官方文档，了解其中各个函数的具体实现。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch-3">
<meta property="og:url" content="https://shajianjian.github.io/2019/06/18/pytorch/PyTorch-3/index.html">
<meta property="og:site_name" content="SJJ">
<meta property="og:description" content="在 PyTorch-2 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的官方文档，了解其中各个函数的具体实现。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-06-18T08:44:44.000Z">
<meta property="article:modified_time" content="2020-04-24T10:34:35.902Z">
<meta property="article:author" content="shajianjian">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://shajianjian.github.io/2019/06/18/pytorch/PyTorch-3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch-3 | SJJ</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">SJJ</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shajianjian.github.io/2019/06/18/pytorch/PyTorch-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="shajianjian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SJJ">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch-3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-18 16:44:44" itemprop="dateCreated datePublished" datetime="2019-06-18T16:44:44+08:00">2019-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-24 18:34:35" itemprop="dateModified" datetime="2020-04-24T18:34:35+08:00">2020-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL-Framework/" itemprop="url" rel="index"><span itemprop="name">DL Framework</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在 <a href="PyTorch-2">PyTorch-2</a> 我们已经了解了 torch 包的初始化过程，接下来便可以愉快查看这个 package 包含哪些字段（包含函数和类）了，再参照 PyTorch 的<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">官方文档</a>，了解其中各个函数的具体实现。</p>
<a id="more"></a>
<h1 id="torch-包"><a href="#torch-包" class="headerlink" title="torch 包"></a>torch 包</h1><p>从 <code>torch/__init__.py</code> 中可以查看所有的 torch 包的所有字段，包括：</p>
<ol>
<li>直接在此文件中定义的函数/字段，如 typename, is_tensor, is_storage, _storage_classes 等</li>
<li>从 torch 包的模块中导入的函数/类，如<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from .random import set_rng_state, get_rng_state, manual_seed, initial_seed</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li>
<li>从 torch._C 中导入的字段/函数/类</li>
<li>从 torch._C._VariableFunctions 导入的字段/函数</li>
</ol>
<p>PyTorch 官方文档中 torch 包有很多函数。这里举几个例子进行说明。</p>
<h2 id="torch-empty"><a href="#torch-empty" class="headerlink" title="torch.empty"></a>torch.empty</h2><p>这个函数实际上来自于 torch._C._VariableFunctions 这个类。文件 torch/csrc/Module.cpp 中调用函数 THPVariable_initModule，跳转到 torch/csrc/autograd/python_variable.cpp 查看函数定义，其定义体中调用 torch::autograd::initTorchFunctions，而这个函数定义位于 torch/csrc/autograd/generated/python_torch_functions.cpp，这个文件是安装 PyTorch 过程中生成的，按以下步骤查看这个文件的生成过程：</p>
<ol>
<li>caffe2/CMakeLists.txt 中的文件生成语句为<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">set(GENERATED_CXX_PYTHON</span><br><span class="line">  ...</span><br><span class="line">  &quot;$&#123;TORCH_SRC_DIR&#125;&#x2F;csrc&#x2F;autograd&#x2F;generated&#x2F;python_torch_functions.cpp&quot;</span><br><span class="line">  ...)</span><br><span class="line">...</span><br><span class="line">add_custom_command(</span><br><span class="line">    OUTPUT</span><br><span class="line">    $&#123;TORCH_GENERATED_CODE&#125;</span><br><span class="line">    COMMAND</span><br><span class="line">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; tools&#x2F;setup_helpers&#x2F;generate_code.py</span><br><span class="line">     ...</span><br><span class="line">    DEPENDS</span><br><span class="line">    ...)</span><br></pre></td></tr></table></figure></li>
<li>执行 tools/setup_helpers/generate_code.py。在函数 generate_code 中调用了以下四个函数生成文件，<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">generate_nn_wrappers</span><br><span class="line">gen_autograd_python</span><br><span class="line">gen_autograd</span><br><span class="line">gen_jit_dispatch</span><br></pre></td></tr></table></figure>
这四个函数的实现都是非常繁琐的，这里以生成 torch/csrc/autograd/generated/python_torch_functions.cpp 为例，实际上是将模板文件 tools/autograd/templates/python_torch_functions.cpp 中的 ${py_methods} 和 ${py_method_defs} 分别替换为对应的方法实现和方法签名，这些方法来自于 torch/share/ATen/Declarations.yaml, tools/autograd/deprecated.yaml, tools/autograd/derivatives.yaml，其中第一个文件又需要动态生成，过程为：</li>
<li>在 caffe2/CMakeLists.txt 中有语句<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include(..&#x2F;cmake&#x2F;Codegen.cmake)</span><br></pre></td></tr></table></figure></li>
<li>在文件 cmake/Codegen.cmake 中调用 <code>gen.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SET(GEN_COMMAND</span><br><span class="line">    &quot;$&#123;PYTHON_EXECUTABLE&#125;&quot; $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;gen.py</span><br><span class="line">    --source-path $&#123;CMAKE_CURRENT_LIST_DIR&#125;&#x2F;..&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class="line">    --install_dir $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen</span><br><span class="line">    $&#123;GEN_ROCM_FLAG&#125;</span><br><span class="line">    $&#123;cwrap_files&#125;)</span><br></pre></td></tr></table></figure>
（在 aten/src/ATen/native/native_functions.yaml 找到 <code>empty</code> 的函数签名）</li>
<li>aten/src/ATen/gen.py 中的 generate_outputs 函数生成 Declarations.yaml 文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_manager.write(&quot;Declarations.yaml&quot;, format_yaml(output_declarations))</span><br></pre></td></tr></table></figure></li>
<li>根据第 2 点，install_dir 为 build/aten/src/ATen，所以 Declarations.yaml 生成路径此时为 build/aten/src/ATen，根据以下步骤安装此文件<ul>
<li>CMakeLists.txt 中的 add_subdirectory(caffe2)</li>
<li>caffe2/CMakeLists.txt 中的 add_subdirectory(../aten aten)</li>
<li>aten/CMakeLists.txt 中的 add_subdirectory(src/ATen)</li>
<li>aten/src/ATen/CMakeLists.txt 中有，<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSTALL(FILES $&#123;CMAKE_BINARY_DIR&#125;&#x2F;aten&#x2F;src&#x2F;ATen&#x2F;Declarations.yaml</span><br><span class="line">  DESTINATION $&#123;AT_INSTALL_SHARE_DIR&#125;&#x2F;ATen)</span><br></pre></td></tr></table></figure>
事实上，除了这里的 Declarations.yaml，在 aten/src/ATen/CMakeLists.txt 中还安装了很多头文件，其中就包括下文将提到的 build/aten/src/ATen/Functions.h，具体参见 aten/src/ATen/CMakeLists.txt 中其他 INSTALL 指令调用。</li>
</ul>
</li>
</ol>
<p>找到这些函数来源后，通过 tools/autograd/gen_python_functions.py 中的函数 create_python_bindings 生成 ${py_methods} 和 ${py_method_defs} 的内容，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">PY_VARIABLE_METHOD_VARARGS &#x3D; CodeTemplate(&quot;&quot;&quot;\</span><br><span class="line">static PyObject * $&#123;pycname&#125;(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class="line">&#123;</span><br><span class="line">    HANDLE_TH_ERRORS</span><br><span class="line">    static PythonArgsParser parser(&#123;</span><br><span class="line">        $&#123;signatures&#125;</span><br><span class="line">    &#125;, &#x2F;*traceable&#x3D;*&#x2F;$&#123;traceable&#125;);</span><br><span class="line">    $&#123;unpack_self&#125;</span><br><span class="line">    ParserArgs&lt;$&#123;max_args&#125;&gt; parsed_args;</span><br><span class="line">    auto r &#x3D; parser.parse(args, kwargs, parsed_args);</span><br><span class="line">    $&#123;declare_namedtuple_return_types&#125;</span><br><span class="line">    $&#123;dispatch&#125;</span><br><span class="line">    Py_RETURN_NONE;</span><br><span class="line">    END_HANDLE_TH_ERRORS</span><br><span class="line">&#125;</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line">...</span><br><span class="line">def create_python_bindings(python_functions, has_self, is_module&#x3D;False):</span><br><span class="line">    def process_function(name, declarations):</span><br><span class="line">        ...</span><br><span class="line">        env &#x3D; &#123;</span><br><span class="line">            &#39;name&#39;: name,</span><br><span class="line">            &#39;dispatch_name&#39;: &#39;dispatch_&#123;&#125;&#39;.format(name),</span><br><span class="line">            &#39;pycname&#39;: &#39;THPVariable_&#123;&#125;&#39;.format(name),</span><br><span class="line">            &#39;signature&#39;: [],</span><br><span class="line">            &#39;max_args&#39;: max(len(o[&#39;arguments&#39;])+len(o[&#39;python_binding_arguments&#39;]) for o in declarations),</span><br><span class="line">            &#39;unpack_self&#39;: [],</span><br><span class="line">            &#39;dispatch&#39;: [],</span><br><span class="line">            &#39;declare_namedtuple_return_types&#39;: &#39;&#39;,</span><br><span class="line">        &#125;</span><br><span class="line">        ... &#x2F;&#x2F; 向 env 增加 key-value pair or 更新 env 中已有 key 的 value</span><br><span class="line">        if len(declarations) &#x3D;&#x3D; 1 and len(declarations[0][&#39;args&#39;]) &#x3D;&#x3D; 1 and has_self:</span><br><span class="line">            ...</span><br><span class="line">        else:</span><br><span class="line">            tmpl &#x3D; PY_VARIABLE_METHOD_VARARGS</span><br><span class="line">            env[&#39;flags&#39;] &#x3D; &#39;METH_VARARGS | METH_KEYWORDS&#39;</span><br><span class="line">        if not is_module and not has_self:</span><br><span class="line">            env[&#39;flags&#39;] +&#x3D; &#39; | METH_STATIC&#39;</span><br><span class="line">        </span><br><span class="line">        py_methods.append(tmpl.substitute(env))</span><br><span class="line">        py_methods_defs.append(PY_VARIABLE_METHOD_DEF.substitute(env))</span><br></pre></td></tr></table></figure>
<p>通过以上代码片段可知，对于函数定义的生成，使用一个函数定义模板 PY_VARIABLE_METHOD_VARARGS，然后对每个函数，来自于 Declarations.yaml, deprecated.yaml, derivatives.yaml，抽取有关字段的值存储到 env 字典中，然后将 PY_VARIABLE_METHOD_VARARGS 中的占位符使用 env 中相应 key 的值替换，就得到这个函数的定义。</p>
<h2 id="empty-定义"><a href="#empty-定义" class="headerlink" title="empty 定义"></a>empty 定义</h2><p>我们看生成后的 empty 函数定义（位于文件 torch/csrc/autograd/generated/python_torch_function.cpp）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">static PyObject * THPVariable_empty(PyObject* self_, PyObject* args, PyObject* kwargs)</span><br><span class="line">&#123;</span><br><span class="line">    HANDLE_TH_ERRORS</span><br><span class="line">    static PythonArgParser parser(&#123;</span><br><span class="line">        &quot;empty(IntList size, *, Tensor out&#x3D;None, ScalarType dtype&#x3D;None, Layout layout&#x3D;torch.strided, Device device&#x3D;None, bool requires_grad&#x3D;False)&quot;,</span><br><span class="line">    &#125;, &#x2F;*tracebalbe*&#x2F;true); &#x2F;&#x2F; 大括号初始化器，得到函数签名的vector</span><br><span class="line">    ParseArgs&lt;6&gt; parsed_args;</span><br><span class="line">    auto r &#x3D; parser.parse(args, kwargs, parseed_args);</span><br><span class="line">    if (r.idx &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F; 函数签名在vector中的下标</span><br><span class="line">        if (r.isNone(1)) &#123;  &#x2F;&#x2F; parameter &#39;out&#39; is None</span><br><span class="line">            auto size &#x3D; r.intlist(0);</span><br><span class="line">            auto dtype &#x3D; r.scalartype(2);</span><br><span class="line">            auto device &#x3D; r.device(4);</span><br><span class="line">            const auto options &#x3D; TensorOptions()</span><br><span class="line">                .dtype(dtype)</span><br><span class="line">                .device(device)</span><br><span class="line">                .layout(r.layout(3).layout)</span><br><span class="line">                .requires_grad(r.toBool(5));</span><br><span class="line">            return wrap(dispatch_empty(size, options));</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            check_out_type_matches(r.tensor(1), r.scalartype(2), r.isNone(2),</span><br><span class="line">                                   r.layout(3), r.isNone(3),</span><br><span class="line">                                   r.device(4), r.isNone(4));</span><br><span class="line">            return wrap(dispatch_empty(r.intlist(0), r.tensor(1)).set_requires_grad(r.toBool(5)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    Py_RETURN_NONE;</span><br><span class="line">    END_HANDLE_TH_ERRORS</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从以上代码中可见，要创建一个 empty 的 Tensor，首先检查调用者是否提供了一个 Tensor，如未提供，则先创建一个 Tensor：</p>
<ol>
<li><code>out</code> 参数为None，则需要根据参数 dtype, device, layout 和 requires_grad 创建 Tensor</li>
<li><code>out</code> 参数不为None, 则检查 <code>out</code> 这个 Tensor 与参数 dtype, layout, device 是否匹配，如果匹配，还需要将 <code>out</code> 的 requires_grad 属性重置为参数 requires_grad</li>
</ol>
<p>然后调用函数 dispatch_empty，这个函数总共有两个重载版本，位于 torch/csrc/autograd/generated/python_torch_functions_dispatch.h，这个文件与同目录下的 python_torch_function.cpp 一样也是动态生成的，生成逻辑也是一样的，将 tools/autograd/templates/python_torch_functions_dispatch.h 中的占位符替换掉，不再具体展开，可参见 tools/autograd/gen_python_functions.py 中的函数 gen_py_torch_functions。dispatch_empty 的两个重载版本为，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; empty 函数调用者提供了 Tensor &#39;out&#39;</span><br><span class="line">inline Tensor dispatch_empty(IntList size, Tensor result) &#123;</span><br><span class="line">    AutoNoGIL no_gil;</span><br><span class="line">    return at::empty_out(result, size);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; empty 函数调用者未提供 Tensor &#39;out&#39;，需要根据参数 options 创建</span><br><span class="line">inline Tensor dispatch_empty(IntList size, const TensorOptions &amp; options) &#123;</span><br><span class="line">    maybe_initialize_cuda(options);</span><br><span class="line">    AutoNoGIL no_gil;</span><br><span class="line">    return torch::empty(size, options);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="有输出-Tensor"><a href="#有输出-Tensor" class="headerlink" title="有输出 Tensor"></a>有输出 Tensor</h3><p>我们看第一个重置版本的定义体，即，调用者提供了输出 Tensor，首先构造一个结构实例 AutoNoGIL，这个结构的构造函数为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AutoNoGIL() : save(PyEval_SaveThread()) &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出，先释放 GIL，因为下一句执行的 at::empty_out 可能会慢很多，为了防止程序使用多线程，但仍然被阻塞在这里，所以释放 GIL，待 at::empty_out 执行完毕，再重新获取 GIL，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~AutoNoGIL() &#123;</span><br><span class="line">    PyEval_RestoreThread(save);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后 at::empty_out 函数位于 torch/lib/include/Aten/Functions.h，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">static inline Tensor &amp; empty_out(Tensor &amp; result, IntList size) &#123;</span><br><span class="line">    return detail::infer_type(result).empty_out(result, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在分析 at::empty_out 函数之前，我们需要知道这里的 Functions.h 也是动态生成的，在项目源码中稍作查询便知，在 aten/src/ATen/gen.py 中的 generate_outputs 函数中使用如下语句生成（与前面的 Declarations.yaml 文件的生成在同一处地方），</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_manager.write(&#39;Functions.h&#39;, FUNCTIONS_H, top_env)</span><br></pre></td></tr></table></figure>
<p>现在回到 at::empty_out 函数定义上来，首先 detail::infer_type(result) 根据调用用传入的 Tensor 实例 result 得到 TypeExtendedInference 类型实例，然后调用实例函数 empty_out。这里相关的结构、类为 TypeExtendedInferface，TypeDefault，位于文件 torch/lib/include/ATen/TypeExtendedInferface.h， torch/lib/include/ATen/TypeDefault.h，此外，TypeDefault类方法实现源文件为 build/aten/src/ATen/TypeDefault.cpp，接口方法 empty_out 的实现正是位于此文件中，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor &amp; TypeDefault::empty_out(Tensor &amp; result, IntList size) const &#123;</span><br><span class="line">    return at::native::empty_out(&#x2F;* native_actuals *&#x2F; result, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先这三个文件是动态生成的（与 Declarations.yaml 相同）。然后我们看方法定义体中，直接调用另一个同名函数 at::native::empty_out 下，函数声明位于文件 torch/lib/include/ATen/NativeFunctions.h，此文件动态生成（与 Declarations.yaml 相同），函数实现位于 aten/src/ATen/native/TensorFactories.cpp，这个文件不是动态生成的（终于来了一个非动态生成的了），在此文件中查看函数定义，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">namespace at &#123;</span><br><span class="line">namespace native &#123;</span><br><span class="line">...</span><br><span class="line">Tensor&amp; empty_out(Tensor&amp; result, IntList size) &#123;</span><br><span class="line">    if (result.is_sparse()) &#123;</span><br><span class="line">        result.sparse_resize_and_clear_(size, size.size(), 0);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        result.resize_(size);</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>显然，根据输出 Tensor 是否是稀疏的进行不同的处理。</p>
<ol>
<li><p>输出 Tensor 是稀疏的</p>
<p>对输出 Tensor 调用方法 sparse_resize_and_clear_，声明位于 torch/lib/include/ATen/core/Tensor.h，此文件动态生成，与 Declarations.yaml 相同，见于 aten/src/ATen/gen.py，但是实际上源码中存在 aten/src/ATen/core/Tensor.h，并且这俩文件完全一样，还有 TensorMethods.h 和 Type.h 均存在这个现象，这里暂时不清楚为啥会这样。sparse_resize_and_clear_ 的函数实现位于 torch/lib/include/ATen/core/TensorMethods.h，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inline Tensor &amp; Tensor::sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class="line">    return type().sparse_resize_and_clear_(*this, size, sparse_dim, dense_dim);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先根据当前 Tensor 获取对应的 Type，然后调用 Type 类型的 sparse_resize_and_clear_ 方法，Type 这个结构是一个接口，其接口函数的具体实现见各个具体 Type 的 .cpp 文件，Type 是由数值类型（如 int,float,double 等）和 Backend（CPU,CUDA,SparseCPU, SparseCUDA 等）组合而成，比如 SparseCPUByteType.h 和 SparseCPUByteType.cpp，此函数的的定义为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tensor &amp; SparseCPUByteType::sparse_resize_and_clear_(Tensor &amp; self, IntList size, int64_t sparse_dim, int64_t dense_dim) const &#123;</span><br><span class="line">    const OptionalDeviceGuard device_guard(device_of(self));</span><br><span class="line">    return at::native::sparse_resize_and_clear_(&#x2F;* actuals *&#x2F; self, size, sparse_dim, dense_dim);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 at::native::sparse_resize_and_clear_ 函数声明位于 torch/lib/include/ATen/NativeFunctions.h，函数实现位于 aten/src/ATen/native/sparse/SparseTensor.cpp，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SparseTensor&amp; sparse_resize_and_clear_(SparseTensor&amp; self, ArrayRef&lt;int64_t&gt; size, int64_t sparse_dim, int64_t dense_dim) &#123;</span><br><span class="line">    get_sparse_impl(self)-&gt;resize_and_clear_(sparse_dim, dense_dim, size);</span><br><span class="line">    return self;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据 Tensor 获取其底层实现 SparseTensorImpl 类对象，然后调用 SparseTensorImpl 的方法 resize_and_clear_。</p>
</li>
<li><p>输出 Tensor 是密集的</p>
<p>Tensor 的 resize_ 方法定义见 TensorMethods.h，为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inline Tensor &amp; Tensor::resize_(IntList size) &#123;</span><br><span class="line">    return type().resize_(*this, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用这个 Tensor 的类型方法 resize_，以 CPUByteType.cpp 为例，定义如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Tensor &amp; <span class="title">CPUByteType::resize_</span><span class="params">(Tensor &amp; self, IntList <span class="built_in">size</span>)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> at::native::resize_cpu_(<span class="comment">/* actuals */</span> self, <span class="built_in">size</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可见，对 Tensor 按给定 size 进行 resize 操作，这个位于 aten/src/ATen/native/Resize.cpp 中的 resize_cpu_ 方法定义为，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Tensor&amp; <span class="title">resize_cpu_</span><span class="params">(Tensor&amp; self, IntList <span class="built_in">size</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span>* self = self.unsafeGetTensorImpl();         <span class="comment">// 获取 Tensor 的底层实现类对象</span></span><br><span class="line">    <span class="comment">// 按给定 size 大小对 Tensor 进行 resize，当 size 大小比 Tensor size 大时，才分配一个更大的内存块</span></span><br><span class="line">    resize_impl_cpu_(self_, <span class="built_in">size</span>, c10::nullopt);     </span><br><span class="line">    self_-&gt;maybe_zero_dim(<span class="built_in">size</span>.<span class="built_in">size</span>()==<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> self;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面这个代码片段中，resize_impl_cpu_ 表示以 cpu 实现方式进行内存 resize 操作，此函数定义位于 aten/src/ATen/native/Resize.h 下，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> TensorImpl* <span class="title">resize_impl_cpu_</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    TensorImpl* self,</span></span></span><br><span class="line"><span class="function"><span class="params">    IntList <span class="built_in">size</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    c10::optional&lt;IntList&gt; stride)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (self-&gt;sizes() == <span class="built_in">size</span> &amp;&amp; (!stride || self-&gt;strides() == stride)) &#123;</span><br><span class="line">        <span class="comment">// 如果当前 size 与将要重新分配 size 相等，且未指定新的步幅，或者当前数据步幅与新的步幅相等，那么无需重新分配内存</span></span><br><span class="line">        <span class="comment">// size 是整型列表，size 相等意味着列表元素数量相等，且对应位置的元素均相等</span></span><br><span class="line">        <span class="keyword">return</span> self;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int64_t</span> storage_size = <span class="number">1</span>;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span>(!stride)&#123;     <span class="comment">// 未指定步幅，则数据布局是近邻的，连续的，即，stride=1</span></span><br><span class="line">        self-&gt;set_sizes_contiguous(<span class="built_in">size</span>);    <span class="comment">// 设置当前 size 为新的 size</span></span><br><span class="line">        storage_size = self-&gt;numel();        <span class="comment">// 设置 size 之后，计算元素数量，例如 size 为 (n1,n2,n3)，那么元素数量为 n1 * n2 * n3</span></span><br><span class="line">    &#125;</span><br><span class="line">    maybe_resize_storage_cpu(self, storage_size);    <span class="comment">// resize 操作</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">maybe_resize_storage_cpu</span><span class="params">(TensorImpl* self, <span class="keyword">int64_t</span> new_size)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (new_size+self-&gt;storage_offset() &gt; self-&gt;storage().numel()) &#123;</span><br><span class="line">        <span class="comment">// self-&gt;storage_offset() 通常返回 0</span></span><br><span class="line">        <span class="comment">// 只有需要更多的元素数量时，才重新分配内存</span></span><br><span class="line">        THStorage_resize(THTensor_getStoragePtr(self), new_size+self-&gt;storage_offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们再来看位于 aten/src/TH/THStorageFunctions.cpp 中的 THStorage_resize 函数定义，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THStorage_resize</span><span class="params">(THStorage* storage, <span class="keyword">ptrdiff_t</span> <span class="built_in">size</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (storage-&gt;resizable()) &#123;</span><br><span class="line">        at::DataPtr new_data;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">size</span> != <span class="number">0</span>) &#123;</span><br><span class="line">            new_data = storage-&gt;allocator()-&gt;allocate(storage-&gt;itemsize()*<span class="built_in">size</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 旧数据为 Tensor 已经存储的数据，新数据为上一步新分配的内存</span></span><br><span class="line">        <span class="comment">// 设置 Tensor 内部存储指向新数据，同时返回旧数据</span></span><br><span class="line">        at::DataPtr old_data = storage-&gt;set_data_ptr(<span class="built_in">std</span>::<span class="built_in">move</span>(new_data));</span><br><span class="line">        <span class="keyword">ptrdiff_t</span> old_size = storage-&gt;numel();   <span class="comment">// 旧数据 size，元素数量</span></span><br><span class="line">        storage-&gt;set_numel(<span class="built_in">size</span>);                <span class="comment">// 设置新的元素熟路</span></span><br><span class="line">        <span class="keyword">if</span> (old_data != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">ptrdiff_t</span> copy_size = old_size;</span><br><span class="line">            <span class="keyword">if</span> (storage-&gt;numel() &lt; copy_size) &#123;</span><br><span class="line">                copy_size = storage_numel();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (copy_size &gt; <span class="number">0</span>) &#123;                 <span class="comment">// 内存数据考虑</span></span><br><span class="line">                <span class="built_in">memcpy</span>(</span><br><span class="line">                    storage-&gt;data(),</span><br><span class="line">                    old_data.<span class="built_in">get</span>(),</span><br><span class="line">                    storage-&gt;itemsize() * copy_size);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的代码片段可见整个 resize 过程，假设原先元素数量为 N1，resize 后的元素数量为 N2，那么</p>
<ol>
<li>N1 &gt;= N2，不重新分配内存，仅仅设置新的 size，标记原来 N1 个元素中前 N2 个元素处于当前使用中</li>
<li>N1 &lt; N2，重新分配内存，并将原来 N1 个元素值拷贝到新内存中前 N1 个位置上，剩余的元素值由 Tensor 内部存储的内存分配器 allocator 决定。</li>
</ol>
</li>
</ol>
<p>实验验证上述 torch.empty 过程，代码如下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">torch.empty(<span class="number">4</span>,<span class="number">5</span>,out=x)  <span class="comment"># resize 到一个较大的 size</span></span><br><span class="line">print(x)</span><br><span class="line">torch.empty(<span class="number">1</span>,<span class="number">2</span>,out=x)  <span class="comment"># resize 到一个较小的 size</span></span><br><span class="line">print(x)</span><br><span class="line">torch.empty(<span class="number">4</span>,<span class="number">4</span>,out=x)  <span class="comment"># 再次 resize 到一个较大的 size</span></span><br></pre></td></tr></table></figure>
<p>本次输出如下，从以下结果可以看出是符合上述过程的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class="line">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class="line">        [0.0576, 0.1087, 0.3033, 0.4694]])</span><br><span class="line">tensor([[4.4638e-02, 1.5454e-01, 5.0591e-01, 6.0266e-01, 4.8720e-01],</span><br><span class="line">        [4.5573e-01, 1.0103e-01, 2.9619e-01, 5.7569e-02, 1.0874e-01],</span><br><span class="line">        [3.0331e-01, 4.6944e-01, 0.0000e+00, 0.0000e+00,        nan],</span><br><span class="line">        [0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])</span><br><span class="line">tensor([[0.0446, 0.1545]])</span><br><span class="line">tensor([[0.0446, 0.1545, 0.5059, 0.6027],</span><br><span class="line">        [0.4872, 0.4557, 0.1010, 0.2962],</span><br><span class="line">        [0.0576, 0.1087, 0.3033, 0.4694],</span><br><span class="line">        [0.0000, 0.0000,    nan, 0.0000]])</span><br></pre></td></tr></table></figure>
<h3 id="无输出-Tensor"><a href="#无输出-Tensor" class="headerlink" title="无输出 Tensor"></a>无输出 Tensor</h3><p>回到 torch/csrc/autograd/generated/python_torch_functions_dispatch.h 这个文件，无输出 tensor 的 dispatch_empty 函数直接调用 torch::empty，此函数位于 torch/csrc/autograd/generated/variable_factories.h，在此函数定义中，我们暂且忽略 jit 跟踪部分的代码（用于跟踪记录有关 Tensor 的操作），核心的实现代码为</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> at::Tensor <span class="title">empty</span><span class="params">(at::IntList <span class="built_in">size</span>, <span class="keyword">const</span> at::TensorOptions &amp; options=&#123;&#125;)</span> </span>&#123;</span><br><span class="line">    ...     <span class="comment">// jit tracing</span></span><br><span class="line">    at::Tensor tensor = at::empty(<span class="built_in">size</span>, at::TensorOptions(options).is_variable(<span class="literal">false</span>));</span><br><span class="line">    <span class="keyword">auto</span> result = autograd::make_variable(tensor, options.requires_grad()); <span class="comment">// 将 Tensor 转为 Variable</span></span><br><span class="line">    ...     <span class="comment">// jit tracing</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 at::empty 位于安装时动态生成的源文件 Functions.h（见上文分析），这个函数定义为</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> Tensor <span class="title">empty</span><span class="params">(IntList <span class="built_in">size</span>, <span class="keyword">const</span> TensorOptions &amp; options)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> at::getType(options).empty(<span class="built_in">size</span>, options);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>与有输出 Tensor 的 empty 函数实现逻辑类似，这里 at::getType(options) 根据给定的 options 构造出 TypeExtendedInterface 接口的具体实现类的 instance，具体而言，根据 options.backend(), options.dtype() 和 options.is_variable() 获取具体类型实例，而类型实例是事先注册好的，以 CPU 为 backend 为例说明，在 aten/src/ATen/Context.cpp 中 Context 的构造函数中，执行函数 register_cpu_types(this) 进行注册，而 register_cpu_type(Context* context) 函数位于 build/aten/src/ATen/RegisterCPU.cpp 文件，此文件由 aten/src/ATen/gen.py 中的 generate_outputs 函数生成（关于 gen.py 文件，上文也有介绍），现在我们来看看 register_cpu_types 中注册哪些类型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CPUByteType</span><br><span class="line">CPUCharType</span><br><span class="line">CPUDoubleType</span><br><span class="line">CPUFloatType</span><br><span class="line">CPUIntType</span><br><span class="line">CPULongType</span><br><span class="line">CPUShortType</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>我们随便选择一个类型，比如 CPUByteType，查看其中 empty 函数实现，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tensor CPUByteType::empty(IntList size, const TensorOptions &amp; options) const &#123;</span><br><span class="line">    const DeviceGuard device_guard(options.device());   &#x2F;&#x2F; 准备在指定 device 上构造 Tensor</span><br><span class="line">    return at::native::empty_cpu(size, options);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上 at::native::empty_cpu 函数位于 aten/src/ATen/native/TensorFactories.cpp 中，函数实现体的部分为</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span>* allocator = at::getCPUAllocator();</span><br><span class="line"><span class="keyword">int64_t</span> nelements = prod_intlist(<span class="built_in">size</span>); <span class="comment">// 连乘（各维度值），得到总元素数量</span></span><br><span class="line"><span class="keyword">auto</span> dtype = options.dtype();</span><br><span class="line"><span class="keyword">auto</span> storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class="line">    dtype,</span><br><span class="line">    nelements,</span><br><span class="line">    allocator-&gt;allocate(nelements*dtype.itemsize()),</span><br><span class="line">    allocator,</span><br><span class="line">    <span class="comment">/*resizeable=*/</span><span class="literal">true</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">auto</span> tensor = detail::make_tensor&lt;TensorImpl&gt;(storage_impl, at::CPUTensorId(), <span class="literal">false</span>);</span><br></pre></td></tr></table></figure>
<p>继续查看 c10::make_intrusive<StorageImpl> 函数定义，不难得知先进行 new StorageImpl(…)，然后 wrap 为 intrusive_ptr，在 <a href="2019/06/13/PyTorch-2">PyTorch-2</a> 中，我们讨论过各种 Tensor 的底层实现都是 StorageImpl，所以 StorageImpl 对象可以通过 detail::make_tensor 转为对应的 Tensor。根据 at::getCPUAllocator 查看其定义得知获得的是 THDefaultAllocator 实例，其 allocate 方法调用 THAlloc 分配内存，THAlloc 内部调用 THAllocInternal 分配内存，而这个函数又使用 malloc（某些情况下也会使用 posix_memalign 申请对齐内存） 申请一块未初始化的内存。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>结果为（每次执行结果可能不同，不固定）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1.6504e-12,3.0637e-41,1.6588e-12],</span><br><span class="line">        [3.0637e-41,4.4842e-44,0.0000e+00]])</span><br></pre></td></tr></table></figure>

<h3 id="Tensor-的由来"><a href="#Tensor-的由来" class="headerlink" title="Tensor 的由来"></a>Tensor 的由来</h3><p>这里我们讨论 torch.empty 函数是如何返回得到 torch.Tensor 对象的。一开始，在 <code>torch/__init__.py</code> 中 <code>import autograd</code>，继而查看 <code>torch/autograd/__init__.py</code>，发现如下调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> torch._C._autograd_init():</span><br></pre></td></tr></table></figure>
<p>_autograd_init 这个 python 函数在 torch/csrc/Module.cpp 中注册，其底层实现是由 THPAutograd_initExtension 完成，这个 c++ 函数声明位于头文件 torch/csrc/autograd/autograd.h 中，函数实现位于 torch/csrc/autograd/init.cpp 中，看下这个函数的部分定义</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载 torch/tensor.py 模块</span></span><br><span class="line"><span class="keyword">auto</span> tensor_module = THPObjectPtr(PyImport_ImportModule(<span class="string">"torch.tensor"</span>));</span><br><span class="line"><span class="comment">// 获取 torch/tensor.py 中的 Tensor 类型</span></span><br><span class="line">THPVariableClass = PyObject_GetAttrString(tensor_module, <span class="string">"Tensor"</span>);</span><br></pre></td></tr></table></figure>
<p>要知道 <code>THPVariableClass</code> 这个类型对象声明位于 torch/csrc/autograd/python_variable.h 中</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">THP_API PyObject *THPVariableClass;</span><br></pre></td></tr></table></figure>
<p>嗯，这是一个 extern 声明，其原本定义位于 torch/csrc/autograd/python_variable.cpp 中。好，现在回到 torch.empty 的底层 c++ 实现部分，即上文 THPVariable_empty 函数定义，在 dispatch_empty 返回一个 Variable 对象后，经过 wrap 包装为 PyObject，来看 wrap 的定义，位于 torch/csrc/autograd/utils/wrap_outputs.h 中，其内部调用 THPVariable_Wrap，这个函数也位于 torch/csrc/autograd/python_variable.cpp，与 THPVariableClass 定义在同一个文件中，前面我们已经知道 THPVariableClass 就是 torch/tensor.py 中的 Tensor 类型，而这里 THPVariable_Wrap 通过调用 THPVariable_NewWithVar 将 Variable 对象包装为 THPVariableClass 对象，即 Tensor 实例。THPVariable_NewWithVar 函数定义的部分代码为</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">THPVariable_NewWithVar</span><span class="params">(PyTypeObject* type, Variable var)</span> </span>&#123;</span><br><span class="line">PyObject *obj=type-&gt;tp_alloc(type, <span class="number">0</span>);      <span class="comment">// 申请 torch.Tensor 所需要的内存</span></span><br><span class="line"><span class="keyword">if</span>(obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span> v = (THPVariable*)obj; <span class="comment">// cast 为 THPVariable 类型指针，即 torch.Tensor 的基类 torch._C._TensorBase 的指针</span></span><br><span class="line">    <span class="keyword">new</span>(&amp;v-&gt;cdata) Variable(<span class="built_in">std</span>::<span class="built_in">move</span>(var));    <span class="comment">// 指定内存中，移动构造 Variable（C++ 版本的 Tensor）</span></span><br><span class="line">    v-&gt;cdata.set_pyobj(obj);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> obj;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(torch.empty(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">Tensor</span>'&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h1><p>好吧，主要是因为内容太多了，樯橹灰飞烟灭，先到此为止吧，就当是梳理了一下方法调用过程，等以后熟悉了整个代码框架，再回头重新整理一番。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/06/16/mAP/" rel="prev" title="mAP">
      <i class="fa fa-chevron-left"></i> mAP
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/06/21/TridentNet/" rel="next" title="TridentNet">
      TridentNet <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#torch-包"><span class="nav-number">1.</span> <span class="nav-text">torch 包</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-empty"><span class="nav-number">1.1.</span> <span class="nav-text">torch.empty</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#empty-定义"><span class="nav-number">1.2.</span> <span class="nav-text">empty 定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#有输出-Tensor"><span class="nav-number">1.2.1.</span> <span class="nav-text">有输出 Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无输出-Tensor"><span class="nav-number">1.2.2.</span> <span class="nav-text">无输出 Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-的由来"><span class="nav-number">1.2.3.</span> <span class="nav-text">Tensor 的由来</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PS"><span class="nav-number">2.</span> <span class="nav-text">PS</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">shajianjian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">shajianjian</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




  















  

  

  

</body>
</html>
