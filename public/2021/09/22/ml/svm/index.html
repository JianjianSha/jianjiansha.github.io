<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shajianjian.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="支持向量机搜索具有最大“边距”的分类器，最大“边距”使得分类器更加健壮。">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机">
<meta property="og:url" content="https://shajianjian.github.io/2021/09/22/ml/svm/index.html">
<meta property="og:site_name" content="SJJ">
<meta property="og:description" content="支持向量机搜索具有最大“边距”的分类器，最大“边距”使得分类器更加健壮。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-09-22T08:04:37.000Z">
<meta property="article:modified_time" content="2021-09-30T10:21:29.710Z">
<meta property="article:author" content="shajianjian">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://shajianjian.github.io/2021/09/22/ml/svm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>支持向量机 | SJJ</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">SJJ</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shajianjian.github.io/2021/09/22/ml/svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="shajianjian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SJJ">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          支持向量机
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-22 16:04:37" itemprop="dateCreated datePublished" datetime="2021-09-22T16:04:37+08:00">2021-09-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-30 18:21:29" itemprop="dateModified" datetime="2021-09-30T18:21:29+08:00">2021-09-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>支持向量机搜索具有最大“边距”的分类器，最大“边距”使得分类器更加健壮。</p>
<span id="more"></span>
<h1 id="边距和-Hard-SVM"><a href="#边距和-Hard-SVM" class="headerlink" title="边距和 Hard-SVM"></a>边距和 Hard-SVM</h1><p>训练集记为 $S=(\mathbf x_1,y_1), \cdots, (\mathbf x_m, y_m)$，其中 $\mathbf x \in \mathbb R^d, \ y \in \{1,-1\}$。假设这个训练集线性可分，那么有</p>
<script type="math/tex; mode=display">\forall i \in [m], \quad y_i (\mathbf w^{\top} \mathbf x_i+b )>0</script><p>满足上式的所有 halfspace $(\mathbf w , b)$，有多解， 均属于 ERM 假设（$L_S(h)=0$），我们根据“边距”最大这一条件来选择最好的 $(\mathbf w, b)$。</p>
<blockquote>
<p>判别超平面与数据集之间的“边距”定义为：超平面与数据集中某样本点之间的最小距离。</p>
</blockquote>
<p>如果这个“最小距离”较大，那么即使对样本点有进行小的扰动，也不影响判别效果。</p>
<h2 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h2><p><strong>样本点 $\mathbf x$ 与超平面 $(\mathbf w, b)$ （其中 $|\mathbf w|=1$）之间的距离为 $|\mathbf w^{\top}\mathbf x+b|$。</strong></p>
<p>这里给了 $\Vert \mathbf w \Vert=1$ 这个约束，如果没有这个约束，那么同一个超平面会有无穷多个表达，即 $a\mathbf w \mathbf x+ab=1, \ \forall a \neq 0$。给定 $\mathbf w$ 约束条件后，$b$ 可以唯一确定，故不需要对其作约束。</p>
<p><strong>证：</strong></p>
<p>这个距离定义为</p>
<script type="math/tex; mode=display">\min\{\Vert\mathbf {x-v}\Vert: \mathbf w^{\top}\mathbf v+b=0\}</script><p>取 $\mathbf v=\mathbf x-(\mathbf w^{\top} \mathbf x+b)\mathbf w$，验证这个值在超平面上，因为</p>
<script type="math/tex; mode=display">\mathbf w^{\top} \mathbf v+b=\mathbf w^{\top}\mathbf x-(\mathbf w^{\top} \mathbf x+b)\|\mathbf w\|^2+b=0</script><p>验证完毕。</p>
<p>此时 $\mathbf x, \ \mathbf v$ 之间的距离为</p>
<script type="math/tex; mode=display">\|\mathbf x-\mathbf v\|=|\mathbf w^{\top} \mathbf x+b|\|\mathbf w\|=|\mathbf w^{\top} \mathbf x+b|</script><p>另一方面，任取超平面上一点 $\mathbf u$，有</p>
<script type="math/tex; mode=display">\begin{aligned} \Vert \mathbf {x-u}\Vert^2 &=\|\mathbf {x-v+v-u}\|^2
\\ &=\|\mathbf {x-v}\|^2+\|\mathbf {v-u}\|^2+2(\mathbf {x-v})^{\top}(\mathbf {v-u})
\\ &\ge\|\mathbf {x-v}\|^2+2(\mathbf {x-v})^{\top}(\mathbf {v-u})
\\ &=\|\mathbf {x-v}\|^2+2(\mathbf w^{\top} \mathbf x+b)\mathbf w^{\top}(\mathbf {v-u})
\\ &=\|\mathbf {x-v}\|^2\end{aligned}</script><p>上式倒数第二个等式用了 $\mathbf v$ 的定义，倒数第一个等式是因为 $\mathbf w^{\top}\mathbf v=\mathbf w^{\top}\mathbf u=-b$。</p>
<p>从上式可见，$\mathbf x$ 与超平面的距离应该为 $|\mathbf {x-v}|^2$，也就是 $|\mathbf w^{\top} \mathbf x+b|$。</p>
<h2 id="Hard-SVM"><a href="#Hard-SVM" class="headerlink" title="Hard-SVM"></a>Hard-SVM</h2><p>Hard-SVM 的学习准则是令超平面与数据集的“边距”最大，即，求解以下最优问题的解，</p>
<script type="math/tex; mode=display">\arg \max_{(\mathbf w, b): \|\mathbf w\|=1} \ \min_{i \in [m]} |\mathbf w^{\top} \mathbf x_i+b| \quad s.t. \ \forall i \in [m], \ y_i(\mathbf w^{\top} \mathbf x_i+b)>0</script><p>在训练集线性可分这一前提条件下，上式等价于</p>
<script type="math/tex; mode=display">\arg \max_{(\mathbf w, b): \|\mathbf w\|=1} \ \min_{i \in [m]} y_i(\mathbf w^{\top} \mathbf x_i+b) \tag{1} \label{1}</script><h3 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h3><p>对满足 $|\mathbf w|=1$ 的任意 $(\mathbf w, b)$, 记 $\gamma=\min_{i \in [m]} \ y_i(\mathbf w^{\top}\mathbf x_i+b)$，于是 $\forall i \in [m]$ 有</p>
<script type="math/tex; mode=display">y_i (\mathbf w^{\top}\mathbf x_i+b) \ge \gamma</script><p>根据数据集线性可分这一假设，有 $\gamma&gt;0$，于是变换上式为</p>
<script type="math/tex; mode=display">y_i (\frac {\mathbf w^{\top}} {\gamma}\mathbf x_i+\frac {b}{\gamma}) \ge 1</script><p>而上面 $\eqref{1}$ 式要求 $\gamma$ 最大，<br>而 $|\mathbf w|=1$，<br>那么意味着 $\frac {|\mathbf w^{\top}|}{\gamma}$ 最小（$b$ 由 $\mathbf w$ 唯一确定，不对其作约束），也就是说，(1) 式可以变换为如下问题：</p>
<script type="math/tex; mode=display">\arg \min_{(\mathbf w, b)}\|\mathbf w\|^2 \quad s.t. \quad \forall i \in [m], \ y_i(\mathbf w^{\top} \mathbf x_i+b) \ge 1 \tag{2}</script><h1 id="Soft-SVM"><a href="#Soft-SVM" class="headerlink" title="Soft-SVM"></a>Soft-SVM</h1><p>Hard-SVM 假设训练集线性可分，但是这是一强假设，对这个假设适当放宽就得到 Soft-SVM，即训练集可能线性不可分。我们在 (2) 式的基础上引入松弛变量 $\{\xi_i:\xi_i \ge 0, \forall i \in [m]\}$，使得约束条件变为 $y_i(\mathbf w^{\top} \mathbf x_i+b) \ge 1-\xi_i$，我们的目的除了使 $\Vert \mathbf w\Vert$ 最小之外，还需要使松弛变量尽量小，即尽量减小这种放宽量，或者说尽量满足 Hard-SVM 中的约束条件，这就是 Soft-SVM 优化问题：</p>
<hr>
<center>Soft-SVM 求解思路</center>

<p><strong>input:</strong> $(\mathbf x_1, y_1), \cdots (\mathbf x_m, y_m)$</p>
<p><strong>parameter:</strong>  $\lambda &gt;0$ （平衡因子）</p>
<p><strong>solve:</strong></p>
<script type="math/tex; mode=display">\min_{\mathbf w, b, \boldsymbol \xi} \left(\lambda \|\mathbf w\|^2 + \frac 1 m \sum_{i=1}^m \xi_i\right)</script><script type="math/tex; mode=display">s.t. \ \forall i, \ y_i(\mathbf w^{\top} \mathbf x_i+b) \ge 1- \xi_i, \ \xi_i \ge 0</script><p><strong>output:</strong> $\mathbf w, b$</p>
<hr>
<p>我们可以使用正则化的损失最小化来改写上式。使用 hinge 损失，</p>
<script type="math/tex; mode=display">l(\mathbf w, b, \mathbf x, y)=\max \{0, 1-y(\mathbf w^{\top}\mathbf x+b)\}</script><p>分类器在训练集 $S$ 上的损失记为 $L_S(\mathbf w, b)$，那么正则化损失最小问题为</p>
<script type="math/tex; mode=display">\min_{\mathbf w, b} (\lambda \|\mathbf w\|^2+L_S(\mathbf w, b))</script><p>固定 $(\mathbf w, b)$ 的值，对于某个样本 $(\mathbf x_i, y_i)$，由于 $\xi_i \ge 0$，所以如果 $y_i(\mathbf w^{\top} \mathbf x_i+b) \ge 1$，那么 $\xi_i=0$，如果 $y_i(\mathbf w^{\top} \mathbf x_i+b) &lt; 1$，那么损失为 $1-y_i(\mathbf w^{\top} \mathbf x_i+b) =\xi_i$，所以 $L_S(\mathbf w, b)=\frac 1 m \sum_{i=1}^m \xi_i$。</p>
<p>另外，如果 $0&lt;\xi&lt;1$，表示样本虽然分类正确，但是太过靠近判别超平面；如果 $\xi_i \ge 1$，这表示第 $i$ 个样本分类错误。故，<strong>Soft-SVM 允许一定程度的错误，这是我们所期望的，因为有时候训练集的数据由于噪声干扰，出现了一些错误数据，如果完全按照训练集误差为零进行训练，得到的分类器在真实数据上性能反而会下降。</strong></p>
<h2 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h2>
SVM 中的“支持向量”这一词语来自 Hard-SVM 中的 $\mathbf w_0=\frac {\mathbf w^{\ast}} {\gamma^{\ast}}$，其中 $\mathbf w^{\ast}$ 是 (1) 式的解，$\gamma^{\ast}$ 则是 (1) 式的值，即 $\forall i \in [m], \ y_i(\mathbf w^{{\ast}\top} \mathbf x_i+b)\ge\gamma^{\ast}$，那么，向量 $\mathbf w_0$ 由训练集中的样本子集$\{(\mathbf x_i, y_i): y_i(\mathbf w^{{\ast}\top} \mathbf x_i+b)=\gamma^{\ast}\}$ 支持（支撑），这个样本子集中的样本到判别超平面距离最小，均为 $\gamma^{\ast}$，称这个样本子集中的数据向量为“支持向量”。

<p>考虑齐次型，即偏置 $b=0$（事实上，可以将 $b$ 作为 $w_1$，且 $\mathbf x$ 前面增加一个元素 $\mathbf x_1=1$，使得非齐次型转换为齐次型）。那么对于 Hard-SVM 有</p>
<script type="math/tex; mode=display">\min_{\mathbf w} \ \|\mathbf w\|^2 \quad s.t. \quad \forall i \in [m], \ y_i \mathbf w^{\top} \mathbf x \ge 1 \tag{3}</script><p>上面所说的 $\mathbf w_0$ 则是上式的解，支持向量则为 $I=\{\mathbf x_i: |\mathbf w_0^{\top}\mathbf x_i|=1\}$。<strong>存在系数 $\alpha_1, \cdots$ 使得</strong></p>
<script type="math/tex; mode=display">\mathbf w_0=\sum_{i \in I} \alpha_i \mathbf x_i</script><p>可以使用拉格朗日乘子法证明上述结论，略。</p>
<h2 id="对偶"><a href="#对偶" class="headerlink" title="对偶"></a>对偶</h2><p>考虑以下函数</p>
<script type="math/tex; mode=display">g(\mathbf w)=\max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \sum_{i=1}^m \alpha_i (1-y_i \mathbf w^{\top}\mathbf x_i)=\begin{cases} 0 & \forall i, \ y_i(\mathbf w^{\top}\mathbf x_i) \ge 1 \\ \infty & \text{otherwise} \end{cases}</script><p>上式中 $\alpha_i$ 全部非负，显然在 $y_i(\mathbf w^{\top}\mathbf x_i) \ge 1$ 条件下，$\forall i , \ \alpha=0$ 可使得 $g(\mathbf w)$ 最大，为 $0$，否则，$\forall i, \ \alpha=\infty$ 可使得 $g(\mathbf w)$ 最大，为 $\infty$。</p>
<p>对于线性可分训练集，考虑齐次型，即 $\eqref{3}$ 式，问题可等价为</p>
<script type="math/tex; mode=display">\min_{\mathbf w}\ (\|\mathbf w\|^2+ g(\mathbf w))</script><p>综合起来就是</p>
<script type="math/tex; mode=display">\min_{\mathbf w} \max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \left(\frac 1 2 \|\mathbf w\|^2+\sum_{i=1}^m \alpha_i (1-y_i \mathbf w^{\top}\mathbf x_i)\right)</script><p>增加 $\frac 1 2$ 因子是为了后面计算方便。现在将最小最大位置对调，那么目标值只可能变小（弱对偶），</p>
<script type="math/tex; mode=display">\min_{\mathbf w} \max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \left(\frac 1 2 \|\mathbf w\|^2+\sum_{i=1}^m \alpha_i (1-y_i \mathbf w^{\top}\mathbf x_i)\right)
\\ \ge
 \max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \min_{\mathbf w} \left(\frac 1 2 \|\mathbf w\|^2+\sum_{i=1}^m \alpha_i (1-y_i \mathbf w^{\top}\mathbf x_i)\right)</script><p>实际上在这里，强对偶也成立，即上式中等式成立，于是问题转化为对偶问题</p>
<script type="math/tex; mode=display">\max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \min_{\mathbf w} \left(\frac 1 2 \|\mathbf w\|^2+\sum_{i=1}^m \alpha_i (1-y_i \mathbf w^{\top}\mathbf x_i)\right)</script><p>当固定 $\boldsymbol \alpha$ 时，优化问题转换为无约束条件且目标可微，根据梯度为 0 求解，得</p>
<script type="math/tex; mode=display">\mathbf w-\sum_{i=1}^m \alpha_i y_i \mathbf x_i=\mathbf 0 \Rightarrow \mathbf w=\sum_{i=1}^m \alpha_i y_i \mathbf x_i</script><p>这表示解 $\mathbf w$ 处于样本向量所张空间中。于是对偶问题变成</p>
<script type="math/tex; mode=display">\max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \left(\frac 1 2 \|\sum_{i=1}^m \alpha_i y_i \mathbf x_i\|^2+\sum_{i=1}^m \alpha_i \left(1-y_i \sum_{j=1}^m \alpha_j y_j \mathbf x_j^{\top}\mathbf x_i \right)\right)</script><p>简化上式，其中第一项，</p>
<script type="math/tex; mode=display">\frac 1 2 \|\sum_{i=1}^m \alpha_i y_i \mathbf x_i\|^2=\frac 1 2 \left(\sum_i \alpha_i y_i \mathbf x_i^{\top}\right)\left(\sum_j \alpha_j y_j \mathbf x_j \right)</script><p>第二项为</p>
<script type="math/tex; mode=display">\sum_{i=1}^m \alpha_i \left(1-y_i \sum_{j=1}^m \alpha_j y_j \mathbf x_j^{\top}\mathbf x_i \right)=\sum_i \alpha_i-\left(\sum_{j=1}^m \alpha_j y_j \mathbf x_j^{\top}\right)\left(\sum_{i=1}^m \alpha_i y_i \mathbf x_i\right)</script><p>于是对偶问题简化为</p>
<script type="math/tex; mode=display">\max_{\boldsymbol \alpha \in \mathbb R^m:\boldsymbol \alpha \ge \mathbf 0} \left(\sum_{i=1}^m \alpha_i-\frac 1 2 \sum_{1=1}^m \sum_{j=1}^m \alpha_i  \alpha_j y_iy_j \mathbf x_i^{\top}\mathbf x_j \right)</script><p>在 <a href="2021/09/26/ml/kernel">核方法</a> 这篇文章中，也有类似的思想，两个地方联系起来看看，会更有心得。</p>
<h2 id="SGD-求解-Soft-SVM"><a href="#SGD-求解-Soft-SVM" class="headerlink" title="SGD 求解 Soft-SVM"></a>SGD 求解 Soft-SVM</h2><p>使用 hinge 损失，那么 Soft-SVM 可写为</p>
<script type="math/tex; mode=display">\min_{\mathbf w} \left(\frac {\lambda} 2 \|\mathbf w\|^2 + \frac 1 m \sum_{i=1}^m \max \{0, 1-y_i \mathbf w^{\top} \mathbf x_i\}\right) \tag{4} \label{4}</script><p>将上式写成 $f(\mathbf w)=\frac {\lambda} 2 |\mathbf w|^2+L_S(\mathbf w)$ 的形式，这是带正则项的经验损失，然而我们使用随机梯度下降算法，需要求真实损失的梯度，即 $t$ 时刻更新的梯度向量 $\mathbf v_t \in \partial l_{\mathcal D}(\mathbf w^{(t)})$，其中 $\partial l_{\mathcal D}(\mathbf w^{(t)})$ 表示损失在真实样本分布 $\mathcal D$ 下的 $\mathbf w^{(t)}$ 处的次梯度集，$l$ 这里表示 hinge 损失函数，由于真实分布 $\mathcal D$ 未知，我们构造其无偏估计，即 从训练集中 $S$ 均匀随机抽取一个样本 $z$，然后计算 $\partial l(\mathbf w^{(t)}, z)$，于是 $\mathbb E[\lambda \mathbf w^{(t)}+\mathbf v_t]$ 就是 $f=\frac {\lambda} 2 |\mathbf w|^2+L_{\mathcal D}(\mathbf w)$ 在 $\mathbf w^{(t)}$ 处的一个次梯度，选择学习率 $\eta=\frac 1 {\lambda t}$，于是更新公式为</p>
<script type="math/tex; mode=display">\begin{aligned}\mathbf w^{(t+1)} &=\mathbf w^{(t)}-\frac 1 {\lambda t}(\lambda \mathbf w^{(t)}+\mathbf v_t)
\\\\ &=\left(1-\frac 1 t\right)\mathbf w^{(t)}-\frac 1 {\lambda t} \mathbf v_t
\\\\ &=\frac {t-1} t \mathbf w^{(t)}-\frac 1 {\lambda t} \mathbf v_t
\\\\ &=\frac {t-1} t \left(\frac {t-2}{t-1}\mathbf w^{(t-1)}-\frac 1 {\lambda (t-1)}\mathbf v_{t-1}\right)-\frac 1 {\lambda t} \mathbf v_t
\\\\ &=\frac {t-2} t \mathbf w^{(t-1)}-\frac 1 {\lambda t} \mathbf v_{t-1}-\frac 1 {\lambda t} \mathbf v_t\end{aligned}</script><p>根据上述迭代公示，可知</p>
<script type="math/tex; mode=display">\mathbf w^{(t+1)}=-\frac 1 {\lambda t}\sum_{i=1}^t \mathbf v_i</script><p>$\mathbf v_i$ 是损失（不包括正则损失）即 hinge 损失函数在 $\mathbf w^{(i)}$ 处的次梯度，当 $y\mathbf w^{(i)\top} \mathbf x \ge 1$ 时，次梯度为 $0$，当 $y\mathbf w^{(i)\top} \mathbf x &lt; 1$ 时，次梯度为 $-y\mathbf x$，记 $\boldsymbol {\theta}^{(t)}=-\sum_{i=1}^t \mathbf v_i$，那么 SGD 学习过程具体步骤为</p>
<hr>
<center>SGD 求解 Soft-SVM</center>

<p><strong>目标：</strong> 求解式 $\eqref{4}$</p>
<p><strong>参数：</strong> $T$ （总迭代次数）</p>
<p><strong>初始化：</strong> $\boldsymbol {\theta}^{(1)}=\mathbf 0$</p>
<p><strong>for</strong> $\ t=1,\cdots, T$</p>
<p>&emsp; $\mathbf w^{(t)}=\frac 1 {\lambda t} \boldsymbol {\theta}^{(t)}$</p>
<p>&emsp; 从 $[m]$ 中均匀随机选择一个值 $i$</p>
<p>&emsp; 若 $\ y_i\mathbf w^{(t)\top} \mathbf x_i &lt; 1$</p>
<p>&emsp; &emsp; $\boldsymbol {\theta}^{(t+1)}=\boldsymbol {\theta}^{(t)}-\mathbf v_t=\boldsymbol {\theta}^{(t)}+y_i \mathbf x_i$</p>
<p>&emsp; 否则</p>
<p>&emsp; &emsp; $\boldsymbol {\theta}^{(t+1)}=\boldsymbol {\theta}^{(t)}$</p>
<p><strong>输出：</strong> $\overline {\mathbf w}=\frac 1 T \sum_{t=1}^T \mathbf w^{(t)}$</p>
<hr>
<p>当然也可以使用 $\mathbf w^{T}$ 或者 $\overline {\mathbf w}=\frac 1 {k} \sum_{t=T-k+1}^T \mathbf w^{(t)}$ （latest k 个 $\mathbf w{(t)}$ 的平均） 作为最终的输出。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/09/19/ml/k_fold/" rel="prev" title="k fold 交叉验证">
      <i class="fa fa-chevron-left"></i> k fold 交叉验证
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/09/26/ml/kernel/" rel="next" title="机器学习中的核方法">
      机器学习中的核方法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BE%B9%E8%B7%9D%E5%92%8C-Hard-SVM"><span class="nav-number">1.</span> <span class="nav-text">边距和 Hard-SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.1.</span> <span class="nav-text">距离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hard-SVM"><span class="nav-number">1.2.</span> <span class="nav-text">Hard-SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E6%8D%A2"><span class="nav-number">1.2.1.</span> <span class="nav-text">变换</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-SVM"><span class="nav-number">2.</span> <span class="nav-text">Soft-SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">支持向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6"><span class="nav-number">2.2.</span> <span class="nav-text">对偶</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD-%E6%B1%82%E8%A7%A3-Soft-SVM"><span class="nav-number">2.3.</span> <span class="nav-text">SGD 求解 Soft-SVM</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">shajianjian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">93</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">shajianjian</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
